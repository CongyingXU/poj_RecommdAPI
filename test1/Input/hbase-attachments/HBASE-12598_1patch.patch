diff --git hbase-client/src/test/java/org/apache/hadoop/hbase/client/TestAsyncProcess.java hbase-client/src/test/java/org/apache/hadoop/hbase/client/TestAsyncProcess.java
index d43ffc0..a56a254 100644
--- hbase-client/src/test/java/org/apache/hadoop/hbase/client/TestAsyncProcess.java
+++ hbase-client/src/test/java/org/apache/hadoop/hbase/client/TestAsyncProcess.java
@@ -662,7 +662,7 @@ public class TestAsyncProcess {
     HTable ht = new HTable();
     MyAsyncProcess ap = new MyAsyncProcess(createHConnection(), conf, true);
     ht.ap = ap;
-    ht.setAutoFlush(true, true);
+    ht.setAutoFlushTo(true);
     if (bufferOn) {
       ht.setWriteBufferSize(1024L * 1024L);
     } else {
@@ -710,7 +710,7 @@ public class TestAsyncProcess {
     HTable ht = new HTable();
     MyAsyncProcess ap = new MyAsyncProcess(createHConnection(), conf, true);
     ht.ap = ap;
-    ht.setAutoFlush(false, true);
+    ht.setAutoFlushTo(false);
     ht.setWriteBufferSize(0);
 
     Put p = createPut(1, false);
@@ -738,7 +738,7 @@ public class TestAsyncProcess {
   public void testWithNoClearOnFail() throws IOException {
     HTable ht = new HTable();
     ht.ap = new MyAsyncProcess(createHConnection(), conf, true);
-    ht.setAutoFlush(false, false);
+    ht.setAutoFlush(false);
 
     Put p = createPut(1, false);
     ht.put(p);
@@ -805,7 +805,7 @@ public class TestAsyncProcess {
     ht.ap.serverTrackerTimeout = 1;
 
     Put p = createPut(1, false);
-    ht.setAutoFlush(false, false);
+    ht.setAutoFlush(false);
     ht.put(p);
 
     try {
@@ -827,7 +827,7 @@ public class TestAsyncProcess {
     Assert.assertNotNull(ht.ap.createServerErrorTracker());
 
     Put p = createPut(1, true);
-    ht.setAutoFlush(false, false);
+    ht.setAutoFlush(false);
     ht.put(p);
 
     try {
diff --git hbase-it/src/test/java/org/apache/hadoop/hbase/test/IntegrationTestBigLinkedList.java hbase-it/src/test/java/org/apache/hadoop/hbase/test/IntegrationTestBigLinkedList.java
index 7493e4b..783d51e 100644
--- hbase-it/src/test/java/org/apache/hadoop/hbase/test/IntegrationTestBigLinkedList.java
+++ hbase-it/src/test/java/org/apache/hadoop/hbase/test/IntegrationTestBigLinkedList.java
@@ -110,7 +110,7 @@ import com.google.common.collect.Sets;
  * lost at scale. This test suite is called continuous ingest. This test runs
  * many ingest clients that continually create linked lists containing 25
  * million nodes. At some point the clients are stopped and a map reduce job is
- * run to ensure no linked list has a hole. A hole indicates data was lost.··
+ * run to ensure no linked list has a hole. A hole indicates data was lost.����
  *
  * The nodes in the linked list are random. This causes each linked list to
  * spread across the table. Therefore if one part of a table loses data, then it
@@ -121,9 +121,9 @@ import com.google.common.collect.Sets;
  * Below is rough sketch of how data is written. For specific details look at
  * the Generator code.
  *
- * 1 Write out 1 million nodes· 2 Flush the client· 3 Write out 1 million that
- * reference previous million· 4 If this is the 25th set of 1 million nodes,
- * then update 1st set of million to point to last· 5 goto 1
+ * 1 Write out 1 million nodes�� 2 Flush the client�� 3 Write out 1 million that
+ * reference previous million�� 4 If this is the 25th set of 1 million nodes,
+ * then update 1st set of million to point to last�� 5 goto 1
  *
  * The key is that nodes only reference flushed nodes. Therefore a node should
  * never reference a missing node, even if the ingest client is killed at any
@@ -131,12 +131,12 @@ import com.google.common.collect.Sets;
  *
  * When running this test suite w/ Accumulo there is a script running in
  * parallel called the Aggitator that randomly and continuously kills server
- * processes.·· The outcome was that many data loss bugs were found in Accumulo
- * by doing this.· This test suite can also help find bugs that impact uptime
- * and stability when· run for days or weeks.··
+ * processes.���� The outcome was that many data loss bugs were found in Accumulo
+ * by doing this.�� This test suite can also help find bugs that impact uptime
+ * and stability when�� run for days or weeks.����
  *
- * This test suite consists the following· - a few Java programs· - a little
- * helper script to run the java programs - a maven script to build it.··
+ * This test suite consists the following�� - a few Java programs�� - a little
+ * helper script to run the java programs - a maven script to build it.����
  *
  * When generating data, its best to have each map task generate a multiple of
  * 25 million. The reason for this is that circular linked list are generated
@@ -147,14 +147,14 @@ import com.google.common.collect.Sets;
  *
  * Below is a description of the Java programs
  *
- * Generator - A map only job that generates data. As stated previously,·
+ * Generator - A map only job that generates data. As stated previously,��
  * its best to generate data in multiples of 25M.
  *
  * Verify - A map reduce job that looks for holes. Look at the counts after running. REFERENCED and
- * UNREFERENCED are· ok, any UNDEFINED counts are bad. Do not run at the· same
+ * UNREFERENCED are�� ok, any UNDEFINED counts are bad. Do not run at the�� same
  * time as the Generator.
  *
- * Walker - A standalong program that start following a linked list· and emits timing info.··
+ * Walker - A standalong program that start following a linked list�� and emits timing info.����
  *
  * Print - A standalone program that prints nodes in the linked list
  *
@@ -340,7 +340,7 @@ public class IntegrationTestBigLinkedList extends IntegrationTestBase {
       byte[] id;
       long count = 0;
       int i;
-      HTable table;
+      Table table;
       long numNodes;
       long wrap;
       int width;
@@ -363,7 +363,7 @@ public class IntegrationTestBigLinkedList extends IntegrationTestBase {
 
       protected void instantiateHTable(Configuration conf) throws IOException {
         table = new HTable(conf, getTableName(conf));
-        table.setAutoFlush(false, true);
+        table.setAutoFlushTo(false);
         table.setWriteBufferSize(4 * 1024 * 1024);
       }
 
@@ -1136,7 +1136,7 @@ public class IntegrationTestBigLinkedList extends IntegrationTestBase {
     System.err.println("                             timing info.");
     System.err.println("  Print                      A standalone program that prints nodes");
     System.err.println("                             in the linked list.");
-    System.err.println("  Delete                     A standalone program that deletes a·");
+    System.err.println("  Delete                     A standalone program that deletes a��");
     System.err.println("                             single node.");
     System.err.println("  Loop                       A program to Loop through Generator and");
     System.err.println("                             Verify steps");
diff --git hbase-it/src/test/java/org/apache/hadoop/hbase/test/IntegrationTestBigLinkedListWithVisibility.java hbase-it/src/test/java/org/apache/hadoop/hbase/test/IntegrationTestBigLinkedListWithVisibility.java
index f80036c..36926b2 100644
--- hbase-it/src/test/java/org/apache/hadoop/hbase/test/IntegrationTestBigLinkedListWithVisibility.java
+++ hbase-it/src/test/java/org/apache/hadoop/hbase/test/IntegrationTestBigLinkedListWithVisibility.java
@@ -184,8 +184,8 @@ public class IntegrationTestBigLinkedListWithVisibility extends IntegrationTestB
       @Override
       protected void instantiateHTable(Configuration conf) throws IOException {
         for (int i = 0; i < DEFAULT_TABLES_COUNT; i++) {
-          HTable table = new HTable(conf, getTableName(i));
-          table.setAutoFlush(true, true);
+          Table table = new HTable(conf, getTableName(i));
+          table.setAutoFlushTo(true);
           //table.setWriteBufferSize(4 * 1024 * 1024);
           this.tables[i] = table;
         }
diff --git hbase-it/src/test/java/org/apache/hadoop/hbase/test/IntegrationTestLoadAndVerify.java hbase-it/src/test/java/org/apache/hadoop/hbase/test/IntegrationTestLoadAndVerify.java
index 24dafa5..1e27626 100644
--- hbase-it/src/test/java/org/apache/hadoop/hbase/test/IntegrationTestLoadAndVerify.java
+++ hbase-it/src/test/java/org/apache/hadoop/hbase/test/IntegrationTestLoadAndVerify.java
@@ -20,12 +20,7 @@ package org.apache.hadoop.hbase.test;
 import static org.junit.Assert.assertEquals;
 import static org.junit.Assert.assertTrue;
 
-import java.io.IOException;
-import java.util.Random;
-import java.util.Set;
-import java.util.UUID;
-import java.util.regex.Matcher;
-import java.util.regex.Pattern;
+import com.google.common.collect.Sets;
 
 import org.apache.commons.cli.CommandLine;
 import org.apache.commons.logging.Log;
@@ -49,6 +44,7 @@ import org.apache.hadoop.hbase.client.Put;
 import org.apache.hadoop.hbase.client.Result;
 import org.apache.hadoop.hbase.client.Scan;
 import org.apache.hadoop.hbase.client.ScannerCallable;
+import org.apache.hadoop.hbase.client.Table;
 import org.apache.hadoop.hbase.io.ImmutableBytesWritable;
 import org.apache.hadoop.hbase.mapreduce.NMapInputFormat;
 import org.apache.hadoop.hbase.mapreduce.TableMapReduceUtil;
@@ -68,7 +64,12 @@ import org.apache.hadoop.util.ToolRunner;
 import org.junit.Test;
 import org.junit.experimental.categories.Category;
 
-import com.google.common.collect.Sets;
+import java.io.IOException;
+import java.util.Random;
+import java.util.Set;
+import java.util.UUID;
+import java.util.regex.Matcher;
+import java.util.regex.Pattern;
 
 /**
  * A large test which loads a lot of data that has internal references, and
@@ -164,7 +165,7 @@ public void cleanUpCluster() throws Exception {
       extends Mapper<NullWritable, NullWritable, NullWritable, NullWritable>
   {
     protected long recordsToWrite;
-    protected HTable table;
+    protected Table table;
     protected Configuration conf;
     protected int numBackReferencesPerRow;
     protected String shortTaskId;
@@ -181,7 +182,7 @@ public void cleanUpCluster() throws Exception {
       numBackReferencesPerRow = conf.getInt(NUM_BACKREFS_KEY, NUM_BACKREFS_DEFAULT);
       table = new HTable(conf, TableName.valueOf(tableName));
       table.setWriteBufferSize(4*1024*1024);
-      table.setAutoFlush(false, true);
+      table.setAutoFlushTo(false);
 
       String taskId = conf.get("mapreduce.task.attempt.id");
       Matcher matcher = Pattern.compile(".+_m_(\\d+_\\d+)").matcher(taskId);
diff --git hbase-it/src/test/java/org/apache/hadoop/hbase/trace/IntegrationTestSendTraceRequests.java hbase-it/src/test/java/org/apache/hadoop/hbase/trace/IntegrationTestSendTraceRequests.java
index 2ec5838..7e63829 100644
--- hbase-it/src/test/java/org/apache/hadoop/hbase/trace/IntegrationTestSendTraceRequests.java
+++ hbase-it/src/test/java/org/apache/hadoop/hbase/trace/IntegrationTestSendTraceRequests.java
@@ -234,12 +234,12 @@ public class IntegrationTestSendTraceRequests extends AbstractHBaseTool {
 
   private LinkedBlockingQueue<Long> insertData() throws IOException, InterruptedException {
     LinkedBlockingQueue<Long> rowKeys = new LinkedBlockingQueue<Long>(25000);
-    HTable ht = new HTable(util.getConfiguration(), this.tableName);
+    Table ht = new HTable(util.getConfiguration(), this.tableName);
     byte[] value = new byte[300];
     for (int x = 0; x < 5000; x++) {
       TraceScope traceScope = Trace.startSpan("insertData", Sampler.ALWAYS);
       try {
-        ht.setAutoFlush(false, true);
+        ht.setAutoFlushTo(false);
         for (int i = 0; i < 5; i++) {
           long rk = random.nextLong();
           rowKeys.add(rk);
diff --git hbase-rest/src/test/java/org/apache/hadoop/hbase/rest/PerformanceEvaluation.java hbase-rest/src/test/java/org/apache/hadoop/hbase/rest/PerformanceEvaluation.java
index 7e17c01..aaf7d59 100644
--- hbase-rest/src/test/java/org/apache/hadoop/hbase/rest/PerformanceEvaluation.java
+++ hbase-rest/src/test/java/org/apache/hadoop/hbase/rest/PerformanceEvaluation.java
@@ -18,22 +18,6 @@
  */
 package org.apache.hadoop.hbase.rest;
 
-import java.io.DataInput;
-import java.io.DataOutput;
-import java.io.IOException;
-import java.io.PrintStream;
-import java.lang.reflect.Constructor;
-import java.text.SimpleDateFormat;
-import java.util.ArrayList;
-import java.util.Arrays;
-import java.util.Date;
-import java.util.List;
-import java.util.Map;
-import java.util.Random;
-import java.util.TreeMap;
-import java.util.regex.Matcher;
-import java.util.regex.Pattern;
-
 import org.apache.commons.logging.Log;
 import org.apache.commons.logging.LogFactory;
 import org.apache.hadoop.conf.Configuration;
@@ -53,11 +37,11 @@ import org.apache.hadoop.hbase.client.Durability;
 import org.apache.hadoop.hbase.client.Get;
 import org.apache.hadoop.hbase.client.HConnection;
 import org.apache.hadoop.hbase.client.HConnectionManager;
-import org.apache.hadoop.hbase.client.HTableInterface;
 import org.apache.hadoop.hbase.client.Put;
 import org.apache.hadoop.hbase.client.Result;
 import org.apache.hadoop.hbase.client.ResultScanner;
 import org.apache.hadoop.hbase.client.Scan;
+import org.apache.hadoop.hbase.client.Table;
 import org.apache.hadoop.hbase.filter.BinaryComparator;
 import org.apache.hadoop.hbase.filter.CompareFilter;
 import org.apache.hadoop.hbase.filter.Filter;
@@ -91,6 +75,22 @@ import org.apache.hadoop.util.LineReader;
 import org.apache.hadoop.util.Tool;
 import org.apache.hadoop.util.ToolRunner;
 
+import java.io.DataInput;
+import java.io.DataOutput;
+import java.io.IOException;
+import java.io.PrintStream;
+import java.lang.reflect.Constructor;
+import java.text.SimpleDateFormat;
+import java.util.ArrayList;
+import java.util.Arrays;
+import java.util.Date;
+import java.util.List;
+import java.util.Map;
+import java.util.Random;
+import java.util.TreeMap;
+import java.util.regex.Matcher;
+import java.util.regex.Pattern;
+
 /**
  * Script used evaluating Stargate performance and scalability.  Runs a SG
  * client that steps through one of a set of hardcoded tests or 'experiments'
@@ -870,7 +870,7 @@ public class PerformanceEvaluation extends Configured implements Tool {
     protected final int totalRows;
     private final Status status;
     protected TableName tableName;
-    protected HTableInterface table;
+    protected Table table;
     protected volatile Configuration conf;
     protected boolean flushCommits;
     protected boolean writeToWAL;
@@ -909,7 +909,7 @@ public class PerformanceEvaluation extends Configured implements Tool {
 
     void testSetup() throws IOException {
       this.table = connection.getTable(tableName);
-      this.table.setAutoFlush(false, true);
+      this.table.setAutoFlushTo(false);
     }
 
     void testTakedown()  throws IOException {
diff --git hbase-server/src/main/java/org/apache/hadoop/hbase/mapreduce/MultiTableOutputFormat.java hbase-server/src/main/java/org/apache/hadoop/hbase/mapreduce/MultiTableOutputFormat.java
index c1d8373..7e82486 100644
--- hbase-server/src/main/java/org/apache/hadoop/hbase/mapreduce/MultiTableOutputFormat.java
+++ hbase-server/src/main/java/org/apache/hadoop/hbase/mapreduce/MultiTableOutputFormat.java
@@ -104,7 +104,7 @@ public class MultiTableOutputFormat extends OutputFormat<ImmutableBytesWritable,
       if (!tables.containsKey(tableName)) {
         LOG.debug("Opening HTable \"" + Bytes.toString(tableName.get())+ "\" for writing");
         HTable table = new HTable(conf, TableName.valueOf(tableName.get()));
-        table.setAutoFlush(false, true);
+        table.setAutoFlushTo(false);
         tables.put(tableName, table);
       }
       return tables.get(tableName);
diff --git hbase-server/src/main/java/org/apache/hadoop/hbase/mapreduce/TableOutputFormat.java hbase-server/src/main/java/org/apache/hadoop/hbase/mapreduce/TableOutputFormat.java
index de29f37..cd69a5b 100644
--- hbase-server/src/main/java/org/apache/hadoop/hbase/mapreduce/TableOutputFormat.java
+++ hbase-server/src/main/java/org/apache/hadoop/hbase/mapreduce/TableOutputFormat.java
@@ -195,7 +195,7 @@ implements Configurable {
       }
       this.connection = ConnectionFactory.createConnection(this.conf);
       this.table = connection.getTable(TableName.valueOf(tableName));
-      ((HTable) this.table).setAutoFlush(false, true);
+      this.table.setAutoFlushTo(false);
       LOG.info("Created table instance for "  + tableName);
     } catch(IOException e) {
       LOG.error(e);
diff --git hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestFromClientSide.java hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestFromClientSide.java
index 35982a2..d1fea02 100644
--- hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestFromClientSide.java
+++ hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestFromClientSide.java
@@ -3906,7 +3906,7 @@ public class TestFromClientSide {
     final int NB_BATCH_ROWS = 10;
     HTable table = TEST_UTIL.createTable(Bytes.toBytes("testRowsPutBufferedOneFlush"),
       new byte [][] {CONTENTS_FAMILY, SMALL_FAMILY});
-    table.setAutoFlush(false, true);
+    table.setAutoFlushTo(false);
     ArrayList<Put> rowsUpdate = new ArrayList<Put>();
     for (int i = 0; i < NB_BATCH_ROWS * 10; i++) {
       byte[] row = Bytes.toBytes("row" + i);
@@ -3947,7 +3947,7 @@ public class TestFromClientSide {
     final int NB_BATCH_ROWS = 10;
     HTable table = TEST_UTIL.createTable(Bytes.toBytes("testRowsPutBufferedManyManyFlushes"),
       new byte[][] {CONTENTS_FAMILY, SMALL_FAMILY });
-    table.setAutoFlush(false, true);
+    table.setAutoFlushTo(false);
     table.setWriteBufferSize(10);
     ArrayList<Put> rowsUpdate = new ArrayList<Put>();
     for (int i = 0; i < NB_BATCH_ROWS * 10; i++) {
diff --git hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestHTableUtil.java hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestHTableUtil.java
index 31db75a..08e3879 100644
--- hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestHTableUtil.java
+++ hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestHTableUtil.java
@@ -69,7 +69,7 @@ public class TestHTableUtil {
   public void testBucketPut() throws Exception {
     byte [] TABLE = Bytes.toBytes("testBucketPut");
     HTable ht = TEST_UTIL.createTable(TABLE, FAMILY);
-    ht.setAutoFlush(false, true);
+    ht.setAutoFlushTo(false);
 
     List<Put> puts = new ArrayList<Put>();
     puts.add( createPut("row1") );
diff --git hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestMultiParallel.java hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestMultiParallel.java
index 26bdb49..4e32f4d 100644
--- hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestMultiParallel.java
+++ hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestMultiParallel.java
@@ -263,8 +263,8 @@ public class TestMultiParallel {
   private void doTestFlushCommits(boolean doAbort) throws Exception {
     // Load the data
     LOG.info("get new table");
-    HTable table = new HTable(UTIL.getConfiguration(), TEST_TABLE);
-    table.setAutoFlush(false, true);
+    Table table = new HTable(UTIL.getConfiguration(), TEST_TABLE);
+    table.setAutoFlushTo(false);
     table.setWriteBufferSize(10 * 1024 * 1024);
 
     LOG.info("constructPutRequests");
diff --git hbase-server/src/test/java/org/apache/hadoop/hbase/coprocessor/TestHTableWrapper.java hbase-server/src/test/java/org/apache/hadoop/hbase/coprocessor/TestHTableWrapper.java
index 8c1e515..2a8ac62 100644
--- hbase-server/src/test/java/org/apache/hadoop/hbase/coprocessor/TestHTableWrapper.java
+++ hbase-server/src/test/java/org/apache/hadoop/hbase/coprocessor/TestHTableWrapper.java
@@ -177,7 +177,7 @@ public class TestHTableWrapper {
     boolean initialAutoFlush = hTableInterface.isAutoFlush();
     hTableInterface.setAutoFlushTo(false);
     assertFalse(hTableInterface.isAutoFlush());
-    hTableInterface.setAutoFlush(true, true);
+    hTableInterface.setAutoFlushTo(true);
     assertTrue(hTableInterface.isAutoFlush());
     hTableInterface.setAutoFlushTo(initialAutoFlush);
   }
diff --git hbase-server/src/test/java/org/apache/hadoop/hbase/master/TestDistributedLogSplitting.java hbase-server/src/test/java/org/apache/hadoop/hbase/master/TestDistributedLogSplitting.java
index 1ad7bc8..15b5080 100644
--- hbase-server/src/test/java/org/apache/hadoop/hbase/master/TestDistributedLogSplitting.java
+++ hbase-server/src/test/java/org/apache/hadoop/hbase/master/TestDistributedLogSplitting.java
@@ -879,7 +879,7 @@ public class TestDistributedLogSplitting {
 
     List<RegionServerThread> rsts = cluster.getLiveRegionServerThreads();
     final ZooKeeperWatcher zkw = new ZooKeeperWatcher(conf, "table-creation", null);
-    HTable ht = installTable(zkw, "table", "family", NUM_REGIONS_TO_CREATE);
+    Table ht = installTable(zkw, "table", "family", NUM_REGIONS_TO_CREATE);
     final SplitLogManager slm = master.getMasterFileSystem().splitLogManager;
 
     Set<HRegionInfo> regionSet = new HashSet<HRegionInfo>();
@@ -919,7 +919,7 @@ public class TestDistributedLogSplitting {
       if (key == null || key.length == 0) {
         key = new byte[] { 0, 0, 0, 0, 1 };
       }
-      ht.setAutoFlush(true, true);
+      ht.setAutoFlushTo(true);
       Put put = new Put(key);
       put.add(Bytes.toBytes("family"), Bytes.toBytes("c1"), new byte[]{'b'});
       ht.put(put);
@@ -1569,7 +1569,7 @@ public class TestDistributedLogSplitting {
    * Load table with puts and deletes with expected values so that we can verify later
    */
   private void prepareData(final HTable t, final byte[] f, final byte[] column) throws IOException {
-    t.setAutoFlush(false, true);
+    t.setAutoFlushTo(false);
     byte[] k = new byte[3];
 
     // add puts
diff --git hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestRegionServerMetrics.java hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestRegionServerMetrics.java
index a808dd9..e7d2285 100644
--- hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestRegionServerMetrics.java
+++ hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestRegionServerMetrics.java
@@ -351,7 +351,7 @@ public class TestRegionServerMetrics {
 
     TEST_UTIL.createTable(tableName, cf);
     HTable t = new HTable(conf, tableName);
-    t.setAutoFlush(false, true);
+    t.setAutoFlushTo(false);
     for (int insertCount =0; insertCount < 100; insertCount++) {
       Put p = new Put(Bytes.toBytes("" + insertCount + "row"));
       p.add(cf, qualifier, val);
diff --git hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/wal/TestLogRolling.java hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/wal/TestLogRolling.java
index cf70d4a..b9dc301 100644
--- hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/wal/TestLogRolling.java
+++ hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/wal/TestLogRolling.java
@@ -442,7 +442,7 @@ public class TestLogRolling  {
       desc.addFamily(new HColumnDescriptor(HConstants.CATALOG_FAMILY));
 
       admin.createTable(desc);
-      HTable table = new HTable(TEST_UTIL.getConfiguration(), desc.getTableName());
+      Table table = new HTable(TEST_UTIL.getConfiguration(), desc.getTableName());
 
       server = TEST_UTIL.getRSForFirstRegionInTable(desc.getTableName());
       this.log = server.getWAL();
@@ -482,7 +482,7 @@ public class TestLogRolling  {
 
       writeData(table, 1002);
 
-      table.setAutoFlush(true, true);
+      table.setAutoFlushTo(true);
 
       long curTime = System.currentTimeMillis();
       long oldFilenum = log.getFilenum();
diff --git hbase-server/src/test/java/org/apache/hadoop/hbase/replication/TestReplicationChangingPeerRegionservers.java hbase-server/src/test/java/org/apache/hadoop/hbase/replication/TestReplicationChangingPeerRegionservers.java
index 64f0a35..c9f5a09 100644
--- hbase-server/src/test/java/org/apache/hadoop/hbase/replication/TestReplicationChangingPeerRegionservers.java
+++ hbase-server/src/test/java/org/apache/hadoop/hbase/replication/TestReplicationChangingPeerRegionservers.java
@@ -22,8 +22,6 @@ import static org.junit.Assert.assertArrayEquals;
 import static org.junit.Assert.assertEquals;
 import static org.junit.Assert.fail;
 
-import java.io.IOException;
-
 import org.apache.commons.logging.Log;
 import org.apache.commons.logging.LogFactory;
 import org.apache.hadoop.hbase.LargeTests;
@@ -36,11 +34,12 @@ import org.apache.hadoop.hbase.client.ResultScanner;
 import org.apache.hadoop.hbase.client.Scan;
 import org.apache.hadoop.hbase.util.Bytes;
 import org.apache.hadoop.hbase.util.JVMClusterUtil;
-import org.apache.hadoop.hbase.util.JVMClusterUtil.RegionServerThread;
 import org.junit.Before;
 import org.junit.Test;
 import org.junit.experimental.categories.Category;
 
+import java.io.IOException;
+
 /**
  * Test handling of changes to the number of a peer's regionservers.
  */
@@ -54,7 +53,7 @@ public class TestReplicationChangingPeerRegionservers extends TestReplicationBas
    */
   @Before
   public void setUp() throws Exception {
-    ((HTable)htable1).setAutoFlush(false, true);
+    htable1.setAutoFlushTo(false);
     // Starting and stopping replication can make us miss new logs,
     // rolling like this makes sure the most recent one gets added to the queue
     for (JVMClusterUtil.RegionServerThread r :
diff --git hbase-server/src/test/java/org/apache/hadoop/hbase/replication/TestReplicationSmallTests.java hbase-server/src/test/java/org/apache/hadoop/hbase/replication/TestReplicationSmallTests.java
index ca5be65..8d6e209 100644
--- hbase-server/src/test/java/org/apache/hadoop/hbase/replication/TestReplicationSmallTests.java
+++ hbase-server/src/test/java/org/apache/hadoop/hbase/replication/TestReplicationSmallTests.java
@@ -68,7 +68,7 @@ public class TestReplicationSmallTests extends TestReplicationBase {
    */
   @Before
   public void setUp() throws Exception {
-    ((HTable)htable1).setAutoFlush(true, true);
+    htable1.setAutoFlushTo(true);
     // Starting and stopping replication can make us miss new logs,
     // rolling like this makes sure the most recent one gets added to the queue
     for ( JVMClusterUtil.RegionServerThread r :
@@ -246,7 +246,7 @@ public class TestReplicationSmallTests extends TestReplicationBase {
     LOG.info("testSmallBatch");
     Put put;
     // normal Batch tests
-    ((HTable)htable1).setAutoFlush(false, true);
+    htable1.setAutoFlushTo(false);
     for (int i = 0; i < NB_ROWS_IN_BATCH; i++) {
       put = new Put(Bytes.toBytes(i));
       put.add(famName, row, row);
@@ -386,7 +386,7 @@ public class TestReplicationSmallTests extends TestReplicationBase {
   public void testLoading() throws Exception {
     LOG.info("Writing out rows to table1 in testLoading");
     htable1.setWriteBufferSize(1024);
-    ((HTable)htable1).setAutoFlush(false, true);
+    htable1.setAutoFlushTo(false);
     for (int i = 0; i < NB_ROWS_IN_BIG_BATCH; i++) {
       Put put = new Put(Bytes.toBytes(i));
       put.add(famName, row, row);
diff --git hbase-server/src/test/java/org/apache/hadoop/hbase/snapshot/SnapshotTestingUtils.java hbase-server/src/test/java/org/apache/hadoop/hbase/snapshot/SnapshotTestingUtils.java
index 186436c..0d87dc2 100644
--- hbase-server/src/test/java/org/apache/hadoop/hbase/snapshot/SnapshotTestingUtils.java
+++ hbase-server/src/test/java/org/apache/hadoop/hbase/snapshot/SnapshotTestingUtils.java
@@ -673,9 +673,9 @@ public class SnapshotTestingUtils {
     loadData(util, new HTable(util.getConfiguration(), tableName), rows, families);
   }
 
-  public static void loadData(final HBaseTestingUtility util, final HTable table, int rows,
+  public static void loadData(final HBaseTestingUtility util, final Table table, int rows,
       byte[]... families) throws IOException, InterruptedException {
-    table.setAutoFlush(false, true);
+    table.setAutoFlushTo(false);
 
     // Ensure one row per region
     assertTrue(rows >= KEYS.length);