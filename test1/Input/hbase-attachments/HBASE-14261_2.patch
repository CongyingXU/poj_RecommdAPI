From 334c716ae349a212103af3bdf1edc706a3b9faad Mon Sep 17 00:00:00 2001
From: Andrew Purtell <apurtell@apache.org>
Date: Fri, 11 Sep 2015 18:31:02 -0700
Subject: [PATCH] Amend HBASE-14261 Enhance Chaos Monkey framework by adding
 zookeeper and datanode fault injections

Fix a Hadoop 1 compilation problem by adding a shim to HadoopShims
for getting the list of live datanodes in a cluster via
DFSClient#datanodeReport(DatanodeReportType)
---
 .../java/org/apache/hadoop/hbase/HadoopShims.java     |  8 ++++++++
 .../java/org/apache/hadoop/hbase/HadoopShimsImpl.java | 19 +++++++++++++++++++
 hbase-hadoop2-compat/pom.xml                          |  5 +++++
 .../java/org/apache/hadoop/hbase/HadoopShimsImpl.java | 17 +++++++++++++++++
 .../chaos/actions/RestartRandomDataNodeAction.java    |  9 +++++----
 5 files changed, 54 insertions(+), 4 deletions(-)

diff --git a/hbase-hadoop-compat/src/test/java/org/apache/hadoop/hbase/HadoopShims.java b/hbase-hadoop-compat/src/test/java/org/apache/hadoop/hbase/HadoopShims.java
index 157327b..5a0df6c 100644
--- a/hbase-hadoop-compat/src/test/java/org/apache/hadoop/hbase/HadoopShims.java
+++ b/hbase-hadoop-compat/src/test/java/org/apache/hadoop/hbase/HadoopShims.java
@@ -18,6 +18,8 @@
 
 package org.apache.hadoop.hbase;
 
+import java.io.IOException;
+
 
 /**
  * A compatibility shim layer for interacting with different versions of Hadoop.
@@ -34,4 +36,10 @@ public interface HadoopShims {
    */
   <T,J> T createTestTaskAttemptContext(final J job, final String taskId);
 
+  /**
+   * Returns an array of DatanodeInfo for all live datanodes in the cluster
+   * @param dfs instance of DistributedFileSystem
+   * @return
+   */
+  <I,DFS> I[] getLiveDatanodes(DFS dfs) throws IOException;
 }
diff --git a/hbase-hadoop1-compat/src/test/java/org/apache/hadoop/hbase/HadoopShimsImpl.java b/hbase-hadoop1-compat/src/test/java/org/apache/hadoop/hbase/HadoopShimsImpl.java
index 34ae174..60acfde 100644
--- a/hbase-hadoop1-compat/src/test/java/org/apache/hadoop/hbase/HadoopShimsImpl.java
+++ b/hbase-hadoop1-compat/src/test/java/org/apache/hadoop/hbase/HadoopShimsImpl.java
@@ -18,6 +18,11 @@
 
 package org.apache.hadoop.hbase;
 
+import java.io.IOException;
+
+import org.apache.hadoop.hdfs.DFSClient;
+import org.apache.hadoop.hdfs.DistributedFileSystem;
+import org.apache.hadoop.hdfs.protocol.FSConstants.DatanodeReportType;
 import org.apache.hadoop.mapreduce.Job;
 import org.apache.hadoop.mapreduce.TaskAttemptContext;
 import org.apache.hadoop.mapreduce.TaskAttemptID;
@@ -40,4 +45,18 @@ public class HadoopShimsImpl implements HadoopShims {
     Job j = (Job)job;
     return (T)new TaskAttemptContext(j.getConfiguration(), TaskAttemptID.forName(taskId));
   }
+
+  /**
+   * Returns an array of DatanodeInfo for all live datanodes in the cluster
+   * @param dfs instance of DistributedFileSystem
+   * @return
+   * @throws IOException 
+   */
+  @Override
+  @SuppressWarnings("unchecked")
+  public <I, DFS> I[] getLiveDatanodes(DFS dfs) throws IOException {
+    DFSClient dfsClient = ((DistributedFileSystem)dfs).getClient();
+    return (I[])dfsClient.datanodeReport(DatanodeReportType.LIVE);
+  }
+
 }
diff --git a/hbase-hadoop2-compat/pom.xml b/hbase-hadoop2-compat/pom.xml
index 36da2f6..17b9968 100644
--- a/hbase-hadoop2-compat/pom.xml
+++ b/hbase-hadoop2-compat/pom.xml
@@ -176,6 +176,11 @@ limitations under the License.
       <version>${hadoop-two.version}</version>
     </dependency>
     <dependency>
+      <groupId>org.apache.hadoop</groupId>
+      <artifactId>hadoop-hdfs</artifactId>
+      <version>${hadoop-two.version}</version>
+    </dependency>
+    <dependency>
       <groupId>commons-lang</groupId>
       <artifactId>commons-lang</artifactId>
     </dependency>
diff --git a/hbase-hadoop2-compat/src/test/java/org/apache/hadoop/hbase/HadoopShimsImpl.java b/hbase-hadoop2-compat/src/test/java/org/apache/hadoop/hbase/HadoopShimsImpl.java
index ce142e8..73b77ff 100644
--- a/hbase-hadoop2-compat/src/test/java/org/apache/hadoop/hbase/HadoopShimsImpl.java
+++ b/hbase-hadoop2-compat/src/test/java/org/apache/hadoop/hbase/HadoopShimsImpl.java
@@ -18,6 +18,11 @@
 
 package org.apache.hadoop.hbase;
 
+import java.io.IOException;
+
+import org.apache.hadoop.hdfs.DFSClient;
+import org.apache.hadoop.hdfs.DistributedFileSystem;
+import org.apache.hadoop.hdfs.protocol.HdfsConstants;
 import org.apache.hadoop.mapreduce.Job;
 import org.apache.hadoop.mapreduce.TaskAttemptID;
 import org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl;
@@ -40,4 +45,16 @@ public class HadoopShimsImpl implements HadoopShims {
     Job j = (Job)job;
     return (T)new TaskAttemptContextImpl(j.getConfiguration(), TaskAttemptID.forName(taskId));
   }
+
+  /**
+   * Returns an array of DatanodeInfo for all live datanodes in the cluster
+   * @param dfs instance of DistributedFileSystem
+   * @return
+   */
+  @Override
+  @SuppressWarnings("unchecked")
+  public <I, DFS> I[] getLiveDatanodes(DFS dfs) throws IOException {
+    DFSClient dfsClient = ((DistributedFileSystem)dfs).getClient();
+    return (I[])dfsClient.datanodeReport(HdfsConstants.DatanodeReportType.LIVE);
+  }
 }
diff --git a/hbase-it/src/test/java/org/apache/hadoop/hbase/chaos/actions/RestartRandomDataNodeAction.java b/hbase-it/src/test/java/org/apache/hadoop/hbase/chaos/actions/RestartRandomDataNodeAction.java
index 7299e79..fea79f3 100644
--- a/hbase-it/src/test/java/org/apache/hadoop/hbase/chaos/actions/RestartRandomDataNodeAction.java
+++ b/hbase-it/src/test/java/org/apache/hadoop/hbase/chaos/actions/RestartRandomDataNodeAction.java
@@ -18,13 +18,13 @@
 
 package org.apache.hadoop.hbase.chaos.actions;
 
+import org.apache.hadoop.hbase.CompatibilitySingletonFactory;
+import org.apache.hadoop.hbase.HadoopShims;
 import org.apache.hadoop.hbase.ServerName;
 import org.apache.hadoop.hbase.chaos.monkies.PolicyBasedChaosMonkey;
 import org.apache.hadoop.hbase.util.FSUtils;
-import org.apache.hadoop.hdfs.DFSClient;
 import org.apache.hadoop.hdfs.DistributedFileSystem;
 import org.apache.hadoop.hdfs.protocol.DatanodeInfo;
-import org.apache.hadoop.hdfs.protocol.HdfsConstants;
 
 import java.io.IOException;
 import java.util.LinkedList;
@@ -48,9 +48,10 @@ public class RestartRandomDataNodeAction extends RestartActionBaseAction {
   public ServerName[] getDataNodes() throws IOException {
     DistributedFileSystem fs = (DistributedFileSystem) FSUtils.getRootDir(getConf())
         .getFileSystem(getConf());
-    DFSClient dfsClient = fs.getClient();
     List<ServerName> hosts = new LinkedList<ServerName>();
-    for (DatanodeInfo dataNode: dfsClient.datanodeReport(HdfsConstants.DatanodeReportType.LIVE)) {
+    HadoopShims hadoop = CompatibilitySingletonFactory.getInstance(HadoopShims.class);
+    DatanodeInfo[] dataNodes = hadoop.getLiveDatanodes(fs);
+    for (DatanodeInfo dataNode: dataNodes) {
       hosts.add(ServerName.valueOf(dataNode.getHostName(), -1, -1));
     }
     return hosts.toArray(new ServerName[hosts.size()]);
-- 
1.9.1

