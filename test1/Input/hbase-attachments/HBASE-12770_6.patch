diff --git hbase-client/src/main/java/org/apache/hadoop/hbase/replication/ReplicationQueues.java hbase-client/src/main/java/org/apache/hadoop/hbase/replication/ReplicationQueues.java
index 3dbbc33..90a3ead 100644
--- hbase-client/src/main/java/org/apache/hadoop/hbase/replication/ReplicationQueues.java
+++ hbase-client/src/main/java/org/apache/hadoop/hbase/replication/ReplicationQueues.java
@@ -23,6 +23,7 @@ import java.util.SortedMap;
 import java.util.SortedSet;
 
 import org.apache.hadoop.hbase.classification.InterfaceAudience;
+import org.apache.hadoop.hbase.util.Pair;
 
 /**
  * This provides an interface for maintaining a region server's replication queues. These queues
@@ -101,6 +102,27 @@ public interface ReplicationQueues {
   SortedMap<String, SortedSet<String>> claimQueues(String regionserver);
 
   /**
+   * Get queueIds from a dead region server, whose queues has not been claimed by other region servers.
+   * @param regionserver
+   * @return queueIds
+   */
+  List<String> getUnClaimedQueueIds(String regionserver);
+  
+  /**
+   * Take ownership for the queue identified by queueId and belongs to a dead region server.
+   * @param regionserver the id of the dead region server
+   * @param queueId the id of the queue
+   * @return the new PeerId and A SortedSet of WALs in its queue. Returns null if no queue was failed-over.
+   */
+  Pair<String, SortedSet<String>> claimQueue(String regionserver, String queueId);
+  
+  /**
+   * Remove the znode of region server if it has no queues
+   * @param regionserver
+   */
+  void removeReplicatorIfEmpty(String regionserver);
+
+  /**
    * Get a list of all region servers that have outstanding replication queues. These servers could
    * be alive, dead or from a previous run of the cluster.
    * @return a list of server names
diff --git hbase-client/src/main/java/org/apache/hadoop/hbase/replication/ReplicationQueuesZKImpl.java hbase-client/src/main/java/org/apache/hadoop/hbase/replication/ReplicationQueuesZKImpl.java
index 635b021..b3adb27 100644
--- hbase-client/src/main/java/org/apache/hadoop/hbase/replication/ReplicationQueuesZKImpl.java
+++ hbase-client/src/main/java/org/apache/hadoop/hbase/replication/ReplicationQueuesZKImpl.java
@@ -19,7 +19,9 @@
 package org.apache.hadoop.hbase.replication;
 
 import java.util.ArrayList;
+import java.util.HashSet;
 import java.util.List;
+import java.util.Set;
 import java.util.SortedMap;
 import java.util.SortedSet;
 import java.util.TreeMap;
@@ -35,6 +37,7 @@ import org.apache.hadoop.hbase.exceptions.DeserializationException;
 import org.apache.hadoop.hbase.protobuf.ProtobufUtil;
 import org.apache.hadoop.hbase.protobuf.generated.ZooKeeperProtos;
 import org.apache.hadoop.hbase.util.Bytes;
+import org.apache.hadoop.hbase.util.Pair;
 import org.apache.hadoop.hbase.zookeeper.ZKUtil;
 import org.apache.hadoop.hbase.zookeeper.ZKUtil.ZKUtilOp;
 import org.apache.hadoop.hbase.zookeeper.ZooKeeperWatcher;
@@ -182,6 +185,69 @@ public class ReplicationQueuesZKImpl extends ReplicationStateZKBase implements R
   }
 
   @Override
+  public Pair<String, SortedSet<String>> claimQueue(String regionserverZnode, String queueId) {
+    // check whether there is multi support. If yes, use it.
+    if (conf.getBoolean(HConstants.ZOOKEEPER_USEMULTI, true)) {
+      LOG.info("Atomically moving " + regionserverZnode + "'s wals of " + queueId + " to my queue");
+      return copyQueueFromRSUsingMulti(regionserverZnode, queueId);
+    } else {
+      LOG.info("Moving " + regionserverZnode + "'s " + queueId + " wals to my queue");
+      if (!lockQueueForOtherRS(regionserverZnode, queueId)) {
+        return null;
+      }
+      Pair<String, SortedSet<String>> queue = copyQueueFromRS(regionserverZnode, queueId);
+      deleteAnotherRSQueue(regionserverZnode, queueId);
+      return queue;
+    }
+  }
+  
+  public List<String> getUnClaimedQueueIds(String regionserverZnode) {
+    String rsZnodePath = ZKUtil.joinZNode(this.queuesZNode, regionserverZnode);
+    try {
+      List<String> children = ZKUtil.listChildrenNoWatch(this.zookeeper, rsZnodePath);
+      if (children == null) return null;
+      
+      if (conf.getBoolean(HConstants.ZOOKEEPER_USEMULTI, true)) {
+        return children;
+      } else {
+        // there might be locks for queues if multi is not support
+        String lockSuffix = "-" + RS_LOCK_ZNODE;
+        List<String> allPeerIds = new ArrayList<String>();
+        Set<String> lockedPeerIds = new HashSet<String>();
+        for (String node : children) {
+          if (node.endsWith(lockSuffix)) {
+            lockedPeerIds.add(node.substring(0, node.lastIndexOf('-')));
+          } else {
+            allPeerIds.add(node);
+          }
+        }
+        List<String> unLockedPeerIds = new ArrayList<String>();
+        for (String peerId : allPeerIds) {
+          if (!lockedPeerIds.contains(peerId)) {
+            unLockedPeerIds.add(peerId);
+          }
+        }
+        return unLockedPeerIds;
+      }
+    } catch (KeeperException e) {
+      LOG.warn("Got exception in getUnClaimedQueueIds: ", e);
+      return null;
+    }
+  }
+ 
+  public void removeReplicatorIfEmpty(String regionserverZnode) {
+    String rsZnodePath = ZKUtil.joinZNode(this.queuesZNode, regionserverZnode);
+    try {
+      List<String> children = ZKUtil.listChildrenNoWatch(this.zookeeper, rsZnodePath);
+      if (children != null && children.size() == 0) {
+        ZKUtil.deleteNodeFailSilent(this.zookeeper, regionserverZnode);
+      }
+    } catch (KeeperException e) {
+      LOG.warn("Got exception in removeReplicatorIfEmpty: ", e);
+    }
+  }
+
+  @Override
   public void removeAllQueues() {
     try {
       ZKUtil.deleteNodeRecursively(this.zookeeper, this.myQueuesZnode);
@@ -250,6 +316,37 @@ public class ReplicationQueuesZKImpl extends ReplicationStateZKBase implements R
     }
     return true;
   }
+  
+  private String getQueueLockZNode(String peerId) {
+    return peerId + "-" + RS_LOCK_ZNODE;
+  }
+  
+  private boolean lockQueueForOtherRS(String znode, String peerId) {
+    try {
+      String parent = ZKUtil.joinZNode(this.queuesZNode, znode);
+      if (parent.equals(this.myQueuesZnode)) {
+        LOG.warn("Won't lock " + peerId + " because this is us, we're dead!");
+        return false;
+      }
+      String p = ZKUtil.joinZNode(parent, getQueueLockZNode(peerId));
+      ZKUtil.createAndWatch(this.zookeeper, p, lockToByteArray(this.myQueuesZnode));
+    } catch (KeeperException e) {
+      // This exception will pop up if the znode under which we're trying to
+      // create the lock is already deleted by another region server, meaning
+      // that the transfer already occurred.
+      // NoNode => transfer is done and znodes are already deleted
+      // NodeExists => lock znode already created by another RS
+      if (e instanceof KeeperException.NoNodeException
+          || e instanceof KeeperException.NodeExistsException) {
+        LOG.info("Won't transfer the queue," + " another RS took care of it because of: "
+            + e.getMessage());
+      } else {
+        LOG.info("Failed lock other rs", e);
+      }
+      return false;
+    }
+    return true;
+  }
 
   /**
    * Delete all the replication queues for a given region server.
@@ -284,6 +381,29 @@ public class ReplicationQueuesZKImpl extends ReplicationStateZKBase implements R
     }
   }
 
+  private void deleteAnotherRSQueue(String regionserverZnode, String cluster) {
+    String fullpath = ZKUtil.joinZNode(this.queuesZNode, regionserverZnode);
+    try {
+      String fullClusterPath = ZKUtil.joinZNode(fullpath, cluster);
+      ZKUtil.deleteNodeRecursively(this.zookeeper, fullClusterPath);
+      String lockPath = ZKUtil.joinZNode(fullpath, getQueueLockZNode(cluster));
+      ZKUtil.deleteNode(this.zookeeper, lockPath);
+    } catch (KeeperException e) {
+      // TODO : confirm this
+      if (e instanceof KeeperException.NoNodeException
+          || e instanceof KeeperException.NotEmptyException) {
+        // Testing a special case where another region server was able to
+        // create a lock just after we deleted it, but then was also able to
+        // delete the RS znode before us or its lock znode is still there.
+        if (e.getPath().equals(fullpath)) {
+          return;
+        }
+      }
+      this.abortable.abort("Failed to delete replication queues for region server: "
+          + regionserverZnode, e);
+    }
+  }
+  
   /**
    * It "atomically" copies all the wals queues from another region server and returns them all
    * sorted per peer cluster (appended with the dead server's znode).
@@ -350,6 +470,67 @@ public class ReplicationQueuesZKImpl extends ReplicationStateZKBase implements R
     }
     return queues;
   }
+  
+  /**
+   * It "atomically" copies the wals queue for peerId from another region server
+   * @param znode pertaining to the region server to copy the queues from
+   * @param peerId the id of the queue
+   * @return new peerId with its WAL queues
+   */
+  private Pair<String, SortedSet<String>> copyQueueFromRSUsingMulti(String znode, String peerId) {
+    Pair<String, SortedSet<String>> queue = null;
+    try {
+        ReplicationQueueInfo replicationQueueInfo = new ReplicationQueueInfo(peerId);
+        if (!peerExists(replicationQueueInfo.getPeerId())) {
+          LOG.warn("Peer " + peerId + " didn't exist, skipping the replay");
+          // Protection against moving orphaned queues
+          return null;
+        }
+        String newPeerId = peerId + "-" + znode;
+        String newPeerZnode = ZKUtil.joinZNode(this.myQueuesZnode, newPeerId);
+        
+        // check the logs queue for the old peer cluster
+        // hbase/replication/rs/deadrs/peerId
+        String oldClusterZnode = ZKUtil.joinZNode(this.queuesZNode, ZKUtil.joinZNode(znode, peerId));
+        List<ZKUtilOp> listOfOps = new ArrayList<ZKUtil.ZKUtilOp>();
+        List<String> wals = ZKUtil.listChildrenNoWatch(this.zookeeper, oldClusterZnode);
+        if (wals == null || wals.size() == 0) {
+          listOfOps.add(ZKUtilOp.deleteNodeFailSilent(oldClusterZnode));
+        } else {
+          // create the new cluster znode
+          queue = new Pair<String, SortedSet<String>>();
+          SortedSet<String> logQueue = new TreeSet<String>();
+          queue.setFirst(newPeerId);
+          queue.setSecond(logQueue);
+          ZKUtilOp op = ZKUtilOp.createAndFailSilent(newPeerZnode, HConstants.EMPTY_BYTE_ARRAY);
+          listOfOps.add(op);
+          // get the offset of the logs and set it to new znodes
+          for (String wal : wals) {
+            String oldWalZnode = ZKUtil.joinZNode(oldClusterZnode, wal);
+            byte[] logOffset = ZKUtil.getData(this.zookeeper, oldWalZnode);
+            LOG.debug("Creating " + wal + " with data " + Bytes.toString(logOffset));
+            String newLogZnode = ZKUtil.joinZNode(newPeerZnode, wal);
+            listOfOps.add(ZKUtilOp.createAndFailSilent(newLogZnode, logOffset));
+            // add ops for deleting
+            listOfOps.add(ZKUtilOp.deleteNodeFailSilent(oldWalZnode));
+            logQueue.add(wal);
+          }
+          // add delete op for peer
+          listOfOps.add(ZKUtilOp.deleteNodeFailSilent(oldClusterZnode));
+        }
+        LOG.debug(" The multi list size is: " + listOfOps.size());
+        ZKUtil.multiOrSequential(this.zookeeper, listOfOps, false);
+        LOG.info("Atomically moved the dead regionserver log, peerId=" + peerId);
+        return queue;
+    } catch (KeeperException e) {
+      // Multi call failed; it looks like some other regionserver took away the logs.
+      LOG.warn("Got exception in copyQueueFromRSUsingMulti and return null: ", e);
+    } catch (InterruptedException e) {
+      LOG.warn("Got exception in copyQueueFromRSUsingMulti and return null: ", e);
+      Thread.currentThread().interrupt();
+    }
+    return null;
+  }
 
   /**
    * This methods copies all the wals queues from another region server and returns them all sorted
@@ -420,6 +601,68 @@ public class ReplicationQueuesZKImpl extends ReplicationStateZKBase implements R
   }
 
   /**
+   * This methods copies all the wals queues for peerId from another region server
+   * @param znode server names to copy
+   * @param cluster the id of the queue
+   * @return new PeerId with its WAL queue, null if an error occurred
+   */
+  private Pair<String, SortedSet<String>> copyQueueFromRS(String znode, String cluster) {
+    // TODO this method isn't atomic enough, we could start copying and then
+    // TODO fail for some reason and we would end up with znodes we don't want.
+    try {
+      String nodePath = ZKUtil.joinZNode(this.queuesZNode, znode);
+      ReplicationQueueInfo replicationQueueInfo = new ReplicationQueueInfo(cluster);
+      if (!peerExists(replicationQueueInfo.getPeerId())) {
+        LOG.warn("Peer " + cluster + " didn't exist, skipping the replay");
+        // Protection against moving orphaned queues
+        // TODO : should return to delete?
+        return null;
+      }
+      // We add the name of the recovered RS to the new znode, we can even
+      // do that for queues that were recovered 10 times giving a znode like
+      // number-startcode-number-otherstartcode-number-anotherstartcode-etc
+      String newCluster = cluster + "-" + znode;
+      String newClusterZnode = ZKUtil.joinZNode(this.myQueuesZnode, newCluster);
+      String clusterPath = ZKUtil.joinZNode(nodePath, cluster);
+      List<String> wals = ZKUtil.listChildrenNoWatch(this.zookeeper, clusterPath);
+      // That region server didn't have anything to replicate for this cluster
+      if (wals == null || wals.size() == 0) {
+        return null;
+      }
+      ZKUtil.createNodeIfNotExistsAndWatch(this.zookeeper, newClusterZnode,
+        HConstants.EMPTY_BYTE_ARRAY);
+      SortedSet<String> logQueue = new TreeSet<String>();
+      Pair<String, SortedSet<String>> queue = new Pair<String, SortedSet<String>>();
+      queue.setFirst(newCluster);
+      queue.setSecond(logQueue);
+      for (String wal : wals) {
+        String z = ZKUtil.joinZNode(clusterPath, wal);
+        byte[] positionBytes = ZKUtil.getData(this.zookeeper, z);
+        long position = 0;
+        try {
+          position = ZKUtil.parseWALPositionFrom(positionBytes);
+        } catch (DeserializationException e) {
+          LOG.warn("Failed parse of wal position from the following znode: " + z + ", Exception: "
+              + e);
+        }
+        LOG.debug("Creating " + wal + " with data " + position);
+        String child = ZKUtil.joinZNode(newClusterZnode, wal);
+        // Position doesn't actually change, we are just deserializing it for
+        // logging, so just use the already serialized version
+        ZKUtil.createAndWatch(this.zookeeper, child, positionBytes);
+        logQueue.add(wal);
+      }
+      return queue;
+    } catch (KeeperException e) {
+      this.abortable.abort("Copy queues from rs", e);
+    } catch (InterruptedException e) {
+      LOG.warn(e);
+      Thread.currentThread().interrupt();
+    }
+    return null;
+  }
+  
+  /**
    * @param lockOwner
    * @return Serialized protobuf of <code>lockOwner</code> with pb magic prefix prepended suitable
    *         for use as content of an replication lock during region server fail over.
diff --git hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSourceManager.java hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSourceManager.java
index 4908ebc..1dd89e7 100644
--- hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSourceManager.java
+++ hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSourceManager.java
@@ -55,6 +55,7 @@ import org.apache.hadoop.hbase.replication.ReplicationPeers;
 import org.apache.hadoop.hbase.replication.ReplicationQueueInfo;
 import org.apache.hadoop.hbase.replication.ReplicationQueues;
 import org.apache.hadoop.hbase.replication.ReplicationTracker;
+import org.apache.hadoop.hbase.util.Pair;
 
 import com.google.common.util.concurrent.ThreadFactoryBuilder;
 
@@ -549,35 +550,44 @@ public class ReplicationSourceManager implements ReplicationListener {
       if (this.rq.isThisOurZnode(rsZnode)) {
         return;
       }
-      // Wait a bit before transferring the queues, we may be shutting down.
-      // This sleep may not be enough in some cases.
-      try {
-        Thread.sleep(sleepBeforeFailover + (long) (rand.nextFloat() * sleepBeforeFailover));
-      } catch (InterruptedException e) {
-        LOG.warn("Interrupted while waiting before transferring a queue.");
-        Thread.currentThread().interrupt();
-      }
-      // We try to lock that rs' queue directory
-      if (server.isStopped()) {
-        LOG.info("Not transferring queue since we are shutting down");
-        return;
-      }
-      SortedMap<String, SortedSet<String>> newQueues = null;
 
-      newQueues = this.rq.claimQueues(rsZnode);
+      List<String> queueIds = this.rq.getUnClaimedQueueIds(rsZnode);
+      if (queueIds == null) return;
+      
+      boolean shouldSleep = true;
+      for (String queueId : queueIds) {
+        if (shouldSleep) {
+          // Wait a bit before transferring the queues, we may be shutting down.
+          // This sleep may not be enough in some cases.
+          try {
+            Thread.sleep(sleepBeforeFailover + (long) (rand.nextFloat() * sleepBeforeFailover));
+          } catch (InterruptedException e) {
+            LOG.warn("Interrupted while waiting before transferring a queue.");
+            Thread.currentThread().interrupt();
+          }
+        } else {
+          shouldSleep = true;
+        }
+        
+        if (server.isStopped()) {
+          LOG.info("Not transferring queue since we are shutting down");
+          return;
+        }
+
+        Pair<String, SortedSet<String>> newQueue = this.rq.claimQueue(rsZnode, queueId);
 
-      // Copying over the failed queue is completed.
-      if (newQueues.isEmpty()) {
-        // We either didn't get the lock or the failed region server didn't have any outstanding
-        // WALs to replicate, so we are done.
-        return;
-      }
+        // Copying over the failed queue is completed.
+        if (newQueue == null) {
+          // We either didn't get the lock or the failed region server didn't have any outstanding
+          // WALs identified by queueId to replicate, so we continue the next queueId without sleep.
+          shouldSleep = false;
+          continue;
+        }
 
-      for (Map.Entry<String, SortedSet<String>> entry : newQueues.entrySet()) {
-        String peerId = entry.getKey();
         try {
+          String newPeerId = newQueue.getFirst();
           // there is not an actual peer defined corresponding to peerId for the failover.
-          ReplicationQueueInfo replicationQueueInfo = new ReplicationQueueInfo(peerId);
+          ReplicationQueueInfo replicationQueueInfo = new ReplicationQueueInfo(newPeerId);
           String actualPeerId = replicationQueueInfo.getPeerId();
           ReplicationPeer peer = replicationPeers.getPeer(actualPeerId);
           ReplicationPeerConfig peerConfig = null;
@@ -592,25 +602,28 @@ public class ReplicationSourceManager implements ReplicationListener {
             continue;
           }
 
-          ReplicationSourceInterface src =
-              getReplicationSource(conf, fs, ReplicationSourceManager.this, this.rq, this.rp,
-                server, peerId, this.clusterId, peerConfig, peer);
+          ReplicationSourceInterface src = getReplicationSource(conf, fs,
+            ReplicationSourceManager.this, this.rq, this.rp, server, newPeerId, this.clusterId,
+            peerConfig, peer);
+          
           if (!this.rp.getPeerIds().contains((src.getPeerClusterId()))) {
             src.terminate("Recovered queue doesn't belong to any current peer");
             break;
           }
           oldsources.add(src);
-          SortedSet<String> walsSet = entry.getValue();
+          SortedSet<String> walsSet = newQueue.getSecond();
           for (String wal : walsSet) {
             src.enqueueLog(new Path(oldLogDir, wal));
           }
           src.startup();
-          walsByIdRecoveredQueues.put(peerId, walsSet);
+          walsByIdRecoveredQueues.put(newPeerId, walsSet);
         } catch (IOException e) {
           // TODO manage it
           LOG.error("Failed creating a source", e);
         }
       }
+      
+      this.rq.removeReplicatorIfEmpty(rsZnode);
     }
   }
 
