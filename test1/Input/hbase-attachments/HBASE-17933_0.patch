diff --git a/hbase-spark/src/main/java/org/apache/hadoop/hbase/spark/example/hbasecontext/JavaHBaseBulkLoadExample.java b/hbase-spark/src/main/java/org/apache/hadoop/hbase/spark/example/hbasecontext/JavaHBaseBulkLoadExample.java
new file mode 100644
index 0000000..b772590
--- /dev/null
+++ b/hbase-spark/src/main/java/org/apache/hadoop/hbase/spark/example/hbasecontext/JavaHBaseBulkLoadExample.java
@@ -0,0 +1,84 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.hbase.spark.example.hbasecontext;
+
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.hbase.HBaseConfiguration;
+import org.apache.hadoop.hbase.HConstants;
+import org.apache.hadoop.hbase.TableName;
+import org.apache.hadoop.hbase.spark.ByteArrayWrapper;
+import org.apache.hadoop.hbase.spark.FamiliesQualifiersValues;
+import org.apache.hadoop.hbase.spark.JavaHBaseContext;
+import org.apache.hadoop.hbase.spark.KeyFamiliesQualifiersValues;
+import org.apache.hadoop.hbase.spark.FamilyHFileWriteOptions;
+import org.apache.hadoop.hbase.util.Bytes;
+import org.apache.spark.api.java.JavaRDD;
+import org.apache.spark.api.java.JavaSparkContext;
+import org.apache.spark.SparkConf;
+
+import java.util.ArrayList;
+import java.util.HashMap;
+import java.util.List;
+
+final public class JavaHBaseBulkLoadExample {
+  private JavaHBaseBulkLoadExample() {}
+
+  public static void main(String[] args) {
+    if (args.length < 1) {
+      System.out.println("JavaHBaseBulkLoadExample  " + "{outputPath}");
+      return;
+    }
+
+    String tableName = "bulkload-table";
+    String columnFamily1 = "f1";
+    String columnFamily2 = "f2";
+
+    SparkConf sparkConf = new SparkConf().setAppName("JavaHBaseBulkPutExample " + tableName);
+    JavaSparkContext jsc = new JavaSparkContext(sparkConf);
+
+    try {
+      List<KeyFamiliesQualifiersValues> list= new ArrayList<KeyFamiliesQualifiersValues>();
+      // row1
+      byte[] row1 = Bytes.toBytes("row1");
+      FamiliesQualifiersValues fqv1 = new FamiliesQualifiersValues();
+      fqv1.add(Bytes.toBytes(columnFamily1), Bytes.toBytes("a"), Bytes.toBytes("1"));
+      list.add(new KeyFamiliesQualifiersValues(new ByteArrayWrapper(row1), fqv1));
+      // row3
+      byte[] row3 = Bytes.toBytes("row3");
+      FamiliesQualifiersValues fqv3 = new FamiliesQualifiersValues();
+      fqv3.add(Bytes.toBytes(columnFamily1), Bytes.toBytes("b"), Bytes.toBytes("1"));
+      fqv3.add(Bytes.toBytes(columnFamily1), Bytes.toBytes("b"), Bytes.toBytes("2"));
+      fqv3.add(Bytes.toBytes(columnFamily2), Bytes.toBytes("a"), Bytes.toBytes("1"));
+      list.add(new KeyFamiliesQualifiersValues(new ByteArrayWrapper(row3), fqv3));
+      // row2
+      byte[] row2 = Bytes.toBytes("row2");
+      FamiliesQualifiersValues fqv2 = new FamiliesQualifiersValues();
+      fqv2.add(Bytes.toBytes(columnFamily1), Bytes.toBytes("b"), Bytes.toBytes("3"));
+      fqv2.add(Bytes.toBytes(columnFamily1), Bytes.toBytes("a"), Bytes.toBytes("3"));
+      list.add(new KeyFamiliesQualifiersValues(new ByteArrayWrapper(row2), fqv2));
+
+      JavaRDD<KeyFamiliesQualifiersValues> rdd = jsc.parallelize(list);
+
+      Configuration conf = HBaseConfiguration.create();
+      JavaHBaseContext hbaseContext = new JavaHBaseContext(jsc, conf);
+      hbaseContext.bulkLoadThinRows(rdd, TableName.valueOf(tableName), args[0],
+          new HashMap<byte[], FamilyHFileWriteOptions>(), false, HConstants.DEFAULT_MAX_FILE_SIZE);
+    } finally {
+      jsc.stop();
+    }
+  }
+}
\ No newline at end of file
diff --git a/hbase-spark/src/main/scala/org/apache/hadoop/hbase/spark/FamiliesQualifiersValues.scala b/hbase-spark/src/main/scala/org/apache/hadoop/hbase/spark/FamiliesQualifiersValues.scala
index 92bb3b7..07f0a71 100644
--- a/hbase-spark/src/main/scala/org/apache/hadoop/hbase/spark/FamiliesQualifiersValues.scala
+++ b/hbase-spark/src/main/scala/org/apache/hadoop/hbase/spark/FamiliesQualifiersValues.scala
@@ -55,4 +55,29 @@ class FamiliesQualifiersValues extends Serializable {
 
     qualifierValues.put(new ByteArrayWrapper(qualifier), value)
   }
+
+  /**
+    * A wrapper for "+=" method above, can be used by Java
+    * @param family    HBase column family
+    * @param qualifier HBase column qualifier
+    * @param value     HBase cell value
+    */
+  def add(family: Array[Byte], qualifier: Array[Byte], value: Array[Byte]): Unit = {
+    this += (family, qualifier, value)
+  }
+}
+
+/**
+  * This is a wrapper class for FamiliesQualifiersValues above
+  * mainly used to support Java BulkLoad API
+  */
+class KeyFamiliesQualifiersValues(rowkey: ByteArrayWrapper, columnAndValue: FamiliesQualifiersValues)
+  extends Serializable {
+  def getRowkey: ByteArrayWrapper = {
+    this.rowkey
+  }
+
+  def getColumnAndValue: FamiliesQualifiersValues = {
+    this.columnAndValue
+  }
 }
diff --git a/hbase-spark/src/main/scala/org/apache/hadoop/hbase/spark/JavaHBaseContext.scala b/hbase-spark/src/main/scala/org/apache/hadoop/hbase/spark/JavaHBaseContext.scala
index 253b386..7dfee9b 100644
--- a/hbase-spark/src/main/scala/org/apache/hadoop/hbase/spark/JavaHBaseContext.scala
+++ b/hbase-spark/src/main/scala/org/apache/hadoop/hbase/spark/JavaHBaseContext.scala
@@ -17,9 +17,11 @@
 
 package org.apache.hadoop.hbase.spark
 
+import java.util.Map
+
 import org.apache.hadoop.conf.Configuration
 import org.apache.hadoop.hbase.TableName
-import org.apache.hadoop.hbase.classification.InterfaceAudience;
+import org.apache.hadoop.hbase.classification.InterfaceAudience
 import org.apache.hadoop.hbase.client.{Connection, Delete, Get, Put, Result, Scan}
 import org.apache.hadoop.hbase.io.ImmutableBytesWritable
 import org.apache.spark.api.java.{JavaRDD, JavaSparkContext}
@@ -268,7 +270,6 @@ class JavaHBaseContext(@transient jsc: JavaSparkContext,
    * generates a new DStream based on Gets and the results
    * they bring back from HBase
    *
-
    * @param tableName     The name of the table to get from
    * @param batchSize     The number of gets to be batched together
    * @param javaDStream   Original DStream with data to iterate over
@@ -292,6 +293,63 @@ class JavaHBaseContext(@transient jsc: JavaSparkContext,
   }
 
   /**
+    * A simple abstraction over the HBaseContext.bulkLoad method.
+    * Caller need to first create JavaRDD[KeyFamilyQualifierValue]
+    * It allow addition support for a user to take a JavaRDD and generate
+    * HFiles in stagingDir for the bulk load
+    *
+    * @param javaRdd                        The javaRDD we are bulk loading from
+    * @param tableName                      The HBase table we are loading into
+    * @param stagingDir                     The location on the FileSystem to bulk load into
+    * @param familyHFileWriteOptionsMap     Options that will define how the HFile for a
+    *                                       column family is written
+    * @param compactionExclude              Compaction excluded for the HFiles
+    * @param maxSize                        Max size for the HFiles before they roll
+    */
+  def bulkLoad(javaRdd: JavaRDD[KeyFamilyQualifierValue],
+                  tableName: TableName,
+                  stagingDir: String,
+                  familyHFileWriteOptionsMap:
+                  Map[Array[Byte], FamilyHFileWriteOptions],
+                  compactionExclude: Boolean,
+                  maxSize: Long):
+  Unit = {
+    hbaseContext.bulkLoad[KeyFamilyQualifierValue](javaRdd.rdd, tableName, t => {
+      val keyFamilyQualifier = t.getKeyFamilyQualifier
+      val value = t.getValue
+      Seq((keyFamilyQualifier, value)).iterator
+    }, stagingDir, familyHFileWriteOptionsMap, compactionExclude, maxSize)
+  }
+
+  /**
+    * A simple abstraction over the HBaseContext.bulkLoadThinRows method.
+    * Caller need to first create JavaRDD[KeyFamiliesQualifiersValues]
+    * It allow addition support for a user to take a JavaRDD and generate
+    * HFiles in stagingDir for the bulk load
+    *
+    * @param javaRdd                        The javaRDD we are bulk loading from
+    * @param tableName                      The HBase table we are loading into
+    * @param stagingDir                     The location on the FileSystem to bulk load into
+    * @param familyHFileWriteOptionsMap     Options that will define how the HFile for a
+    *                                       column family is written
+    * @param compactionExclude              Compaction excluded for the HFiles
+    * @param maxSize                        Max size for the HFiles before they roll
+    */
+  def bulkLoadThinRows(javaRdd: JavaRDD[KeyFamiliesQualifiersValues],
+                          tableName: TableName,
+                          stagingDir: String,
+                          familyHFileWriteOptionsMap:
+                          Map[Array[Byte], FamilyHFileWriteOptions],
+                          compactionExclude: Boolean,
+                          maxSize: Long):
+  Unit = {
+    hbaseContext.bulkLoadThinRows[KeyFamiliesQualifiersValues](javaRdd.rdd,
+      tableName, (t: KeyFamiliesQualifiersValues) => {
+      (t.getRowkey, t.getColumnAndValue)
+    }, stagingDir, familyHFileWriteOptionsMap, compactionExclude, maxSize)
+  }
+
+  /**
    * This function will use the native HBase TableInputFormat with the
    * given scan object to generate a new JavaRDD
    *
@@ -341,4 +399,5 @@ class JavaHBaseContext(@transient jsc: JavaSparkContext,
    */
   private[spark]
   def fakeClassTag[T]: ClassTag[T] = ClassTag.AnyRef.asInstanceOf[ClassTag[T]]
+
 }
diff --git a/hbase-spark/src/main/scala/org/apache/hadoop/hbase/spark/KeyFamilyQualifier.scala b/hbase-spark/src/main/scala/org/apache/hadoop/hbase/spark/KeyFamilyQualifier.scala
index 722fdae..0122aaf 100644
--- a/hbase-spark/src/main/scala/org/apache/hadoop/hbase/spark/KeyFamilyQualifier.scala
+++ b/hbase-spark/src/main/scala/org/apache/hadoop/hbase/spark/KeyFamilyQualifier.scala
@@ -46,3 +46,22 @@ class KeyFamilyQualifier(val rowKey:Array[Byte], val family:Array[Byte], val qua
     Bytes.toString(rowKey) + ":" + Bytes.toString(family) + ":" + Bytes.toString(qualifier)
   }
 }
+
+/**
+ * This is a wrapper class for KeyFamilyQualifier
+ * mainly used to support Java BulkLoad API
+ */
+class KeyFamilyQualifierValue (val key:Array[Byte], val fam:Array[Byte], val qual:Array[Byte], val value:Array[Byte])
+  extends Serializable{
+  val keyFamilyQualifier = new KeyFamilyQualifier(key, fam, qual)
+  def getKeyFamilyQualifier: KeyFamilyQualifier = {
+    this.keyFamilyQualifier
+  }
+  def getValue: Array[Byte] = {
+    this.value
+  }
+  override def toString: String = {
+    Bytes.toString(key) + ":" + Bytes.toString(fam) + ":" + Bytes.toString(qual) +
+      ":" + Bytes.toString(value)
+  }
+}
diff --git a/hbase-spark/src/test/java/org/apache/hadoop/hbase/spark/TestJavaHBaseContext.java b/hbase-spark/src/test/java/org/apache/hadoop/hbase/spark/TestJavaHBaseContext.java
index da6b724..47d169b 100644
--- a/hbase-spark/src/test/java/org/apache/hadoop/hbase/spark/TestJavaHBaseContext.java
+++ b/hbase-spark/src/test/java/org/apache/hadoop/hbase/spark/TestJavaHBaseContext.java
@@ -19,16 +19,22 @@ package org.apache.hadoop.hbase.spark;
 import java.io.File;
 import java.io.IOException;
 import java.io.Serializable;
-
-import java.util.*;
+import java.util.ArrayList;
+import java.util.Iterator;
+import java.util.HashMap;
+import java.util.List;
 
 import org.apache.commons.logging.Log;
 import org.apache.commons.logging.LogFactory;
 import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.fs.FileSystem;
+import org.apache.hadoop.fs.Path;
 import org.apache.hadoop.hbase.Cell;
 import org.apache.hadoop.hbase.CellUtil;
+import org.apache.hadoop.hbase.HConstants;
 import org.apache.hadoop.hbase.HBaseTestingUtility;
 import org.apache.hadoop.hbase.TableName;
+import org.apache.hadoop.hbase.client.Admin;
 import org.apache.hadoop.hbase.client.Connection;
 import org.apache.hadoop.hbase.client.ConnectionFactory;
 import org.apache.hadoop.hbase.client.Delete;
@@ -38,39 +44,48 @@ import org.apache.hadoop.hbase.client.Result;
 import org.apache.hadoop.hbase.client.Scan;
 import org.apache.hadoop.hbase.client.Table;
 import org.apache.hadoop.hbase.io.ImmutableBytesWritable;
+import org.apache.hadoop.hbase.mapreduce.LoadIncrementalHFiles;
 import org.apache.hadoop.hbase.spark.example.hbasecontext.JavaHBaseBulkDeleteExample;
 import org.apache.hadoop.hbase.testclassification.MediumTests;
 import org.apache.hadoop.hbase.testclassification.MiscTests;
 import org.apache.hadoop.hbase.util.Bytes;
-import org.apache.spark.api.java.*;
+
+import org.apache.spark.api.java.JavaRDD;
+import org.apache.spark.api.java.JavaSparkContext;
 import org.apache.spark.api.java.function.Function;
-import org.junit.*;
+
+import org.junit.After;
+import org.junit.Assert;
+import org.junit.Before;
+import org.junit.Test;
 import org.junit.experimental.categories.Category;
 
 import scala.Tuple2;
-
 import com.google.common.io.Files;
 
 @Category({MiscTests.class, MediumTests.class})
 public class TestJavaHBaseContext implements Serializable {
-  private transient JavaSparkContext jsc;
-  HBaseTestingUtility htu;
   protected static final Log LOG = LogFactory.getLog(TestJavaHBaseContext.class);
 
+  private transient JavaSparkContext jsc;
+  private static HBaseTestingUtility htu;
+
 
   byte[] tableName = Bytes.toBytes("t1");
   byte[] columnFamily = Bytes.toBytes("c");
+  byte[] columnFamily1 = Bytes.toBytes("d");
   String columnFamilyStr = Bytes.toString(columnFamily);
+  String columnFamilyStr1 = Bytes.toString(columnFamily1);
+
 
   @Before
   public void setUp() {
     jsc = new JavaSparkContext("local", "JavaHBaseContextSuite");
-    jsc.addJar("spark.jar");
 
     File tempDir = Files.createTempDir();
     tempDir.deleteOnExit();
 
-    htu = HBaseTestingUtility.createLocalHTU();
+    htu = new HBaseTestingUtility();
     try {
       LOG.info("cleaning up test dir");
 
@@ -91,7 +106,7 @@ public class TestJavaHBaseContext implements Serializable {
 
       LOG.info(" - creating table " + Bytes.toString(tableName));
       htu.createTable(TableName.valueOf(tableName),
-              columnFamily);
+          new byte[][]{columnFamily, columnFamily1});
       LOG.info(" - created table");
     } catch (Exception e1) {
       throw new RuntimeException(e1);
@@ -278,6 +293,161 @@ public class TestJavaHBaseContext implements Serializable {
     Assert.assertEquals(stringJavaRDD.count(), 5);
   }
 
+  @Test
+  public void testBulkLoad() throws Exception {
+
+    Path output = htu.getDataTestDir("testBulkLoad");
+    List<KeyFamilyQualifierValue> list= new ArrayList<KeyFamilyQualifierValue>();
+    // row1
+    byte[] row1 = Bytes.toBytes("1");
+    KeyFamilyQualifierValue k11 = new KeyFamilyQualifierValue(row1, columnFamily, Bytes.toBytes("b"), Bytes.toBytes("1"));
+    list.add(k11);
+    // row3
+    byte[] row3 = Bytes.toBytes("3");
+    KeyFamilyQualifierValue k31 = new KeyFamilyQualifierValue(row3, columnFamily, Bytes.toBytes("b"), Bytes.toBytes("1"));
+    KeyFamilyQualifierValue k32 = new KeyFamilyQualifierValue(row3, columnFamily, Bytes.toBytes("a"), Bytes.toBytes("2"));
+    KeyFamilyQualifierValue k33 = new KeyFamilyQualifierValue(row3, columnFamily1, Bytes.toBytes("a"), Bytes.toBytes("1"));
+    list.add(k31);list.add(k32);list.add(k33);
+    // row2
+    byte[] row2 = Bytes.toBytes("2");
+    KeyFamilyQualifierValue k21 = new KeyFamilyQualifierValue(row2, columnFamily, Bytes.toBytes("b"), Bytes.toBytes("3"));
+    KeyFamilyQualifierValue k22 = new KeyFamilyQualifierValue(row2, columnFamily, Bytes.toBytes("a"), Bytes.toBytes("3"));
+    list.add(k21);list.add(k22);
+
+
+    JavaRDD<KeyFamilyQualifierValue> rdd = jsc.parallelize(list);
+
+    Configuration conf = htu.getConfiguration();
+    JavaHBaseContext hbaseContext = new JavaHBaseContext(jsc, conf);
+
+
+    hbaseContext.bulkLoad(rdd, TableName.valueOf(tableName), output.toUri().getPath(),
+        new HashMap<byte[], FamilyHFileWriteOptions>(), false, HConstants.DEFAULT_MAX_FILE_SIZE);
+
+    FileSystem fs = output.getFileSystem(conf);
+    Assert.assertEquals(fs.listStatus(output).length, 2);
+    Assert.assertEquals(fs.listStatus(output)[0].getPath().getName(), columnFamilyStr);
+    Assert.assertEquals(fs.listStatus(output)[1].getPath().getName(), columnFamilyStr1);
+    try (Connection conn = ConnectionFactory.createConnection(conf); Admin admin = conn.getAdmin()) {
+      Table table = conn.getTable(TableName.valueOf(tableName));
+
+      // Do bulk load
+      LoadIncrementalHFiles load = new LoadIncrementalHFiles(conf);
+      load.doBulkLoad(output, admin, table, conn.getRegionLocator(TableName.valueOf(tableName)));
+
+
+
+      // Check row1
+      List<Cell> cell1 = table.get(new Get(Bytes.toBytes("1"))).listCells();
+      Assert.assertEquals(cell1.size(), 1);
+      Assert.assertEquals(Bytes.toString(CellUtil.cloneFamily(cell1.get(0))), columnFamilyStr);
+      Assert.assertEquals(Bytes.toString(CellUtil.cloneQualifier(cell1.get(0))), "b");
+      Assert.assertEquals(Bytes.toString(CellUtil.cloneValue(cell1.get(0))), "1");
+
+      // Check row3
+      List<Cell> cell3 = table.get(new Get(Bytes.toBytes("3"))).listCells();
+      Assert.assertEquals(cell3.size(), 3);
+      Assert.assertEquals(Bytes.toString(CellUtil.cloneFamily(cell3.get(0))), columnFamilyStr);
+      Assert.assertEquals(Bytes.toString(CellUtil.cloneQualifier(cell3.get(0))), "a");
+      Assert.assertEquals(Bytes.toString(CellUtil.cloneValue(cell3.get(0))), "2");
+      Assert.assertEquals(Bytes.toString(CellUtil.cloneFamily(cell3.get(1))), columnFamilyStr);
+      Assert.assertEquals(Bytes.toString(CellUtil.cloneQualifier(cell3.get(1))), "b");
+      Assert.assertEquals(Bytes.toString(CellUtil.cloneValue(cell3.get(1))), "1");
+      Assert.assertEquals(Bytes.toString(CellUtil.cloneFamily(cell3.get(2))), columnFamilyStr1);
+      Assert.assertEquals(Bytes.toString(CellUtil.cloneQualifier(cell3.get(2))), "a");
+      Assert.assertEquals(Bytes.toString(CellUtil.cloneValue(cell3.get(2))), "1");
+
+      // Check row2
+      List<Cell> cell2 = table.get(new Get(Bytes.toBytes("2"))).listCells();
+      Assert.assertEquals(cell2.size(), 2);
+      Assert.assertEquals(Bytes.toString(CellUtil.cloneFamily(cell2.get(0))), columnFamilyStr);
+      Assert.assertEquals(Bytes.toString(CellUtil.cloneQualifier(cell2.get(0))), "a");
+      Assert.assertEquals(Bytes.toString(CellUtil.cloneValue(cell2.get(0))), "3");
+      Assert.assertEquals(Bytes.toString(CellUtil.cloneFamily(cell2.get(1))), columnFamilyStr);
+      Assert.assertEquals(Bytes.toString(CellUtil.cloneQualifier(cell2.get(1))), "b");
+      Assert.assertEquals(Bytes.toString(CellUtil.cloneValue(cell2.get(1))), "3");
+    }
+
+  }
+
+  @Test
+  public void testBulkLoadThinRows() throws Exception {
+    Path output = htu.getDataTestDir("testBulkLoadThinRows");
+    //String output  = dir.getName() + "/" + String.valueOf(EnvironmentEdgeManager.currentTime());
+    List<KeyFamiliesQualifiersValues> list = new ArrayList<KeyFamiliesQualifiersValues>();
+    // row1
+    byte[] row1 = Bytes.toBytes("1");
+    FamiliesQualifiersValues fqv1 = new FamiliesQualifiersValues();
+    fqv1.add(columnFamily, Bytes.toBytes("b"), Bytes.toBytes("1"));
+    list.add(new KeyFamiliesQualifiersValues(new ByteArrayWrapper(row1), fqv1));
+    // row3
+    byte[] row3 = Bytes.toBytes("3");
+    FamiliesQualifiersValues fqv3 = new FamiliesQualifiersValues();
+    fqv3.add(columnFamily1, Bytes.toBytes("a"), Bytes.toBytes("1"));
+    fqv3.add(columnFamily, Bytes.toBytes("b"), Bytes.toBytes("1"));
+    fqv3.add(columnFamily, Bytes.toBytes("a"), Bytes.toBytes("2"));
+    list.add(new KeyFamiliesQualifiersValues(new ByteArrayWrapper(row3), fqv3));
+    // row2
+    byte[] row2 = Bytes.toBytes("2");
+    FamiliesQualifiersValues fqv2 = new FamiliesQualifiersValues();
+    fqv2.add(columnFamily, Bytes.toBytes("a"), Bytes.toBytes("3"));
+    fqv2.add(columnFamily, Bytes.toBytes("b"), Bytes.toBytes("3"));
+    list.add(new KeyFamiliesQualifiersValues(new ByteArrayWrapper(row2), fqv2));
+
+    JavaRDD<KeyFamiliesQualifiersValues> rdd = jsc.parallelize(list);
+
+    Configuration conf = htu.getConfiguration();
+    JavaHBaseContext hbaseContext = new JavaHBaseContext(jsc, conf);
+
+    FileSystem fs = output.getFileSystem(conf);
+    hbaseContext.bulkLoadThinRows(rdd, TableName.valueOf(tableName), output.toString(),
+        new HashMap<byte[], FamilyHFileWriteOptions>(), false, HConstants.DEFAULT_MAX_FILE_SIZE);
+
+    Assert.assertEquals(fs.listStatus(output).length, 2);
+    Assert.assertEquals(fs.listStatus(output)[0].getPath().getName(), columnFamilyStr);
+    Assert.assertEquals(fs.listStatus(output)[1].getPath().getName(), columnFamilyStr1);
+
+    try (Connection conn = ConnectionFactory.createConnection(conf); Admin admin = conn.getAdmin()) {
+      Table table = conn.getTable(TableName.valueOf(tableName));
+      // Do bulk load
+      LoadIncrementalHFiles load = new LoadIncrementalHFiles(conf);
+      load.doBulkLoad(output, admin, table, conn.getRegionLocator(TableName.valueOf(tableName)));
+
+
+
+      // Check row1
+      List<Cell> cell1 = table.get(new Get(Bytes.toBytes("1"))).listCells();
+      Assert.assertEquals(cell1.size(), 1);
+      Assert.assertEquals(Bytes.toString(CellUtil.cloneFamily(cell1.get(0))), columnFamilyStr);
+      Assert.assertEquals(Bytes.toString(CellUtil.cloneQualifier(cell1.get(0))), "b");
+      Assert.assertEquals(Bytes.toString(CellUtil.cloneValue(cell1.get(0))), "1");
+
+      // Check row3
+      List<Cell> cell3 = table.get(new Get(Bytes.toBytes("3"))).listCells();
+      Assert.assertEquals(cell3.size(), 3);
+      Assert.assertEquals(Bytes.toString(CellUtil.cloneFamily(cell3.get(0))), columnFamilyStr);
+      Assert.assertEquals(Bytes.toString(CellUtil.cloneQualifier(cell3.get(0))), "a");
+      Assert.assertEquals(Bytes.toString(CellUtil.cloneValue(cell3.get(0))), "2");
+      Assert.assertEquals(Bytes.toString(CellUtil.cloneFamily(cell3.get(1))), columnFamilyStr);
+      Assert.assertEquals(Bytes.toString(CellUtil.cloneQualifier(cell3.get(1))), "b");
+      Assert.assertEquals(Bytes.toString(CellUtil.cloneValue(cell3.get(1))), "1");
+      Assert.assertEquals(Bytes.toString(CellUtil.cloneFamily(cell3.get(2))), columnFamilyStr1);
+      Assert.assertEquals(Bytes.toString(CellUtil.cloneQualifier(cell3.get(2))), "a");
+      Assert.assertEquals(Bytes.toString(CellUtil.cloneValue(cell3.get(2))), "1");
+
+      // Check row2
+      List<Cell> cell2 = table.get(new Get(Bytes.toBytes("2"))).listCells();
+      Assert.assertEquals(cell2.size(), 2);
+      Assert.assertEquals(Bytes.toString(CellUtil.cloneFamily(cell2.get(0))), columnFamilyStr);
+      Assert.assertEquals(Bytes.toString(CellUtil.cloneQualifier(cell2.get(0))), "a");
+      Assert.assertEquals(Bytes.toString(CellUtil.cloneValue(cell2.get(0))), "3");
+      Assert.assertEquals(Bytes.toString(CellUtil.cloneFamily(cell2.get(1))), columnFamilyStr);
+      Assert.assertEquals(Bytes.toString(CellUtil.cloneQualifier(cell2.get(1))), "b");
+      Assert.assertEquals(Bytes.toString(CellUtil.cloneValue(cell2.get(1))), "3");
+    }
+
+  }
+
   public static class GetFunction implements Function<byte[], Get> {
 
     private static final long serialVersionUID = 1L;
@@ -319,7 +489,7 @@ public class TestJavaHBaseContext implements Serializable {
   }
 
   private void populateTableWithMockData(Configuration conf, TableName tableName)
-          throws IOException {
+      throws IOException {
     try (
       Connection conn = ConnectionFactory.createConnection(conf);
       Table table = conn.getTable(tableName)) {
@@ -335,4 +505,4 @@ public class TestJavaHBaseContext implements Serializable {
     }
   }
 
-}
\ No newline at end of file
+}
diff --git a/hbase-spark/src/test/scala/org/apache/hadoop/hbase/spark/BulkLoadSuite.scala b/hbase-spark/src/test/scala/org/apache/hadoop/hbase/spark/BulkLoadSuite.scala
index 795ce6d..e121892 100644
--- a/hbase-spark/src/test/scala/org/apache/hadoop/hbase/spark/BulkLoadSuite.scala
+++ b/hbase-spark/src/test/scala/org/apache/hadoop/hbase/spark/BulkLoadSuite.scala
@@ -103,8 +103,7 @@ BeforeAndAfterEach with BeforeAndAfterAll  with Logging {
 
     val hbaseContext = new HBaseContext(sc, config)
 
-    testFolder.create()
-    val stagingFolder = testFolder.newFolder()
+    val stagingFolder = TEST_UTIL.getDataTestDir("testWideRowBulkLoad1")
 
     hbaseContext.bulkLoad[(Array[Byte], (Array[Byte], Array[Byte], Array[Byte]))](rdd,
       TableName.valueOf(tableName),
@@ -118,17 +117,17 @@ BeforeAndAfterEach with BeforeAndAfterAll  with Logging {
 
         Seq((keyFamilyQualifier, value)).iterator
       },
-      stagingFolder.getPath)
+      stagingFolder.toUri.getPath)
 
     val fs = FileSystem.get(config)
-    assert(fs.listStatus(new Path(stagingFolder.getPath)).length == 2)
+    assert(fs.listStatus(stagingFolder).length == 2)
 
     val conn = ConnectionFactory.createConnection(config)
 
     val load = new LoadIncrementalHFiles(config)
     val table = conn.getTable(TableName.valueOf(tableName))
     try {
-      load.doBulkLoad(new Path(stagingFolder.getPath), conn.getAdmin, table,
+      load.doBulkLoad(stagingFolder, conn.getAdmin, table,
         conn.getRegionLocator(TableName.valueOf(tableName)))
 
       val cells5 = table.get(new Get(Bytes.toBytes("5"))).listCells()
@@ -183,10 +182,7 @@ BeforeAndAfterEach with BeforeAndAfterAll  with Logging {
       } finally {
         admin.close()
       }
-      fs.delete(new Path(stagingFolder.getPath), true)
-
-      testFolder.delete()
-
+      fs.delete(stagingFolder, true)
     }
   }
 
@@ -226,8 +222,7 @@ BeforeAndAfterEach with BeforeAndAfterAll  with Logging {
 
     val hbaseContext = new HBaseContext(sc, config)
 
-    testFolder.create()
-    val stagingFolder = testFolder.newFolder()
+    val stagingFolder = TEST_UTIL.getDataTestDir("testWideRowBulkLoad2")
 
     rdd.hbaseBulkLoad(hbaseContext,
       TableName.valueOf(tableName),
@@ -241,22 +236,22 @@ BeforeAndAfterEach with BeforeAndAfterAll  with Logging {
 
         Seq((keyFamilyQualifier, value)).iterator
       },
-      stagingFolder.getPath,
+      stagingFolder.toUri.getPath,
       new java.util.HashMap[Array[Byte], FamilyHFileWriteOptions],
       compactionExclude = false,
       20)
 
     val fs = FileSystem.get(config)
-    assert(fs.listStatus(new Path(stagingFolder.getPath)).length == 1)
+    assert(fs.listStatus(stagingFolder).length == 1)
 
-    assert(fs.listStatus(new Path(stagingFolder.getPath+ "/f1")).length == 5)
+    assert(fs.listStatus(new Path(stagingFolder.toUri.getPath + "/f1")).length == 5)
 
     val conn = ConnectionFactory.createConnection(config)
 
     val load = new LoadIncrementalHFiles(config)
     val table = conn.getTable(TableName.valueOf(tableName))
     try {
-      load.doBulkLoad(new Path(stagingFolder.getPath),
+      load.doBulkLoad(stagingFolder,
         conn.getAdmin, table, conn.getRegionLocator(TableName.valueOf(tableName)))
 
       val cells5 = table.get(new Get(Bytes.toBytes("5"))).listCells()
@@ -310,9 +305,7 @@ BeforeAndAfterEach with BeforeAndAfterAll  with Logging {
       } finally {
         admin.close()
       }
-      fs.delete(new Path(stagingFolder.getPath), true)
-
-      testFolder.delete()
+      fs.delete(stagingFolder, true)
     }
   }
 
@@ -357,8 +350,7 @@ BeforeAndAfterEach with BeforeAndAfterAll  with Logging {
 
     val hbaseContext = new HBaseContext(sc, config)
 
-    testFolder.create()
-    val stagingFolder = testFolder.newFolder()
+    val stagingFolder = TEST_UTIL.getDataTestDir("testWideRowBulkLoad3")
 
     val familyHBaseWriterOptions = new java.util.HashMap[Array[Byte], FamilyHFileWriteOptions]
 
@@ -379,15 +371,15 @@ BeforeAndAfterEach with BeforeAndAfterAll  with Logging {
 
         Seq((keyFamilyQualifier, value)).iterator
       },
-      stagingFolder.getPath,
+      stagingFolder.toUri.getPath,
       familyHBaseWriterOptions,
       compactionExclude = false,
       HConstants.DEFAULT_MAX_FILE_SIZE)
 
     val fs = FileSystem.get(config)
-    assert(fs.listStatus(new Path(stagingFolder.getPath)).length == 2)
+    assert(fs.listStatus(stagingFolder).length == 2)
 
-    val f1FileList = fs.listStatus(new Path(stagingFolder.getPath +"/f1"))
+    val f1FileList = fs.listStatus(new Path(stagingFolder.toUri.getPath + "/f1"))
     for ( i <- 0 until f1FileList.length) {
       val reader = HFile.createReader(fs, f1FileList(i).getPath,
         new CacheConfig(config), config)
@@ -397,7 +389,7 @@ BeforeAndAfterEach with BeforeAndAfterAll  with Logging {
 
     assert( 3 ==  f1FileList.length)
 
-    val f2FileList = fs.listStatus(new Path(stagingFolder.getPath +"/f2"))
+    val f2FileList = fs.listStatus(new Path(stagingFolder.toUri.getPath + "/f2"))
     for ( i <- 0 until f2FileList.length) {
       val reader = HFile.createReader(fs, f2FileList(i).getPath,
         new CacheConfig(config), config)
@@ -413,7 +405,7 @@ BeforeAndAfterEach with BeforeAndAfterAll  with Logging {
     val load = new LoadIncrementalHFiles(config)
     val table = conn.getTable(TableName.valueOf(tableName))
     try {
-      load.doBulkLoad(new Path(stagingFolder.getPath),
+      load.doBulkLoad(stagingFolder,
         conn.getAdmin, table, conn.getRegionLocator(TableName.valueOf(tableName)))
 
       val cells5 = table.get(new Get(Bytes.toBytes("5"))).listCells()
@@ -468,9 +460,7 @@ BeforeAndAfterEach with BeforeAndAfterAll  with Logging {
       } finally {
         admin.close()
       }
-      fs.delete(new Path(stagingFolder.getPath), true)
-
-      testFolder.delete()
+      fs.delete(stagingFolder, true)
 
     }
   }
@@ -571,8 +561,7 @@ BeforeAndAfterEach with BeforeAndAfterAll  with Logging {
 
     val hbaseContext = new HBaseContext(sc, config)
 
-    testFolder.create()
-    val stagingFolder = testFolder.newFolder()
+    val stagingFolder = TEST_UTIL.getDataTestDir("testThinRowBulkLoad1")
 
     hbaseContext.bulkLoadThinRows[(String, Iterable[(Array[Byte], Array[Byte], Array[Byte])])](rdd,
       TableName.valueOf(tableName),
@@ -589,17 +578,17 @@ BeforeAndAfterEach with BeforeAndAfterAll  with Logging {
         })
         (new ByteArrayWrapper(rowKey), familyQualifiersValues)
       },
-      stagingFolder.getPath)
+      stagingFolder.toUri.getPath)
 
     val fs = FileSystem.get(config)
-    assert(fs.listStatus(new Path(stagingFolder.getPath)).length == 2)
+    assert(fs.listStatus(stagingFolder).length == 2)
 
     val conn = ConnectionFactory.createConnection(config)
 
     val load = new LoadIncrementalHFiles(config)
     val table = conn.getTable(TableName.valueOf(tableName))
     try {
-      load.doBulkLoad(new Path(stagingFolder.getPath), conn.getAdmin, table,
+      load.doBulkLoad(stagingFolder, conn.getAdmin, table,
         conn.getRegionLocator(TableName.valueOf(tableName)))
 
       val cells5 = table.get(new Get(Bytes.toBytes("5"))).listCells()
@@ -654,10 +643,7 @@ BeforeAndAfterEach with BeforeAndAfterAll  with Logging {
       } finally {
         admin.close()
       }
-      fs.delete(new Path(stagingFolder.getPath), true)
-
-      testFolder.delete()
-
+      fs.delete(stagingFolder, true)
     }
   }
 
@@ -698,8 +684,7 @@ BeforeAndAfterEach with BeforeAndAfterAll  with Logging {
 
     val hbaseContext = new HBaseContext(sc, config)
 
-    testFolder.create()
-    val stagingFolder = testFolder.newFolder()
+    val stagingFolder = TEST_UTIL.getDataTestDir("testThinRowBulkLoad2")
 
     rdd.hbaseBulkLoadThinRows(hbaseContext,
       TableName.valueOf(tableName),
@@ -716,22 +701,22 @@ BeforeAndAfterEach with BeforeAndAfterAll  with Logging {
         })
         (new ByteArrayWrapper(Bytes.toBytes(rowKey)), familyQualifiersValues)
       },
-      stagingFolder.getPath,
+      stagingFolder.toUri.getPath,
       new java.util.HashMap[Array[Byte], FamilyHFileWriteOptions],
       compactionExclude = false,
       20)
 
     val fs = FileSystem.get(config)
-    assert(fs.listStatus(new Path(stagingFolder.getPath)).length == 1)
+    assert(fs.listStatus(stagingFolder).length == 1)
 
-    assert(fs.listStatus(new Path(stagingFolder.getPath+ "/f1")).length == 5)
+    assert(fs.listStatus(new Path(stagingFolder.toUri.getPath + "/f1")).length == 5)
 
     val conn = ConnectionFactory.createConnection(config)
 
     val load = new LoadIncrementalHFiles(config)
     val table = conn.getTable(TableName.valueOf(tableName))
     try {
-      load.doBulkLoad(new Path(stagingFolder.getPath),
+      load.doBulkLoad(stagingFolder,
         conn.getAdmin, table, conn.getRegionLocator(TableName.valueOf(tableName)))
 
       val cells5 = table.get(new Get(Bytes.toBytes("5"))).listCells()
@@ -785,9 +770,7 @@ BeforeAndAfterEach with BeforeAndAfterAll  with Logging {
       } finally {
         admin.close()
       }
-      fs.delete(new Path(stagingFolder.getPath), true)
-
-      testFolder.delete()
+      fs.delete(stagingFolder, true)
     }
   }
 
@@ -833,8 +816,7 @@ BeforeAndAfterEach with BeforeAndAfterAll  with Logging {
 
     val hbaseContext = new HBaseContext(sc, config)
 
-    testFolder.create()
-    val stagingFolder = testFolder.newFolder()
+    val stagingFolder = TEST_UTIL.getDataTestDir("testThinRowBulkLoad3")
 
     val familyHBaseWriterOptions = new java.util.HashMap[Array[Byte], FamilyHFileWriteOptions]
 
@@ -858,15 +840,15 @@ BeforeAndAfterEach with BeforeAndAfterAll  with Logging {
         })
         (new ByteArrayWrapper(Bytes.toBytes(rowKey)), familyQualifiersValues)
       },
-      stagingFolder.getPath,
+      stagingFolder.toUri.getPath,
       familyHBaseWriterOptions,
       compactionExclude = false,
       HConstants.DEFAULT_MAX_FILE_SIZE)
 
     val fs = FileSystem.get(config)
-    assert(fs.listStatus(new Path(stagingFolder.getPath)).length == 2)
+    assert(fs.listStatus(stagingFolder).length == 2)
 
-    val f1FileList = fs.listStatus(new Path(stagingFolder.getPath +"/f1"))
+    val f1FileList = fs.listStatus(new Path(stagingFolder.toUri.getPath + "/f1"))
     for ( i <- 0 until f1FileList.length) {
       val reader = HFile.createReader(fs, f1FileList(i).getPath,
         new CacheConfig(config), config)
@@ -876,7 +858,7 @@ BeforeAndAfterEach with BeforeAndAfterAll  with Logging {
 
     assert( 3 ==  f1FileList.length)
 
-    val f2FileList = fs.listStatus(new Path(stagingFolder.getPath +"/f2"))
+    val f2FileList = fs.listStatus(new Path(stagingFolder.toUri.getPath + "/f2"))
     for ( i <- 0 until f2FileList.length) {
       val reader = HFile.createReader(fs, f2FileList(i).getPath,
         new CacheConfig(config), config)
@@ -892,7 +874,7 @@ BeforeAndAfterEach with BeforeAndAfterAll  with Logging {
     val load = new LoadIncrementalHFiles(config)
     val table = conn.getTable(TableName.valueOf(tableName))
     try {
-      load.doBulkLoad(new Path(stagingFolder.getPath),
+      load.doBulkLoad(stagingFolder,
         conn.getAdmin, table, conn.getRegionLocator(TableName.valueOf(tableName)))
 
       val cells5 = table.get(new Get(Bytes.toBytes("5"))).listCells()
@@ -947,9 +929,7 @@ BeforeAndAfterEach with BeforeAndAfterAll  with Logging {
       } finally {
         admin.close()
       }
-      fs.delete(new Path(stagingFolder.getPath), true)
-
-      testFolder.delete()
+      fs.delete(stagingFolder, true)
 
     }
   }
