From c15c05a7ad2e0b5b7f01ffd50874fc6a42086de1 Mon Sep 17 00:00:00 2001
From: Malcolm Taylor <malcolm@semmle.com>
Date: Sat, 22 Jul 2017 10:40:27 +0100
Subject: [PATCH] fix some alerts raised by lgtm.com

---
 .../hadoop/hbase/ipc/NettyRpcDuplexHandler.java    | 139 +++++++++++----------
 .../org/apache/hadoop/hbase/zookeeper/ZKUtil.java  |  26 ++--
 .../hbase/io/hfile/bucket/BucketAllocator.java     |   2 +-
 .../hadoop/hbase/master/MasterDumpServlet.java     | 101 +++++++--------
 .../hadoop/hbase/master/MasterRpcServices.java     |   2 +-
 .../hadoop/hbase/regionserver/RSDumpServlet.java   |  85 ++++++-------
 .../compactions/FIFOCompactionPolicy.java          |  10 +-
 .../org/apache/hadoop/hbase/util/RegionMover.java  |   4 +-
 .../org/apache/hadoop/hbase/wal/WALSplitter.java   |   3 +-
 .../hbase/io/hfile/bucket/TestBucketAllocator.java |  46 +++++++
 10 files changed, 234 insertions(+), 184 deletions(-)
 create mode 100644 hbase-server/src/test/java/org/apache/hadoop/hbase/io/hfile/bucket/TestBucketAllocator.java

diff --git a/hbase-client/src/main/java/org/apache/hadoop/hbase/ipc/NettyRpcDuplexHandler.java b/hbase-client/src/main/java/org/apache/hadoop/hbase/ipc/NettyRpcDuplexHandler.java
index 08533b4..2075d6b 100644
--- a/hbase-client/src/main/java/org/apache/hadoop/hbase/ipc/NettyRpcDuplexHandler.java
+++ b/hbase-client/src/main/java/org/apache/hadoop/hbase/ipc/NettyRpcDuplexHandler.java
@@ -91,21 +91,22 @@ class NettyRpcDuplexHandler extends ChannelDuplexHandler {
         : sizeWithoutCellBlock;
     ByteBuf buf = ctx.alloc().buffer(sizeWithoutCellBlock + 4);
     buf.writeInt(totalSize);
-    ByteBufOutputStream bbos = new ByteBufOutputStream(buf);
-    requestHeader.writeDelimitedTo(bbos);
-    if (call.param != null) {
-      call.param.writeDelimitedTo(bbos);
-    }
-    if (cellBlock != null) {
-      ChannelPromise withoutCellBlockPromise = ctx.newPromise();
-      ctx.write(buf, withoutCellBlockPromise);
-      ChannelPromise cellBlockPromise = ctx.newPromise();
-      ctx.write(cellBlock, cellBlockPromise);
-      PromiseCombiner combiner = new PromiseCombiner();
-      combiner.addAll(withoutCellBlockPromise, cellBlockPromise);
-      combiner.finish(promise);
-    } else {
-      ctx.write(buf, promise);
+    try (ByteBufOutputStream bbos = new ByteBufOutputStream(buf)) {
+      requestHeader.writeDelimitedTo(bbos);
+      if (call.param != null) {
+        call.param.writeDelimitedTo(bbos);
+      }
+      if (cellBlock != null) {
+        ChannelPromise withoutCellBlockPromise = ctx.newPromise();
+        ctx.write(buf, withoutCellBlockPromise);
+        ChannelPromise cellBlockPromise = ctx.newPromise();
+        ctx.write(cellBlock, cellBlockPromise);
+        PromiseCombiner combiner = new PromiseCombiner();
+        combiner.addAll(withoutCellBlockPromise, cellBlockPromise);
+        combiner.finish(promise);
+      } else {
+        ctx.write(buf, promise);
+      }
     }
   }
 
@@ -121,64 +122,66 @@ class NettyRpcDuplexHandler extends ChannelDuplexHandler {
 
   private void readResponse(ChannelHandlerContext ctx, ByteBuf buf) throws IOException {
     int totalSize = buf.readInt();
-    ByteBufInputStream in = new ByteBufInputStream(buf);
-    ResponseHeader responseHeader = ResponseHeader.parseDelimitedFrom(in);
-    int id = responseHeader.getCallId();
-    if (LOG.isTraceEnabled()) {
-      LOG.trace("got response header " + TextFormat.shortDebugString(responseHeader)
+    try (ByteBufInputStream in = new ByteBufInputStream(buf)) {
+      ResponseHeader responseHeader = ResponseHeader.parseDelimitedFrom(in);
+      int id = responseHeader.getCallId();
+      if (LOG.isTraceEnabled()) {
+        LOG.trace("got response header " + TextFormat.shortDebugString(responseHeader)
           + ", totalSize: " + totalSize + " bytes");
-    }
-    RemoteException remoteExc;
-    if (responseHeader.hasException()) {
-      ExceptionResponse exceptionResponse = responseHeader.getException();
-      remoteExc = IPCUtil.createRemoteException(exceptionResponse);
-      if (IPCUtil.isFatalConnectionException(exceptionResponse)) {
-        // Here we will cleanup all calls so do not need to fall back, just return.
-        exceptionCaught(ctx, remoteExc);
+      }
+      RemoteException remoteExc;
+      if (responseHeader.hasException()) {
+        ExceptionResponse exceptionResponse = responseHeader.getException();
+        remoteExc = IPCUtil.createRemoteException(exceptionResponse);
+        if (IPCUtil.isFatalConnectionException(exceptionResponse)) {
+          // Here we will cleanup all calls so do not need to fall back, just return.
+          exceptionCaught(ctx, remoteExc);
+          return;
+        }
+      } else {
+        remoteExc = null;
+      }
+      Call call = id2Call.remove(id);
+      if (call == null) {
+        // So we got a response for which we have no corresponding 'call' here on the client-side.
+        // We probably timed out waiting, cleaned up all references, and now the server decides
+        // to return a response. There is nothing we can do w/ the response at this stage. Clean
+        // out the wire of the response so its out of the way and we can get other responses on
+        // this connection.
+        int readSoFar = IPCUtil.getTotalSizeWhenWrittenDelimited(responseHeader);
+        int whatIsLeftToRead = totalSize - readSoFar;
+        if (LOG.isDebugEnabled()) {
+          LOG.debug("Unknown callId: " + id + ", skipping over this response of "
+              + whatIsLeftToRead + " bytes");
+        }
         return;
       }
-    } else {
-      remoteExc = null;
-    }
-    Call call = id2Call.remove(id);
-    if (call == null) {
-      // So we got a response for which we have no corresponding 'call' here on the client-side.
-      // We probably timed out waiting, cleaned up all references, and now the server decides
-      // to return a response. There is nothing we can do w/ the response at this stage. Clean
-      // out the wire of the response so its out of the way and we can get other responses on
-      // this connection.
-      int readSoFar = IPCUtil.getTotalSizeWhenWrittenDelimited(responseHeader);
-      int whatIsLeftToRead = totalSize - readSoFar;
-      if (LOG.isDebugEnabled()) {
-        LOG.debug("Unknown callId: " + id + ", skipping over this response of " + whatIsLeftToRead
-            + " bytes");
+      if (remoteExc != null) {
+        call.setException(remoteExc);
+        return;
       }
-      return;
-    }
-    if (remoteExc != null) {
-      call.setException(remoteExc);
-      return;
-    }
-    Message value;
-    if (call.responseDefaultType != null) {
-      Builder builder = call.responseDefaultType.newBuilderForType();
-      builder.mergeDelimitedFrom(in);
-      value = builder.build();
-    } else {
-      value = null;
-    }
-    CellScanner cellBlockScanner;
-    if (responseHeader.hasCellBlockMeta()) {
-      int size = responseHeader.getCellBlockMeta().getLength();
-      // Maybe we could read directly from the ByteBuf.
-      // The problem here is that we do not know when to release it.
-      byte[] cellBlock = new byte[size];
-      buf.readBytes(cellBlock);
-      cellBlockScanner = cellBlockBuilder.createCellScanner(this.codec, this.compressor, cellBlock);
-    } else {
-      cellBlockScanner = null;
+      Message value;
+      if (call.responseDefaultType != null) {
+        Builder builder = call.responseDefaultType.newBuilderForType();
+        builder.mergeDelimitedFrom(in);
+        value = builder.build();
+      } else {
+        value = null;
+      }
+      CellScanner cellBlockScanner;
+      if (responseHeader.hasCellBlockMeta()) {
+        int size = responseHeader.getCellBlockMeta().getLength();
+        // Maybe we could read directly from the ByteBuf.
+        // The problem here is that we do not know when to release it.
+        byte[] cellBlock = new byte[size];
+        buf.readBytes(cellBlock);
+        cellBlockScanner =
+            cellBlockBuilder.createCellScanner(this.codec, this.compressor, cellBlock);
+      } else {
+        cellBlockScanner = null;
+      }
+      call.setResponse(value, cellBlockScanner);
     }
-    call.setResponse(value, cellBlockScanner);
   }
 
   @Override
diff --git a/hbase-client/src/main/java/org/apache/hadoop/hbase/zookeeper/ZKUtil.java b/hbase-client/src/main/java/org/apache/hadoop/hbase/zookeeper/ZKUtil.java
index 08b059e..7fa74b1 100644
--- a/hbase-client/src/main/java/org/apache/hadoop/hbase/zookeeper/ZKUtil.java
+++ b/hbase-client/src/main/java/org/apache/hadoop/hbase/zookeeper/ZKUtil.java
@@ -1921,21 +1921,21 @@ public class ZKUtil {
       socket.connect(sockAddr, timeout);
 
       socket.setSoTimeout(timeout);
-      PrintWriter out = new PrintWriter(socket.getOutputStream(), true);
-      BufferedReader in = new BufferedReader(new InputStreamReader(
-        socket.getInputStream()));
-      out.println("stat");
-      out.flush();
-      ArrayList<String> res = new ArrayList<>();
-      while (true) {
-        String line = in.readLine();
-        if (line != null) {
-          res.add(line);
-        } else {
-          break;
+      try (PrintWriter out = new PrintWriter(socket.getOutputStream(), true);
+          BufferedReader in = new BufferedReader(new InputStreamReader(socket.getInputStream()))) {
+        out.println("stat");
+        out.flush();
+        ArrayList<String> res = new ArrayList<>();
+        while (true) {
+          String line = in.readLine();
+          if (line != null) {
+            res.add(line);
+          } else {
+            break;
+          }
         }
+        return res.toArray(new String[res.size()]);
       }
-      return res.toArray(new String[res.size()]);
     }
   }
 
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/bucket/BucketAllocator.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/bucket/BucketAllocator.java
index 5c8fa1b..d1b8bb7 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/bucket/BucketAllocator.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/bucket/BucketAllocator.java
@@ -315,7 +315,7 @@ public final class BucketAllocator {
     this.bucketSizes = bucketSizes == null ? DEFAULT_BUCKET_SIZES : bucketSizes;
     Arrays.sort(this.bucketSizes);
     this.bigItemSize = Ints.max(this.bucketSizes);
-    this.bucketCapacity = FEWEST_ITEMS_IN_BUCKET * bigItemSize;
+    this.bucketCapacity = FEWEST_ITEMS_IN_BUCKET * (long) bigItemSize;
     buckets = new Bucket[(int) (availableSpace / bucketCapacity)];
     if (buckets.length < this.bucketSizes.length)
       throw new BucketAllocatorException("Bucket allocator size too small (" + buckets.length +
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/master/MasterDumpServlet.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/master/MasterDumpServlet.java
index a48444c..86dc014 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/master/MasterDumpServlet.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/master/MasterDumpServlet.java
@@ -54,60 +54,61 @@ public class MasterDumpServlet extends StateDumpServlet {
 
     response.setContentType("text/plain");
     OutputStream os = response.getOutputStream();
-    PrintWriter out = new PrintWriter(os);
+    try (PrintWriter out = new PrintWriter(os)) {
 
-    out.println("Master status for " + master.getServerName()
+      out.println("Master status for " + master.getServerName()
         + " as of " + new Date());
 
-    out.println("\n\nVersion Info:");
-    out.println(LINE);
-    dumpVersionInfo(out);
-
-    out.println("\n\nTasks:");
-    out.println(LINE);
-    TaskMonitor.get().dumpAsText(out);
-
-    out.println("\n\nServers:");
-    out.println(LINE);
-    dumpServers(master, out);
-
-    out.println("\n\nRegions-in-transition:");
-    out.println(LINE);
-    dumpRIT(master, out);
-
-    out.println("\n\nExecutors:");
-    out.println(LINE);
-    dumpExecutors(master.getExecutorService(), out);
-
-    out.println("\n\nStacks:");
-    out.println(LINE);
-    out.flush();
-    PrintStream ps = new PrintStream(response.getOutputStream(), false, "UTF-8");
-    Threads.printThreadInfo(ps, "");
-    ps.flush();
-
-    out.println("\n\nMaster configuration:");
-    out.println(LINE);
-    Configuration conf = master.getConfiguration();
-    out.flush();
-    conf.writeXml(os);
-    os.flush();
-
-    out.println("\n\nRecent regionserver aborts:");
-    out.println(LINE);
-    master.getRegionServerFatalLogBuffer().dumpTo(out);
-
-    out.println("\n\nLogs");
-    out.println(LINE);
-    long tailKb = getTailKbParam(request);
-    LogMonitoring.dumpTailOfLogs(out, tailKb);
-
-    out.println("\n\nRS Queue:");
-    out.println(LINE);
-    if(isShowQueueDump(conf)) {
-      RSDumpServlet.dumpQueue(master, out);
+      out.println("\n\nVersion Info:");
+      out.println(LINE);
+      dumpVersionInfo(out);
+
+      out.println("\n\nTasks:");
+      out.println(LINE);
+      TaskMonitor.get().dumpAsText(out);
+
+      out.println("\n\nServers:");
+      out.println(LINE);
+      dumpServers(master, out);
+
+      out.println("\n\nRegions-in-transition:");
+      out.println(LINE);
+      dumpRIT(master, out);
+
+      out.println("\n\nExecutors:");
+      out.println(LINE);
+      dumpExecutors(master.getExecutorService(), out);
+
+      out.println("\n\nStacks:");
+      out.println(LINE);
+      out.flush();
+      PrintStream ps = new PrintStream(response.getOutputStream(), false, "UTF-8");
+      Threads.printThreadInfo(ps, "");
+      ps.flush();
+
+      out.println("\n\nMaster configuration:");
+      out.println(LINE);
+      Configuration conf = master.getConfiguration();
+      out.flush();
+      conf.writeXml(os);
+      os.flush();
+
+      out.println("\n\nRecent regionserver aborts:");
+      out.println(LINE);
+      master.getRegionServerFatalLogBuffer().dumpTo(out);
+
+      out.println("\n\nLogs");
+      out.println(LINE);
+      long tailKb = getTailKbParam(request);
+      LogMonitoring.dumpTailOfLogs(out, tailKb);
+
+      out.println("\n\nRS Queue:");
+      out.println(LINE);
+      if (isShowQueueDump(conf)) {
+        RSDumpServlet.dumpQueue(master, out);
+      }
+      out.flush();
     }
-    out.flush();
   }
 
 
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/master/MasterRpcServices.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/master/MasterRpcServices.java
index 8b9266c..aa64caa 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/master/MasterRpcServices.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/master/MasterRpcServices.java
@@ -1291,7 +1291,7 @@ public class MasterRpcServices extends RSRpcServices
     throws ServiceException {
     try {
       master.checkInitialized();
-      Boolean result = master.getHFileCleaner().runCleaner() && master.getLogCleaner().runCleaner();
+      boolean result = master.getHFileCleaner().runCleaner() && master.getLogCleaner().runCleaner();
       return ResponseConverter.buildRunCleanerChoreResponse(result);
     } catch (IOException ioe) {
       throw new ServiceException(ioe);
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/RSDumpServlet.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/RSDumpServlet.java
index 9499a79..997ffcc 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/RSDumpServlet.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/RSDumpServlet.java
@@ -55,52 +55,53 @@ public class RSDumpServlet extends StateDumpServlet {
     }
 
     OutputStream os = response.getOutputStream();
-    PrintWriter out = new PrintWriter(os);
+    try (PrintWriter out = new PrintWriter(os)) {
 
-    out.println("RegionServer status for " + hrs.getServerName()
+      out.println("RegionServer status for " + hrs.getServerName()
         + " as of " + new Date());
 
-    out.println("\n\nVersion Info:");
-    out.println(LINE);
-    dumpVersionInfo(out);
-
-    out.println("\n\nTasks:");
-    out.println(LINE);
-    TaskMonitor.get().dumpAsText(out);
-
-    out.println("\n\nRowLocks:");
-    out.println(LINE);
-    dumpRowLock(hrs, out);
-
-    out.println("\n\nExecutors:");
-    out.println(LINE);
-    dumpExecutors(hrs.getExecutorService(), out);
-
-    out.println("\n\nStacks:");
-    out.println(LINE);
-    PrintStream ps = new PrintStream(response.getOutputStream(), false, "UTF-8");
-    Threads.printThreadInfo(ps, "");
-    ps.flush();
-
-    out.println("\n\nRS Configuration:");
-    out.println(LINE);
-    Configuration conf = hrs.getConfiguration();
-    out.flush();
-    conf.writeXml(os);
-    os.flush();
-
-    out.println("\n\nLogs");
-    out.println(LINE);
-    long tailKb = getTailKbParam(request);
-    LogMonitoring.dumpTailOfLogs(out, tailKb);
-
-    out.println("\n\nRS Queue:");
-    out.println(LINE);
-    if(isShowQueueDump(conf)) {
-      dumpQueue(hrs, out);
-    }
+      out.println("\n\nVersion Info:");
+      out.println(LINE);
+      dumpVersionInfo(out);
+
+      out.println("\n\nTasks:");
+      out.println(LINE);
+      TaskMonitor.get().dumpAsText(out);
+
+      out.println("\n\nRowLocks:");
+      out.println(LINE);
+      dumpRowLock(hrs, out);
+
+      out.println("\n\nExecutors:");
+      out.println(LINE);
+      dumpExecutors(hrs.getExecutorService(), out);
+
+      out.println("\n\nStacks:");
+      out.println(LINE);
+      PrintStream ps = new PrintStream(response.getOutputStream(), false, "UTF-8");
+      Threads.printThreadInfo(ps, "");
+      ps.flush();
+
+      out.println("\n\nRS Configuration:");
+      out.println(LINE);
+      Configuration conf = hrs.getConfiguration();
+      out.flush();
+      conf.writeXml(os);
+      os.flush();
+
+      out.println("\n\nLogs");
+      out.println(LINE);
+      long tailKb = getTailKbParam(request);
+      LogMonitoring.dumpTailOfLogs(out, tailKb);
+
+      out.println("\n\nRS Queue:");
+      out.println(LINE);
+      if (isShowQueueDump(conf)) {
+        dumpQueue(hrs, out);
+      }
 
-    out.flush();
+      out.flush();
+    }
   }
 
   public static void dumpRowLock(HRegionServer hrs, PrintWriter out) {
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/compactions/FIFOCompactionPolicy.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/compactions/FIFOCompactionPolicy.java
index 97b8387..64ca771 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/compactions/FIFOCompactionPolicy.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/compactions/FIFOCompactionPolicy.java
@@ -101,10 +101,9 @@ public class FIFOCompactionPolicy extends ExploringCompactionPolicy {
     long currentTime = EnvironmentEdgeManager.currentTime();
     for(StoreFile sf: files){
       // Check MIN_VERSIONS is in HStore removeUnneededFiles
-      Long maxTs = sf.getReader().getMaxTimestamp();
+      long maxTs = sf.getReader().getMaxTimestamp();
       long maxTtl = storeConfigInfo.getStoreFileTtl();
-      if(maxTs == null 
-          || maxTtl == Long.MAX_VALUE
+      if (maxTtl == Long.MAX_VALUE
           || (currentTime - maxTtl < maxTs)){
         continue; 
       } else{
@@ -120,10 +119,9 @@ public class FIFOCompactionPolicy extends ExploringCompactionPolicy {
     Collection<StoreFile> expiredStores = new ArrayList<>();
     for(StoreFile sf: files){
       // Check MIN_VERSIONS is in HStore removeUnneededFiles
-      Long maxTs = sf.getReader().getMaxTimestamp();
+      long maxTs = sf.getReader().getMaxTimestamp();
       long maxTtl = storeConfigInfo.getStoreFileTtl();
-      if(maxTs == null 
-          || maxTtl == Long.MAX_VALUE
+      if (maxTtl == Long.MAX_VALUE
           || (currentTime - maxTtl < maxTs)){
         continue; 
       } else if(filesCompacting == null || filesCompacting.contains(sf) == false){
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/util/RegionMover.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/util/RegionMover.java
index ce018da..65651a4 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/util/RegionMover.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/util/RegionMover.java
@@ -427,7 +427,7 @@ public class RegionMover extends AbstractHBaseTool {
     moveRegionsPool.shutdown();
     long timeoutInSeconds =
         regionsToMove.size()
-            * admin.getConfiguration().getInt(MOVE_WAIT_MAX_KEY, DEFAULT_MOVE_WAIT_MAX);
+            * (long) admin.getConfiguration().getInt(MOVE_WAIT_MAX_KEY, DEFAULT_MOVE_WAIT_MAX);
     try {
       if (!moveRegionsPool.awaitTermination(timeoutInSeconds, TimeUnit.SECONDS)) {
         moveRegionsPool.shutdownNow();
@@ -501,7 +501,7 @@ public class RegionMover extends AbstractHBaseTool {
       moveRegionsPool.shutdown();
       long timeoutInSeconds =
           regionsToMove.size()
-              * admin.getConfiguration().getInt(MOVE_WAIT_MAX_KEY, DEFAULT_MOVE_WAIT_MAX);
+              * (long) admin.getConfiguration().getInt(MOVE_WAIT_MAX_KEY, DEFAULT_MOVE_WAIT_MAX);
       try {
         if (!moveRegionsPool.awaitTermination(timeoutInSeconds, TimeUnit.SECONDS)) {
           moveRegionsPool.shutdownNow();
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/wal/WALSplitter.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/wal/WALSplitter.java
index da828de..df4c1bd 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/wal/WALSplitter.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/wal/WALSplitter.java
@@ -711,7 +711,8 @@ public class WALSplitter {
         for (FileStatus status : files) {
           String fileName = status.getPath().getName();
           try {
-            Long tmpSeqId = Long.parseLong(fileName.substring(0, fileName.length()
+            long tmpSeqId =
+                Long.parseLong(fileName.substring(0, fileName.length()
                 - SEQUENCE_ID_FILE_SUFFIX_LENGTH));
             maxSeqId = Math.max(tmpSeqId, maxSeqId);
           } catch (NumberFormatException ex) {
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/io/hfile/bucket/TestBucketAllocator.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/io/hfile/bucket/TestBucketAllocator.java
new file mode 100644
index 0000000..fedabbf
--- /dev/null
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/io/hfile/bucket/TestBucketAllocator.java
@@ -0,0 +1,46 @@
+/**
+ * Copyright The Apache Software Foundation
+ *
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements. See the NOTICE file distributed with this
+ * work for additional information regarding copyright ownership. The ASF
+ * licenses this file to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
+ * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
+ * License for the specific language governing permissions and limitations
+ * under the License.
+ */
+package org.apache.hadoop.hbase.io.hfile.bucket;
+
+import static org.junit.Assert.assertTrue;
+
+import org.apache.hadoop.hbase.testclassification.IOTests;
+import org.apache.hadoop.hbase.testclassification.SmallTests;
+import org.junit.Test;
+import org.junit.experimental.categories.Category;
+
+/**
+ * Basic test of BucketAllocator.
+ * <p>
+ * Tests will ensure that casts from int to long do not overflow
+ */
+@Category({ IOTests.class, SmallTests.class })
+public class TestBucketAllocator {
+
+
+  @Test
+  public void testBucketAllocatorLargeBuckets() throws BucketAllocatorException {
+    long availableSpace = 20 * 1024L * 1024 * 1024;
+    int[] bucketSizes = new int[] { 1024, 1024 * 1024, 1024 * 1024 * 1024 };
+    BucketAllocator allocator = new BucketAllocator(availableSpace, bucketSizes);
+    assertTrue(allocator.getBuckets().length > 0);
+  }
+
+
+}
-- 
2.7.4

