diff --git a/hbase-examples/src/main/java/org/apache/hadoop/hbase/coprocessor/example/BulkDeleteEndpoint.java b/hbase-examples/src/main/java/org/apache/hadoop/hbase/coprocessor/example/BulkDeleteEndpoint.java
index 0b65341..e2f2cb0 100644
--- a/hbase-examples/src/main/java/org/apache/hadoop/hbase/coprocessor/example/BulkDeleteEndpoint.java
+++ b/hbase-examples/src/main/java/org/apache/hadoop/hbase/coprocessor/example/BulkDeleteEndpoint.java
@@ -46,8 +46,8 @@ import org.apache.hadoop.hbase.coprocessor.example.generated.BulkDeleteProtos.Bu
 import org.apache.hadoop.hbase.filter.FirstKeyOnlyFilter;
 import org.apache.hadoop.hbase.protobuf.ProtobufUtil;
 import org.apache.hadoop.hbase.protobuf.ResponseConverter;
-import org.apache.hadoop.hbase.regionserver.HRegion;
 import org.apache.hadoop.hbase.regionserver.OperationStatus;
+import org.apache.hadoop.hbase.regionserver.Region;
 import org.apache.hadoop.hbase.regionserver.RegionScanner;
 import org.apache.hadoop.hbase.util.Bytes;
 
@@ -112,7 +112,7 @@ public class BulkDeleteEndpoint extends BulkDeleteService implements Coprocessor
       RpcCallback<BulkDeleteResponse> done) {
     long totalRowsDeleted = 0L;
     long totalVersionsDeleted = 0L;
-    HRegion region = env.getRegion();
+    Region region = env.getRegion();
     int rowBatchSize = request.getRowBatchSize();
     Long timestamp = null;
     if (request.hasTimestamp()) {
@@ -151,7 +151,8 @@ public class BulkDeleteEndpoint extends BulkDeleteService implements Coprocessor
           for (List<Cell> deleteRow : deleteRows) {
             deleteArr[i++] = createDeleteMutation(deleteRow, deleteType, timestamp);
           }
-          OperationStatus[] opStatus = region.batchMutate(deleteArr);
+          OperationStatus[] opStatus = region.batchMutate(deleteArr, HConstants.NO_NONCE,
+            HConstants.NO_NONCE);
           for (i = 0; i < opStatus.length; i++) {
             if (opStatus[i].getOperationStatusCode() != OperationStatusCode.SUCCESS) {
               break;
diff --git a/hbase-server/src/main/jamon/org/apache/hadoop/hbase/tmpl/regionserver/RegionListTmpl.jamon b/hbase-server/src/main/jamon/org/apache/hadoop/hbase/tmpl/regionserver/RegionListTmpl.jamon
index 4a17785..3dcd634 100644
--- a/hbase-server/src/main/jamon/org/apache/hadoop/hbase/tmpl/regionserver/RegionListTmpl.jamon
+++ b/hbase-server/src/main/jamon/org/apache/hadoop/hbase/tmpl/regionserver/RegionListTmpl.jamon
@@ -26,7 +26,7 @@
         org.apache.hadoop.hbase.regionserver.HRegionServer;
         org.apache.hadoop.hbase.util.Bytes;
         org.apache.hadoop.hbase.HRegionInfo;
-        org.apache.hadoop.hbase.regionserver.HRegion;
+        org.apache.hadoop.hbase.regionserver.Region;
         org.apache.hadoop.hbase.ServerName;
         org.apache.hadoop.hbase.HBaseConfiguration;
         org.apache.hadoop.hbase.protobuf.ProtobufUtil;
@@ -244,7 +244,7 @@
 
         <%for HRegionInfo r: onlineRegions %>
         <%java>    
-            HRegion region = regionServer.getFromOnlineRegions(r.getEncodedName());
+            Region region = regionServer.getFromOnlineRegions(r.getEncodedName());
             MetricsRegionWrapper mWrap = region == null ? null: region.getMetrics().getRegionWrapper();
         </%java>
         
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/client/ClientSideRegionScanner.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/client/ClientSideRegionScanner.java
index 587eff7..5a5c3a0 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/client/ClientSideRegionScanner.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/client/ClientSideRegionScanner.java
@@ -32,6 +32,7 @@ import org.apache.hadoop.hbase.HTableDescriptor;
 import org.apache.hadoop.hbase.KeyValueUtil;
 import org.apache.hadoop.hbase.client.metrics.ScanMetrics;
 import org.apache.hadoop.hbase.regionserver.HRegion;
+import org.apache.hadoop.hbase.regionserver.Region.Operation;
 import org.apache.hadoop.hbase.regionserver.RegionScanner;
 import org.mortbay.log.Log;
 
@@ -67,7 +68,7 @@ public class ClientSideRegionScanner extends AbstractClientScanner {
     } else {
       this.scanMetrics = scanMetrics;
     }
-    region.startRegionOperation();
+    region.startRegionOperation(Operation.ANY);
   }
 
   @Override
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/coprocessor/AggregateImplementation.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/coprocessor/AggregateImplementation.java
index edaca1c..73533b0 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/coprocessor/AggregateImplementation.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/coprocessor/AggregateImplementation.java
@@ -113,7 +113,7 @@ extends AggregateService implements CoprocessorService, Coprocessor {
       }
     }
     log.info("Maximum from this region is "
-        + env.getRegion().getRegionNameAsString() + ": " + max);
+        + env.getRegionInfo().getRegionNameAsString() + ": " + max);
     done.run(response);
   }
 
@@ -165,7 +165,7 @@ extends AggregateService implements CoprocessorService, Coprocessor {
       }
     }
     log.info("Minimum from this region is "
-        + env.getRegion().getRegionNameAsString() + ": " + min);
+        + env.getRegionInfo().getRegionNameAsString() + ": " + min);
     done.run(response);
   }
 
@@ -219,7 +219,7 @@ extends AggregateService implements CoprocessorService, Coprocessor {
       }
     }
     log.debug("Sum from this region is "
-        + env.getRegion().getRegionNameAsString() + ": " + sum);
+        + env.getRegionInfo().getRegionNameAsString() + ": " + sum);
     done.run(response);
   }
 
@@ -270,7 +270,7 @@ extends AggregateService implements CoprocessorService, Coprocessor {
       }
     }
     log.info("Row counter from this region is "
-        + env.getRegion().getRegionNameAsString() + ": " + counter);
+        + env.getRegionInfo().getRegionNameAsString() + ": " + counter);
     done.run(response);
   }
 
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/coprocessor/BaseRegionObserver.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/coprocessor/BaseRegionObserver.java
index 58e2e63..321b82b 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/coprocessor/BaseRegionObserver.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/coprocessor/BaseRegionObserver.java
@@ -45,8 +45,8 @@ import org.apache.hadoop.hbase.io.FSDataInputStreamWrapper;
 import org.apache.hadoop.hbase.io.Reference;
 import org.apache.hadoop.hbase.io.hfile.CacheConfig;
 import org.apache.hadoop.hbase.regionserver.DeleteTracker;
-import org.apache.hadoop.hbase.regionserver.HRegion;
-import org.apache.hadoop.hbase.regionserver.HRegion.Operation;
+import org.apache.hadoop.hbase.regionserver.Region;
+import org.apache.hadoop.hbase.regionserver.Region.Operation;
 import org.apache.hadoop.hbase.regionserver.InternalScanner;
 import org.apache.hadoop.hbase.regionserver.KeyValueScanner;
 import org.apache.hadoop.hbase.regionserver.MiniBatchOperationInProgress;
@@ -154,7 +154,7 @@ public abstract class BaseRegionObserver implements RegionObserver {
   }
 
   @Override
-  public void postSplit(ObserverContext<RegionCoprocessorEnvironment> e, HRegion l, HRegion r)
+  public void postSplit(ObserverContext<RegionCoprocessorEnvironment> e, Region l, Region r)
       throws IOException {
   }
 
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/coprocessor/BaseRegionServerObserver.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/coprocessor/BaseRegionServerObserver.java
index 1f34f88..9fc130f 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/coprocessor/BaseRegionServerObserver.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/coprocessor/BaseRegionServerObserver.java
@@ -25,7 +25,7 @@ import org.apache.hadoop.hbase.CoprocessorEnvironment;
 import org.apache.hadoop.hbase.HBaseInterfaceAudience;
 import org.apache.hadoop.hbase.client.Mutation;
 import org.apache.hadoop.hbase.protobuf.generated.AdminProtos.WALEntry;
-import org.apache.hadoop.hbase.regionserver.HRegion;
+import org.apache.hadoop.hbase.regionserver.Region;
 import org.apache.hadoop.hbase.replication.ReplicationEndpoint;
 
 /**
@@ -48,28 +48,28 @@ public class BaseRegionServerObserver implements RegionServerObserver {
   public void stop(CoprocessorEnvironment env) throws IOException { }
 
   @Override
-  public void preMerge(ObserverContext<RegionServerCoprocessorEnvironment> ctx, HRegion regionA,
-      HRegion regionB) throws IOException { }
+  public void preMerge(ObserverContext<RegionServerCoprocessorEnvironment> ctx, Region regionA,
+      Region regionB) throws IOException { }
 
   @Override
-  public void postMerge(ObserverContext<RegionServerCoprocessorEnvironment> c, HRegion regionA,
-      HRegion regionB, HRegion mergedRegion) throws IOException { }
+  public void postMerge(ObserverContext<RegionServerCoprocessorEnvironment> c, Region regionA,
+      Region regionB, Region mergedRegion) throws IOException { }
 
   @Override
   public void preMergeCommit(ObserverContext<RegionServerCoprocessorEnvironment> ctx,
-      HRegion regionA, HRegion regionB, List<Mutation> metaEntries) throws IOException { }
+      Region regionA, Region regionB, List<Mutation> metaEntries) throws IOException { }
 
   @Override
   public void postMergeCommit(ObserverContext<RegionServerCoprocessorEnvironment> ctx,
-      HRegion regionA, HRegion regionB, HRegion mergedRegion) throws IOException { }
+      Region regionA, Region regionB, Region mergedRegion) throws IOException { }
 
   @Override
   public void preRollBackMerge(ObserverContext<RegionServerCoprocessorEnvironment> ctx,
-      HRegion regionA, HRegion regionB) throws IOException { }
+      Region regionA, Region regionB) throws IOException { }
 
   @Override
   public void postRollBackMerge(ObserverContext<RegionServerCoprocessorEnvironment> ctx,
-      HRegion regionA, HRegion regionB) throws IOException { }
+      Region regionA, Region regionB) throws IOException { }
 
   @Override
   public void preRollWALWriterRequest(ObserverContext<RegionServerCoprocessorEnvironment> ctx)
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/coprocessor/BaseRowProcessorEndpoint.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/coprocessor/BaseRowProcessorEndpoint.java
index 7b841aa..b800ea4 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/coprocessor/BaseRowProcessorEndpoint.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/coprocessor/BaseRowProcessorEndpoint.java
@@ -31,7 +31,7 @@ import org.apache.hadoop.hbase.protobuf.ResponseConverter;
 import org.apache.hadoop.hbase.protobuf.generated.RowProcessorProtos.ProcessRequest;
 import org.apache.hadoop.hbase.protobuf.generated.RowProcessorProtos.ProcessResponse;
 import org.apache.hadoop.hbase.protobuf.generated.RowProcessorProtos.RowProcessorService;
-import org.apache.hadoop.hbase.regionserver.HRegion;
+import org.apache.hadoop.hbase.regionserver.Region;
 import org.apache.hadoop.hbase.regionserver.RowProcessor;
 
 import com.google.protobuf.ByteString;
@@ -68,7 +68,7 @@ extends RowProcessorService implements CoprocessorService, Coprocessor {
     ProcessResponse resultProto = null;
     try {
       RowProcessor<S,T> processor = constructRowProcessorFromRequest(request);
-      HRegion region = env.getRegion();
+      Region region = env.getRegion();
       long nonceGroup = request.hasNonceGroup() ? request.getNonceGroup() : HConstants.NO_NONCE;
       long nonce = request.hasNonce() ? request.getNonce() : HConstants.NO_NONCE;
       region.processRowsWithLocks(processor, nonceGroup, nonce);
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/coprocessor/RegionCoprocessorEnvironment.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/coprocessor/RegionCoprocessorEnvironment.java
index ccb16bf..a577748 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/coprocessor/RegionCoprocessorEnvironment.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/coprocessor/RegionCoprocessorEnvironment.java
@@ -26,14 +26,14 @@ import org.apache.hadoop.hbase.classification.InterfaceStability;
 import org.apache.hadoop.hbase.CoprocessorEnvironment;
 import org.apache.hadoop.hbase.HBaseInterfaceAudience;
 import org.apache.hadoop.hbase.HRegionInfo;
-import org.apache.hadoop.hbase.regionserver.HRegion;
+import org.apache.hadoop.hbase.regionserver.Region;
 import org.apache.hadoop.hbase.regionserver.RegionServerServices;
 
 @InterfaceAudience.LimitedPrivate(HBaseInterfaceAudience.COPROC)
 @InterfaceStability.Evolving
 public interface RegionCoprocessorEnvironment extends CoprocessorEnvironment {
   /** @return the region associated with this coprocessor */
-  HRegion getRegion();
+  Region getRegion();
 
   /** @return region information for the region this coprocessor is running on */
   HRegionInfo getRegionInfo();
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/coprocessor/RegionObserver.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/coprocessor/RegionObserver.java
index 7233aad..81f6e5b 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/coprocessor/RegionObserver.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/coprocessor/RegionObserver.java
@@ -43,12 +43,11 @@ import org.apache.hadoop.hbase.io.FSDataInputStreamWrapper;
 import org.apache.hadoop.hbase.io.Reference;
 import org.apache.hadoop.hbase.io.hfile.CacheConfig;
 import org.apache.hadoop.hbase.regionserver.DeleteTracker;
-import org.apache.hadoop.hbase.regionserver.HRegion;
-import org.apache.hadoop.hbase.regionserver.HRegion.Operation;
 import org.apache.hadoop.hbase.regionserver.InternalScanner;
 import org.apache.hadoop.hbase.regionserver.KeyValueScanner;
 import org.apache.hadoop.hbase.regionserver.MiniBatchOperationInProgress;
 import org.apache.hadoop.hbase.regionserver.OperationStatus;
+import org.apache.hadoop.hbase.regionserver.Region;
 import org.apache.hadoop.hbase.regionserver.RegionScanner;
 import org.apache.hadoop.hbase.regionserver.ScanType;
 import org.apache.hadoop.hbase.regionserver.Store;
@@ -359,8 +358,8 @@ public interface RegionObserver extends Coprocessor {
    * @throws IOException if an error occurred on the coprocessor
    * @deprecated Use postCompleteSplit() instead
    */
-  void postSplit(final ObserverContext<RegionCoprocessorEnvironment> c, final HRegion l,
-      final HRegion r) throws IOException;
+  void postSplit(final ObserverContext<RegionCoprocessorEnvironment> c, final Region l,
+      final Region r) throws IOException;
 
   /**
    * This will be called before PONR step as part of split transaction. Calling
@@ -626,7 +625,7 @@ public interface RegionObserver extends Coprocessor {
    * called after acquiring the locks on the mutating rows and after applying the proper timestamp
    * for each Mutation at the server. The batch may contain Put/Delete. By setting OperationStatus
    * of Mutations ({@link MiniBatchOperationInProgress#setOperationStatus(int, OperationStatus)}),
-   * {@link RegionObserver} can make HRegion to skip these Mutations.
+   * {@link RegionObserver} can make Region to skip these Mutations.
    * @param c the environment provided by the region server
    * @param miniBatchOp batch of Mutations getting applied to region.
    * @throws IOException if an error occurred on the coprocessor
@@ -646,22 +645,22 @@ public interface RegionObserver extends Coprocessor {
 
   /**
    * This will be called for region operations where read lock is acquired in
-   * {@link HRegion#startRegionOperation()}.
+   * {@link Region#startRegionOperation()}.
    * @param ctx
    * @param operation The operation is about to be taken on the region
    * @throws IOException
    */
   void postStartRegionOperation(final ObserverContext<RegionCoprocessorEnvironment> ctx,
-      Operation operation) throws IOException;
+      Region.Operation operation) throws IOException;
 
   /**
-   * Called after releasing read lock in {@link HRegion#closeRegionOperation(Operation)}.
+   * Called after releasing read lock in {@link Region#closeRegionOperation(Operation)}.
    * @param ctx
    * @param operation
    * @throws IOException
    */
   void postCloseRegionOperation(final ObserverContext<RegionCoprocessorEnvironment> ctx,
-      Operation operation) throws IOException;
+      Region.Operation operation) throws IOException;
 
   /**
    * Called after the completion of batch put/delete and will be called even if the batch operation
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/coprocessor/RegionServerObserver.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/coprocessor/RegionServerObserver.java
index d89e424..03eea6d 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/coprocessor/RegionServerObserver.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/coprocessor/RegionServerObserver.java
@@ -26,7 +26,7 @@ import org.apache.hadoop.hbase.Coprocessor;
 import org.apache.hadoop.hbase.MetaMutationAnnotation;
 import org.apache.hadoop.hbase.client.Mutation;
 import org.apache.hadoop.hbase.protobuf.generated.AdminProtos.WALEntry;
-import org.apache.hadoop.hbase.regionserver.HRegion;
+import org.apache.hadoop.hbase.regionserver.Region;
 import org.apache.hadoop.hbase.replication.ReplicationEndpoint;
 
 public interface RegionServerObserver extends Coprocessor {
@@ -50,7 +50,7 @@ public interface RegionServerObserver extends Coprocessor {
    * @throws IOException
    */
   void preMerge(final ObserverContext<RegionServerCoprocessorEnvironment> ctx,
-      final HRegion regionA, final HRegion regionB) throws IOException;
+      final Region regionA, final Region regionB) throws IOException;
 
   /**
    * called after the regions merge.
@@ -61,7 +61,7 @@ public interface RegionServerObserver extends Coprocessor {
    * @throws IOException
    */
   void postMerge(final ObserverContext<RegionServerCoprocessorEnvironment> c,
-      final HRegion regionA, final HRegion regionB, final HRegion mergedRegion) throws IOException;
+      final Region regionA, final Region regionB, final Region mergedRegion) throws IOException;
 
   /**
    * This will be called before PONR step as part of regions merge transaction. Calling
@@ -74,7 +74,7 @@ public interface RegionServerObserver extends Coprocessor {
    * @throws IOException
    */
   void preMergeCommit(final ObserverContext<RegionServerCoprocessorEnvironment> ctx,
-      final HRegion regionA, final HRegion regionB,
+      final Region regionA, final Region regionB,
       @MetaMutationAnnotation List<Mutation> metaEntries) throws IOException;
 
   /**
@@ -86,7 +86,7 @@ public interface RegionServerObserver extends Coprocessor {
    * @throws IOException
    */
   void postMergeCommit(final ObserverContext<RegionServerCoprocessorEnvironment> ctx,
-      final HRegion regionA, final HRegion regionB, final HRegion mergedRegion) throws IOException;
+      final Region regionA, final Region regionB, final Region mergedRegion) throws IOException;
 
   /**
    * This will be called before the roll back of the regions merge.
@@ -96,7 +96,7 @@ public interface RegionServerObserver extends Coprocessor {
    * @throws IOException
    */
   void preRollBackMerge(final ObserverContext<RegionServerCoprocessorEnvironment> ctx,
-      final HRegion regionA, final HRegion regionB) throws IOException;
+      final Region regionA, final Region regionB) throws IOException;
 
   /**
    * This will be called after the roll back of the regions merge.
@@ -106,7 +106,7 @@ public interface RegionServerObserver extends Coprocessor {
    * @throws IOException
    */
   void postRollBackMerge(final ObserverContext<RegionServerCoprocessorEnvironment> ctx,
-      final HRegion regionA, final HRegion regionB) throws IOException;
+      final Region regionA, final Region regionB) throws IOException;
 
   /**
    * This will be called before executing user request to roll a region server WAL.
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/master/HMasterCommandLine.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/master/HMasterCommandLine.java
index 479c27f..82e91bc 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/master/HMasterCommandLine.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/master/HMasterCommandLine.java
@@ -173,9 +173,7 @@ public class HMasterCommandLine extends ServerCommandLine {
         }
         conf.set(HConstants.ZOOKEEPER_CLIENT_PORT,
                  Integer.toString(clientPort));
-        int localZKClusterSessionTimeout =
-            conf.getInt(HConstants.ZK_SESSION_TIMEOUT + ".localHBaseCluster", 10*1000);
-        conf.setInt(HConstants.ZK_SESSION_TIMEOUT, localZKClusterSessionTimeout);
+        conf.setInt(HConstants.ZK_SESSION_TIMEOUT, 10 *1000);
         // Need to have the zk cluster shutdown when master is shutdown.
         // Run a subclass that does the zk cluster shutdown on its way out.
         LocalHBaseCluster cluster = new LocalHBaseCluster(conf, conf.getInt("hbase.masters", 1),
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/master/RegionStateStore.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/master/RegionStateStore.java
index 45c4fd2..2d0868e 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/master/RegionStateStore.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/master/RegionStateStore.java
@@ -32,11 +32,10 @@ import org.apache.hadoop.hbase.ServerName;
 import org.apache.hadoop.hbase.TableName;
 import org.apache.hadoop.hbase.catalog.CatalogTracker;
 import org.apache.hadoop.hbase.catalog.MetaEditor;
-import org.apache.hadoop.hbase.client.HTable;
 import org.apache.hadoop.hbase.client.Put;
 import org.apache.hadoop.hbase.client.Result;
 import org.apache.hadoop.hbase.master.RegionState.State;
-import org.apache.hadoop.hbase.regionserver.HRegion;
+import org.apache.hadoop.hbase.regionserver.Region;
 import org.apache.hadoop.hbase.regionserver.RegionServerServices;
 import org.apache.hadoop.hbase.util.Bytes;
 import org.apache.hadoop.hbase.util.ConfigUtil;
@@ -54,8 +53,7 @@ import com.google.common.base.Preconditions;
 public class RegionStateStore {
   private static final Log LOG = LogFactory.getLog(RegionStateStore.class);
 
-  private volatile HRegion metaRegion;
-  private volatile HRegion rootRegion;
+  private volatile Region metaRegion;
   private volatile boolean initialized;
 
   private final boolean noPersistence;
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/AnnotationReadingPriorityFunction.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/AnnotationReadingPriorityFunction.java
index 009a244..8a4bae0 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/AnnotationReadingPriorityFunction.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/AnnotationReadingPriorityFunction.java
@@ -132,7 +132,7 @@ class AnnotationReadingPriorityFunction implements PriorityFunction {
   }
 
   public boolean isMetaRegion(byte[] regionName) {
-    HRegion region;
+    Region region;
     try {
       region = hRegionServer.getRegion(regionName);
     } catch (NotServingRegionException ignored) {
@@ -169,10 +169,11 @@ class AnnotationReadingPriorityFunction implements PriorityFunction {
       if (hasRegion != null && (Boolean)hasRegion.invoke(param, (Object[])null)) {
         Method getRegion = methodMap.get("getRegion").get(rpcArgClass);
         regionSpecifier = (RegionSpecifier)getRegion.invoke(param, (Object[])null);
-        HRegion region = hRegionServer.getRegion(regionSpecifier);
+        Region region = hRegionServer.getRegion(regionSpecifier);
         if (region.getRegionInfo().isMetaTable()) {
           if (LOG.isTraceEnabled()) {
-            LOG.trace("High priority because region=" + region.getRegionNameAsString());
+            LOG.trace("High priority because region=" +
+              region.getRegionInfo().getRegionNameAsString());
           }
           return HConstants.HIGH_QOS;
         }
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/BaseRowProcessor.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/BaseRowProcessor.java
index 65375b8..1d267ef 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/BaseRowProcessor.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/BaseRowProcessor.java
@@ -39,19 +39,19 @@ public abstract class BaseRowProcessor<S extends Message,T extends Message>
 implements RowProcessor<S,T> {
 
   @Override
-  public void preProcess(HRegion region, WALEdit walEdit) throws IOException {
+  public void preProcess(Region region, WALEdit walEdit) throws IOException {
   }
 
   @Override
-  public void preBatchMutate(HRegion region, WALEdit walEdit) throws IOException {
+  public void preBatchMutate(Region region, WALEdit walEdit) throws IOException {
   }
 
   @Override
-  public void postBatchMutate(HRegion region) throws IOException {
+  public void postBatchMutate(Region region) throws IOException {
   }
 
   @Override
-  public void postProcess(HRegion region, WALEdit walEdit, boolean success) throws IOException {
+  public void postProcess(Region region, WALEdit walEdit, boolean success) throws IOException {
   }
 
   @Override
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/CompactSplitThread.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/CompactSplitThread.java
index f9b12f7..e1e5a4d 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/CompactSplitThread.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/CompactSplitThread.java
@@ -198,8 +198,8 @@ public class CompactSplitThread implements CompactionRequestor {
     return queueLists.toString();
   }
 
-  public synchronized void requestRegionsMerge(final HRegion a,
-      final HRegion b, final boolean forcible) {
+  public synchronized void requestRegionsMerge(final Region a,
+      final Region b, final boolean forcible) {
     try {
       mergePool.execute(new RegionMergeRequest(a, b, this.server, forcible));
       if (LOG.isDebugEnabled()) {
@@ -212,10 +212,10 @@ public class CompactSplitThread implements CompactionRequestor {
     }
   }
 
-  public synchronized boolean requestSplit(final HRegion r) {
+  public synchronized boolean requestSplit(final Region r) {
     // don't split regions that are blocking
-    if (shouldSplitRegion() && r.getCompactPriority() >= Store.PRIORITY_USER) {
-      byte[] midKey = r.checkSplit();
+    if (shouldSplitRegion() && ((HRegion)r).getCompactPriority() >= Store.PRIORITY_USER) {
+      byte[] midKey = ((HRegion)r).checkSplit();
       if (midKey != null) {
         requestSplit(r, midKey);
         return true;
@@ -224,12 +224,12 @@ public class CompactSplitThread implements CompactionRequestor {
     return false;
   }
 
-  public synchronized void requestSplit(final HRegion r, byte[] midKey) {
+  public synchronized void requestSplit(final Region r, byte[] midKey) {
     if (midKey == null) {
-      LOG.debug("Region " + r.getRegionNameAsString() +
+      LOG.debug("Region " + r.getRegionInfo().getRegionNameAsString() +
         " not splittable because midkey=null");
-      if (r.shouldForceSplit()) {
-        r.clearSplit();
+      if (((HRegion)r).shouldForceSplit()) {
+        ((HRegion)r).clearSplit();
       }
       return;
     }
@@ -244,36 +244,36 @@ public class CompactSplitThread implements CompactionRequestor {
   }
 
   @Override
-  public synchronized List<CompactionRequest> requestCompaction(final HRegion r, final String why)
+  public synchronized List<CompactionRequest> requestCompaction(final Region r, final String why)
       throws IOException {
     return requestCompaction(r, why, null);
   }
 
   @Override
-  public synchronized List<CompactionRequest> requestCompaction(final HRegion r, final String why,
+  public synchronized List<CompactionRequest> requestCompaction(final Region r, final String why,
       List<Pair<CompactionRequest, Store>> requests) throws IOException {
     return requestCompaction(r, why, Store.NO_PRIORITY, requests);
   }
 
   @Override
-  public synchronized CompactionRequest requestCompaction(final HRegion r, final Store s,
+  public synchronized CompactionRequest requestCompaction(final Region r, final Store s,
       final String why, CompactionRequest request) throws IOException {
     return requestCompaction(r, s, why, Store.NO_PRIORITY, request);
   }
 
   @Override
-  public synchronized List<CompactionRequest> requestCompaction(final HRegion r, final String why,
+  public synchronized List<CompactionRequest> requestCompaction(final Region r, final String why,
       int p, List<Pair<CompactionRequest, Store>> requests) throws IOException {
     return requestCompactionInternal(r, why, p, requests, true);
   }
 
-  private List<CompactionRequest> requestCompactionInternal(final HRegion r, final String why,
+  private List<CompactionRequest> requestCompactionInternal(final Region r, final String why,
       int p, List<Pair<CompactionRequest, Store>> requests, boolean selectNow) throws IOException {
     // not a special compaction request, so make our own list
     List<CompactionRequest> ret = null;
     if (requests == null) {
       ret = selectNow ? new ArrayList<CompactionRequest>(r.getStores().size()) : null;
-      for (Store s : r.getStores().values()) {
+      for (Store s : r.getStores()) {
         CompactionRequest cr = requestCompactionInternal(r, s, why, p, null, selectNow);
         if (selectNow) ret.add(cr);
       }
@@ -287,30 +287,30 @@ public class CompactSplitThread implements CompactionRequestor {
     return ret;
   }
 
-  public CompactionRequest requestCompaction(final HRegion r, final Store s,
+  public CompactionRequest requestCompaction(final Region r, final Store s,
       final String why, int priority, CompactionRequest request) throws IOException {
     return requestCompactionInternal(r, s, why, priority, request, true);
   }
 
   public synchronized void requestSystemCompaction(
-      final HRegion r, final String why) throws IOException {
+      final Region r, final String why) throws IOException {
     requestCompactionInternal(r, why, Store.NO_PRIORITY, null, false);
   }
 
   public void requestSystemCompaction(
-      final HRegion r, final Store s, final String why) throws IOException {
+      final Region r, final Store s, final String why) throws IOException {
     requestCompactionInternal(r, s, why, Store.NO_PRIORITY, null, false);
   }
 
   /**
-   * @param r HRegion store belongs to
+   * @param r Region store belongs to
    * @param s Store to request compaction on
    * @param why Why compaction requested -- used in debug messages
    * @param priority override the default priority (NO_PRIORITY == decide)
    * @param request custom compaction request. Can be <tt>null</tt> in which case a simple
    *          compaction will be used.
    */
-  private synchronized CompactionRequest requestCompactionInternal(final HRegion r, final Store s,
+  private synchronized CompactionRequest requestCompactionInternal(final Region r, final Store s,
       final String why, int priority, CompactionRequest request, boolean selectNow)
           throws IOException {
     if (this.server.isStopped()
@@ -338,12 +338,12 @@ public class CompactSplitThread implements CompactionRequestor {
     return selectNow ? compaction.getRequest() : null;
   }
 
-  private CompactionContext selectCompaction(final HRegion r, final Store s,
+  private CompactionContext selectCompaction(final Region r, final Store s,
       int priority, CompactionRequest request) throws IOException {
     CompactionContext compaction = s.requestCompaction(priority, request);
     if (compaction == null) {
       if(LOG.isDebugEnabled()) {
-        LOG.debug("Not compacting " + r.getRegionNameAsString() +
+        LOG.debug("Not compacting " + r.getRegionInfo().getRegionNameAsString() +
             " because compaction request was cancelled");
       }
       return null;
@@ -432,11 +432,11 @@ public class CompactSplitThread implements CompactionRequestor {
     private int queuedPriority;
     private ThreadPoolExecutor parent;
 
-    public CompactionRunner(Store store, HRegion region,
+    public CompactionRunner(Store store, Region region,
         CompactionContext compaction, ThreadPoolExecutor parent) {
       super();
       this.store = store;
-      this.region = region;
+      this.region = (HRegion)region;
       this.compaction = compaction;
       this.queuedPriority = (this.compaction == null)
           ? store.getCompactPriority() : compaction.getRequest().getPriority();
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/CompactionRequestor.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/CompactionRequestor.java
index 93a73e9..a6f1940 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/CompactionRequestor.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/CompactionRequestor.java
@@ -21,24 +21,27 @@ package org.apache.hadoop.hbase.regionserver;
 import java.io.IOException;
 import java.util.List;
 
+import org.apache.hadoop.hbase.HBaseInterfaceAudience;
 import org.apache.hadoop.hbase.classification.InterfaceAudience;
+import org.apache.hadoop.hbase.classification.InterfaceStability;
 import org.apache.hadoop.hbase.regionserver.compactions.CompactionRequest;
 import org.apache.hadoop.hbase.util.Pair;
 
-@InterfaceAudience.Private
+@InterfaceAudience.LimitedPrivate(HBaseInterfaceAudience.COPROC)
+@InterfaceStability.Evolving
 public interface CompactionRequestor {
   /**
-   * @param r Region to compact
+   * @param r region to compact
    * @param why Why compaction was requested -- used in debug messages
    * @return The created {@link CompactionRequest CompactionRequests} or an empty list if no
    *         compactions were started
    * @throws IOException
    */
-  List<CompactionRequest> requestCompaction(final HRegion r, final String why)
+  List<CompactionRequest> requestCompaction(final Region r, final String why)
       throws IOException;
 
   /**
-   * @param r Region to compact
+   * @param r region to compact
    * @param why Why compaction was requested -- used in debug messages
    * @param requests custom compaction requests. Each compaction must specify the store on which it
    *          is acting. Can be <tt>null</tt> in which case a compaction will be attempted on all
@@ -48,25 +51,25 @@ public interface CompactionRequestor {
    * @throws IOException
    */
   List<CompactionRequest> requestCompaction(
-    final HRegion r, final String why, List<Pair<CompactionRequest, Store>> requests
+    final Region r, final String why, List<Pair<CompactionRequest, Store>> requests
   )
       throws IOException;
 
   /**
-   * @param r Region to compact
+   * @param r region to compact
    * @param s Store within region to compact
    * @param why Why compaction was requested -- used in debug messages
-   * @param request custom compaction request for the {@link HRegion} and {@link Store}. Custom
+   * @param request custom compaction request for the {@link Region} and {@link Store}. Custom
    *          request must be <tt>null</tt> or be constructed with matching region and store.
    * @return The created {@link CompactionRequest} or <tt>null</tt> if no compaction was started.
    * @throws IOException
    */
   CompactionRequest requestCompaction(
-    final HRegion r, final Store s, final String why, CompactionRequest request
+    final Region r, final Store s, final String why, CompactionRequest request
   ) throws IOException;
 
   /**
-   * @param r Region to compact
+   * @param r region to compact
    * @param why Why compaction was requested -- used in debug messages
    * @param pri Priority of this compaction. minHeap. <=0 is critical
    * @param requests custom compaction requests. Each compaction must specify the store on which it
@@ -77,7 +80,7 @@ public interface CompactionRequestor {
    * @throws IOException
    */
   List<CompactionRequest> requestCompaction(
-    final HRegion r, final String why, int pri, List<Pair<CompactionRequest, Store>> requests
+    final Region r, final String why, int pri, List<Pair<CompactionRequest, Store>> requests
   ) throws IOException;
 
   /**
@@ -85,12 +88,12 @@ public interface CompactionRequestor {
    * @param s Store within region to compact
    * @param why Why compaction was requested -- used in debug messages
    * @param pri Priority of this compaction. minHeap. <=0 is critical
-   * @param request custom compaction request to run. {@link Store} and {@link HRegion} for the
+   * @param request custom compaction request to run. {@link Store} and {@link Region} for the
    *          request must match the region and store specified here.
    * @return The created {@link CompactionRequest} or <tt>null</tt> if no compaction was started
    * @throws IOException
    */
   CompactionRequest requestCompaction(
-    final HRegion r, final Store s, final String why, int pri, CompactionRequest request
+    final Region r, final Store s, final String why, int pri, CompactionRequest request
   ) throws IOException;
 }
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/ConstantSizeRegionSplitPolicy.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/ConstantSizeRegionSplitPolicy.java
index fba5b2a..2459ae6 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/ConstantSizeRegionSplitPolicy.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/ConstantSizeRegionSplitPolicy.java
@@ -55,7 +55,7 @@ public class ConstantSizeRegionSplitPolicy extends RegionSplitPolicy {
     boolean force = region.shouldForceSplit();
     boolean foundABigStore = false;
 
-    for (Store store : region.getStores().values()) {
+    for (Store store : region.getStores()) {
       // If any of the stores are unable to split (eg they contain reference files)
       // then don't split
       if ((!store.canSplit())) {
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/FavoredNodesForRegion.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/FavoredNodesForRegion.java
index d978a2d..f516ecd 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/FavoredNodesForRegion.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/FavoredNodesForRegion.java
@@ -21,15 +21,18 @@ package org.apache.hadoop.hbase.regionserver;
 import java.net.InetSocketAddress;
 import java.util.List; 
 
+import org.apache.hadoop.hbase.HBaseInterfaceAudience;
 import org.apache.hadoop.hbase.classification.InterfaceAudience;
+import org.apache.hadoop.hbase.classification.InterfaceStability;
 import org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.ServerName;
 
 /**
  * Abstraction that allows different modules in RegionServer to update/get
  * the favored nodes information for regions. 
  */
-@InterfaceAudience.Private
-interface FavoredNodesForRegion {
+@InterfaceAudience.LimitedPrivate(HBaseInterfaceAudience.COPROC)
+@InterfaceStability.Evolving
+public interface FavoredNodesForRegion {
   /**
    * Used to update the favored nodes mapping when required.
    * @param encodedRegionName
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/FlushRequester.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/FlushRequester.java
index d43a087..7191b68 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/FlushRequester.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/FlushRequester.java
@@ -19,24 +19,27 @@
 
 package org.apache.hadoop.hbase.regionserver;
 
+import org.apache.hadoop.hbase.HBaseInterfaceAudience;
 import org.apache.hadoop.hbase.classification.InterfaceAudience;
+import org.apache.hadoop.hbase.classification.InterfaceStability;
 
 /**
  * Request a flush.
  */
-@InterfaceAudience.Private
+@InterfaceAudience.LimitedPrivate(HBaseInterfaceAudience.COPROC)
+@InterfaceStability.Evolving
 public interface FlushRequester {
   /**
    * Tell the listener the cache needs to be flushed.
    *
    * @param region the HRegion requesting the cache flush
    */
-  void requestFlush(HRegion region);
+  void requestFlush(Region region);
   /**
    * Tell the listener the cache needs to be flushed after a delay
    *
    * @param region the HRegion requesting the cache flush
    * @param delay after how much time should the flush happen
    */
-  void requestDelayedFlush(HRegion region, long delay);
+  void requestDelayedFlush(Region region, long delay);
 }
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java
index 28ca92d..cda7e70 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java
@@ -203,7 +203,7 @@ import com.google.protobuf.Service;
  * defines the keyspace for this HRegion.
  */
 @InterfaceAudience.Private
-public class HRegion implements HeapSize { // , Writable{
+public class HRegion implements HeapSize, Region {
   public static final Log LOG = LogFactory.getLog(HRegion.class);
 
   public static final String LOAD_CFS_ON_DEMAND_CONFIG_KEY =
@@ -232,17 +232,6 @@ public class HRegion implements HeapSize { // , Writable{
    */
   private final AtomicLong sequenceId = new AtomicLong(-1L);
 
-  /**
-   * Operation enum is used in {@link HRegion#startRegionOperation} to provide operation context for
-   * startRegionOperation to possibly invoke different checks before any region operations. Not all
-   * operations have to be defined here. It's only needed when a special check is need in
-   * startRegionOperation
-   */
-  public enum Operation {
-    ANY, GET, PUT, DELETE, SCAN, APPEND, INCREMENT, SPLIT_REGION, MERGE_REGION, BATCH_MUTATE,
-    REPLAY_BATCH_MUTATE, COMPACT_REGION
-  }
-
   //////////////////////////////////////////////////////////////////////////////
   // Members
   //////////////////////////////////////////////////////////////////////////////
@@ -278,13 +267,6 @@ public class HRegion implements HeapSize { // , Writable{
   // Number of requests blocked by memstore size.
   private final Counter blockedRequestsCount = new Counter();
 
-  /**
-   * @return the number of blocked requests count.
-   */
-  public long getBlockedRequestsCount() {
-    return this.blockedRequestsCount.get();
-  }
-
   // Compaction counters
   final AtomicLong compactionsFinished = new AtomicLong(0L);
   final AtomicLong compactionNumFilesCompacted = new AtomicLong(0L);
@@ -374,6 +356,7 @@ public class HRegion implements HeapSize { // , Writable{
     }
     return minimumReadPoint;
   }
+
   /*
    * Data structure of write state flags used coordinating flushes,
    * compactions and closes.
@@ -647,7 +630,7 @@ public class HRegion implements HeapSize { // , Writable{
       this.metricsRegionWrapper = new MetricsRegionWrapperImpl(this);
       this.metricsRegion = new MetricsRegion(this.metricsRegionWrapper);
 
-      Map<String, HRegion> recoveringRegions = rsServices.getRecoveringRegions();
+      Map<String, Region> recoveringRegions = rsServices.getRecoveringRegions();
       String encodedName = getRegionInfo().getEncodedName();
       if (recoveringRegions != null && recoveringRegions.containsKey(encodedName)) {
         this.isRecovering = true;
@@ -717,8 +700,8 @@ public class HRegion implements HeapSize { // , Writable{
       // nextSeqid will be -1 if the initialization fails.
       // At least it will be 0 otherwise.
       if (nextSeqId == -1) {
-        status
-            .abort("Exception during region " + this.getRegionNameAsString() + " initialization.");
+        status.abort("Exception during region " + getRegionInfo().getRegionNameAsString() +
+          " initialization.");
       }
     }
   }
@@ -862,11 +845,7 @@ public class HRegion implements HeapSize { // , Writable{
     return false;
   }
 
-  /**
-   * This function will return the HDFS blocks distribution based on the data
-   * captured when HFile is created
-   * @return The HDFS blocks distribution for the region.
-   */
+  @Override
   public HDFSBlocksDistribution getHDFSBlocksDistribution() {
     HDFSBlocksDistribution hdfsBlocksDistribution =
       new HDFSBlocksDistribution();
@@ -923,24 +902,25 @@ public class HRegion implements HeapSize { // , Writable{
     return hdfsBlocksDistribution;
   }
 
-  public AtomicLong getMemstoreSize() {
-    return memstoreSize;
-  }
-
   /**
    * Increase the size of mem store in this region and the size of global mem
    * store
    * @param memStoreSize
    * @return the size of memstore in this region
    */
-  public long addAndGetGlobalMemstoreSize(long memStoreSize) {
+  long addAndGetGlobalMemstoreSize(long memStoreSize) {
     if (this.rsAccounting != null) {
       rsAccounting.addAndGetGlobalMemstoreSize(memStoreSize);
     }
     return this.memstoreSize.addAndGet(memStoreSize);
   }
 
-  /** @return a HRegionInfo object for this region */
+  @Override
+  public HTableDescriptor getTableDesc() {
+    return this.htableDescriptor;
+  }
+
+  @Override
   public HRegionInfo getRegionInfo() {
     return this.fs.getRegionInfo();
   }
@@ -960,28 +940,67 @@ public class HRegion implements HeapSize { // , Writable{
     return this.splitPolicy;
   }
 
-  /** @return readRequestsCount for this region */
-  long getReadRequestsCount() {
+  @Override
+  public long getReadRequestsCount() {
     return this.readRequestsCount.get();
   }
 
-  /** @return writeRequestsCount for this region */
-  long getWriteRequestsCount() {
+  @Override
+  public void updateReadRequestsCount(long i) {
+    this.readRequestsCount.add(i);
+  }
+
+  @Override
+  public long getWriteRequestsCount() {
     return this.writeRequestsCount.get();
   }
 
+  @Override
+  public void updateWriteRequestsCount(long i) {
+    this.writeRequestsCount.add(i);
+  }
+
+  @Override
+  public long getMemstoreSize() {
+    return this.memstoreSize.get();
+  }
+
+  @Override
+  public long getNumMutationsWithoutWAL() {
+    return this.numMutationsWithoutWAL.get();
+  }
+
+  @Override
+  public long getDataInMemoryWithoutWAL() {
+    return this.dataInMemoryWithoutWAL.get();
+  }
+
+  @Override
+  public long getBlockedRequestsCount() {
+    return this.blockedRequestsCount.get();
+  }
+
+  @Override
+  public long getCheckAndMutateChecksPassed() {
+    return this.checkAndMutateChecksPassed.get();
+  }
+
+  @Override
+  public long getCheckAndMutateChecksFailed() {
+    return this.checkAndMutateChecksFailed.get();
+  }
+
+  @Override
   public MetricsRegion getMetrics() {
     return metricsRegion;
   }
 
-  /** @return true if region is closed */
+  @Override
   public boolean isClosed() {
     return this.closed.get();
   }
 
-  /**
-   * @return True if closing process has started.
-   */
+  @Override
   public boolean isClosing() {
     return this.closing.get();
   }
@@ -999,34 +1018,30 @@ public class HRegion implements HeapSize { // , Writable{
     }
   }
 
-  /**
-   * @return True if current region is in recovering
-   */
+  @Override
   public boolean isRecovering() {
     return this.isRecovering;
   }
 
-  /** @return true if region is available (not closed and not closing) */
+  @Override
   public boolean isAvailable() {
     return !isClosed() && !isClosing();
   }
 
-  /** @return true if region is splittable */
+  /** @return true if the region is splittable */
   public boolean isSplittable() {
     return isAvailable() && !hasReferences();
   }
 
-  /**
-   * @return true if region is mergeable
-   */
+  /** @return true if the region is mergeable */
   public boolean isMergeable() {
     if (!isAvailable()) {
-      LOG.debug("Region " + this.getRegionNameAsString()
+      LOG.debug("Region " + getRegionInfo().getRegionNameAsString()
           + " is not mergeable because it is closing or closed");
       return false;
     }
     if (hasReferences()) {
-      LOG.debug("Region " + this.getRegionNameAsString()
+      LOG.debug("Region " + getRegionInfo().getRegionNameAsString()
           + " is not mergeable because it has references");
       return false;
     }
@@ -1044,9 +1059,7 @@ public class HRegion implements HeapSize { // , Writable{
      return mvcc;
    }
 
-   /*
-    * Returns readpoint considering given IsolationLevel
-    */
+   @Override
    public long getReadpoint(IsolationLevel isolationLevel) {
      if (isolationLevel == IsolationLevel.READ_UNCOMMITTED) {
        // This scan can read even uncommitted transactions
@@ -1055,6 +1068,7 @@ public class HRegion implements HeapSize { // , Writable{
      return mvcc.memstoreReadPoint();
    }
 
+   @Override
    public boolean isLoadingCfsOnDemandDefault() {
      return this.isLoadingCfsOnDemandDefault;
    }
@@ -1150,7 +1164,7 @@ public class HRegion implements HeapSize { // , Writable{
     // the close flag?
     if (!abort && worthPreFlushing()) {
       status.setStatus("Pre-flushing region before close");
-      LOG.info("Running close preflush of " + this.getRegionNameAsString());
+      LOG.info("Running close preflush of " + getRegionInfo().getRegionNameAsString());
       try {
         internalFlushcache(status);
       } catch (IOException ioe) {
@@ -1173,7 +1187,7 @@ public class HRegion implements HeapSize { // , Writable{
       // Don't flush the cache if we are aborting
       if (!abort) {
         int flushCount = 0;
-        while (this.getMemstoreSize().get() > 0) {
+        while (getMemstoreSize() > 0) {
           try {
             if (flushCount++ > 0) {
               int actualFlushes = flushCount - 1;
@@ -1181,7 +1195,8 @@ public class HRegion implements HeapSize { // , Writable{
                 // If we tried 5 times and are unable to clear memory, abort
                 // so we do not lose data
                 throw new DroppedSnapshotException("Failed clearing memory after " +
-                  actualFlushes + " attempts on region: " + Bytes.toStringBinary(getRegionName()));
+                  actualFlushes + " attempts on region: " +
+                  Bytes.toStringBinary(getRegionInfo().getRegionName()));
               }
               LOG.info("Running extra flush, " + actualFlushes +
                 " (carrying snapshot?) " + this);
@@ -1203,7 +1218,8 @@ public class HRegion implements HeapSize { // , Writable{
       if (!stores.isEmpty()) {
         // initialize the thread pool for closing stores in parallel.
         ThreadPoolExecutor storeCloserThreadPool =
-          getStoreOpenAndCloseThreadPool("StoreCloserThread-" + this.getRegionNameAsString());
+          getStoreOpenAndCloseThreadPool("StoreCloserThread-" +
+            getRegionInfo().getRegionNameAsString());
         CompletionService<Pair<byte[], Collection<StoreFile>>> completionService =
           new ExecutorCompletionService<Pair<byte[], Collection<StoreFile>>>(storeCloserThreadPool);
 
@@ -1239,7 +1255,7 @@ public class HRegion implements HeapSize { // , Writable{
         }
       }
       this.closed.set(true);
-      if (memstoreSize.get() != 0) LOG.error("Memstore size is " + memstoreSize.get());
+      if (getMemstoreSize() != 0) LOG.error("Memstore size is " + getMemstoreSize());
       if (coprocessorHost != null) {
         status.setStatus("Running coprocessor post-close hooks");
         this.coprocessorHost.postClose(abort);
@@ -1314,7 +1330,7 @@ public class HRegion implements HeapSize { // , Writable{
     * @return True if its worth doing a flush before we put up the close flag.
     */
   private boolean worthPreFlushing() {
-    return this.memstoreSize.get() >
+    return getMemstoreSize() >
       this.conf.getLong("hbase.hregion.preclose.flush.size", 1024 * 1024 * 5);
   }
 
@@ -1322,36 +1338,6 @@ public class HRegion implements HeapSize { // , Writable{
   // HRegion accessors
   //////////////////////////////////////////////////////////////////////////////
 
-  /** @return start key for region */
-  public byte [] getStartKey() {
-    return this.getRegionInfo().getStartKey();
-  }
-
-  /** @return end key for region */
-  public byte [] getEndKey() {
-    return this.getRegionInfo().getEndKey();
-  }
-
-  /** @return region id */
-  public long getRegionId() {
-    return this.getRegionInfo().getRegionId();
-  }
-
-  /** @return region name */
-  public byte [] getRegionName() {
-    return this.getRegionInfo().getRegionName();
-  }
-
-  /** @return region name as string for logging */
-  public String getRegionNameAsString() {
-    return this.getRegionInfo().getRegionNameAsString();
-  }
-
-  /** @return HTableDescriptor for this region */
-  public HTableDescriptor getTableDesc() {
-    return this.htableDescriptor;
-  }
-
   /** @return HLog in use for this region */
   public HLog getLog() {
     return this.log;
@@ -1378,7 +1364,7 @@ public class HRegion implements HeapSize { // , Writable{
     return this.fs;
   }
 
-  /** @return the last time the region was flushed */
+  @Override
   public long getLastFlushTime() {
     return this.lastFlushTime;
   }
@@ -1416,7 +1402,8 @@ public class HRegion implements HeapSize { // , Writable{
   protected void doRegionCompactionPrep() throws IOException {
   }
 
-  void triggerMajorCompaction() {
+  @Override
+  public void triggerMajorCompaction() {
     for (Store h : stores.values()) {
       h.triggerMajorCompaction();
     }
@@ -1444,7 +1431,7 @@ public class HRegion implements HeapSize { // , Writable{
    * @throws IOException e
    */
   public void compactStores() throws IOException {
-    for (Store s : getStores().values()) {
+    for (Store s : getStores()) {
       CompactionContext compaction = s.requestCompaction();
       if (compaction != null) {
         compact(compaction, s, NoLimitCompactionThroughputController.INSTANCE);
@@ -1454,17 +1441,22 @@ public class HRegion implements HeapSize { // , Writable{
 
   /**
    * Called by compaction thread and after region is opened to compact the
-   * HStores if necessary.
+   * Stores if necessary.
    *
    * <p>This operation could block for a long time, so don't call it from a
    * time-sensitive thread.
    *
-   * Note that no locking is necessary at this level because compaction only
+   * <p>Note that no locking is done because the assumption is compaction only
    * conflicts with a region split, and that cannot happen because the region
-   * server does them sequentially and not in parallel.
-   *
-   * @param compaction Compaction details, obtained by requestCompaction()
-   * @return whether the compaction completed
+   * server does them sequentially and not in parallel. Do not call this
+   * directly from somewhere else unless you are prepared to handling locking
+   * details yourself.
+   * 
+   * @param compaction
+   * @param store
+   * @param compactionThroughputController
+   * @return
+   * @throws IOException
    */
   public boolean compact(CompactionContext compaction, Store store,
       CompactionThroughputController throughputController) throws IOException {
@@ -1630,9 +1622,7 @@ public class HRegion implements HeapSize { // , Writable{
     }
   }
 
-  /**
-   * Should the memstore be flushed now
-   */
+  /** @return true if the region thinks it should flush */
   boolean shouldFlush() {
     // This is a rough measure.
     if (this.completeSequenceId > 0
@@ -1649,7 +1639,7 @@ public class HRegion implements HeapSize { // , Writable{
     }
     //since we didn't flush in the recent past, flush now if certain conditions
     //are met. Return true on first such memstore hit.
-    for (Store s : this.getStores().values()) {
+    for (Store s : this.getStores()) {
       if (s.timeOfOldestEdit() < now - flushCheckInterval) {
         // we have an old enough edit in the memstore, flush
         return true;
@@ -1717,7 +1707,7 @@ public class HRegion implements HeapSize { // , Writable{
     final long startTime = EnvironmentEdgeManager.currentTimeMillis();
     // Clear flush flag.
     // If nothing to flush, return and avoid logging start/stop flush.
-    if (this.memstoreSize.get() <= 0) {
+    if (getMemstoreSize() <= 0) {
       if(LOG.isDebugEnabled()) {
         LOG.debug("Empty memstore size for the current region "+this);
       }
@@ -1726,7 +1716,7 @@ public class HRegion implements HeapSize { // , Writable{
 
     LOG.info("Started memstore flush for " + this +
       ", current region memstore size " +
-      StringUtils.humanReadableInt(this.memstoreSize.get()) +
+      StringUtils.humanReadableInt(getMemstoreSize()) +
       ((wal != null)? "": "; wal is null, using passed sequenceid=" + myseqid));
 
     // Stop updates while we snapshot the memstore of all stores. We only have
@@ -1838,7 +1828,7 @@ public class HRegion implements HeapSize { // , Writable{
         wal.abortCacheFlush(this.getRegionInfo().getEncodedNameAsBytes());
       }
       DroppedSnapshotException dse = new DroppedSnapshotException("region: " +
-          Bytes.toStringBinary(getRegionName()));
+          Bytes.toStringBinary(getRegionInfo().getRegionName()));
       dse.initCause(t);
       status.abort("Flush failed: " + StringUtils.stringifyException(t));
       throw dse;
@@ -1862,7 +1852,7 @@ public class HRegion implements HeapSize { // , Writable{
     }
 
     long time = EnvironmentEdgeManager.currentTimeMillis() - startTime;
-    long memstoresize = this.memstoreSize.get();
+    long memstoresize = getMemstoreSize();
     String msg = "Finished memstore flush of ~" +
       StringUtils.humanReadableInt(totalFlushableSize) + "/" + totalFlushableSize +
       ", currentsize=" +
@@ -1881,30 +1871,8 @@ public class HRegion implements HeapSize { // , Writable{
   //////////////////////////////////////////////////////////////////////////////
   // get() methods for client use.
   //////////////////////////////////////////////////////////////////////////////
-  /**
-   * Return all the data for the row that matches <i>row</i> exactly,
-   * or the one that immediately preceeds it, at or immediately before
-   * <i>ts</i>.
-   *
-   * @param row row key
-   * @return map of values
-   * @throws IOException
-   */
-  Result getClosestRowBefore(final byte [] row)
-  throws IOException{
-    return getClosestRowBefore(row, HConstants.CATALOG_FAMILY);
-  }
 
-  /**
-   * Return all the data for the row that matches <i>row</i> exactly,
-   * or the one that immediately preceeds it, at or immediately before
-   * <i>ts</i>.
-   *
-   * @param row row key
-   * @param family column family to find on
-   * @return map of values
-   * @throws IOException read exceptions
-   */
+  @Override
   public Result getClosestRowBefore(final byte [] row, final byte [] family)
   throws IOException {
     if (coprocessorHost != null) {
@@ -1937,37 +1905,23 @@ public class HRegion implements HeapSize { // , Writable{
     }
   }
 
-  /**
-   * Return an iterator that scans over the HRegion, returning the indicated
-   * columns and rows specified by the {@link Scan}.
-   * <p>
-   * This Iterator must be closed by the caller.
-   *
-   * @param scan configured {@link Scan}
-   * @return RegionScanner
-   * @throws IOException read exceptions
-   */
+  @Override
   public RegionScanner getScanner(Scan scan) throws IOException {
    return getScanner(scan, null);
   }
 
-  void prepareScanner(Scan scan) throws IOException {
-    if(!scan.hasFamilies()) {
-      // Adding all families to scanner
-      for(byte[] family: this.htableDescriptor.getFamiliesKeys()){
-        scan.addFamily(family);
-      }
-    }
-  }
-
   protected RegionScanner getScanner(Scan scan,
       List<KeyValueScanner> additionalScanners) throws IOException {
     startRegionOperation(Operation.SCAN);
     try {
       // Verify families are all valid
-      prepareScanner(scan);
-      if(scan.hasFamilies()) {
-        for(byte [] family : scan.getFamilyMap().keySet()) {
+      if (!scan.hasFamilies()) {
+        // Adding all families to scanner
+        for (byte[] family: this.htableDescriptor.getFamiliesKeys()){
+          scan.addFamily(family);
+        }
+      } else {
+        for (byte [] family : scan.getFamilyMap().keySet()) {
           checkFamily(family);
         }
       }
@@ -1988,10 +1942,8 @@ public class HRegion implements HeapSize { // , Writable{
     return new RegionScannerImpl(scan, additionalScanners, this);
   }
 
-  /*
-   * @param delete The passed delete is modified by this method. WARNING!
-   */
-  void prepareDelete(Delete delete) throws IOException {
+  @Override
+  public void prepareDelete(Delete delete) throws IOException {
     // Check to see if this is a deleteRow insert
     if(delete.getFamilyCellMap().isEmpty()){
       for(byte [] family : this.htableDescriptor.getFamiliesKeys()){
@@ -2011,12 +1963,9 @@ public class HRegion implements HeapSize { // , Writable{
   //////////////////////////////////////////////////////////////////////////////
   // set() methods for client use.
   //////////////////////////////////////////////////////////////////////////////
-  /**
-   * @param delete delete object
-   * @throws IOException read exceptions
-   */
-  public void delete(Delete delete)
-  throws IOException {
+
+  @Override
+  public void delete(Delete delete) throws IOException {
     checkReadOnly();
     checkResources();
     startRegionOperation(Operation.DELETE);
@@ -2039,6 +1988,7 @@ public class HRegion implements HeapSize { // , Writable{
    * @param durability
    * @throws IOException
    */
+  @VisibleForTesting
   void delete(NavigableMap<byte[], List<Cell>> familyMap,
       Durability durability) throws IOException {
     Delete delete = new Delete(FOR_UNIT_TESTS_ONLY);
@@ -2047,15 +1997,8 @@ public class HRegion implements HeapSize { // , Writable{
     doBatchMutate(delete);
   }
 
-  /**
-   * Setup correct timestamps in the KVs in Delete object.
-   * Caller should have the row and region locks.
-   * @param mutation
-   * @param familyMap
-   * @param byteNow
-   * @throws IOException
-   */
-  void prepareDeleteTimestamps(Mutation mutation, Map<byte[], List<Cell>> familyMap,
+  @Override
+  public void prepareDeleteTimestamps(Mutation mutation, Map<byte[], List<Cell>> familyMap,
       byte[] byteNow) throws IOException {
     for (Map.Entry<byte[], List<Cell>> e : familyMap.entrySet()) {
 
@@ -2117,12 +2060,8 @@ public class HRegion implements HeapSize { // , Writable{
         getkv.getTimestampOffset(), Bytes.SIZEOF_LONG);
   }
 
-  /**
-   * @param put
-   * @throws IOException
-   */
-  public void put(Put put)
-  throws IOException {
+  @Override
+  public void put(Put put) throws IOException {
     checkReadOnly();
 
     // Do a rough check that we have resources to accept a write.  The check is
@@ -2235,14 +2174,7 @@ public class HRegion implements HeapSize { // , Writable{
     }
   }
 
-  /**
-   * Perform a batch of mutations.
-   * It supports only Put and Delete mutations and will ignore other types passed.
-   * @param mutations the list of mutations
-   * @return an array of OperationStatus which internally contains the
-   *         OperationStatusCode and the exceptionMessage if any.
-   * @throws IOException
-   */
+  @Override
   public OperationStatus[] batchMutate(
       Mutation[] mutations, long nonceGroup, long nonce) throws IOException {
     // As it stands, this is used for 3 things
@@ -2252,17 +2184,7 @@ public class HRegion implements HeapSize { // , Writable{
     return batchMutate(new MutationBatch(mutations, nonceGroup, nonce));
   }
 
-  public OperationStatus[] batchMutate(Mutation[] mutations) throws IOException {
-    return batchMutate(mutations, HConstants.NO_NONCE, HConstants.NO_NONCE);
-  }
-
-  /**
-   * Replay a batch of mutations.
-   * @param mutations mutations to replay.
-   * @return an array of OperationStatus which internally contains the
-   *         OperationStatusCode and the exceptionMessage if any.
-   * @throws IOException
-   */
+  @Override
   public OperationStatus[] batchReplay(HLogSplitter.MutationReplay[] mutations)
       throws IOException {
     return batchMutate(new ReplayBatch(mutations));
@@ -2276,7 +2198,7 @@ public class HRegion implements HeapSize { // , Writable{
    *         OperationStatusCode and the exceptionMessage if any.
    * @throws IOException
    */
-  OperationStatus[] batchMutate(BatchOperationInProgress<?> batchOp) throws IOException {
+  private OperationStatus[] batchMutate(BatchOperationInProgress<?> batchOp) throws IOException {
     boolean initialized = false;
     Operation op = batchOp.isInReplay() ? Operation.REPLAY_BATCH_MUTATE : Operation.BATCH_MUTATE;
     startRegionOperation(op);
@@ -2713,22 +2635,11 @@ public class HRegion implements HeapSize { // , Writable{
   //the getting of the lock happens before, so that you would just pass it into
   //the methods. So in the case of checkAndMutate you could just do lockRow,
   //get, put, unlockRow or something
-  /**
-   *
-   * @param row
-   * @param family
-   * @param qualifier
-   * @param compareOp
-   * @param comparator
-   * @param w
-   * @param writeToWAL
-   * @throws IOException
-   * @return true if the new put was executed, false otherwise
-   */
+
+  @Override
   public boolean checkAndMutate(byte [] row, byte [] family, byte [] qualifier,
       CompareOp compareOp, ByteArrayComparable comparator, Mutation w,
-      boolean writeToWAL)
-  throws IOException{
+      boolean writeToWAL) throws IOException {
     checkReadOnly();
     //TODO, add check for value length or maybe even better move this to the
     //client if this becomes a global setting
@@ -2742,7 +2653,7 @@ public class HRegion implements HeapSize { // , Writable{
           "getRow must match the passed row");
     }
 
-    startRegionOperation();
+    startRegionOperation(Operation.ANY);
     try {
       Get get = new Get(row);
       checkFamily(family);
@@ -2753,13 +2664,13 @@ public class HRegion implements HeapSize { // , Writable{
       // wait for all previous transactions to complete (with lock held)
       mvcc.completeMemstoreInsert(mvcc.beginMemstoreInsert());
       try {
-        if (this.getCoprocessorHost() != null) {
+        if (this.coprocessorHost != null) {
           Boolean processed = null;
           if (w instanceof Put) {
-            processed = this.getCoprocessorHost().preCheckAndPutAfterRowLock(row, family,
+            processed = this.coprocessorHost.preCheckAndPutAfterRowLock(row, family,
                 qualifier, compareOp, comparator, (Put) w);
           } else if (w instanceof Delete) {
-            processed = this.getCoprocessorHost().preCheckAndDeleteAfterRowLock(row, family,
+            processed = this.coprocessorHost.preCheckAndDeleteAfterRowLock(row, family,
                 qualifier, compareOp, comparator, (Delete) w);
           }
           if (processed != null) {
@@ -2830,6 +2741,7 @@ public class HRegion implements HeapSize { // , Writable{
    * @throws IOException
    * @return true if the new put was executed, false otherwise
    */
+  @Override
   public boolean checkAndRowMutate(byte [] row, byte [] family, byte [] qualifier,
       CompareOp compareOp, ByteArrayComparable comparator, RowMutations rm,
       boolean writeToWAL)
@@ -2839,7 +2751,7 @@ public class HRegion implements HeapSize { // , Writable{
     //client if this becomes a global setting
     checkResources();
 
-    startRegionOperation();
+    startRegionOperation(Operation.ANY);
     try {
       Get get = new Get(row);
       checkFamily(family);
@@ -2939,11 +2851,8 @@ public class HRegion implements HeapSize { // , Writable{
     manifest.addRegion(this);
   }
 
-  /**
-   * Replaces any KV timestamps set to {@link HConstants#LATEST_TIMESTAMP} with the
-   * provided current timestamp.
-   */
-  void updateKVTimestamps(final Iterable<List<Cell>> keyLists, final byte[] now) {
+  @Override
+  public void updateKVTimestamps(final Iterable<List<Cell>> keyLists, final byte[] now) {
     for (List<Cell> cells: keyLists) {
       if (cells == null) continue;
       assert cells instanceof RandomAccess;
@@ -3020,7 +2929,7 @@ public class HRegion implements HeapSize { // , Writable{
     // If catalog region, do not impose resource constraints or block updates.
     if (this.getRegionInfo().isMetaRegion()) return;
 
-    if (this.memstoreSize.get() > this.blockingMemStoreSize) {
+    if (getMemstoreSize() > this.blockingMemStoreSize) {
       blockedRequestsCount.increment();
       requestFlush();
       throw new RegionTooBusyException("Above memstore limit, " +
@@ -3028,7 +2937,7 @@ public class HRegion implements HeapSize { // , Writable{
           this.getRegionInfo().getRegionNameAsString()) +
           ", server=" + (this.getRegionServerServices() == null ? "unknown" :
           this.getRegionServerServices().getServerName()) +
-          ", memstoreSize=" + memstoreSize.get() +
+          ", memstoreSize=" + getMemstoreSize() +
           ", blockingMemStoreSize=" + blockingMemStoreSize);
     }
   }
@@ -3050,8 +2959,7 @@ public class HRegion implements HeapSize { // , Writable{
    * @praram now
    * @throws IOException
    */
-  private void put(final byte [] row, byte [] family, List<Cell> edits)
-  throws IOException {
+  private void put(final byte [] row, byte [] family, List<Cell> edits) throws IOException {
     NavigableMap<byte[], List<Cell>> familyMap;
     familyMap = new TreeMap<byte[], List<Cell>>(Bytes.BYTES_COMPARATOR);
 
@@ -3142,12 +3050,8 @@ public class HRegion implements HeapSize { // , Writable{
         " keyvalues from start:" + start + " to end:" + end);
   }
 
-  /**
-   * Check the collection of families for validity.
-   * @throws NoSuchColumnFamilyException if a family does not exist.
-   */
-  void checkFamilies(Collection<byte[]> families)
-  throws NoSuchColumnFamilyException {
+  @Override
+  public void checkFamilies(Collection<byte[]> families) throws NoSuchColumnFamilyException {
     for (byte[] family : families) {
       checkFamily(family);
     }
@@ -3177,8 +3081,9 @@ public class HRegion implements HeapSize { // , Writable{
     }
   }
 
-  void checkTimestamps(final Map<byte[], List<Cell>> familyMap,
-      long now) throws FailedSanityCheckException {
+  @Override
+  public void checkTimestamps(final Map<byte[], List<Cell>> familyMap, long now)
+      throws FailedSanityCheckException {
     if (timestampSlop == HConstants.LATEST_TIMESTAMP) {
       return;
     }
@@ -3345,7 +3250,7 @@ public class HRegion implements HeapSize { // , Writable{
     // The edits size added into rsAccounting during this replaying will not
     // be required any more. So just clear it.
     if (this.rsAccounting != null) {
-      this.rsAccounting.clearRegionReplayEditsSize(this.getRegionName());
+      this.rsAccounting.clearRegionReplayEditsSize(getRegionInfo().getRegionName());
     }
     if (seqid > minSeqIdForTheRegion) {
       // Then we added some edits to memory. Flush and cleanup split edit files.
@@ -3541,10 +3446,9 @@ public class HRegion implements HeapSize { // , Writable{
    * Call to complete a compaction. Its for the case where we find in the WAL a compaction
    * that was not finished.  We could find one recovering a WAL after a regionserver crash.
    * See HBASE-2331.
-   * @param compaction
+   * @param compaction compaction descriptor
    */
-  void completeCompactionMarker(CompactionDescriptor compaction)
-      throws IOException {
+  void completeCompactionMarker(CompactionDescriptor compaction) throws IOException {
     Store store = this.getStore(compaction.getFamilyName().toByteArray());
     if (store == null) {
       LOG.warn("Found Compaction WAL edit for deleted family:" +
@@ -3563,7 +3467,7 @@ public class HRegion implements HeapSize { // , Writable{
   protected boolean restoreEdit(final Store s, final KeyValue kv) {
     long kvSize = s.add(kv);
     if (this.rsAccounting != null) {
-      rsAccounting.addAndGetRegionReplayEditsSize(this.getRegionName(), kvSize);
+      rsAccounting.addAndGetRegionReplayEditsSize(getRegionInfo().getRegionName(), kvSize);
     }
     return isFlushSize(this.addAndGetGlobalMemstoreSize(kvSize));
   }
@@ -3587,28 +3491,19 @@ public class HRegion implements HeapSize { // , Writable{
     return new HStore(this, family, this.conf);
   }
 
-  /**
-   * Return HStore instance.
-   * Use with caution.  Exposed for use of fixup utilities.
-   * @param column Name of column family hosted by this region.
-   * @return Store that goes with the family on passed <code>column</code>.
-   * TODO: Make this lookup faster.
-   */
+  @Override
   public Store getStore(final byte[] column) {
     return this.stores.get(column);
   }
 
-  public Map<byte[], Store> getStores() {
-    return this.stores;
+  @Override
+  public List<Store> getStores() {
+    List<Store> result = Lists.newArrayList();
+    result.addAll(stores.values());
+    return result;
   }
 
-  /**
-   * Return list of storeFiles for the set of CFs.
-   * Uses closeLock to prevent the race condition where a region closes
-   * in between the for loop - closing the stores one by one, some stores
-   * will return 0 files.
-   * @return List of storeFiles.
-   */
+  @Override
   public List<String> getStoreFileList(final byte [][] columns)
     throws IllegalArgumentException {
     List<String> storeFileNames = new ArrayList<String>();
@@ -3626,6 +3521,7 @@ public class HRegion implements HeapSize { // , Writable{
     }
     return storeFileNames;
   }
+
   //////////////////////////////////////////////////////////////////////////////
   // Support code
   //////////////////////////////////////////////////////////////////////////////
@@ -3635,23 +3531,15 @@ public class HRegion implements HeapSize { // , Writable{
     if (!rowIsInRange(getRegionInfo(), row)) {
       throw new WrongRegionException("Requested row out of range for " +
           op + " on HRegion " + this + ", startKey='" +
-          Bytes.toStringBinary(getStartKey()) + "', getEndKey()='" +
-          Bytes.toStringBinary(getEndKey()) + "', row='" +
+          Bytes.toStringBinary(getRegionInfo().getStartKey()) + "', getEndKey()='" +
+          Bytes.toStringBinary(getRegionInfo().getEndKey()) + "', row='" +
           Bytes.toStringBinary(row) + "'");
     }
   }
 
-  /**
-   * Tries to acquire a lock on the given row.
-   * @param waitForLock if true, will block until the lock is available.
-   *        Otherwise, just tries to obtain the lock and returns
-   *        false if unavailable.
-   * @return the row lock if acquired,
-   *   null if waitForLock was false and the lock was not acquired
-   * @throws IOException if waitForLock was true and the lock could not be acquired after waiting
-   */
+  @Override
   public RowLock getRowLock(byte[] row, boolean waitForLock) throws IOException {
-    startRegionOperation();
+    startRegionOperation(Operation.ANY);
     try {
       return getRowLockInternal(row, waitForLock);
     } finally {
@@ -3717,13 +3605,11 @@ public class HRegion implements HeapSize { // , Writable{
    * @return the acquired row lock
    * @throws IOException if the lock could not be acquired after waiting
    */
-  public RowLock getRowLock(byte[] row) throws IOException {
+  RowLock getRowLock(byte[] row) throws IOException {
     return getRowLock(row, true);
   }
 
-  /**
-   * If the given list of row locks is not null, releases all locks.
-   */
+  @Override
   public void releaseRowLocks(List<RowLock> rowLocks) {
     if (rowLocks != null) {
       for (RowLock rowLock : rowLocks) {
@@ -3740,7 +3626,7 @@ public class HRegion implements HeapSize { // , Writable{
    * @param familyPaths List of Pair<byte[] column family, String hfilePath>
    */
   private static boolean hasMultipleColumnFamilies(
-      List<Pair<byte[], String>> familyPaths) {
+      Collection<Pair<byte[], String>> familyPaths) {
     boolean multipleFamilies = false;
     byte[] family = null;
     for (Pair<byte[], String> pair : familyPaths) {
@@ -3755,12 +3641,6 @@ public class HRegion implements HeapSize { // , Writable{
     return multipleFamilies;
   }
 
-
-  public boolean bulkLoadHFiles(List<Pair<byte[], String>> familyPaths,
-                                boolean assignSeqId) throws IOException {
-    return bulkLoadHFiles(familyPaths, assignSeqId, null);
-  }
-
   /**
    * Attempts to atomically load a group of hfiles.  This is critical for loading
    * rows with multiple column families atomically.
@@ -3772,7 +3652,7 @@ public class HRegion implements HeapSize { // , Writable{
    * @return true if successful, false if failed recoverably
    * @throws IOException if failed unrecoverably.
    */
-  public boolean bulkLoadHFiles(List<Pair<byte[], String>> familyPaths, boolean assignSeqId,
+  public boolean bulkLoadHFiles(Collection<Pair<byte[], String>> familyPaths, boolean assignSeqId,
       BulkLoadListener bulkLoadListener) throws IOException {
     Preconditions.checkNotNull(familyPaths);
     // we need writeLock for multi-family bulk load
@@ -3882,18 +3762,18 @@ public class HRegion implements HeapSize { // , Writable{
 
   @Override
   public boolean equals(Object o) {
-    return o instanceof HRegion && Bytes.equals(this.getRegionName(),
-                                                ((HRegion) o).getRegionName());
+    return o instanceof HRegion && Bytes.equals(getRegionInfo().getRegionName(),
+      ((HRegion) o).getRegionInfo().getRegionName());
   }
 
   @Override
   public int hashCode() {
-    return Bytes.hashCode(this.getRegionName());
+    return Bytes.hashCode(getRegionInfo().getRegionName());
   }
 
   @Override
   public String toString() {
-    return this.getRegionNameAsString();
+    return getRegionInfo().getRegionNameAsString();
   }
 
   /**
@@ -4142,9 +4022,8 @@ public class HRegion implements HeapSize { // , Writable{
           long afterTime = rpcCall.disconnectSince();
           if (afterTime >= 0) {
             throw new CallerDisconnectedException(
-                "Aborting on region " + getRegionNameAsString() + ", call " +
-                    this + " after " + afterTime + " ms, since " +
-                    "caller disconnected");
+                "Aborting on region " + getRegionInfo().getRegionNameAsString() + ", call " +
+                    this + " after " + afterTime + " ms, since caller disconnected");
           }
         }
 
@@ -4286,8 +4165,8 @@ public class HRegion implements HeapSize { // , Writable{
       resetFilters();
       // Calling the hook in CP which allows it to do a fast forward
       return this.region.getCoprocessorHost() == null
-          || this.region.getCoprocessorHost()
-              .postScannerFilterRow(this, currentRow, offset, length);
+          || this.region.getCoprocessorHost().postScannerFilterRow(this, currentRow, offset,
+               length);
     }
 
     protected boolean isStopRow(byte[] currentRow, int offset, short length) {
@@ -4322,7 +4201,7 @@ public class HRegion implements HeapSize { // , Writable{
         throw new IllegalArgumentException("Row cannot be null.");
       }
       boolean result = false;
-      startRegionOperation();
+      startRegionOperation(Operation.ANY);
       try {
         KeyValue kv = KeyValue.createFirstOnRow(row);
         // use request seek to make use of the lazy seek option. See HBASE-5520
@@ -4768,7 +4647,7 @@ public class HRegion implements HeapSize { // , Writable{
   HRegion createMergedRegionFromMerges(final HRegionInfo mergedRegionInfo,
       final HRegion region_b) throws IOException {
     HRegion r = HRegion.newHRegion(this.fs.getTableDir(), this.getLog(),
-        fs.getFileSystem(), this.getBaseConf(), mergedRegionInfo,
+        fs.getFileSystem(), getBaseConf(), mergedRegionInfo,
         this.getTableDesc(), this.rsServices);
     r.readRequestsCount.set(this.getReadRequestsCount()
         + region_b.getReadRequestsCount());
@@ -4793,7 +4672,7 @@ public class HRegion implements HeapSize { // , Writable{
   public static void addRegionToMETA(final HRegion meta, final HRegion r) throws IOException {
     meta.checkResources();
     // The row key is the region name
-    byte[] row = r.getRegionName();
+    byte[] row = r.getRegionInfo().getRegionName();
     final long now = EnvironmentEdgeManager.currentTimeMillis();
     final List<Cell> cells = new ArrayList<Cell>(2);
     cells.add(new KeyValue(row, HConstants.CATALOG_FAMILY,
@@ -4858,21 +4737,23 @@ public class HRegion implements HeapSize { // , Writable{
   throws IOException {
     HRegion a = srcA;
     HRegion b = srcB;
+    HRegionInfo hri_a = a.getRegionInfo();
+    HRegionInfo hri_b = b.getRegionInfo();
 
     // Make sure that srcA comes first; important for key-ordering during
     // write of the merged file.
-    if (srcA.getStartKey() == null) {
-      if (srcB.getStartKey() == null) {
+    if (hri_a.getStartKey() == null) {
+      if (hri_b.getStartKey() == null) {
         throw new IOException("Cannot merge two regions with null start key");
       }
       // A's start key is null but B's isn't. Assume A comes before B
-    } else if ((srcB.getStartKey() == null) ||
-      (Bytes.compareTo(srcA.getStartKey(), srcB.getStartKey()) > 0)) {
+    } else if ((hri_b.getStartKey() == null) ||
+        (Bytes.compareTo(hri_a.getStartKey(), hri_b.getStartKey()) > 0)) {
       a = srcB;
       b = srcA;
     }
 
-    if (!(Bytes.compareTo(a.getEndKey(), b.getStartKey()) == 0)) {
+    if (!(Bytes.compareTo(hri_a.getEndKey(), hri_b.getStartKey()) == 0)) {
       throw new IOException("Cannot merge non-adjacent regions");
     }
     return merge(a, b);
@@ -4921,7 +4802,7 @@ public class HRegion implements HeapSize { // , Writable{
         + Bytes.toStringBinary(mergedRegionInfo.getEndKey()) + ">");
     HRegion dstRegion;
     try {
-      dstRegion = rmt.execute(null, null);
+      dstRegion = (HRegion)rmt.execute(null, null);
     } catch (IOException ioe) {
       rmt.rollback(null, null);
       throw new IOException("Failed merging region " + a + " and " + b
@@ -4948,27 +4829,7 @@ public class HRegion implements HeapSize { // , Writable{
     return dstRegion;
   }
 
-  /**
-   * @return True if needs a major compaction.
-   * @throws IOException
-   */
-  boolean isMajorCompaction() throws IOException {
-    for (Store store : this.stores.values()) {
-      if (store.isMajorCompaction()) {
-        return true;
-      }
-    }
-    return false;
-  }
-
-  //
-  // HBASE-880
-  //
-  /**
-   * @param get get object
-   * @return result
-   * @throws IOException read exceptions
-   */
+  @Override
   public Result get(final Get get) throws IOException {
     checkRow(get.getRow(), "Get");
     // Verify families are all valid
@@ -4985,11 +4846,7 @@ public class HRegion implements HeapSize { // , Writable{
     return Result.create(results, get.isCheckExistenceOnly() ? !results.isEmpty() : null);
   }
 
-  /*
-   * Do a get based on the get parameter.
-   * @param withCoprocessor invoke coprocessor or not. We don't want to
-   * always invoke cp for this private method.
-   */
+  @Override
   public List<Cell> get(Get get, boolean withCoprocessor)
   throws IOException {
 
@@ -5032,73 +4889,39 @@ public class HRegion implements HeapSize { // , Writable{
     return results;
   }
 
+  @Override
   public void mutateRow(RowMutations rm) throws IOException {
     // Don't need nonces here - RowMutations only supports puts and deletes
-    mutateRowsWithLocks(rm.getMutations(), Collections.singleton(rm.getRow()));
-  }
-
-  /**
-   * Perform atomic mutations within the region w/o nonces.
-   * See {@link #mutateRowsWithLocks(Collection, Collection, long, long)}
-   */
-  public void mutateRowsWithLocks(Collection<Mutation> mutations,
-      Collection<byte[]> rowsToLock) throws IOException {
-    mutateRowsWithLocks(mutations, rowsToLock, HConstants.NO_NONCE, HConstants.NO_NONCE);
+    mutateRowsWithLocks(rm.getMutations(), Collections.singleton(rm.getRow()),
+      HConstants.NO_NONCE, HConstants.NO_NONCE);
   }
 
-  /**
-   * Perform atomic mutations within the region.
-   * @param mutations The list of mutations to perform.
-   * <code>mutations</code> can contain operations for multiple rows.
-   * Caller has to ensure that all rows are contained in this region.
-   * @param rowsToLock Rows to lock
-   * @param nonceGroup Optional nonce group of the operation (client Id)
-   * @param nonce Optional nonce of the operation (unique random id to ensure "more idempotence")
-   * If multiple rows are locked care should be taken that
-   * <code>rowsToLock</code> is sorted in order to avoid deadlocks.
-   * @throws IOException
-   */
+  @Override
   public void mutateRowsWithLocks(Collection<Mutation> mutations,
       Collection<byte[]> rowsToLock, long nonceGroup, long nonce) throws IOException {
     MultiRowMutationProcessor proc = new MultiRowMutationProcessor(mutations, rowsToLock);
     processRowsWithLocks(proc, -1, nonceGroup, nonce);
   }
 
-  /**
-   * @return the current load statistics for the the region
-   */
+  /** @return the current load statistics for the the region */
   public ClientProtos.RegionLoadStats getRegionStats() {
     if (!regionStatsEnabled) {
       return null;
     }
     ClientProtos.RegionLoadStats.Builder stats = ClientProtos.RegionLoadStats.newBuilder();
-    stats.setMemstoreLoad((int) (Math.min(100, (this.memstoreSize.get() * 100) / this
+    stats.setMemstoreLoad((int) (Math.min(100, (getMemstoreSize() * 100) / this
         .memstoreFlushSize)));
     stats.setHeapOccupancy((int)rsServices.getHeapMemoryManager().getHeapOccupancyPercent()*100);
     return stats.build();
   }
 
-  /**
-   * Performs atomic multiple reads and writes on a given row.
-   *
-   * @param processor The object defines the reads and writes to a row.
-   * @param nonceGroup Optional nonce group of the operation (client Id)
-   * @param nonce Optional nonce of the operation (unique random id to ensure "more idempotence")
-   */
+  @Override
   public void processRowsWithLocks(RowProcessor<?,?> processor, long nonceGroup, long nonce)
       throws IOException {
     processRowsWithLocks(processor, rowProcessorTimeout, nonceGroup, nonce);
   }
 
-  /**
-   * Performs atomic multiple reads and writes on a given row.
-   *
-   * @param processor The object defines the reads and writes to a row.
-   * @param timeout The timeout of the processor.process() execution
-   *                Use a negative number to switch off the time bound
-   * @param nonceGroup Optional nonce group of the operation (client Id)
-   * @param nonce Optional nonce of the operation (unique random id to ensure "more idempotence")
-   */
+  @Override
   public void processRowsWithLocks(RowProcessor<?,?> processor, long timeout,
       long nonceGroup, long nonce) throws IOException {
 
@@ -5110,7 +4933,7 @@ public class HRegion implements HeapSize { // , Writable{
     }
     checkResources();
 
-    startRegionOperation();
+    startRegionOperation(Operation.ANY);
     WALEdit walEdit = new WALEdit();
 
     // 1. Run pre-process hook
@@ -5292,20 +5115,11 @@ public class HRegion implements HeapSize { // , Writable{
     }
   }
 
-  public Result append(Append append) throws IOException {
-    return append(append, HConstants.NO_NONCE, HConstants.NO_NONCE);
-  }
-
   // TODO: There's a lot of boiler plate code identical to increment.
   // We should refactor append and increment as local get-mutate-put
   // transactions, so all stores only go through one code path for puts.
-  /**
-   * Perform one or more append operations on a row.
-   *
-   * @param append
-   * @return new keyvalues after increment
-   * @throws IOException
-   */
+
+  @Override
   public Result append(Append append, long nonceGroup, long nonce)
       throws IOException {
     byte[] row = append.getRow();
@@ -5538,19 +5352,11 @@ public class HRegion implements HeapSize { // , Writable{
     return append.isReturnResults() ? Result.create(allKVs) : null;
   }
 
-  public Result increment(Increment increment) throws IOException {
-    return increment(increment, HConstants.NO_NONCE, HConstants.NO_NONCE);
-  }
-
   // TODO: There's a lot of boiler plate code identical to append.
   // We should refactor append and increment as local get-mutate-put
   // transactions, so all stores only go through one code path for puts.
-  /**
-   * Perform one or more increment operations on a row.
-   * @param increment
-   * @return new keyvalues after increment
-   * @throws IOException
-   */
+
+  @Override
   public Result increment(Increment increment, long nonceGroup, long nonce)
   throws IOException {
     byte [] row = increment.getRow();
@@ -5826,22 +5632,7 @@ public class HRegion implements HeapSize { // , Writable{
     System.exit(1);
   }
 
-  /**
-   * Registers a new protocol buffer {@link Service} subclass as a coprocessor endpoint to
-   * be available for handling
-   * {@link HRegion#execService(com.google.protobuf.RpcController,
-   *    org.apache.hadoop.hbase.protobuf.generated.ClientProtos.CoprocessorServiceCall)}} calls.
-   *
-   * <p>
-   * Only a single instance may be registered per region for a given {@link Service} subclass (the
-   * instances are keyed on {@link com.google.protobuf.Descriptors.ServiceDescriptor#getFullName()}.
-   * After the first registration, subsequent calls with the same service name will fail with
-   * a return value of {@code false}.
-   * </p>
-   * @param instance the {@code Service} subclass instance to expose as a coprocessor endpoint
-   * @return {@code true} if the registration was successful, {@code false}
-   * otherwise
-   */
+  @Override
   public boolean registerService(Service instance) {
     /*
      * No stacking of instances is allowed for a single service name
@@ -5857,25 +5648,13 @@ public class HRegion implements HeapSize { // , Writable{
     coprocessorServiceHandlers.put(serviceDesc.getFullName(), instance);
     if (LOG.isDebugEnabled()) {
       LOG.debug("Registered coprocessor service: region="+
-          Bytes.toStringBinary(getRegionName())+" service="+serviceDesc.getFullName());
+          Bytes.toStringBinary(getRegionInfo().getRegionName())+" service="+
+          serviceDesc.getFullName());
     }
     return true;
   }
 
-  /**
-   * Executes a single protocol buffer coprocessor endpoint {@link Service} method using
-   * the registered protocol handlers.  {@link Service} implementations must be registered via the
-   * {@link HRegion#registerService(com.google.protobuf.Service)}
-   * method before they are available.
-   *
-   * @param controller an {@code RpcContoller} implementation to pass to the invoked service
-   * @param call a {@code CoprocessorServiceCall} instance identifying the service, method,
-   *     and parameters for the method invocation
-   * @return a protocol buffer {@code Message} instance containing the method's result
-   * @throws IOException if no registered service handler is found or an error
-   *     occurs during the invocation
-   * @see org.apache.hadoop.hbase.regionserver.HRegion#registerService(com.google.protobuf.Service)
-   */
+  @Override
   public Message execService(RpcController controller, CoprocessorServiceCall call)
       throws IOException {
     String serviceName = call.getServiceName();
@@ -5883,7 +5662,7 @@ public class HRegion implements HeapSize { // , Writable{
     if (!coprocessorServiceHandlers.containsKey(serviceName)) {
       throw new UnknownProtocolException(null,
           "No registered coprocessor service found for name "+serviceName+
-          " in region "+Bytes.toStringBinary(getRegionName()));
+          " in region "+Bytes.toStringBinary(getRegionInfo().getRegionName()));
     }
 
     Service service = coprocessorServiceHandlers.get(serviceName);
@@ -5892,7 +5671,7 @@ public class HRegion implements HeapSize { // , Writable{
     if (methodDesc == null) {
       throw new UnknownProtocolException(service.getClass(),
           "Unknown method "+methodName+" called on service "+serviceName+
-              " in region "+Bytes.toStringBinary(getRegionName()));
+              " in region "+Bytes.toStringBinary(getRegionInfo().getRegionName()));
     }
 
     Message request = service.getRequestPrototype(methodDesc).newBuilderForType()
@@ -5969,6 +5748,7 @@ public class HRegion implements HeapSize { // , Writable{
     }
   }
 
+  /** @return true if the region thinks a split should be forced */
   boolean shouldForceSplit() {
     return this.splitRequest;
   }
@@ -5977,7 +5757,7 @@ public class HRegion implements HeapSize { // , Writable{
     return this.explicitSplitPoint;
   }
 
-  void forceSplit(byte[] sp) {
+  public void forceSplit(byte[] sp) {
     // This HRegion will go away after the forced split is successful
     // But if a forced split fails, we need to clear forced split.
     this.splitRequest = true;
@@ -5986,24 +5766,13 @@ public class HRegion implements HeapSize { // , Writable{
     }
   }
 
+  /** Clear out state for any pending split request. */
   void clearSplit() {
     this.splitRequest = false;
     this.explicitSplitPoint = null;
   }
 
-  /**
-   * Give the region a chance to prepare before it is split.
-   */
-  protected void prepareToSplit() {
-    // nothing
-  }
-
-  /**
-   * Return the splitpoint. null indicates the region isn't splittable
-   * If the splitpoint isn't explicitly specified, it will go over the stores
-   * to find the best splitpoint. Currently the criteria of best splitpoint
-   * is based on the size of the store.
-   */
+  /** @return the region splitpoint. null indicates the region isn't splittable */
   public byte[] checkSplit() {
     // Can't split META
     if (this.getRegionInfo().isMetaTable() ||
@@ -6037,9 +5806,7 @@ public class HRegion implements HeapSize { // , Writable{
     return ret;
   }
 
-  /**
-   * @return The priority that this region should have in the compaction queue
-   */
+  /** @return The priority that this region should have in the compaction queue */
   public int getCompactPriority() {
     int count = Integer.MAX_VALUE;
     for (Store store : stores.values()) {
@@ -6062,7 +5829,21 @@ public class HRegion implements HeapSize { // , Writable{
     return false;
   }
 
+  /**
+   * @return True if needs a major compaction.
+   * @throws IOException
+   */
+  boolean needsMajorCompaction() throws IOException {
+    for (Store store : this.stores.values()) {
+      if (store.isMajorCompaction()) {
+        return true;
+      }
+    }
+    return false;
+  }
+
   /** @return the coprocessor host */
+  @Override
   public RegionCoprocessorHost getCoprocessorHost() {
     return coprocessorHost;
   }
@@ -6072,22 +5853,8 @@ public class HRegion implements HeapSize { // , Writable{
     this.coprocessorHost = coprocessorHost;
   }
 
-  /**
-   * This method needs to be called before any public call that reads or
-   * modifies data. It has to be called just before a try.
-   * #closeRegionOperation needs to be called in the try's finally block
-   * Acquires a read lock and checks if the region is closing or closed.
-   * @throws IOException
-   */
-  public void startRegionOperation() throws IOException {
-    startRegionOperation(Operation.ANY);
-  }
-
-  /**
-   * @param op The operation is about to be taken on the region
-   * @throws IOException
-   */
-  protected void startRegionOperation(Operation op) throws IOException {
+  @Override
+  public void startRegionOperation(Operation op) throws IOException {
     switch (op) {
     case INCREMENT:
     case APPEND:
@@ -6102,7 +5869,7 @@ public class HRegion implements HeapSize { // , Writable{
       // when a region is in recovering state, no read, split or merge is allowed
       if (isRecovering() && (this.disallowWritesInRecovering ||
               (op != Operation.PUT && op != Operation.DELETE && op != Operation.BATCH_MUTATE))) {
-        throw new RegionInRecoveryException(this.getRegionNameAsString() +
+        throw new RegionInRecoveryException(getRegionInfo().getRegionNameAsString() +
           " is recovering; cannot take reads");
       }
       break;
@@ -6116,12 +5883,14 @@ public class HRegion implements HeapSize { // , Writable{
       return;
     }
     if (this.closing.get()) {
-      throw new NotServingRegionException(getRegionNameAsString() + " is closing");
+      throw new NotServingRegionException(getRegionInfo().getRegionNameAsString() +
+        " is closing");
     }
     lock(lock.readLock());
     if (this.closed.get()) {
       lock.readLock().unlock();
-      throw new NotServingRegionException(getRegionNameAsString() + " is closed");
+      throw new NotServingRegionException(getRegionInfo().getRegionNameAsString() +
+        " is closed");
     }
     try {
       if (coprocessorHost != null) {
@@ -6133,11 +5902,7 @@ public class HRegion implements HeapSize { // , Writable{
     }
   }
 
-  /**
-   * Closes the lock. This needs to be called in the finally block corresponding
-   * to the try block of #startRegionOperation
-   * @throws IOException
-   */
+  @Override
   public void closeRegionOperation() throws IOException {
     closeRegionOperation(Operation.ANY);
   }
@@ -6167,14 +5932,14 @@ public class HRegion implements HeapSize { // , Writable{
   private void startBulkRegionOperation(boolean writeLockNeeded)
       throws NotServingRegionException, RegionTooBusyException, InterruptedIOException {
     if (this.closing.get()) {
-      throw new NotServingRegionException(getRegionNameAsString() + " is closing");
+      throw new NotServingRegionException(getRegionInfo().getRegionNameAsString() + " is closing");
     }
     if (writeLockNeeded) lock(lock.writeLock());
     else lock(lock.readLock());
     if (this.closed.get()) {
       if (writeLockNeeded) lock.writeLock().unlock();
       else lock.readLock().unlock();
-      throw new NotServingRegionException(getRegionNameAsString() + " is closed");
+      throw new NotServingRegionException(getRegionInfo().getRegionNameAsString() + " is closed");
     }
   }
 
@@ -6346,24 +6111,17 @@ public class HRegion implements HeapSize { // , Writable{
     }
   }
 
-  /**
-   * Gets the latest sequence number that was read from storage when this region was opened.
-   */
+  @Override
   public long getOpenSeqNum() {
     return this.openSeqNum;
   }
 
-  /**
-   * Gets max sequence ids of stores that was read from storage when this region was opened. WAL
-   * Edits with smaller or equal sequence number will be skipped from replay.
-   */
+  @Override
   public Map<byte[], Long> getMaxStoreSeqIdForLogReplay() {
     return this.maxSeqIdInStores;
   }
 
-  /**
-   * @return if a given region is in compaction now.
-   */
+  /** @return if a given region is in compaction now */
   public CompactionState getCompactionState() {
     boolean hasMajor = majorInProgress.get() > 0, hasMinor = minorInProgress.get() > 0;
     return (hasMajor ? (hasMinor ? CompactionState.MAJOR_AND_MINOR : CompactionState.MAJOR)
@@ -6400,39 +6158,6 @@ public class HRegion implements HeapSize { // , Writable{
     this.sequenceId.set(value);
   }
 
-  /**
-   * Listener class to enable callers of
-   * bulkLoadHFile() to perform any necessary
-   * pre/post processing of a given bulkload call
-   */
-  public interface BulkLoadListener {
-
-    /**
-     * Called before an HFile is actually loaded
-     * @param family family being loaded to
-     * @param srcPath path of HFile
-     * @return final path to be used for actual loading
-     * @throws IOException
-     */
-    String prepareBulkLoad(byte[] family, String srcPath) throws IOException;
-
-    /**
-     * Called after a successful HFile load
-     * @param family family being loaded to
-     * @param srcPath path of HFile
-     * @throws IOException
-     */
-    void doneBulkLoad(byte[] family, String srcPath) throws IOException;
-
-    /**
-     * Called after a failed HFile load
-     * @param family family being loaded to
-     * @param srcPath path of HFile
-     * @throws IOException
-     */
-    void failedBulkLoad(byte[] family, String srcPath) throws IOException;
-  }
-
   @VisibleForTesting class RowLockContext {
     private final HashedBytes row;
     private final CountDownLatch latch = new CountDownLatch(1);
@@ -6450,7 +6175,9 @@ public class HRegion implements HeapSize { // , Writable{
 
     RowLock newLock() {
       lockCount++;
-      return new RowLock(this);
+      RowLockImpl rl = new RowLockImpl();
+      rl.setContext(this);
+      return rl;
     }
 
     void releaseLock() {
@@ -6476,19 +6203,21 @@ public class HRegion implements HeapSize { // , Writable{
    * One thread may acquire multiple locks on the same row simultaneously.
    * The locks must be released by calling release() from the same thread.
    */
-  public static class RowLock {
-    @VisibleForTesting final RowLockContext context;
+  public static class RowLockImpl implements RowLock {
+    private RowLockContext context;
     private boolean released = false;
 
-    @VisibleForTesting RowLock(RowLockContext context) {
+    @VisibleForTesting
+    RowLockContext getContext() {
+      return context;
+    }
+
+    @VisibleForTesting
+    void setContext(RowLockContext context) {
       this.context = context;
     }
 
-    /**
-     * Release the given lock.  If there are no remaining locks held by the current thread
-     * then unlock the row and allow other threads to acquire the lock.
-     * @throws IllegalArgumentException if called by a different thread than the lock owning thread
-     */
+    @Override
     public void release() {
       if (!released) {
         context.releaseLock();
@@ -6497,20 +6226,9 @@ public class HRegion implements HeapSize { // , Writable{
     }
   }
 
-  /**
-   * Lock the updates' readLock first, so that we could safely append logs in coprocessors.
-   * @throws RegionTooBusyException
-   * @throws InterruptedIOException
-   */
-  public void updatesLock() throws RegionTooBusyException, InterruptedIOException {
-    lock(updatesLock.readLock());
+  @Override
+  public long getCompleteSequenceId() {
+    return this.completeSequenceId;
   }
 
-  /**
-   * Unlock the updates' readLock after appending logs in coprocessors.
-   * @throws InterruptedIOException
-   */
-  public void updatesUnlock() throws InterruptedIOException {
-    updatesLock.readLock().unlock();
-  }
 }
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java
index edb799c..b4579a2 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java
@@ -54,6 +54,8 @@ import java.util.concurrent.locks.ReentrantReadWriteLock;
 import javax.management.ObjectName;
 
 import com.google.common.annotations.VisibleForTesting;
+import com.google.common.base.Preconditions;
+import com.google.common.collect.Lists;
 import com.google.common.collect.Maps;
 
 import org.apache.hadoop.hbase.util.ByteStringer;
@@ -204,7 +206,7 @@ import org.apache.hadoop.hbase.protobuf.generated.RegionServerStatusProtos.Repor
 import org.apache.hadoop.hbase.protobuf.generated.RegionServerStatusProtos.ReportRegionStateTransitionRequest;
 import org.apache.hadoop.hbase.protobuf.generated.RegionServerStatusProtos.ReportRegionStateTransitionResponse;
 import org.apache.hadoop.hbase.protobuf.generated.WALProtos.CompactionDescriptor;
-import org.apache.hadoop.hbase.regionserver.HRegion.Operation;
+import org.apache.hadoop.hbase.regionserver.Region.Operation;
 import org.apache.hadoop.hbase.regionserver.Leases.LeaseStillHeldException;
 import org.apache.hadoop.hbase.regionserver.compactions.CompactionProgress;
 import org.apache.hadoop.hbase.regionserver.handler.CloseMetaHandler;
@@ -328,8 +330,8 @@ public class HRegionServer implements ClientProtos.ClientService.BlockingInterfa
    * Map of regions currently being served by this region server. Key is the
    * encoded region name.  All access should be synchronized.
    */
-  protected final Map<String, HRegion> onlineRegions =
-    new ConcurrentHashMap<String, HRegion>();
+  protected final Map<String, Region> onlineRegions =
+    new ConcurrentHashMap<String, Region>();
 
   /**
    * Map of encoded region names to the DataNode locations they should be hosted on
@@ -347,8 +349,8 @@ public class HRegionServer implements ClientProtos.ClientService.BlockingInterfa
    * Set of regions currently being in recovering state which means it can accept writes(edits from
    * previous failed region server) but not reads. A recovering region is also an online region.
    */
-  protected final Map<String, HRegion> recoveringRegions = Collections
-      .synchronizedMap(new HashMap<String, HRegion>());
+  protected final Map<String, Region> recoveringRegions = Collections
+      .synchronizedMap(new HashMap<String, Region>());
 
   // Leases
   protected Leases leases;
@@ -1084,7 +1086,7 @@ public class HRegionServer implements ClientProtos.ClientService.BlockingInterfa
   private boolean areAllUserRegionsOffline() {
     if (getNumberOfOnlineRegions() > 2) return false;
     boolean allUserRegionsOffline = true;
-    for (Map.Entry<String, HRegion> e: this.onlineRegions.entrySet()) {
+    for (Map.Entry<String, Region> e: onlineRegions.entrySet()) {
       if (!e.getValue().getRegionInfo().isMetaTable()) {
         allUserRegionsOffline = false;
         break;
@@ -1098,7 +1100,7 @@ public class HRegionServer implements ClientProtos.ClientService.BlockingInterfa
    */
   private long getWriteRequestCount() {
     int writeCount = 0;
-    for (Map.Entry<String, HRegion> e: this.onlineRegions.entrySet()) {
+    for (Map.Entry<String, Region> e: onlineRegions.entrySet()) {
       writeCount += e.getValue().getWriteRequestsCount();
     }
     return writeCount;
@@ -1144,7 +1146,6 @@ public class HRegionServer implements ClientProtos.ClientService.BlockingInterfa
     // improved; Additionally the load balancer will be able to take advantage of a more complete
     // history.
     MetricsRegionServerWrapper regionServerWrapper = this.metricsRegionServer.getRegionServerWrapper();
-    Collection<HRegion> regions = getOnlineRegionsLocalContext();
     MemoryUsage memory =
       ManagementFactory.getMemoryMXBean().getHeapMemoryUsage();
 
@@ -1161,7 +1162,7 @@ public class HRegionServer implements ClientProtos.ClientService.BlockingInterfa
     }
     RegionLoad.Builder regionLoadBldr = RegionLoad.newBuilder();
     RegionSpecifier.Builder regionSpecifier = RegionSpecifier.newBuilder();
-    for (HRegion region : regions) {
+    for (Region region : getOnlineRegionsLocalContext()) {
       serverLoad.addRegionLoads(createRegionLoad(region, regionLoadBldr, regionSpecifier));
     }
     serverLoad.setReportStartTime(reportStartTime);
@@ -1192,7 +1193,7 @@ public class HRegionServer implements ClientProtos.ClientService.BlockingInterfa
 
   String getOnlineRegionsAsPrintableString() {
     StringBuilder sb = new StringBuilder();
-    for (HRegion r: this.onlineRegions.values()) {
+    for (Region r: onlineRegions.values()) {
       if (sb.length() > 0) sb.append(", ");
       sb.append(r.getRegionInfo().getEncodedName());
     }
@@ -1226,7 +1227,7 @@ public class HRegionServer implements ClientProtos.ClientService.BlockingInterfa
       // Ensure all user regions have been sent a close. Use this to
       // protect against the case where an open comes in after we start the
       // iterator of onlineRegions to close all user regions.
-      for (Map.Entry<String, HRegion> e : this.onlineRegions.entrySet()) {
+      for (Map.Entry<String, Region> e : onlineRegions.entrySet()) {
         HRegionInfo hri = e.getValue().getRegionInfo();
         if (!this.regionsInTransitionInRS.containsKey(hri.getEncodedNameAsBytes())
             && !closedRegions.contains(hri.getEncodedName())) {
@@ -1387,50 +1388,43 @@ public class HRegionServer implements ClientProtos.ClientService.BlockingInterfa
   }
 
   /*
-   * @param r Region to get RegionLoad for.
+   * @param r region to get RegionLoad for.
    * @param regionLoadBldr the RegionLoad.Builder, can be null
    * @param regionSpecifier the RegionSpecifier.Builder, can be null
    * @return RegionLoad instance.
    *
    * @throws IOException
    */
-  private RegionLoad createRegionLoad(final HRegion r, RegionLoad.Builder regionLoadBldr,
+  private RegionLoad createRegionLoad(final Region r, RegionLoad.Builder regionLoadBldr,
       RegionSpecifier.Builder regionSpecifier) {
-    byte[] name = r.getRegionName();
+    byte[] name = r.getRegionInfo().getRegionName();
     int stores = 0;
     int storefiles = 0;
     int storeUncompressedSizeMB = 0;
     int storefileSizeMB = 0;
-    int memstoreSizeMB = (int) (r.memstoreSize.get() / 1024 / 1024);
+    int memstoreSizeMB = (int) (r.getMemstoreSize() / 1024 / 1024);
     int storefileIndexSizeMB = 0;
     int rootIndexSizeKB = 0;
     int totalStaticIndexSizeKB = 0;
     int totalStaticBloomSizeKB = 0;
     long totalCompactingKVs = 0;
     long currentCompactedKVs = 0;
-    synchronized (r.stores) {
-      stores += r.stores.size();
-      for (Store store : r.stores.values()) {
-        storefiles += store.getStorefilesCount();
-        storeUncompressedSizeMB += (int) (store.getStoreSizeUncompressed()
-            / 1024 / 1024);
-        storefileSizeMB += (int) (store.getStorefilesSize() / 1024 / 1024);
-        storefileIndexSizeMB += (int) (store.getStorefilesIndexSize() / 1024 / 1024);
-        CompactionProgress progress = store.getCompactionProgress();
-        if (progress != null) {
-          totalCompactingKVs += progress.totalCompactingKVs;
-          currentCompactedKVs += progress.currentCompactedKVs;
-        }
-
-        rootIndexSizeKB +=
-            (int) (store.getStorefilesIndexSize() / 1024);
-
-        totalStaticIndexSizeKB +=
-          (int) (store.getTotalStaticIndexSize() / 1024);
-
-        totalStaticBloomSizeKB +=
-          (int) (store.getTotalStaticBloomSize() / 1024);
-      }
+    Collection<Store> storeList = r.getStores();
+    stores += storeList.size();
+    for (Store store : storeList) {
+      storefiles += store.getStorefilesCount();
+      storeUncompressedSizeMB += (int) (store.getStoreSizeUncompressed()
+          / 1024 / 1024);
+      storefileSizeMB += (int) (store.getStorefilesSize() / 1024 / 1024);
+      storefileIndexSizeMB += (int) (store.getStorefilesIndexSize() / 1024 / 1024);
+      CompactionProgress progress = store.getCompactionProgress();
+      if (progress != null) {
+        totalCompactingKVs += progress.totalCompactingKVs;
+        currentCompactedKVs += progress.currentCompactedKVs;
+      }
+      rootIndexSizeKB += (int) (store.getStorefilesIndexSize() / 1024); 
+      totalStaticIndexSizeKB += (int) (store.getTotalStaticIndexSize() / 1024);
+      totalStaticBloomSizeKB += (int) (store.getTotalStaticBloomSize() / 1024);
     }
     float dataLocality =
         r.getHDFSBlocksDistribution().getBlockLocalityIndex(serverName.getHostname());
@@ -1452,11 +1446,11 @@ public class HRegionServer implements ClientProtos.ClientService.BlockingInterfa
       .setRootIndexSizeKB(rootIndexSizeKB)
       .setTotalStaticIndexSizeKB(totalStaticIndexSizeKB)
       .setTotalStaticBloomSizeKB(totalStaticBloomSizeKB)
-      .setReadRequestsCount(r.readRequestsCount.get())
-      .setWriteRequestsCount(r.writeRequestsCount.get())
+      .setReadRequestsCount(r.getReadRequestsCount())
+      .setWriteRequestsCount(r.getWriteRequestsCount())
       .setTotalCompactingKVs(totalCompactingKVs)
       .setCurrentCompactedKVs(currentCompactedKVs)
-      .setCompleteSequenceId(r.completeSequenceId)
+      .setCompleteSequenceId(r.getCompleteSequenceId())
       .setDataLocality(dataLocality);
 
     return regionLoadBldr.build();
@@ -1467,8 +1461,7 @@ public class HRegionServer implements ClientProtos.ClientService.BlockingInterfa
    * @return An instance of RegionLoad.
    */
   public RegionLoad createRegionLoad(final String encodedRegionName) {
-    HRegion r = null;
-    r = this.onlineRegions.get(encodedRegionName);
+    Region r = onlineRegions.get(encodedRegionName);
     return r != null ? createRegionLoad(r, null, null) : null;
   }
 
@@ -1497,10 +1490,10 @@ public class HRegionServer implements ClientProtos.ClientService.BlockingInterfa
 
     @Override
     protected void chore() {
-      for (HRegion r : this.instance.onlineRegions.values()) {
+      for (Region r : instance.onlineRegions.values()) {
         if (r == null)
           continue;
-        for (Store s : r.getStores().values()) {
+        for (Store s : r.getStores()) {
           try {
             long multiplier = s.getCompactionCheckMultiplier();
             assert multiplier > 0;
@@ -1511,7 +1504,7 @@ public class HRegionServer implements ClientProtos.ClientService.BlockingInterfa
                   + " requests compaction");
             } else if (s.isMajorCompaction()) {
               if (majorCompactPriority == DEFAULT_PRIORITY
-                  || majorCompactPriority > r.getCompactPriority()) {
+                  || majorCompactPriority > ((HRegion)r).getCompactPriority()) {
                 this.instance.compactSplitThread.requestCompaction(r, s, getName()
                     + " requests major compaction; use default priority", null);
               } else {
@@ -1540,15 +1533,15 @@ public class HRegionServer implements ClientProtos.ClientService.BlockingInterfa
 
     @Override
     protected void chore() {
-      for (HRegion r : this.server.onlineRegions.values()) {
+      for (Region r : server.onlineRegions.values()) {
         if (r == null)
           continue;
-        if (r.shouldFlush()) {
+        if (((HRegion)r).shouldFlush()) {
           FlushRequester requester = server.getFlushRequester();
           if (requester != null) {
             long randomDelay = rand.nextInt(RANGE_OF_DELAY) + MIN_DELAY_TIME;
-            LOG.info(getName() + " requesting flush for region " + r.getRegionNameAsString() +
-                " after a delay of " + randomDelay);
+            LOG.info(getName() + " requesting flush for region " +
+              r.getRegionInfo().getRegionNameAsString() + " after a delay of " + randomDelay);
             //Throttle the flushes by putting a delay. If we don't throttle, and there
             //is a balanced write-load on the regions in a table, we might end up
             //overwhelming the filesystem with too many flushes at once.
@@ -1856,12 +1849,12 @@ public class HRegionServer implements ClientProtos.ClientService.BlockingInterfa
   }
 
   @Override
-  public void postOpenDeployTasks(final HRegion r, final CatalogTracker ct)
+  public void postOpenDeployTasks(final Region r, final CatalogTracker ct)
   throws KeeperException, IOException {
     checkOpen();
-    LOG.info("Post open deploy tasks for region=" + r.getRegionNameAsString());
+    LOG.info("Post open deploy tasks for region=" + r.getRegionInfo().getRegionNameAsString());
     // Do checks to see if we need to compact (references or too many files)
-    for (Store s : r.getStores().values()) {
+    for (Store s : r.getStores()) {
       if (s.hasReferences() || s.needsCompaction()) {
        this.compactSplitThread.requestSystemCompaction(r, s, "Opening Region");
       }
@@ -1869,12 +1862,13 @@ public class HRegionServer implements ClientProtos.ClientService.BlockingInterfa
     long openSeqNum = r.getOpenSeqNum();
     if (openSeqNum == HConstants.NO_SEQNUM) {
       // If we opened a region, we should have read some sequence number from it.
-      LOG.error("No sequence number found when opening " + r.getRegionNameAsString());
+      LOG.error("No sequence number found when opening " +
+        r.getRegionInfo().getRegionNameAsString());
       openSeqNum = 0;
     }
 
     // Update flushed sequence id of a recovering region in ZK
-    updateRecoveringRegionLastFlushedSequenceId(r);
+    updateRecoveringRegionLastFlushedSequenceId((HRegion)r);
 
     if (useZKForAssignment) {
       if (r.getRegionInfo().isMetaRegion()) {
@@ -1890,11 +1884,10 @@ public class HRegionServer implements ClientProtos.ClientService.BlockingInterfa
      if (!useZKForAssignment
         && !reportRegionStateTransition(TransitionCode.OPENED, openSeqNum, r.getRegionInfo())) {
       throw new IOException("Failed to report opened region to master: "
-          + r.getRegionNameAsString());
+          + r.getRegionInfo().getRegionNameAsString());
     }
 
-    LOG.info("Finished post open deploy task for " + r.getRegionNameAsString());
-
+    LOG.info("Finished post open deploy task for " + r.getRegionInfo().getRegionNameAsString());
   }
 
   @Override
@@ -2021,8 +2014,10 @@ public class HRegionServer implements ClientProtos.ClientService.BlockingInterfa
   }
 
   @Override
-  public boolean reportRegionStateTransition(TransitionCode code, long openSeqNum, HRegionInfo... hris) {
-    ReportRegionStateTransitionRequest.Builder builder = ReportRegionStateTransitionRequest.newBuilder();
+  public boolean reportRegionStateTransition(TransitionCode code, long openSeqNum,
+      HRegionInfo... hris) {
+    ReportRegionStateTransitionRequest.Builder builder =
+        ReportRegionStateTransitionRequest.newBuilder();
     builder.setServer(ProtobufUtil.toServerName(serverName));
     RegionStateTransition.Builder transition = builder.addTransitionBuilder();
     transition.setTransitionCode(code);
@@ -2040,7 +2035,8 @@ public class HRegionServer implements ClientProtos.ClientService.BlockingInterfa
           createRegionServerStatusStub();
           continue;
         }
-        ReportRegionStateTransitionResponse response = rss.reportRegionStateTransition(null, request);
+        ReportRegionStateTransitionResponse response = rss.reportRegionStateTransition(null,
+          request);
         if (response.hasErrorMessage()) {
           LOG.info("Failed to transition " + hris[0] + " to " + code + ": "
               + response.getErrorMessage());
@@ -2094,7 +2090,7 @@ public class HRegionServer implements ClientProtos.ClientService.BlockingInterfa
       new InetSocketAddress(sn.getHostname(), sn.getPort());
       try {
         BlockingRpcChannel channel =
-            this.rpcClient.createBlockingRpcChannel(sn, userProvider.getCurrent(), this.rpcTimeout);
+          this.rpcClient.createBlockingRpcChannel(sn, userProvider.getCurrent(), this.rpcTimeout);
         intf = RegionServerStatusService.newBlockingStub(channel);
         break;
       } catch (IOException e) {
@@ -2196,10 +2192,10 @@ public class HRegionServer implements ClientProtos.ClientService.BlockingInterfa
    * @param abort Whether we're running an abort.
    */
   void closeMetaTableRegions(final boolean abort) {
-    HRegion meta = null;
+    Region meta = null;
     this.lock.writeLock().lock();
     try {
-      for (Map.Entry<String, HRegion> e: onlineRegions.entrySet()) {
+      for (Map.Entry<String, Region> e: onlineRegions.entrySet()) {
         HRegionInfo hri = e.getValue().getRegionInfo();
         if (hri.isMetaRegion()) {
           meta = e.getValue();
@@ -2221,8 +2217,8 @@ public class HRegionServer implements ClientProtos.ClientService.BlockingInterfa
   void closeUserRegions(final boolean abort) {
     this.lock.writeLock().lock();
     try {
-      for (Map.Entry<String, HRegion> e: this.onlineRegions.entrySet()) {
-        HRegion r = e.getValue();
+      for (Map.Entry<String, Region> e: onlineRegions.entrySet()) {
+        Region r = e.getValue();
         if (!r.getRegionInfo().isMetaTable() && r.isAvailable()) {
           // Don't update zk with this close transition; pass false.
           closeRegionIgnoreErrors(r.getRegionInfo(), abort);
@@ -2252,8 +2248,10 @@ public class HRegionServer implements ClientProtos.ClientService.BlockingInterfa
   }
 
   @Override
-  public Map<String, HRegion> getRecoveringRegions() {
-    return this.recoveringRegions;
+  public Map<String, Region> getRecoveringRegions() {
+    Map<String, Region> result = Maps.newHashMap();
+    result.putAll(recoveringRegions);
+    return result;
   }
 
   /**
@@ -2283,14 +2281,17 @@ public class HRegionServer implements ClientProtos.ClientService.BlockingInterfa
    * This method will only work if HRegionServer is in the same JVM as client;
    * HRegion cannot be serialized to cross an rpc.
    */
-  public Collection<HRegion> getOnlineRegionsLocalContext() {
-    Collection<HRegion> regions = this.onlineRegions.values();
-    return Collections.unmodifiableCollection(regions);
+  @VisibleForTesting
+  public List<Region> getOnlineRegionsLocalContext() {
+    List<Region> result = Lists.newArrayList();
+    result.addAll(onlineRegions.values());
+    return result;
   }
 
   @Override
-  public void addToOnlineRegions(HRegion region) {
-    this.onlineRegions.put(region.getRegionInfo().getEncodedName(), region);
+  public void addToOnlineRegions(Region region) {
+    Preconditions.checkArgument(region instanceof HRegion, "Region must be a HRegion");
+    this.onlineRegions.put(region.getRegionInfo().getEncodedName(), (HRegion)region);
   }
 
   /**
@@ -2298,9 +2299,9 @@ public class HRegionServer implements ClientProtos.ClientService.BlockingInterfa
    * biggest.  If two regions are the same size, then the last one found wins; i.e. this method
    * may NOT return all regions.
    */
-  SortedMap<Long, HRegion> getCopyOfOnlineRegionsSortedBySize() {
+  SortedMap<Long, Region> getCopyOfOnlineRegionsSortedBySize() {
     // we'll sort the regions in reverse
-    SortedMap<Long, HRegion> sortedRegions = new TreeMap<Long, HRegion>(
+    SortedMap<Long, Region> sortedRegions = new TreeMap<Long, Region>(
         new Comparator<Long>() {
           @Override
           public int compare(Long a, Long b) {
@@ -2308,8 +2309,8 @@ public class HRegionServer implements ClientProtos.ClientService.BlockingInterfa
           }
         });
     // Copy over all regions. Regions are sorted by size with biggest first.
-    for (HRegion region : this.onlineRegions.values()) {
-      sortedRegions.put(region.memstoreSize.get(), region);
+    for (Region region : onlineRegions.values()) {
+      sortedRegions.put(region.getMemstoreSize(), region);
     }
     return sortedRegions;
   }
@@ -2335,7 +2336,7 @@ public class HRegionServer implements ClientProtos.ClientService.BlockingInterfa
    */
   protected HRegionInfo[] getMostLoadedRegions() {
     ArrayList<HRegionInfo> regions = new ArrayList<HRegionInfo>();
-    for (HRegion r : onlineRegions.values()) {
+    for (Region r : onlineRegions.values()) {
       if (!r.isAvailable()) {
         continue;
       }
@@ -2551,25 +2552,24 @@ public class HRegionServer implements ClientProtos.ClientService.BlockingInterfa
    * @return Online regions from <code>tableName</code>
    */
   @Override
-  public List<HRegion> getOnlineRegions(TableName tableName) {
-     List<HRegion> tableRegions = new ArrayList<HRegion>();
-     synchronized (this.onlineRegions) {
-       for (HRegion region: this.onlineRegions.values()) {
-         HRegionInfo regionInfo = region.getRegionInfo();
-         if(regionInfo.getTable().equals(tableName)) {
-           tableRegions.add(region);
-         }
-       }
-     }
-     return tableRegions;
-   }
+  public List<Region> getOnlineRegions(TableName tableName) {
+    List<Region> tableRegions = Lists.newArrayList();
+    synchronized (onlineRegions) {
+      for (Region region: onlineRegions.values()) {
+        HRegionInfo regionInfo = region.getRegionInfo();
+        if(regionInfo.getTable().equals(tableName)) {
+          tableRegions.add(region);
+        }
+      }
+    }
+    return tableRegions;
+  }
 
   // used by org/apache/hbase/tmpl/regionserver/RSStatusTmpl.jamon (HBASE-4070).
   public String[] getCoprocessors() {
     TreeSet<String> coprocessors = new TreeSet<String>(
         this.hlog.getCoprocessorHost().getCoprocessors());
-    Collection<HRegion> regions = getOnlineRegionsLocalContext();
-    for (HRegion region: regions) {
+    for (Region region: getOnlineRegionsLocalContext()) {
       coprocessors.addAll(region.getCoprocessorHost().getCoprocessors());
     }
     return coprocessors.toArray(new String[coprocessors.size()]);
@@ -2594,14 +2594,17 @@ public class HRegionServer implements ClientProtos.ClientService.BlockingInterfa
         LOG.info("Scanner " + this.scannerName + " lease expired on region "
             + s.getRegionInfo().getRegionNameAsString());
         try {
-          HRegion region = getRegion(s.getRegionInfo().getRegionName());
-          if (region != null && region.getCoprocessorHost() != null) {
-            region.getCoprocessorHost().preScannerClose(s);
+          RegionCoprocessorHost cpHost = null;
+          Region region = getRegion(s.getRegionInfo().getRegionName());
+          if (region != null) {
+            cpHost = region.getCoprocessorHost();
+          }
+          if (cpHost != null) {
+            cpHost.preScannerClose(s);
           }
-
           s.close();
-          if (region != null && region.getCoprocessorHost() != null) {
-            region.getCoprocessorHost().postScannerClose(s);
+          if (cpHost != null) {
+            cpHost.postScannerClose(s);
           }
         } catch (IOException e) {
           LOG.error("Closing scanner for "
@@ -2658,7 +2661,7 @@ public class HRegionServer implements ClientProtos.ClientService.BlockingInterfa
    *   If a close was in progress, this new request will be ignored, and an exception thrown.
    * </p>
    *
-   * @param encodedName Region to close
+   * @param encodedName region to close
    * @param abort True if we are aborting
    * @param zk True if we are to update zk about the region close; if the close
    * was orchestrated by master, then update zk.  If the close is being run by
@@ -2673,12 +2676,16 @@ public class HRegionServer implements ClientProtos.ClientService.BlockingInterfa
       final boolean zk, final int versionOfClosingNode, final ServerName sn)
       throws NotServingRegionException, RegionAlreadyInTransitionException {
     //Check for permissions to close.
-    HRegion actualRegion = this.getFromOnlineRegions(encodedName);
-    if ((actualRegion != null) && (actualRegion.getCoprocessorHost() != null)) {
+    RegionCoprocessorHost cpHost = null;
+    Region actualRegion = getFromOnlineRegions(encodedName);
+    if (actualRegion != null) {
+      cpHost = actualRegion.getCoprocessorHost();
+    }
+    if (cpHost != null) {
       try {
-        actualRegion.getCoprocessorHost().preClose(false);
+        cpHost.preClose(false);
       } catch (IOException exp) {
-        LOG.warn("Unable to close region: the coprocessor launched an error ", exp);
+        LOG.warn("Unable to close region, the coprocessor threw an exception: ", exp);
         return false;
       }
     }
@@ -2697,7 +2704,7 @@ public class HRegionServer implements ClientProtos.ClientService.BlockingInterfa
         return closeRegion(encodedName, abort, zk, versionOfClosingNode, sn);
       }
       // Let's get the region from the online region list again
-      actualRegion = this.getFromOnlineRegions(encodedName);
+      actualRegion = getFromOnlineRegions(encodedName);
       if (actualRegion == null) { // If already online, we still need to close it.
         LOG.info("The opening previously in progress has been cancelled by a CLOSE request.");
         // The master deletes the znode when it receives this exception.
@@ -2741,9 +2748,8 @@ public class HRegionServer implements ClientProtos.ClientService.BlockingInterfa
    * @return HRegion for the passed binary <code>regionName</code> or null if
    *         named region is not member of the online regions.
    */
-  public HRegion getOnlineRegion(final byte[] regionName) {
-    String encodedRegionName = HRegionInfo.encodeRegionName(regionName);
-    return this.onlineRegions.get(encodedRegionName);
+  public Region getOnlineRegion(final byte[] regionName) {
+    return onlineRegions.get(HRegionInfo.encodeRegionName(regionName));
   }
 
   public InetSocketAddress[] getRegionBlockLocations(final String encodedRegionName) {
@@ -2751,15 +2757,13 @@ public class HRegionServer implements ClientProtos.ClientService.BlockingInterfa
   }
 
   @Override
-  public HRegion getFromOnlineRegions(final String encodedRegionName) {
-    return this.onlineRegions.get(encodedRegionName);
+  public Region getFromOnlineRegions(final String encodedRegionName) {
+    return onlineRegions.get(encodedRegionName);
   }
 
-
   @Override
-  public boolean removeFromOnlineRegions(final HRegion r, ServerName destination) {
-    HRegion toReturn = this.onlineRegions.remove(r.getRegionInfo().getEncodedName());
-
+  public boolean removeFromOnlineRegions(final Region r, ServerName destination) {
+    Region toReturn = this.onlineRegions.remove(r.getRegionInfo().getEncodedName());
     if (destination != null) {
       HLog wal = getWAL();
       long closeSeqNum = wal.getEarliestMemstoreSeqNum(r.getRegionInfo().getEncodedNameAsBytes());
@@ -2780,24 +2784,24 @@ public class HRegionServer implements ClientProtos.ClientService.BlockingInterfa
    * Protected utility method for safely obtaining an HRegion handle.
    *
    * @param regionName
-   *          Name of online {@link HRegion} to return
-   * @return {@link HRegion} for <code>regionName</code>
+   *          Name of online {@link Region} to return
+   * @return {@link Region} for <code>regionName</code>
    * @throws NotServingRegionException
    */
-  protected HRegion getRegion(final byte[] regionName)
+  protected Region getRegion(final byte[] regionName)
       throws NotServingRegionException {
     String encodedRegionName = HRegionInfo.encodeRegionName(regionName);
     return getRegionByEncodedName(regionName, encodedRegionName);
   }
 
-  protected HRegion getRegionByEncodedName(String encodedRegionName)
+  protected Region getRegionByEncodedName(String encodedRegionName)
       throws NotServingRegionException {
     return getRegionByEncodedName(null, encodedRegionName);
   }
 
-  protected HRegion getRegionByEncodedName(byte[] regionName, String encodedRegionName)
+  protected Region getRegionByEncodedName(byte[] regionName, String encodedRegionName)
     throws NotServingRegionException {
-    HRegion region = this.onlineRegions.get(encodedRegionName);
+    Region region = onlineRegions.get(encodedRegionName);
     if (region == null) {
       MovedRegionInfo moveInfo = getMovedRegion(encodedRegionName);
       if (moveInfo != null) {
@@ -2912,7 +2916,7 @@ public class HRegionServer implements ClientProtos.ClientService.BlockingInterfa
     return this.fsOk;
   }
 
-  protected long addScanner(RegionScanner s, HRegion r) throws LeaseStillHeldException {
+  protected long addScanner(RegionScanner s, Region r) throws LeaseStillHeldException {
     long scannerId = this.scannerIdGen.incrementAndGet();
     String scannerName = String.valueOf(scannerId);
 
@@ -2942,7 +2946,7 @@ public class HRegionServer implements ClientProtos.ClientService.BlockingInterfa
     try {
       checkOpen();
       requestCount.increment();
-      HRegion region = getRegion(request.getRegion());
+      Region region = getRegion(request.getRegion());
 
       GetResponse.Builder builder = GetResponse.newBuilder();
       ClientProtos.Get get = request.getGet();
@@ -2960,15 +2964,16 @@ public class HRegionServer implements ClientProtos.ClientService.BlockingInterfa
         r = region.getClosestRowBefore(row, family);
       } else {
         Get clientGet = ProtobufUtil.toGet(get);
-        if (get.getExistenceOnly() && region.getCoprocessorHost() != null) {
-          existence = region.getCoprocessorHost().preExists(clientGet);
+        RegionCoprocessorHost cpHost = region.getCoprocessorHost();
+        if (get.getExistenceOnly() && cpHost != null) {
+          existence = cpHost.preExists(clientGet);
         }
         if (existence == null) {
           r = region.get(clientGet);
           if (get.getExistenceOnly()) {
             boolean exists = r.getExists();
-            if (region.getCoprocessorHost() != null) {
-              exists = region.getCoprocessorHost().postExists(clientGet, exists);
+            if (cpHost != null) {
+              exists = cpHost.postExists(clientGet, exists);
             }
             existence = exists;
           }
@@ -3009,7 +3014,7 @@ public class HRegionServer implements ClientProtos.ClientService.BlockingInterfa
     try {
       checkOpen();
       requestCount.increment();
-      HRegion region = getRegion(request.getRegion());
+      Region region = getRegion(request.getRegion());
       MutateResponse.Builder builder = MutateResponse.newBuilder();
       MutationProto mutation = request.getMutation();
       if (!region.getRegionInfo().isMetaTable()) {
@@ -3039,16 +3044,16 @@ public class HRegionServer implements ClientProtos.ClientService.BlockingInterfa
           CompareOp compareOp = CompareOp.valueOf(condition.getCompareType().name());
           ByteArrayComparable comparator =
             ProtobufUtil.toComparator(condition.getComparator());
-          if (region.getCoprocessorHost() != null) {
-            processed = region.getCoprocessorHost().preCheckAndPut(
-              row, family, qualifier, compareOp, comparator, put);
+          RegionCoprocessorHost cpHost = region.getCoprocessorHost();
+          if (cpHost != null) {
+            processed = cpHost.preCheckAndPut(row, family, qualifier, compareOp, comparator, put);
           }
           if (processed == null) {
             boolean result = region.checkAndMutate(row, family,
               qualifier, compareOp, comparator, put, true);
-            if (region.getCoprocessorHost() != null) {
-              result = region.getCoprocessorHost().postCheckAndPut(row, family,
-                qualifier, compareOp, comparator, put, result);
+            if (cpHost != null) {
+              result = cpHost.postCheckAndPut(row, family, qualifier, compareOp, comparator, put,
+                result);
             }
             processed = result;
           }
@@ -3065,18 +3070,18 @@ public class HRegionServer implements ClientProtos.ClientService.BlockingInterfa
           byte[] family = condition.getFamily().toByteArray();
           byte[] qualifier = condition.getQualifier().toByteArray();
           CompareOp compareOp = CompareOp.valueOf(condition.getCompareType().name());
-          ByteArrayComparable comparator =
-            ProtobufUtil.toComparator(condition.getComparator());
-          if (region.getCoprocessorHost() != null) {
-            processed = region.getCoprocessorHost().preCheckAndDelete(
-              row, family, qualifier, compareOp, comparator, delete);
+          ByteArrayComparable comparator = ProtobufUtil.toComparator(condition.getComparator());
+          RegionCoprocessorHost cpHost = region.getCoprocessorHost();
+          if (cpHost != null) {
+            processed = cpHost.preCheckAndDelete(row, family, qualifier, compareOp, comparator,
+              delete);
           }
           if (processed == null) {
             boolean result = region.checkAndMutate(row, family,
               qualifier, compareOp, comparator, delete, true);
-            if (region.getCoprocessorHost() != null) {
-              result = region.getCoprocessorHost().postCheckAndDelete(row, family,
-                qualifier, compareOp, comparator, delete, result);
+            if (cpHost != null) {
+              result = cpHost.postCheckAndDelete(row, family, qualifier, compareOp, comparator,
+                delete, result);
             }
             processed = result;
           }
@@ -3163,7 +3168,7 @@ public class HRegionServer implements ClientProtos.ClientService.BlockingInterfa
       requestCount.increment();
 
       int ttl = 0;
-      HRegion region = null;
+      Region region = null;
       RegionScanner scanner = null;
       RegionScannerHolder rsh = null;
       boolean moreResults = true;
@@ -3200,15 +3205,21 @@ public class HRegionServer implements ClientProtos.ClientService.BlockingInterfa
           scan.setLoadColumnFamiliesOnDemand(region.isLoadingCfsOnDemandDefault());
         }
         scan.getAttribute(Scan.SCAN_ATTRIBUTES_METRICS_ENABLE);
-        region.prepareScanner(scan);
-        if (region.getCoprocessorHost() != null) {
-          scanner = region.getCoprocessorHost().preScannerOpen(scan);
+        if (!scan.hasFamilies()) {
+          // Add all families to scanner of not specified
+          for (byte[] family: region.getTableDesc().getFamiliesKeys()){
+            scan.addFamily(family);
+          }
+        }
+        RegionCoprocessorHost cpHost = region.getCoprocessorHost();
+        if (cpHost != null) {
+          scanner = cpHost.preScannerOpen(scan);
         }
         if (scanner == null) {
           scanner = region.getScanner(scan);
         }
-        if (region.getCoprocessorHost() != null) {
-          scanner = region.getCoprocessorHost().postScannerOpen(scan, scanner);
+        if (cpHost != null) {
+          scanner = cpHost.postScannerOpen(scan, scanner);
         }
         scannerId = addScanner(scanner, region);
         scannerName = String.valueOf(scannerId);
@@ -3243,9 +3254,12 @@ public class HRegionServer implements ClientProtos.ClientService.BlockingInterfa
 
           boolean done = false;
           // Call coprocessor. Get region info from scanner.
-          if (region != null && region.getCoprocessorHost() != null) {
-            Boolean bypass = region.getCoprocessorHost().preScannerNext(
-              scanner, results, rows);
+          RegionCoprocessorHost cpHost = null;
+          if (region != null) {
+            cpHost = region.getCoprocessorHost();
+          }
+          if (cpHost != null) {
+            Boolean bypass = cpHost.preScannerNext(scanner, results, rows);
             if (!results.isEmpty()) {
               for (Result r : results) {
                 if (maxScannerResultSize < Long.MAX_VALUE){
@@ -3295,15 +3309,15 @@ public class HRegionServer implements ClientProtos.ClientService.BlockingInterfa
                   values.clear();
                 }
               }
-              region.readRequestsCount.add(i);
+              region.updateReadRequestsCount(i);
               region.getMetrics().updateScanNext(totalKvSize);
             } finally {
               region.closeRegionOperation();
             }
 
             // coprocessor postNext hook
-            if (region != null && region.getCoprocessorHost() != null) {
-              region.getCoprocessorHost().postScannerNext(scanner, results, rows, true);
+            if (cpHost != null) {
+              cpHost.postScannerNext(scanner, results, rows, true);
             }
           }
 
@@ -3329,18 +3343,20 @@ public class HRegionServer implements ClientProtos.ClientService.BlockingInterfa
       if (!moreResults || closeScanner) {
         ttl = 0;
         moreResults = false;
-        if (region != null && region.getCoprocessorHost() != null) {
-          if (region.getCoprocessorHost().preScannerClose(scanner)) {
-            return builder.build(); // bypass
-          }
+        RegionCoprocessorHost cpHost = null;
+        if (region != null) {
+          cpHost = region.getCoprocessorHost();
+        }
+        if (cpHost != null && cpHost.preScannerClose(scanner)) {
+          return builder.build(); // bypass
         }
         rsh = scanners.remove(scannerName);
         if (rsh != null) {
           scanner = rsh.s;
           scanner.close();
           leases.cancelLease(scannerName);
-          if (region != null && region.getCoprocessorHost() != null) {
-            region.getCoprocessorHost().postScannerClose(scanner);
+          if (cpHost != null) {
+            cpHost.postScannerClose(scanner);
           }
         }
       }
@@ -3397,22 +3413,23 @@ public class HRegionServer implements ClientProtos.ClientService.BlockingInterfa
     try {
       checkOpen();
       requestCount.increment();
-      HRegion region = getRegion(request.getRegion());
+      Region region = getRegion(request.getRegion());
       List<Pair<byte[], String>> familyPaths = new ArrayList<Pair<byte[], String>>();
       for (FamilyPath familyPath: request.getFamilyPathList()) {
         familyPaths.add(new Pair<byte[], String>(familyPath.getFamily().toByteArray(),
           familyPath.getPath()));
       }
       boolean bypass = false;
-      if (region.getCoprocessorHost() != null) {
-        bypass = region.getCoprocessorHost().preBulkLoadHFile(familyPaths);
+      RegionCoprocessorHost cpHost = region.getCoprocessorHost();
+      if (cpHost != null) {
+        bypass = cpHost.preBulkLoadHFile(familyPaths);
       }
       boolean loaded = false;
       if (!bypass) {
-        loaded = region.bulkLoadHFiles(familyPaths, request.getAssignSeqNum());
+        loaded = region.bulkLoadHFiles(familyPaths, request.getAssignSeqNum(), null);
       }
-      if (region.getCoprocessorHost() != null) {
-        loaded = region.getCoprocessorHost().postBulkLoadHFile(familyPaths, loaded);
+      if (cpHost != null) {
+        loaded = cpHost.postBulkLoadHFile(familyPaths, loaded);
       }
       BulkLoadHFileResponse.Builder builder = BulkLoadHFileResponse.newBuilder();
       builder.setLoaded(loaded);
@@ -3428,12 +3445,12 @@ public class HRegionServer implements ClientProtos.ClientService.BlockingInterfa
     try {
       checkOpen();
       requestCount.increment();
-      HRegion region = getRegion(request.getRegion());
+      Region region = getRegion(request.getRegion());
       Message result = execServiceOnRegion(region, request.getCall());
       CoprocessorServiceResponse.Builder builder =
           CoprocessorServiceResponse.newBuilder();
       builder.setRegion(RequestConverter.buildRegionSpecifier(
-          RegionSpecifierType.REGION_NAME, region.getRegionName()));
+          RegionSpecifierType.REGION_NAME, region.getRegionInfo().getRegionName()));
       builder.setValue(
           builder.getValueBuilder().setName(result.getClass().getName())
               .setValue(result.toByteString()));
@@ -3443,7 +3460,7 @@ public class HRegionServer implements ClientProtos.ClientService.BlockingInterfa
     }
   }
 
-  private Message execServiceOnRegion(HRegion region,
+  private Message execServiceOnRegion(Region region,
       final ClientProtos.CoprocessorServiceCall serviceCall) throws IOException {
     // ignore the passed in controller (from the serialized call)
     ServerRpcController execController = new ServerRpcController();
@@ -3550,7 +3567,7 @@ public class HRegionServer implements ClientProtos.ClientService.BlockingInterfa
 
     for (RegionAction regionAction : request.getRegionActionList()) {
       this.requestCount.add(regionAction.getActionCount());
-      HRegion region;
+      Region region;
       regionActionResultBuilder.clear();
       try {
         region = getRegion(regionAction.getRegion());
@@ -3614,7 +3631,7 @@ public class HRegionServer implements ClientProtos.ClientService.BlockingInterfa
    * method returns as a 'result'.
    * @return Return the <code>cellScanner</code> passed
    */
-  private List<CellScannable> doNonAtomicRegionMutation(final HRegion region,
+  private List<CellScannable> doNonAtomicRegionMutation(final Region region,
       final RegionAction actions, final CellScanner cellScanner,
       final RegionActionResult.Builder builder, List<CellScannable> cellsToReturn, long nonceGroup) {
     // Gather up CONTIGUOUS Puts and Deletes in this mutations List.  Idea is that rather than do
@@ -3716,12 +3733,12 @@ public class HRegionServer implements ClientProtos.ClientService.BlockingInterfa
     try {
       checkOpen();
       requestCount.increment();
-      HRegion region = getRegion(request.getRegion());
+      Region region = getRegion(request.getRegion());
       HRegionInfo info = region.getRegionInfo();
       GetRegionInfoResponse.Builder builder = GetRegionInfoResponse.newBuilder();
       builder.setRegionInfo(HRegionInfo.convert(info));
       if (request.hasCompactionState() && request.getCompactionState()) {
-        builder.setCompactionState(region.getCompactionState());
+        builder.setCompactionState(((HRegion)region).getCompactionState());
       }
       builder.setIsRecovering(region.isRecovering());
       return builder.build();
@@ -3735,13 +3752,14 @@ public class HRegionServer implements ClientProtos.ClientService.BlockingInterfa
       final GetStoreFileRequest request) throws ServiceException {
     try {
       checkOpen();
-      HRegion region = getRegion(request.getRegion());
+      Region region = getRegion(request.getRegion());
       requestCount.increment();
-      Set<byte[]> columnFamilies;
+      Set<byte[]> columnFamilies = new TreeSet<byte[]>(Bytes.BYTES_RAWCOMPARATOR);
       if (request.getFamilyCount() == 0) {
-        columnFamilies = region.getStores().keySet();
+        for (byte[] cf: region.getTableDesc().getFamiliesKeys()) {
+          columnFamilies.add(cf);
+        }
       } else {
-        columnFamilies = new TreeSet<byte[]>(Bytes.BYTES_RAWCOMPARATOR);
         for (ByteString cf: request.getFamilyList()) {
           columnFamilies.add(cf.toByteArray());
         }
@@ -3765,7 +3783,7 @@ public class HRegionServer implements ClientProtos.ClientService.BlockingInterfa
       checkOpen();
       requestCount.increment();
       List<HRegionInfo> list = new ArrayList<HRegionInfo>(onlineRegions.size());
-      for (HRegion region: this.onlineRegions.values()) {
+      for (Region region: this.onlineRegions.values()) {
         list.add(region.getRegionInfo());
       }
       Collections.sort(list);
@@ -3825,7 +3843,7 @@ public class HRegionServer implements ClientProtos.ClientService.BlockingInterfa
         new HashMap<TableName, HTableDescriptor>(regionCount);
     final boolean isBulkAssign = regionCount > 1;
     for (RegionOpenInfo regionOpenInfo : request.getOpenInfoList()) {
-      final HRegionInfo region = HRegionInfo.convert(regionOpenInfo.getRegion());
+      final HRegionInfo regionInfo = HRegionInfo.convert(regionOpenInfo.getRegion());
 
       int versionOfOfflineNode = -1;
       if (regionOpenInfo.hasVersionOfOfflineNode()) {
@@ -3833,7 +3851,7 @@ public class HRegionServer implements ClientProtos.ClientService.BlockingInterfa
       }
       HTableDescriptor htd;
       try {
-        final HRegion onlineRegion = getFromOnlineRegions(region.getEncodedName());
+        final Region onlineRegion = getFromOnlineRegions(regionInfo.getEncodedName());
         if (onlineRegion != null) {
           //Check if the region can actually be opened.
           if (onlineRegion.getCoprocessorHost() != null) {
@@ -3842,83 +3860,83 @@ public class HRegionServer implements ClientProtos.ClientService.BlockingInterfa
           // See HBASE-5094. Cross check with hbase:meta if still this RS is owning
           // the region.
           Pair<HRegionInfo, ServerName> p = MetaReader.getRegion(
-              this.catalogTracker, region.getRegionName());
+              this.catalogTracker, regionInfo.getRegionName());
           if (this.getServerName().equals(p.getSecond())) {
-            Boolean closing = regionsInTransitionInRS.get(region.getEncodedNameAsBytes());
+            Boolean closing = regionsInTransitionInRS.get(regionInfo.getEncodedNameAsBytes());
             // Map regionsInTransitionInRSOnly has an entry for a region only if the region
             // is in transition on this RS, so here closing can be null. If not null, it can
             // be true or false. True means the region is opening on this RS; while false
             // means the region is closing. Only return ALREADY_OPENED if not closing (i.e.
             // not in transition any more, or still transition to open.
             if (!Boolean.FALSE.equals(closing)
-                && getFromOnlineRegions(region.getEncodedName()) != null) {
-              LOG.warn("Attempted open of " + region.getEncodedName()
+                && getFromOnlineRegions(regionInfo.getEncodedName()) != null) {
+              LOG.warn("Attempted open of " + regionInfo.getEncodedName()
                 + " but already online on this server");
               builder.addOpeningState(RegionOpeningState.ALREADY_OPENED);
               continue;
             }
           } else {
-            LOG.warn("The region " + region.getEncodedName() + " is online on this server" +
+            LOG.warn("The region " + regionInfo.getEncodedName() + " is online on this server" +
                 " but hbase:meta does not have this server - continue opening.");
             removeFromOnlineRegions(onlineRegion, null);
           }
         }
-        LOG.info("Open " + region.getRegionNameAsString());
-        htd = htds.get(region.getTable());
+        LOG.info("Open " + regionInfo.getRegionNameAsString());
+        htd = htds.get(regionInfo.getTable());
         if (htd == null) {
-          htd = this.tableDescriptors.get(region.getTable());
-          htds.put(region.getTable(), htd);
+          htd = this.tableDescriptors.get(regionInfo.getTable());
+          htds.put(regionInfo.getTable(), htd);
         }
 
         final Boolean previous = this.regionsInTransitionInRS.putIfAbsent(
-            region.getEncodedNameAsBytes(), Boolean.TRUE);
+            regionInfo.getEncodedNameAsBytes(), Boolean.TRUE);
 
         if (Boolean.FALSE.equals(previous)) {
           // There is a close in progress. We need to mark this open as failed in ZK.
           OpenRegionHandler.
-              tryTransitionFromOfflineToFailedOpen(this, region, versionOfOfflineNode);
+              tryTransitionFromOfflineToFailedOpen(this, regionInfo, versionOfOfflineNode);
 
           throw new RegionAlreadyInTransitionException("Received OPEN for the region:" +
-              region.getRegionNameAsString() + " , which we are already trying to CLOSE ");
+              regionInfo.getRegionNameAsString() + " , which we are already trying to CLOSE ");
         }
 
         if (Boolean.TRUE.equals(previous)) {
           // An open is in progress. This is supported, but let's log this.
           LOG.info("Receiving OPEN for the region:" +
-              region.getRegionNameAsString() + " , which we are already trying to OPEN" +
+              regionInfo.getRegionNameAsString() + " , which we are already trying to OPEN" +
               " - ignoring this new request for this region.");
         }
 
         // We are opening this region. If it moves back and forth for whatever reason, we don't
         // want to keep returning the stale moved record while we are opening/if we close again.
-        removeFromMovedRegions(region.getEncodedName());
+        removeFromMovedRegions(regionInfo.getEncodedName());
 
         if (previous == null) {
           // check if the region to be opened is marked in recovering state in ZK
           if (SplitLogManager.isRegionMarkedRecoveringInZK(this.zooKeeper,
-                region.getEncodedName())) {
+                regionInfo.getEncodedName())) {
             // check if current region open is for distributedLogReplay. This check is to support
             // rolling restart/upgrade where we want to Master/RS see same configuration
             if (!regionOpenInfo.hasOpenForDistributedLogReplay()
                   || regionOpenInfo.getOpenForDistributedLogReplay()) {
-              this.recoveringRegions.put(region.getEncodedName(), null);
+              this.recoveringRegions.put(regionInfo.getEncodedName(), null);
             } else {
               // remove stale recovery region from ZK when we open region not for recovering which
               // could happen when turn distributedLogReplay off from on.
               List<String> tmpRegions = new ArrayList<String>();
-              tmpRegions.add(region.getEncodedName());
+              tmpRegions.add(regionInfo.getEncodedName());
               SplitLogManager.deleteRecoveringRegionZNodes(this.zooKeeper, tmpRegions);
             }
           }
           // If there is no action in progress, we can submit a specific handler.
           // Need to pass the expected version in the constructor.
-          if (region.isMetaRegion()) {
-            this.service.submit(new OpenMetaHandler(this, this, region, htd,
+          if (regionInfo.isMetaRegion()) {
+            this.service.submit(new OpenMetaHandler(this, this, regionInfo, htd,
                 versionOfOfflineNode));
           } else {
-            updateRegionFavoredNodesMapping(region.getEncodedName(),
+            updateRegionFavoredNodesMapping(regionInfo.getEncodedName(),
                 regionOpenInfo.getFavoredNodesList());
-            this.service.submit(new OpenRegionHandler(this, this, region, htd,
+            this.service.submit(new OpenRegionHandler(this, this, regionInfo, htd,
                 versionOfOfflineNode));
           }
         }
@@ -3929,7 +3947,7 @@ public class HRegionServer implements ClientProtos.ClientService.BlockingInterfa
         LOG.error("Can't retrieve recovering state from zookeeper", zooKeeperEx);
         throw new ServiceException(zooKeeperEx);
       } catch (IOException ie) {
-        LOG.warn("Failed opening region " + region.getRegionNameAsString(), ie);
+        LOG.warn("Failed opening region " + regionInfo.getRegionNameAsString(), ie);
         if (isBulkAssign) {
           builder.addOpeningState(RegionOpeningState.FAILED_OPENING);
         } else {
@@ -3998,8 +4016,8 @@ public class HRegionServer implements ClientProtos.ClientService.BlockingInterfa
       final String encodedRegionName = ProtobufUtil.getRegionEncodedName(request.getRegion());
 
       // Can be null if we're calling close on a region that's not online
-      final HRegion region = this.getFromOnlineRegions(encodedRegionName);
-      if ((region  != null) && (region .getCoprocessorHost() != null)) {
+      final Region region = getFromOnlineRegions(encodedRegionName);
+      if ((region  != null) && (region.getCoprocessorHost() != null)) {
         region.getCoprocessorHost().preClose(false);
       }
 
@@ -4029,8 +4047,8 @@ public class HRegionServer implements ClientProtos.ClientService.BlockingInterfa
     try {
       checkOpen();
       requestCount.increment();
-      HRegion region = getRegion(request.getRegion());
-      LOG.info("Flushing " + region.getRegionNameAsString());
+      Region region = getRegion(request.getRegion());
+      LOG.info("Flushing " + region.getRegionInfo().getRegionNameAsString());
       boolean shouldFlush = true;
       if (request.hasIfOlderThanTs()) {
         shouldFlush = region.getLastFlushTime() < request.getIfOlderThanTs();
@@ -4038,7 +4056,7 @@ public class HRegionServer implements ClientProtos.ClientService.BlockingInterfa
       FlushRegionResponse.Builder builder = FlushRegionResponse.newBuilder();
       if (shouldFlush) {
         long startTime = EnvironmentEdgeManager.currentTimeMillis();
-        HRegion.FlushResult flushResult = region.flushcache();
+        HRegion.FlushResult flushResult = ((HRegion)region).flushcache();
         if (flushResult.isFlushSucceeded()) {
           long endTime = EnvironmentEdgeManager.currentTimeMillis();
           metricsRegionServer.updateFlushTime(endTime - startTime);
@@ -4078,9 +4096,9 @@ public class HRegionServer implements ClientProtos.ClientService.BlockingInterfa
     try {
       checkOpen();
       requestCount.increment();
-      HRegion region = getRegion(request.getRegion());
+      HRegion region = (HRegion)getRegion(request.getRegion());
       region.startRegionOperation(Operation.SPLIT_REGION);
-      LOG.info("Splitting " + region.getRegionNameAsString());
+      LOG.info("Splitting " + region.getRegionInfo().getRegionNameAsString());
       long startTime = EnvironmentEdgeManager.currentTimeMillis();
       HRegion.FlushResult flushResult = region.flushcache();
       if (flushResult.isFlushSucceeded()) {
@@ -4114,21 +4132,21 @@ public class HRegionServer implements ClientProtos.ClientService.BlockingInterfa
     try {
       checkOpen();
       requestCount.increment();
-      HRegion regionA = getRegion(request.getRegionA());
-      HRegion regionB = getRegion(request.getRegionB());
+      Region regionA = getRegion(request.getRegionA());
+      Region regionB = getRegion(request.getRegionB());
       boolean forcible = request.getForcible();
       regionA.startRegionOperation(Operation.MERGE_REGION);
       regionB.startRegionOperation(Operation.MERGE_REGION);
       LOG.info("Receiving merging request for  " + regionA + ", " + regionB
           + ",forcible=" + forcible);
       long startTime = EnvironmentEdgeManager.currentTimeMillis();
-      HRegion.FlushResult flushResult = regionA.flushcache();
+      HRegion.FlushResult flushResult = ((HRegion)regionA).flushcache();
       if (flushResult.isFlushSucceeded()) {
         long endTime = EnvironmentEdgeManager.currentTimeMillis();
         metricsRegionServer.updateFlushTime(endTime - startTime);
       }
       startTime = EnvironmentEdgeManager.currentTimeMillis();
-      flushResult = regionB.flushcache();
+      flushResult = ((HRegion)regionB).flushcache();
       if (flushResult.isFlushSucceeded()) {
         long endTime = EnvironmentEdgeManager.currentTimeMillis();
         metricsRegionServer.updateFlushTime(endTime - startTime);
@@ -4154,9 +4172,9 @@ public class HRegionServer implements ClientProtos.ClientService.BlockingInterfa
     try {
       checkOpen();
       requestCount.increment();
-      HRegion region = getRegion(request.getRegion());
+      Region region = getRegion(request.getRegion());
       region.startRegionOperation(Operation.COMPACT_REGION);
-      LOG.info("Compacting " + region.getRegionNameAsString());
+      LOG.info("Compacting " + region.getRegionInfo().getRegionNameAsString());
       boolean major = false;
       byte [] family = null;
       Store store = null;
@@ -4165,7 +4183,7 @@ public class HRegionServer implements ClientProtos.ClientService.BlockingInterfa
         store = region.getStore(family);
         if (store == null) {
           throw new ServiceException(new IOException("column family " + Bytes.toString(family) +
-            " does not exist in region " + region.getRegionNameAsString()));
+            " does not exist in region " + region.getRegionInfo().getRegionNameAsString()));
         }
       }
       if (request.hasMajor()) {
@@ -4181,7 +4199,7 @@ public class HRegionServer implements ClientProtos.ClientService.BlockingInterfa
 
       String familyLogMsg = (family != null)?" for column family: " + Bytes.toString(family):"";
       LOG.trace("User-triggered compaction requested for region " +
-        region.getRegionNameAsString() + familyLogMsg);
+        region.getRegionInfo().getRegionNameAsString() + familyLogMsg);
       String log = "User-triggered " + (major ? "major " : "") + "compaction" + familyLogMsg;
       if(family != null) {
         compactSplitThread.requestCompaction(region, store, log,
@@ -4245,9 +4263,9 @@ public class HRegionServer implements ClientProtos.ClientService.BlockingInterfa
         // empty input
         return ReplicateWALEntryResponse.newBuilder().build();
       }
-      HRegion region = this.getRegionByEncodedName(
+      Region region = this.getRegionByEncodedName(
         entries.get(0).getKey().getEncodedRegionName().toStringUtf8());
-      RegionCoprocessorHost coprocessorHost = region.getCoprocessorHost();
+      RegionCoprocessorHost cpHost = region.getCoprocessorHost();
       List<Pair<HLogKey, WALEdit>> walEntries = new ArrayList<Pair<HLogKey, WALEdit>>();
       List<HLogSplitter.MutationReplay> mutations = new ArrayList<HLogSplitter.MutationReplay>();
       // when tag is enabled, we need tag replay edits with log sequence number
@@ -4259,14 +4277,14 @@ public class HRegionServer implements ClientProtos.ClientService.BlockingInterfa
           long nonce = entry.getKey().hasNonce() ? entry.getKey().getNonce() : HConstants.NO_NONCE;
           nonceManager.reportOperationFromWal(nonceGroup, nonce, entry.getKey().getWriteTime());
         }
-        Pair<HLogKey, WALEdit> walEntry = (coprocessorHost == null) ? null :
+        Pair<HLogKey, WALEdit> walEntry = (cpHost == null) ? null :
           new Pair<HLogKey, WALEdit>();
         List<HLogSplitter.MutationReplay> edits = HLogSplitter.getMutationsFromWALEntry(entry,
           cells, walEntry, needAddReplayTag);
-        if (coprocessorHost != null) {
+        if (cpHost != null) {
           // Start coprocessor replay here. The coprocessor is for each WALEdit instead of a
           // KeyValue.
-          if (coprocessorHost.preWALRestore(region.getRegionInfo(), walEntry.getFirst(),
+          if (cpHost.preWALRestore(region.getRegionInfo(), walEntry.getFirst(),
             walEntry.getSecond())) {
             // if bypass this log entry, ignore it ...
             continue;
@@ -4285,9 +4303,9 @@ public class HRegionServer implements ClientProtos.ClientService.BlockingInterfa
           }
         }
       }
-      if (coprocessorHost != null) {
+      if (cpHost != null) {
         for (Pair<HLogKey, WALEdit> wal : walEntries) {
-          coprocessorHost.postWALRestore(region.getRegionInfo(), wal.getFirst(),
+          cpHost.postWALRestore(region.getRegionInfo(), wal.getFirst(),
             wal.getSecond());
         }
       }
@@ -4367,14 +4385,14 @@ public class HRegionServer implements ClientProtos.ClientService.BlockingInterfa
 // End Admin methods
 
   /**
-   * Find the HRegion based on a region specifier
+   * Find the region based on a region specifier
    *
    * @param regionSpecifier the region specifier
    * @return the corresponding region
    * @throws IOException if the specifier is not null,
    *    but failed to find the region
    */
-  protected HRegion getRegion(
+  protected Region getRegion(
       final RegionSpecifier regionSpecifier) throws IOException {
     return getRegionByEncodedName(regionSpecifier.getValue().toByteArray(),
         ProtobufUtil.getRegionEncodedName(regionSpecifier));
@@ -4390,13 +4408,14 @@ public class HRegionServer implements ClientProtos.ClientService.BlockingInterfa
    * bypassed as indicated by RegionObserver, null otherwise
    * @throws IOException
    */
-  protected Result append(final HRegion region,
+  protected Result append(final Region region,
       final MutationProto m, final CellScanner cellScanner, long nonceGroup) throws IOException {
     long before = EnvironmentEdgeManager.currentTimeMillis();
     Append append = ProtobufUtil.toAppend(m, cellScanner);
     Result r = null;
-    if (region.getCoprocessorHost() != null) {
-      r = region.getCoprocessorHost().preAppend(append);
+    RegionCoprocessorHost cpHost = region.getCoprocessorHost();
+    if (cpHost != null) {
+      r = cpHost.preAppend(append);
     }
     if (r == null) {
       long nonce = startNonceOperation(m, nonceGroup);
@@ -4407,8 +4426,8 @@ public class HRegionServer implements ClientProtos.ClientService.BlockingInterfa
       } finally {
         endNonceOperation(m, nonceGroup, success);
       }
-      if (region.getCoprocessorHost() != null) {
-        region.getCoprocessorHost().postAppend(append, r);
+      if (cpHost != null) {
+        cpHost.postAppend(append, r);
       }
     }
     metricsRegionServer.updateAppend(EnvironmentEdgeManager.currentTimeMillis() - before);
@@ -4423,13 +4442,14 @@ public class HRegionServer implements ClientProtos.ClientService.BlockingInterfa
    * @return the Result
    * @throws IOException
    */
-  protected Result increment(final HRegion region, final MutationProto mutation,
+  protected Result increment(final Region region, final MutationProto mutation,
       final CellScanner cells, long nonceGroup) throws IOException {
     long before = EnvironmentEdgeManager.currentTimeMillis();
     Increment increment = ProtobufUtil.toIncrement(mutation, cells);
     Result r = null;
-    if (region.getCoprocessorHost() != null) {
-      r = region.getCoprocessorHost().preIncrement(increment);
+    RegionCoprocessorHost cpHost = region.getCoprocessorHost();
+    if (cpHost != null) {
+      r = cpHost.preIncrement(increment);
     }
     if (r == null) {
       long nonce = startNonceOperation(mutation, nonceGroup);
@@ -4440,8 +4460,8 @@ public class HRegionServer implements ClientProtos.ClientService.BlockingInterfa
       } finally {
         endNonceOperation(mutation, nonceGroup, success);
       }
-      if (region.getCoprocessorHost() != null) {
-        r = region.getCoprocessorHost().postIncrement(increment, r);
+      if (cpHost != null) {
+        r = cpHost.postIncrement(increment, r);
       }
     }
     metricsRegionServer.updateIncrement(EnvironmentEdgeManager.currentTimeMillis() - before);
@@ -4497,7 +4517,7 @@ public class HRegionServer implements ClientProtos.ClientService.BlockingInterfa
    * @param region
    * @param mutations
    */
-  protected void doBatchOp(final RegionActionResult.Builder builder, final HRegion region,
+  protected void doBatchOp(final RegionActionResult.Builder builder, final Region region,
       final List<ClientProtos.Action> mutations, final CellScanner cells) {
     Mutation[] mArray = new Mutation[mutations.size()];
     long before = EnvironmentEdgeManager.currentTimeMillis();
@@ -4521,7 +4541,8 @@ public class HRegionServer implements ClientProtos.ClientService.BlockingInterfa
         cacheFlusher.reclaimMemStoreMemory();
       }
 
-      OperationStatus codes[] = region.batchMutate(mArray);
+      OperationStatus codes[] = region.batchMutate(mArray, HConstants.NO_NONCE,
+        HConstants.NO_NONCE);
       for (i = 0; i < codes.length; i++) {
         int index = mutations.get(i).getIndex();
         Exception e = null;
@@ -4543,7 +4564,8 @@ public class HRegionServer implements ClientProtos.ClientService.BlockingInterfa
 
           case SUCCESS:
             builder.addResultOrException(getResultOrException(
-              ClientProtos.Result.getDefaultInstance(), index, region.getRegionStats()));
+              ClientProtos.Result.getDefaultInstance(), index, 
+              ((HRegion)region).getRegionStats()));
             break;
         }
       }
@@ -4584,7 +4606,7 @@ public class HRegionServer implements ClientProtos.ClientService.BlockingInterfa
    *         exceptionMessage if any
    * @throws IOException
    */
-  protected OperationStatus [] doReplayBatchOp(final HRegion region,
+  protected OperationStatus [] doReplayBatchOp(final Region region,
       final List<HLogSplitter.MutationReplay> mutations) throws IOException {
 
     long before = EnvironmentEdgeManager.currentTimeMillis();
@@ -4603,7 +4625,7 @@ public class HRegionServer implements ClientProtos.ClientService.BlockingInterfa
           for (Cell metaCell : metaCells) {
             CompactionDescriptor compactionDesc = WALEdit.getCompaction(metaCell);
             if (compactionDesc != null) {
-              region.completeCompactionMarker(compactionDesc);
+              ((HRegion)region).completeCompactionMarker(compactionDesc);
             }
           }
           it.remove();
@@ -4634,7 +4656,7 @@ public class HRegionServer implements ClientProtos.ClientService.BlockingInterfa
    * @param cellScanner if non-null, the mutation data -- the Cell content.
    * @throws IOException
    */
-  protected ClientProtos.RegionLoadStats mutateRows(final HRegion region,
+  protected ClientProtos.RegionLoadStats mutateRows(final Region region,
       final List<ClientProtos.Action> actions, final CellScanner cellScanner)
       throws IOException {
     if (!region.getRegionInfo().isMetaTable()) {
@@ -4662,7 +4684,7 @@ public class HRegionServer implements ClientProtos.ClientService.BlockingInterfa
       }
     }
     region.mutateRow(rm);
-    return region.getRegionStats();
+    return ((HRegion)region).getRegionStats();
   }
 
   /**
@@ -4678,7 +4700,7 @@ public class HRegionServer implements ClientProtos.ClientService.BlockingInterfa
    * @param comparator
    * @throws IOException
    */
-  private boolean checkAndRowMutate(final HRegion region, final List<ClientProtos.Action> actions,
+  private boolean checkAndRowMutate(final Region region, final List<ClientProtos.Action> actions,
       final CellScanner cellScanner, byte[] row, byte[] family, byte[] qualifier,
       CompareOp compareOp, ByteArrayComparable comparator) throws IOException {
     if (!region.getRegionInfo().isMetaTable()) {
@@ -4836,9 +4858,9 @@ public class HRegionServer implements ClientProtos.ClientService.BlockingInterfa
   private static class RegionScannerHolder {
     private RegionScanner s;
     private long nextCallSeq = 0L;
-    private HRegion r;
+    private Region r;
 
-    public RegionScannerHolder(RegionScanner s, HRegion r) {
+    public RegionScannerHolder(RegionScanner s, Region r) {
       this.s = s;
       this.r = r;
     }
@@ -4964,8 +4986,8 @@ public class HRegionServer implements ClientProtos.ClientService.BlockingInterfa
   @Override
   public double getCompactionPressure() {
     double max = 0;
-    for (HRegion region : onlineRegions.values()) {
-      for (Store store : region.getStores().values()) {
+    for (Region region : onlineRegions.values()) {
+      for (Store store : region.getStores()) {
         double normCount = store.getCompactionPressure();
         if (normCount > max) {
           max = normCount;
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/IncreasingToUpperBoundRegionSplitPolicy.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/IncreasingToUpperBoundRegionSplitPolicy.java
index 18bb376..d7a9be5 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/IncreasingToUpperBoundRegionSplitPolicy.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/IncreasingToUpperBoundRegionSplitPolicy.java
@@ -72,7 +72,7 @@ extends ConstantSizeRegionSplitPolicy {
     // Get size to check
     long sizeToCheck = getSizeToCheck(tableRegionsCount);
 
-    for (Store store : region.getStores().values()) {
+    for (Store store : region.getStores()) {
       // If any of the stores is unable to split (eg they contain reference files)
       // then don't split
       if ((!store.canSplit())) {
@@ -114,7 +114,7 @@ extends ConstantSizeRegionSplitPolicy {
     TableName tablename = this.region.getTableDesc().getTableName();
     int tableRegionsCount = 0;
     try {
-      List<HRegion> hri = rss.getOnlineRegions(tablename);
+      List<Region> hri = rss.getOnlineRegions(tablename);
       tableRegionsCount = hri == null || hri.isEmpty()? 0: hri.size();
     } catch (IOException e) {
       LOG.debug("Failed getOnlineRegions " + tablename, e);
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/LogRoller.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/LogRoller.java
index 45722e3..b661f13 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/LogRoller.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/LogRoller.java
@@ -125,7 +125,7 @@ class LogRoller extends HasThread implements WALActionsListener {
    */
   private void scheduleFlush(final byte [] encodedRegionName) {
     boolean scheduled = false;
-    HRegion r = this.services.getFromOnlineRegions(Bytes.toString(encodedRegionName));
+    Region r = this.services.getFromOnlineRegions(Bytes.toString(encodedRegionName));
     FlushRequester requester = null;
     if (r != null) {
       requester = this.services.getFlushRequester();
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/MemStoreFlusher.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/MemStoreFlusher.java
index 4d175d0..a402eef 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/MemStoreFlusher.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/MemStoreFlusher.java
@@ -69,8 +69,8 @@ class MemStoreFlusher implements FlushRequester {
   // a corresponding entry in the other.
   private final BlockingQueue<FlushQueueEntry> flushQueue =
     new DelayQueue<FlushQueueEntry>();
-  private final Map<HRegion, FlushRegionEntry> regionsInQueue =
-    new HashMap<HRegion, FlushRegionEntry>();
+  private final Map<Region, FlushRegionEntry> regionsInQueue =
+    new HashMap<Region, FlushRegionEntry>();
   private AtomicBoolean wakeupPending = new AtomicBoolean();
 
   private final long threadWakeFrequency;
@@ -160,19 +160,17 @@ class MemStoreFlusher implements FlushRequester {
    * @return true if successful
    */
   private boolean flushOneForGlobalPressure() {
-    SortedMap<Long, HRegion> regionsBySize =
-        server.getCopyOfOnlineRegionsSortedBySize();
-
-    Set<HRegion> excludedRegions = new HashSet<HRegion>();
+    SortedMap<Long, Region> regionsBySize = server.getCopyOfOnlineRegionsSortedBySize();
+    Set<Region> excludedRegions = new HashSet<Region>();
 
     boolean flushedOne = false;
     while (!flushedOne) {
       // Find the biggest region that doesn't have too many storefiles
       // (might be null!)
-      HRegion bestFlushableRegion = getBiggestMemstoreRegion(
+      Region bestFlushableRegion = getBiggestMemstoreRegion(
           regionsBySize, excludedRegions, true);
       // Find the biggest region, total, even if it might have too many flushes.
-      HRegion bestAnyRegion = getBiggestMemstoreRegion(
+      Region bestAnyRegion = getBiggestMemstoreRegion(
           regionsBySize, excludedRegions, false);
 
       if (bestAnyRegion == null) {
@@ -180,20 +178,20 @@ class MemStoreFlusher implements FlushRequester {
         return false;
       }
 
-      HRegion regionToFlush;
+      Region regionToFlush;
       if (bestFlushableRegion != null &&
-          bestAnyRegion.memstoreSize.get() > 2 * bestFlushableRegion.memstoreSize.get()) {
+          bestAnyRegion.getMemstoreSize() > 2 * bestFlushableRegion.getMemstoreSize()) {
         // Even if it's not supposed to be flushed, pick a region if it's more than twice
         // as big as the best flushable one - otherwise when we're under pressure we make
         // lots of little flushes and cause lots of compactions, etc, which just makes
         // life worse!
         if (LOG.isDebugEnabled()) {
           LOG.debug("Under global heap pressure: " +
-            "Region " + bestAnyRegion.getRegionNameAsString() + " has too many " +
+            "Region " + bestAnyRegion.getRegionInfo().getRegionNameAsString() + " has too many " +
             "store files, but is " +
-            StringUtils.humanReadableInt(bestAnyRegion.memstoreSize.get()) +
+            StringUtils.humanReadableInt(bestAnyRegion.getMemstoreSize()) +
             " vs best flushable region's " +
-            StringUtils.humanReadableInt(bestFlushableRegion.memstoreSize.get()) +
+            StringUtils.humanReadableInt(bestFlushableRegion.getMemstoreSize()) +
             ". Choosing the bigger.");
         }
         regionToFlush = bestAnyRegion;
@@ -205,10 +203,10 @@ class MemStoreFlusher implements FlushRequester {
         }
       }
 
-      Preconditions.checkState(regionToFlush.memstoreSize.get() > 0);
+      Preconditions.checkState(regionToFlush.getMemstoreSize() > 0);
 
       LOG.info("Flush of region " + regionToFlush + " due to global heap pressure");
-      flushedOne = flushRegion(regionToFlush, true);
+      flushedOne = flushRegion((HRegion)regionToFlush, true);
       if (!flushedOne) {
         LOG.info("Excluding unflushable region " + regionToFlush +
           " - trying to find a different region to flush.");
@@ -282,20 +280,19 @@ class MemStoreFlusher implements FlushRequester {
     }
   }
 
-  private HRegion getBiggestMemstoreRegion(
-      SortedMap<Long, HRegion> regionsBySize,
-      Set<HRegion> excludedRegions,
+  private Region getBiggestMemstoreRegion(
+      SortedMap<Long, Region> regionsBySize,
+      Set<Region> excludedRegions,
       boolean checkStoreFileCount) {
     synchronized (regionsInQueue) {
-      for (HRegion region : regionsBySize.values()) {
+      for (Region region: regionsBySize.values()) {
         if (excludedRegions.contains(region)) {
           continue;
         }
-
-        if (region.writestate.flushing || !region.writestate.writesEnabled) {
+        if (((HRegion)region).writestate.flushing || 
+            !((HRegion)region).writestate.writesEnabled) {
           continue;
         }
-
         if (checkStoreFileCount && isTooManyStoreFiles(region)) {
           continue;
         }
@@ -321,23 +318,25 @@ class MemStoreFlusher implements FlushRequester {
       getGlobalMemstoreSize() >= globalMemStoreLimitLowMark;
   }
 
-  public void requestFlush(HRegion r) {
+  @Override
+  public void requestFlush(Region r) {
     synchronized (regionsInQueue) {
       if (!regionsInQueue.containsKey(r)) {
         // This entry has no delay so it will be added at the top of the flush
         // queue.  It'll come out near immediately.
-        FlushRegionEntry fqe = new FlushRegionEntry(r);
+        FlushRegionEntry fqe = new FlushRegionEntry((HRegion)r);
         this.regionsInQueue.put(r, fqe);
         this.flushQueue.add(fqe);
       }
     }
   }
 
-  public void requestDelayedFlush(HRegion r, long delay) {
+  @Override
+  public void requestDelayedFlush(Region r, long delay) {
     synchronized (regionsInQueue) {
       if (!regionsInQueue.containsKey(r)) {
         // This entry has some delay
-        FlushRegionEntry fqe = new FlushRegionEntry(r);
+        FlushRegionEntry fqe = new FlushRegionEntry((HRegion)r);
         fqe.requeue(delay);
         this.regionsInQueue.put(r, fqe);
         this.flushQueue.add(fqe);
@@ -399,19 +398,19 @@ class MemStoreFlusher implements FlushRequester {
    * not flushed.
    */
   private boolean flushRegion(final FlushRegionEntry fqe) {
-    HRegion region = fqe.region;
+    HRegion region = (HRegion)fqe.region;
     if (!region.getRegionInfo().isMetaRegion() &&
         isTooManyStoreFiles(region)) {
       if (fqe.isMaximumWait(this.blockingWaitTime)) {
         LOG.info("Waited " + (System.currentTimeMillis() - fqe.createTime) +
           "ms on a compaction to clean up 'too many store files'; waited " +
           "long enough... proceeding with flush of " +
-          region.getRegionNameAsString());
+          region.getRegionInfo().getRegionNameAsString());
       } else {
         // If this is first time we've been put off, then emit a log message.
         if (fqe.getRequeueCount() <= 0) {
           // Note: We don't impose blockingStoreFiles constraint on meta regions
-          LOG.warn("Region " + region.getRegionNameAsString() + " has too many " +
+          LOG.warn("Region " + region.getRegionInfo().getRegionNameAsString() + " has too many " +
             "store files; delaying flush up to " + this.blockingWaitTime + "ms");
           if (!this.server.compactSplitThread.requestSplit(region)) {
             try {
@@ -419,7 +418,8 @@ class MemStoreFlusher implements FlushRequester {
                   region, Thread.currentThread().getName());
             } catch (IOException e) {
               LOG.error(
-                "Cache flush failed for region " + Bytes.toStringBinary(region.getRegionName()),
+                "Cache flush failed for region " +
+                  Bytes.toStringBinary(region.getRegionInfo().getRegionName()),
                 RemoteExceptionHandler.checkIOException(e));
             }
           }
@@ -493,7 +493,8 @@ class MemStoreFlusher implements FlushRequester {
       return false;
     } catch (IOException ex) {
       LOG.error("Cache flush failed" +
-        (region != null ? (" for region " + Bytes.toStringBinary(region.getRegionName())) : ""),
+        (region != null ? (" for region " +
+          Bytes.toStringBinary(region.getRegionInfo().getRegionName())) : ""),
         RemoteExceptionHandler.checkIOException(ex));
       if (!server.checkFileSystem()) {
         return false;
@@ -511,8 +512,8 @@ class MemStoreFlusher implements FlushRequester {
     }
   }
 
-  private boolean isTooManyStoreFiles(HRegion region) {
-    for (Store store : region.stores.values()) {
+  private boolean isTooManyStoreFiles(Region region) {
+    for (Store store : region.getStores()) {
       if (store.hasTooManyStoreFiles()) {
         return true;
       }
@@ -682,7 +683,7 @@ class MemStoreFlusher implements FlushRequester {
 
     @Override
     public String toString() {
-      return "[flush region " + Bytes.toStringBinary(region.getRegionName()) + "]";
+      return "[flush region " + Bytes.toStringBinary(region.getRegionInfo().getRegionName()) + "]";
     }
 
     @Override
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/MetricsRegionServerWrapperImpl.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/MetricsRegionServerWrapperImpl.java
index 784a4be..ecc4fce 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/MetricsRegionServerWrapperImpl.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/MetricsRegionServerWrapperImpl.java
@@ -152,7 +152,7 @@ class MetricsRegionServerWrapperImpl
 
   @Override
   public long getNumOnlineRegions() {
-    Collection<HRegion> onlineRegionsLocalContext = regionServer.getOnlineRegionsLocalContext();
+    Collection<Region> onlineRegionsLocalContext = regionServer.getOnlineRegionsLocalContext();
     if (onlineRegionsLocalContext == null) {
       return 0;
     }
@@ -438,16 +438,16 @@ class MetricsRegionServerWrapperImpl
       long tempMajorCompactedCellsSize = 0;
       long tempBlockedRequestsCount = 0L;
 
-      for (HRegion r : regionServer.getOnlineRegionsLocalContext()) {
-        tempNumMutationsWithoutWAL += r.numMutationsWithoutWAL.get();
-        tempDataInMemoryWithoutWAL += r.dataInMemoryWithoutWAL.get();
-        tempReadRequestsCount += r.readRequestsCount.get();
-        tempWriteRequestsCount += r.writeRequestsCount.get();
-        tempCheckAndMutateChecksFailed += r.checkAndMutateChecksFailed.get();
-        tempCheckAndMutateChecksPassed += r.checkAndMutateChecksPassed.get();
+      for (Region r : regionServer.getOnlineRegionsLocalContext()) {
+        tempNumMutationsWithoutWAL += r.getNumMutationsWithoutWAL();
+        tempDataInMemoryWithoutWAL += r.getDataInMemoryWithoutWAL();
+        tempReadRequestsCount += r.getReadRequestsCount();
+        tempWriteRequestsCount += r.getWriteRequestsCount();
+        tempCheckAndMutateChecksFailed += r.getCheckAndMutateChecksFailed();
+        tempCheckAndMutateChecksPassed += r.getCheckAndMutateChecksPassed();
         tempBlockedRequestsCount += r.getBlockedRequestsCount();
-        tempNumStores += r.stores.size();
-        for (Store store : r.stores.values()) {
+        tempNumStores += r.getStores().size();
+        for (Store store : r.getStores()) {
           tempNumStoreFiles += store.getStorefilesCount();
           tempMemstoreSize += store.getMemStoreSize();
           tempStoreFileSize += store.getStorefilesSize();
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/MultiRowMutationProcessor.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/MultiRowMutationProcessor.java
index 3b6bdff..68e9bab 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/MultiRowMutationProcessor.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/MultiRowMutationProcessor.java
@@ -70,7 +70,7 @@ MultiRowMutationProcessorResponse> {
 
   @Override
   public void process(long now,
-                      HRegion region,
+                      Region region,
                       List<Mutation> mutationsToApply,
                       WALEdit walEdit) throws IOException {
     byte[] byteNow = Bytes.toBytes(now);
@@ -104,7 +104,7 @@ MultiRowMutationProcessorResponse> {
   }
 
   @Override
-  public void preProcess(HRegion region, WALEdit walEdit) throws IOException {
+  public void preProcess(Region region, WALEdit walEdit) throws IOException {
     RegionCoprocessorHost coprocessorHost = region.getCoprocessorHost();
     if (coprocessorHost != null) {
       for (Mutation m : mutations) {
@@ -126,7 +126,7 @@ MultiRowMutationProcessorResponse> {
   }
 
   @Override
-  public void preBatchMutate(HRegion region, WALEdit walEdit) throws IOException {
+  public void preBatchMutate(Region region, WALEdit walEdit) throws IOException {
     // TODO we should return back the status of this hook run to HRegion so that those Mutations
     // with OperationStatus as SUCCESS or FAILURE should not get applied to memstore.
     RegionCoprocessorHost coprocessorHost = region.getCoprocessorHost();
@@ -155,7 +155,7 @@ MultiRowMutationProcessorResponse> {
   }
 
   @Override
-  public void postBatchMutate(HRegion region) throws IOException {
+  public void postBatchMutate(Region region) throws IOException {
     RegionCoprocessorHost coprocessorHost = region.getCoprocessorHost();
     if (coprocessorHost != null) {
       assert miniBatch != null;
@@ -165,7 +165,7 @@ MultiRowMutationProcessorResponse> {
   }
 
   @Override
-  public void postProcess(HRegion region, WALEdit walEdit, boolean success) throws IOException {
+  public void postProcess(Region region, WALEdit walEdit, boolean success) throws IOException {
     RegionCoprocessorHost coprocessorHost = region.getCoprocessorHost();
     if (coprocessorHost != null) {
       for (Mutation m : mutations) {
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/OnlineRegions.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/OnlineRegions.java
index 1cde7e3..60fc9fb 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/OnlineRegions.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/OnlineRegions.java
@@ -22,46 +22,49 @@ import java.io.IOException;
 import java.util.List;
 
 import org.apache.hadoop.hbase.classification.InterfaceAudience;
+import org.apache.hadoop.hbase.classification.InterfaceStability;
+import org.apache.hadoop.hbase.HBaseInterfaceAudience;
 import org.apache.hadoop.hbase.TableName;
 import org.apache.hadoop.hbase.Server;
 import org.apache.hadoop.hbase.ServerName;
 
 /**
  * Interface to Map of online regions.  In the  Map, the key is the region's
- * encoded name and the value is an {@link HRegion} instance.
+ * encoded name and the value is an {@link Region} instance.
  */
-@InterfaceAudience.Private
-interface OnlineRegions extends Server {
+@InterfaceAudience.LimitedPrivate(HBaseInterfaceAudience.COPROC)
+@InterfaceStability.Evolving
+public interface OnlineRegions extends Server {
   /**
    * Add to online regions.
    * @param r
    */
-  void addToOnlineRegions(final HRegion r);
+  void addToOnlineRegions(final Region r);
 
   /**
-   * This method removes HRegion corresponding to hri from the Map of onlineRegions.
+   * This method removes Region corresponding to hri from the Map of onlineRegions.
    *
    * @param r Region to remove.
    * @param destination Destination, if any, null otherwise.
    * @return True if we removed a region from online list.
    */
-  boolean removeFromOnlineRegions(final HRegion r, ServerName destination);
+  boolean removeFromOnlineRegions(final Region r, ServerName destination);
 
   /**
-   * Return {@link HRegion} instance.
-   * Only works if caller is in same context, in same JVM. HRegion is not
+   * Return {@link Region} instance.
+   * Only works if caller is in same context, in same JVM. Region is not
    * serializable.
    * @param encodedRegionName
-   * @return HRegion for the passed encoded <code>encodedRegionName</code> or
+   * @return Region for the passed encoded <code>encodedRegionName</code> or
    * null if named region is not member of the online regions.
    */
-  HRegion getFromOnlineRegions(String encodedRegionName);
+  Region getFromOnlineRegions(String encodedRegionName);
 
    /**
     * Get all online regions of a table in this RS.
     * @param tableName
-    * @return List of HRegion
+    * @return List of Region
     * @throws java.io.IOException
     */
-   List<HRegion> getOnlineRegions(TableName tableName) throws IOException;
+   List<Region> getOnlineRegions(TableName tableName) throws IOException;
 }
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/Region.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/Region.java
new file mode 100644
index 0000000..2a35671
--- /dev/null
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/Region.java
@@ -0,0 +1,560 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.hbase.regionserver;
+
+import java.io.IOException;
+import java.util.Collection;
+import java.util.List;
+import java.util.Map;
+
+import org.apache.hadoop.hbase.Cell;
+import org.apache.hadoop.hbase.HBaseInterfaceAudience;
+import org.apache.hadoop.hbase.HConstants;
+import org.apache.hadoop.hbase.HDFSBlocksDistribution;
+import org.apache.hadoop.hbase.HRegionInfo;
+import org.apache.hadoop.hbase.HTableDescriptor;
+import org.apache.hadoop.hbase.classification.InterfaceAudience;
+import org.apache.hadoop.hbase.classification.InterfaceStability;
+import org.apache.hadoop.hbase.client.Append;
+import org.apache.hadoop.hbase.client.Delete;
+import org.apache.hadoop.hbase.client.Get;
+import org.apache.hadoop.hbase.client.Increment;
+import org.apache.hadoop.hbase.client.IsolationLevel;
+import org.apache.hadoop.hbase.client.Mutation;
+import org.apache.hadoop.hbase.client.Put;
+import org.apache.hadoop.hbase.client.Result;
+import org.apache.hadoop.hbase.client.RowMutations;
+import org.apache.hadoop.hbase.client.Scan;
+import org.apache.hadoop.hbase.exceptions.FailedSanityCheckException;
+import org.apache.hadoop.hbase.filter.ByteArrayComparable;
+import org.apache.hadoop.hbase.filter.CompareFilter.CompareOp;
+import org.apache.hadoop.hbase.protobuf.generated.ClientProtos.CoprocessorServiceCall;
+import org.apache.hadoop.hbase.regionserver.wal.HLogSplitter;
+import org.apache.hadoop.hbase.util.Pair;
+
+import com.google.protobuf.Message;
+import com.google.protobuf.RpcController;
+import com.google.protobuf.Service;
+
+/**
+ * Regions store data for a certain region of a table.  It stores all columns
+ * for each row. A given table consists of one or more Regions.
+ *
+ * <p>An Region is defined by its table and its key extent.
+ *
+ * <p>Locking at the Region level serves only one purpose: preventing the
+ * region from being closed (and consequently split) while other operations
+ * are ongoing. Each row level operation obtains both a row lock and a region
+ * read lock for the duration of the operation. While a scanner is being
+ * constructed, getScanner holds a read lock. If the scanner is successfully
+ * constructed, it holds a read lock until it is closed. A close takes out a
+ * write lock and consequently will block for ongoing operations and will block
+ * new operations from starting while the close is in progress.
+ */
+@InterfaceAudience.LimitedPrivate(HBaseInterfaceAudience.COPROC)
+@InterfaceStability.Evolving
+public interface Region {
+
+  ///////////////////////////////////////////////////////////////////////////
+  // Region state
+
+  /** @return region information for this region */
+  HRegionInfo getRegionInfo();
+
+  /** @return table descriptor for this region */
+  HTableDescriptor getTableDesc();
+
+  /** @return true if region is available (not closed and not closing) */
+  boolean isAvailable();
+
+  /** @return true if region is closed */
+  boolean isClosed();
+
+  /** @return True if closing process has started */
+  boolean isClosing();
+
+  /** @return True if region is in recovering state */
+  boolean isRecovering();
+
+  /**
+   * Return the list of Stores managed by this region
+   * <p>Use with caution.  Exposed for use of fixup utilities.
+   * @return a list of the Stores managed by this region
+   */
+  List<Store> getStores();
+
+  /**
+   * Return the Store for the given family
+   * <p>Use with caution.  Exposed for use of fixup utilities.
+   * @return the Store for the given family
+   */
+  Store getStore(byte[] family);
+
+  /** @return list of store file names for the given families */
+  List<String> getStoreFileList(byte [][] columns);
+
+  /** @return the latest sequence number that was read from storage when this region was opened */
+  long getOpenSeqNum();
+
+  /** @return the last flushed sequence id for this region */
+  long getCompleteSequenceId();
+
+  /** @return the last time the region was flushed */
+  long getLastFlushTime();
+
+  /** @return the current read point for the given IsolationLevel */
+  long getReadpoint(IsolationLevel isolationLevel);
+
+  /**
+   * @return max sequence ids of stores that was read from storage when this region was opened.
+   * <p>WALEdits with smaller or equal sequence number will be skipped from replay.
+   */
+  Map<byte[], Long> getMaxStoreSeqIdForLogReplay();
+
+  /** @return true if configured to load CFs on demand by default */
+  boolean isLoadingCfsOnDemandDefault();
+
+  ///////////////////////////////////////////////////////////////////////////
+  // Metrics
+  
+  /** @return read requests count for this region */
+  long getReadRequestsCount();
+
+  /**
+   * Update the read request count for this region
+   * @param i increment
+   */
+  void updateReadRequestsCount(long i);
+
+  /** @return write request count for this region */
+  long getWriteRequestsCount();
+
+  /**
+   * Update the write request count for this region
+   * @param i increment
+   */
+  void updateWriteRequestsCount(long i);
+
+  /** @return memstore size for this region, in bytes */
+  long getMemstoreSize();
+
+  /** @return the number of mutations processed bypassing the WAL */
+  long getNumMutationsWithoutWAL();
+  
+  /** @return the size of data processed bypassing the WAL, in bytes */
+  long getDataInMemoryWithoutWAL();
+
+  /** @return the number of blocked requests */
+  long getBlockedRequestsCount();
+
+  /** @return the number of checkAndMutate guards that passed */
+  long getCheckAndMutateChecksPassed();
+
+  /** @return the number of failed checkAndMutate guards */
+  long getCheckAndMutateChecksFailed();
+
+  /** @return the MetricsRegion for this region */
+  MetricsRegion getMetrics();
+
+  /** @return the block distribution for all Stores managed by this region */
+  HDFSBlocksDistribution getHDFSBlocksDistribution();
+
+  ///////////////////////////////////////////////////////////////////////////
+  // Locking
+
+  // Region read locks
+
+  /**
+   * Operation enum is used in {@link Region#startRegionOperation} to provide context for
+   * various checks before any region operation begins.
+   */
+  enum Operation {
+    ANY, GET, PUT, DELETE, SCAN, APPEND, INCREMENT, SPLIT_REGION, MERGE_REGION, BATCH_MUTATE,
+    REPLAY_BATCH_MUTATE, COMPACT_REGION
+  }
+
+  /**
+   * This method needs to be called before any public call that reads or
+   * modifies data. 
+   * Acquires a read lock and checks if the region is closing or closed.
+   * <p>{@link #closeRegionOperation} MUST then always be called after
+   * the operation has completed, whether it succeeded or failed.
+   * @param op The operation is about to be taken on the region
+   * @throws IOException
+   */
+  void startRegionOperation(Operation op) throws IOException;
+
+  /**
+   * Closes the region operation lock.
+   * @throws IOException
+   */
+  void closeRegionOperation() throws IOException;
+
+  // Row write locks
+
+  /**
+   * Row lock held by a given thread.
+   * One thread may acquire multiple locks on the same row simultaneously.
+   * The locks must be released by calling release() from the same thread.
+   */
+  public interface RowLock {
+    /**
+     * Release the given lock.  If there are no remaining locks held by the current thread
+     * then unlock the row and allow other threads to acquire the lock.
+     * @throws IllegalArgumentException if called by a different thread than the lock owning
+     *     thread
+     */
+    void release();
+  }
+
+  /**
+   * Tries to acquire a lock on the given row.
+   * @param waitForLock if true, will block until the lock is available.
+   *        Otherwise, just tries to obtain the lock and returns
+   *        false if unavailable.
+   * @return the row lock if acquired,
+   *   null if waitForLock was false and the lock was not acquired
+   * @throws IOException if waitForLock was true and the lock could not be acquired after waiting
+   */
+  RowLock getRowLock(byte[] row, boolean waitForLock) throws IOException;
+
+  /**
+   * If the given list of row locks is not null, releases all locks.
+   */
+  void releaseRowLocks(List<RowLock> rowLocks);
+
+  ///////////////////////////////////////////////////////////////////////////
+  // Region operations
+
+  /**
+   * Perform one or more append operations on a row.
+   * @param append
+   * @param nonceGroup
+   * @param nonce
+   * @return
+   * @throws IOException
+   */
+  Result append(Append append, long nonceGroup, long nonce) throws IOException;
+
+  /**
+   * Perform a batch of mutations.
+   * <p>
+   * Note this supports only Put and Delete mutations and will ignore other types passed.
+   * @param mutations the list of mutations
+   * @param nonceGroup
+   * @param nonce
+   * @return an array of OperationStatus which internally contains the
+   *         OperationStatusCode and the exceptionMessage if any.
+   * @throws IOException
+   */
+  OperationStatus[] batchMutate(Mutation[] mutations, long nonceGroup, long nonce)
+      throws IOException;
+
+  /**
+   * Replay a batch of mutations.
+   * @param mutations mutations to replay.
+   * @return an array of OperationStatus which internally contains the
+   *         OperationStatusCode and the exceptionMessage if any.
+   * @throws IOException
+   */
+  OperationStatus[] batchReplay(HLogSplitter.MutationReplay[] mutations) throws IOException;
+
+  /**
+   * Atomically checks if a row/family/qualifier value matches the expected val
+   * If it does, it performs the row mutations.  If the passed value is null, t
+   * is for the lack of column (ie: non-existence)
+   * @param row to check
+   * @param family column family to check
+   * @param qualifier column qualifier to check
+   * @param compareOp the comparison operator
+   * @param comparator
+   * @param put
+   * @param writeToWAL
+   * @return
+   * @throws IOException
+   */
+  boolean checkAndMutate(byte [] row, byte [] family, byte [] qualifier, CompareOp compareOp,
+      ByteArrayComparable comparator, Mutation w, boolean writeToWAL) throws IOException;
+
+  /**
+   * Atomically checks if a row/family/qualifier value matches the expected val
+   * If it does, it performs the row mutations.  If the passed value is null, t
+   * is for the lack of column (ie: non-existence)
+   * @param row to check
+   * @param family column family to check
+   * @param qualifier column qualifier to check
+   * @param compareOp the comparison operator
+   * @param comparator
+   * @param rm
+   * @param writeToWAL
+   * @return
+   * @throws IOException
+   */
+  boolean checkAndRowMutate(byte [] row, byte [] family, byte [] qualifier, CompareOp compareOp,
+      ByteArrayComparable comparator, RowMutations rm, boolean writeToWAL) throws IOException;
+
+  /**
+   * Deletes the specified cells/row.
+   * @param delete
+   * @throws IOException
+   */
+  void delete(Delete delete) throws IOException;
+
+  /**
+   * Do a get based on the get parameter.
+   * @param get
+   */
+  Result get(Get get) throws IOException;
+
+  /**
+   * Do a get based on the get parameter.
+   * @param get
+   * @param withCoprocessor invoke coprocessor or not. We don't want to
+   * always invoke cp.
+   */
+  List<Cell> get(Get get, boolean withCoprocessor) throws IOException;
+
+  /**
+   * Return all the data for the row that matches <i>row</i> exactly,
+   * or the one that immediately preceeds it, at or immediately before
+   * <i>ts</i>.
+   * @param row
+   * @param family
+   * @return
+   * @throws IOException
+   */
+  Result getClosestRowBefore(byte[] row, byte[] family) throws IOException;
+
+  /**
+   * Return an iterator that scans over the HRegion, returning the indicated
+   * columns and rows specified by the {@link Scan}.
+   * <p>
+   * This Iterator must be closed by the caller.
+   *
+   * @param scan configured {@link Scan}
+   * @return RegionScanner
+   * @throws IOException read exceptions
+   */
+  RegionScanner getScanner(Scan scan) throws IOException;
+
+  /**
+   * Perform one or more increment operations on a row.
+   * @param increment
+   * @param nonceGroup
+   * @param nonce
+   * @return
+   * @throws IOException
+   */
+  Result increment(Increment increment, long nonceGroup, long nonce) throws IOException;
+
+  /**
+   * Performs multiple mutations atomically on a single row. Currently
+   * {@link Put} and {@link Delete} are supported.
+   *
+   * @param rm object that specifies the set of mutations to perform atomically
+   * @throws IOException
+   */
+  void mutateRow(RowMutations rm) throws IOException;
+
+  /**
+   * Perform atomic mutations within the region.
+   * 
+   * @param mutations The list of mutations to perform.
+   * <code>mutations</code> can contain operations for multiple rows.
+   * Caller has to ensure that all rows are contained in this region.
+   * @param rowsToLock Rows to lock
+   * @param nonceGroup Optional nonce group of the operation (client Id)
+   * @param nonce Optional nonce of the operation (unique random id to ensure "more idempotence")
+   * If multiple rows are locked care should be taken that
+   * <code>rowsToLock</code> is sorted in order to avoid deadlocks.
+   * @throws IOException
+   */
+  void mutateRowsWithLocks(Collection<Mutation> mutations, Collection<byte[]> rowsToLock,
+      long nonceGroup, long nonce) throws IOException;
+
+  /**
+   * Performs atomic multiple reads and writes on a given row.
+   *
+   * @param processor The object defines the reads and writes to a row.
+   * @param nonceGroup Optional nonce group of the operation (client Id)
+   * @param nonce Optional nonce of the operation (unique random id to ensure "more idempotence")
+   */
+  void processRowsWithLocks(RowProcessor<?,?> processor, long nonceGroup, long nonce)
+      throws IOException;
+
+  /**
+   * Performs atomic multiple reads and writes on a given row.
+   *
+   * @param processor The object defines the reads and writes to a row.
+   * @param timeout The timeout of the processor.process() execution
+   *                Use a negative number to switch off the time bound
+   * @param nonceGroup Optional nonce group of the operation (client Id)
+   * @param nonce Optional nonce of the operation (unique random id to ensure "more idempotence")
+   */
+  void processRowsWithLocks(RowProcessor<?,?> processor, long timeout, long nonceGroup, long nonce)
+      throws IOException;
+
+  /**
+   * Puts some data in the table.
+   * @param put
+   * @throws IOException
+   */
+  void put(Put put) throws IOException;
+
+  /**
+   * Listener class to enable callers of
+   * bulkLoadHFile() to perform any necessary
+   * pre/post processing of a given bulkload call
+   */
+  public interface BulkLoadListener {
+
+    /**
+     * Called before an HFile is actually loaded
+     * @param family family being loaded to
+     * @param srcPath path of HFile
+     * @return final path to be used for actual loading
+     * @throws IOException
+     */
+    String prepareBulkLoad(byte[] family, String srcPath) throws IOException;
+
+    /**
+     * Called after a successful HFile load
+     * @param family family being loaded to
+     * @param srcPath path of HFile
+     * @throws IOException
+     */
+    void doneBulkLoad(byte[] family, String srcPath) throws IOException;
+
+    /**
+     * Called after a failed HFile load
+     * @param family family being loaded to
+     * @param srcPath path of HFile
+     * @throws IOException
+     */
+    void failedBulkLoad(byte[] family, String srcPath) throws IOException;
+  }
+
+  /**
+   * Attempts to atomically load a group of hfiles.  This is critical for loading
+   * rows with multiple column families atomically.
+   *
+   * @param familyPaths List of Pair<byte[] column family, String hfilePath>
+   * @param bulkLoadListener Internal hooks enabling massaging/preparation of a
+   * file about to be bulk loaded
+   * @param assignSeqId
+   * @return true if successful, false if failed recoverably
+   * @throws IOException if failed unrecoverably.
+   */
+  public boolean bulkLoadHFiles(Collection<Pair<byte[], String>> familyPaths, boolean assignSeqId,
+      BulkLoadListener bulkLoadListener) throws IOException;
+
+  ///////////////////////////////////////////////////////////////////////////
+  // Coprocessors
+
+  /** @return the coprocessor host */
+  public RegionCoprocessorHost getCoprocessorHost();
+
+  /**
+   * Executes a single protocol buffer coprocessor endpoint {@link Service} method using
+   * the registered protocol handlers.  {@link Service} implementations must be registered via the
+   * {@link Region#registerService(com.google.protobuf.Service)}
+   * method before they are available.
+   *
+   * @param controller an {@code RpcContoller} implementation to pass to the invoked service
+   * @param call a {@code CoprocessorServiceCall} instance identifying the service, method,
+   *     and parameters for the method invocation
+   * @return a protocol buffer {@code Message} instance containing the method's result
+   * @throws IOException if no registered service handler is found or an error
+   *     occurs during the invocation
+   * @see org.apache.hadoop.hbase.regionserver.Region#registerService(com.google.protobuf.Service)
+   */
+  public Message execService(RpcController controller, CoprocessorServiceCall call)
+    throws IOException;
+
+  /**
+   * Registers a new protocol buffer {@link Service} subclass as a coprocessor endpoint to
+   * be available for handling
+   * {@link Region#execService(com.google.protobuf.RpcController,
+   *    org.apache.hadoop.hbase.protobuf.generated.ClientProtos.CoprocessorServiceCall)}} calls.
+   *
+   * <p>
+   * Only a single instance may be registered per region for a given {@link Service} subclass (the
+   * instances are keyed on {@link com.google.protobuf.Descriptors.ServiceDescriptor#getFullName()}.
+   * After the first registration, subsequent calls with the same service name will fail with
+   * a return value of {@code false}.
+   * </p>
+   * @param instance the {@code Service} subclass instance to expose as a coprocessor endpoint
+   * @return {@code true} if the registration was successful, {@code false}
+   * otherwise
+   */
+  public boolean registerService(Service instance);
+
+  ///////////////////////////////////////////////////////////////////////////
+  // RowMutation processor support
+
+  /**
+   * Check the collection of families for validity.
+   * @param families
+   * @throws NoSuchColumnFamilyException
+   */
+  public void checkFamilies(Collection<byte[]> families) throws NoSuchColumnFamilyException;
+
+  /**
+   * Check the collection of families for valid timestamps
+   * @param familyMap
+   * @param now current timestamp
+   * @throws FailedSanityCheckException
+   */
+  public void checkTimestamps(Map<byte[], List<Cell>> familyMap, long now)
+      throws FailedSanityCheckException;
+
+  /**
+   * Prepare a delete for a row mutation processor
+   * @param delete The passed delete is modified by this method. WARNING!
+   * @throws IOException
+   */
+  public void prepareDelete(Delete delete) throws IOException;
+
+  /**
+   * Set up correct timestamps in the KVs in Delete object.
+   * <p>Caller should have the row and region locks.
+   * @param mutation
+   * @param familyCellMap
+   * @param now
+   * @throws IOException
+   */
+  public void prepareDeleteTimestamps(Mutation mutation, Map<byte[], List<Cell>> familyCellMap,
+      byte[] now) throws IOException;
+
+  /**
+   * Replace any KV timestamps set to {@link HConstants#LATEST_TIMESTAMP} with the
+   * provided current timestamp.
+   * @param values
+   * @param now
+   */
+  public void updateKVTimestamps(Iterable<List<Cell>> keyLists, byte[] now);
+
+  ///////////////////////////////////////////////////////////////////////////
+  // Compactions, splits, etc.
+
+  /**
+   * Trigger a major compaction on the region 
+   */
+  public void triggerMajorCompaction(); // Exposed because Store does too
+
+}
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/RegionCoprocessorHost.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/RegionCoprocessorHost.java
index f45e89c..9964f7c7 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/RegionCoprocessorHost.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/RegionCoprocessorHost.java
@@ -45,7 +45,6 @@ import org.apache.hadoop.fs.FileSystem;
 import org.apache.hadoop.fs.Path;
 import org.apache.hadoop.hbase.Cell;
 import org.apache.hadoop.hbase.Coprocessor;
-import org.apache.hadoop.hbase.CoprocessorEnvironment;
 import org.apache.hadoop.hbase.HBaseConfiguration;
 import org.apache.hadoop.hbase.HBaseInterfaceAudience;
 import org.apache.hadoop.hbase.HConstants;
@@ -73,7 +72,7 @@ import org.apache.hadoop.hbase.io.FSDataInputStreamWrapper;
 import org.apache.hadoop.hbase.io.ImmutableBytesWritable;
 import org.apache.hadoop.hbase.io.Reference;
 import org.apache.hadoop.hbase.io.hfile.CacheConfig;
-import org.apache.hadoop.hbase.regionserver.HRegion.Operation;
+import org.apache.hadoop.hbase.regionserver.Region.Operation;
 import org.apache.hadoop.hbase.regionserver.compactions.CompactionRequest;
 import org.apache.hadoop.hbase.regionserver.wal.HLogKey;
 import org.apache.hadoop.hbase.regionserver.wal.WALEdit;
@@ -380,31 +379,6 @@ public class RegionCoprocessorHost
   }
 
   /**
-   * HBASE-4014 : This is used by coprocessor hooks which are not declared to throw exceptions.
-   *
-   * For example, {@link
-   * org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost#preOpen()} and
-   * {@link org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost#postOpen()} are such hooks.
-   *
-   * See also
-   * {@link org.apache.hadoop.hbase.master.MasterCoprocessorHost#handleCoprocessorThrowable(
-   *    CoprocessorEnvironment, Throwable)}
-   * @param env The coprocessor that threw the exception.
-   * @param e The exception that was thrown.
-   */
-  private void handleCoprocessorThrowableNoRethrow(
-      final CoprocessorEnvironment env, final Throwable e) {
-    try {
-      handleCoprocessorThrowable(env,e);
-    } catch (IOException ioe) {
-      // We cannot throw exceptions from the caller hook, so ignore.
-      LOG.warn(
-        "handleCoprocessorThrowable() threw an IOException while attempting to handle Throwable " +
-        e + ". Ignoring.",e);
-    }
-  }
-
-  /**
    * Invoked before a region open.
    *
    * @throws IOException Signals that an I/O exception has occurred.
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/RegionMergeRequest.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/RegionMergeRequest.java
index 5e3db0c..ba30606 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/RegionMergeRequest.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/RegionMergeRequest.java
@@ -36,13 +36,13 @@ import com.google.common.base.Preconditions;
 @InterfaceAudience.Private
 class RegionMergeRequest implements Runnable {
   static final Log LOG = LogFactory.getLog(RegionMergeRequest.class);
-  private final HRegion region_a;
-  private final HRegion region_b;
+  private final Region region_a;
+  private final Region region_b;
   private final HRegionServer server;
   private final boolean forcible;
   private TableLock tableLock;
 
-  RegionMergeRequest(HRegion a, HRegion b, HRegionServer hrs, boolean forcible) {
+  RegionMergeRequest(Region a, Region b, HRegionServer hrs, boolean forcible) {
     Preconditions.checkNotNull(hrs);
     this.region_a = a;
     this.region_b = b;
@@ -70,8 +70,9 @@ class RegionMergeRequest implements Runnable {
 
       //acquire a shared read lock on the table, so that table schema modifications
       //do not happen concurrently
-      tableLock = server.getTableLockManager().readLock(region_a.getTableDesc().getTableName()
-          , "MERGE_REGIONS:" + region_a.getRegionNameAsString() + ", " + region_b.getRegionNameAsString());
+      tableLock = server.getTableLockManager().readLock(region_a.getTableDesc().getTableName(),
+        "MERGE_REGIONS:" + region_a.getRegionInfo().getRegionNameAsString() + ", " +
+        region_b.getRegionInfo().getRegionNameAsString());
       try {
         tableLock.acquire();
       } catch (IOException ex) {
@@ -132,9 +133,9 @@ class RegionMergeRequest implements Runnable {
         this.tableLock.release();
       } catch (IOException ex) {
         LOG.error("Could not release the table lock (something is really wrong). " 
-           + "Aborting this server to avoid holding the lock forever.");
+          + "Aborting this server to avoid holding the lock forever.");
         this.server.abort("Abort; we got an error when releasing the table lock "
-                         + "on " + region_a.getRegionNameAsString());
+          + "on " + region_a.getRegionInfo().getRegionNameAsString());
       }
     }
   }
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/RegionMergeTransaction.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/RegionMergeTransaction.java
index 054da41..bb0d462 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/RegionMergeTransaction.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/RegionMergeTransaction.java
@@ -42,7 +42,6 @@ import org.apache.hadoop.hbase.catalog.CatalogTracker;
 import org.apache.hadoop.hbase.catalog.MetaEditor;
 import org.apache.hadoop.hbase.catalog.MetaReader;
 import org.apache.hadoop.hbase.client.Delete;
-import org.apache.hadoop.hbase.client.HTable;
 import org.apache.hadoop.hbase.client.Mutation;
 import org.apache.hadoop.hbase.client.Put;
 import org.apache.hadoop.hbase.executor.EventType;
@@ -159,14 +158,14 @@ public class RegionMergeTransaction {
    * @param b region b to merge
    * @param forcible if false, we will only merge adjacent regions
    */
-  public RegionMergeTransaction(final HRegion a, final HRegion b,
+  public RegionMergeTransaction(final Region a, final Region b,
       final boolean forcible) {
     if (a.getRegionInfo().compareTo(b.getRegionInfo()) <= 0) {
-      this.region_a = a;
-      this.region_b = b;
+      this.region_a = (HRegion)a;
+      this.region_b = (HRegion)b;
     } else {
-      this.region_a = b;
-      this.region_b = a;
+      this.region_a = (HRegion)b;
+      this.region_b = (HRegion)a;
     }
     this.forcible = forcible;
     this.mergesdir = region_a.getRegionFileSystem().getMergesDir();
@@ -185,15 +184,15 @@ public class RegionMergeTransaction {
           + " because they do not belong to the same table");
       return false;
     }
-    if (region_a.getRegionInfo().equals(region_b.getRegionInfo())) {
+    HRegionInfo hriA = region_a.getRegionInfo();
+    HRegionInfo hriB = region_b.getRegionInfo();
+    if (hriA.equals(hriB)) {
       LOG.info("Can't merge the same region " + region_a);
       return false;
     }
-    if (!forcible && !HRegionInfo.areAdjacent(region_a.getRegionInfo(),
-            region_b.getRegionInfo())) {
-      String msg = "Skip merging " + this.region_a.getRegionNameAsString()
-          + " and " + this.region_b.getRegionNameAsString()
-          + ", because they are not adjacent.";
+    if (!forcible && !HRegionInfo.areAdjacent(hriA, hriB)) {
+      String msg = "Skip merging " + hriA.getRegionNameAsString() + " and " +
+        hriB.getRegionNameAsString() + ", because they are not adjacent.";
       LOG.info(msg);
       return false;
     }
@@ -202,18 +201,18 @@ public class RegionMergeTransaction {
     }
     try {
       boolean regionAHasMergeQualifier = hasMergeQualifierInMeta(services,
-          region_a.getRegionName());
+          hriA.getRegionName());
       if (regionAHasMergeQualifier ||
-          hasMergeQualifierInMeta(services, region_b.getRegionName())) {
-        LOG.debug("Region " + (regionAHasMergeQualifier ? region_a.getRegionNameAsString()
-                : region_b.getRegionNameAsString())
+          hasMergeQualifierInMeta(services, hriB.getRegionName())) {
+        LOG.debug("Region " + (regionAHasMergeQualifier ? hriA.getRegionNameAsString()
+                : hriB.getRegionNameAsString())
             + " is not mergeable because it has merge qualifier in META");
         return false;
       }
     } catch (IOException e) {
       LOG.warn("Failed judging whether merge transaction is available for "
-              + region_a.getRegionNameAsString() + " and "
-              + region_b.getRegionNameAsString(), e);
+              + hriA.getRegionNameAsString() + " and "
+              + hriB.getRegionNameAsString(), e);
       return false;
     }
 
@@ -224,8 +223,7 @@ public class RegionMergeTransaction {
     // Since HBASE-7721, we don't need fix up daughters any more. so here do
     // nothing
 
-    this.mergedRegionInfo = getMergedRegionInfo(region_a.getRegionInfo(),
-        region_b.getRegionInfo());
+    this.mergedRegionInfo = getMergedRegionInfo(hriA, hriB);
     return true;
   }
 
@@ -273,7 +271,7 @@ public class RegionMergeTransaction {
   HRegion createMergedRegion(final Server server,
       final RegionServerServices services) throws IOException {
     LOG.info("Starting merge of " + region_a + " and "
-        + region_b.getRegionNameAsString() + ", forcible=" + forcible);
+        + region_b.getRegionInfo().getRegionNameAsString() + ", forcible=" + forcible);
     if ((server != null && server.isStopped())
         || (services != null && services.isStopping())) {
       throw new IOException("Server is stopped or stopping");
@@ -365,7 +363,6 @@ public class RegionMergeTransaction {
     addLocation(putOfMerged, serverName, 1);
   }
 
-  @SuppressWarnings("deprecation")
   public Put addLocation(final Put p, final ServerName sn, long openSeqNum) {
     p.add(HConstants.CATALOG_FAMILY, HConstants.SERVER_QUALIFIER, Bytes
         .toBytes(sn.getHostAndPort()));
@@ -439,8 +436,8 @@ public class RegionMergeTransaction {
     // stuff in fs that needs cleanup -- a storefile or two. Thats why we
     // add entry to journal BEFORE rather than AFTER the change.
     this.journal.add(JournalEntry.STARTED_MERGED_REGION_CREATION);
-    HRegion mergedRegion = createMergedRegionFromMerges(this.region_a,
-        this.region_b, this.mergedRegionInfo);
+    HRegion mergedRegion = this.region_a.createMergedRegionFromMerges(
+      this.mergedRegionInfo, this.region_b);
     return mergedRegion;
   }
 
@@ -557,7 +554,7 @@ public class RegionMergeTransaction {
     boolean stopped = server != null && server.isStopped();
     boolean stopping = services != null && services.isStopping();
     if (stopped || stopping) {
-      LOG.info("Not opening merged region  " + merged.getRegionNameAsString()
+      LOG.info("Not opening merged region  " + merged.getRegionInfo().getRegionNameAsString()
           + " because stopping=" + stopping + ", stopped=" + stopped);
       return;
     }
@@ -565,7 +562,7 @@ public class RegionMergeTransaction {
     LoggingProgressable reporter = server == null ? null
         : new LoggingProgressable(hri, server.getConfiguration().getLong(
             "hbase.regionserver.regionmerge.open.log.interval", 10000));
-    merged.openHRegion(reporter);
+    ((HRegion)merged).openHRegion(reporter);
 
     if (services != null) {
       try {
@@ -795,7 +792,7 @@ public class RegionMergeTransaction {
             this.region_a.initialize();
           } catch (IOException e) {
             LOG.error("Failed rollbacking CLOSED_REGION_A of region "
-                + this.region_a.getRegionNameAsString(), e);
+                + this.region_a.getRegionInfo().getRegionNameAsString(), e);
             throw new RuntimeException(e);
           }
           break;
@@ -810,7 +807,7 @@ public class RegionMergeTransaction {
             this.region_b.initialize();
           } catch (IOException e) {
             LOG.error("Failed rollbacking CLOSED_REGION_A of region "
-                + this.region_b.getRegionNameAsString(), e);
+                + this.region_b.getRegionInfo().getRegionNameAsString(), e);
             throw new RuntimeException(e);
           }
           break;
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/RegionServerCoprocessorHost.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/RegionServerCoprocessorHost.java
index 988e39a..4b8bdc0 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/RegionServerCoprocessorHost.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/RegionServerCoprocessorHost.java
@@ -90,7 +90,7 @@ public class RegionServerCoprocessorHost extends
     });
   }
 
-  public boolean preMerge(final HRegion regionA, final HRegion regionB) throws IOException {
+  public boolean preMerge(final Region regionA, final Region regionB) throws IOException {
     return execOperation(coprocessors.isEmpty() ? null : new CoprocessorOperation() {
       @Override
       public void call(RegionServerObserver oserver,
@@ -100,7 +100,7 @@ public class RegionServerCoprocessorHost extends
     });
   }
 
-  public void postMerge(final HRegion regionA, final HRegion regionB, final HRegion mergedRegion)
+  public void postMerge(final Region regionA, final Region regionB, final Region mergedRegion)
       throws IOException {
     execOperation(coprocessors.isEmpty() ? null : new CoprocessorOperation() {
       @Override
@@ -111,7 +111,7 @@ public class RegionServerCoprocessorHost extends
     });
   }
 
-  public boolean preMergeCommit(final HRegion regionA, final HRegion regionB,
+  public boolean preMergeCommit(final Region regionA, final Region regionB,
       final @MetaMutationAnnotation List<Mutation> metaEntries) throws IOException {
     return execOperation(coprocessors.isEmpty() ? null : new CoprocessorOperation() {
       @Override
@@ -122,8 +122,8 @@ public class RegionServerCoprocessorHost extends
     });
   }
 
-  public void postMergeCommit(final HRegion regionA, final HRegion regionB,
-      final HRegion mergedRegion) throws IOException {
+  public void postMergeCommit(final Region regionA, final Region regionB,
+      final Region mergedRegion) throws IOException {
     execOperation(coprocessors.isEmpty() ? null : new CoprocessorOperation() {
       @Override
       public void call(RegionServerObserver oserver,
@@ -133,7 +133,7 @@ public class RegionServerCoprocessorHost extends
     });
   }
 
-  public void preRollBackMerge(final HRegion regionA, final HRegion regionB) throws IOException {
+  public void preRollBackMerge(final Region regionA, final Region regionB) throws IOException {
     execOperation(coprocessors.isEmpty() ? null : new CoprocessorOperation() {
       @Override
       public void call(RegionServerObserver oserver,
@@ -143,7 +143,7 @@ public class RegionServerCoprocessorHost extends
     });
   }
 
-  public void postRollBackMerge(final HRegion regionA, final HRegion regionB) throws IOException {
+  public void postRollBackMerge(final Region regionA, final Region regionB) throws IOException {
     execOperation(coprocessors.isEmpty() ? null : new CoprocessorOperation() {
       @Override
       public void call(RegionServerObserver oserver,
@@ -301,7 +301,7 @@ public class RegionServerCoprocessorHost extends
         final Configuration conf, final RegionServerServices services) {
       super(impl, priority, seq, conf);
       this.regionServerServices = services;
-      for (Class c : implClass.getInterfaces()) {
+      for (Class<?> c : implClass.getInterfaces()) {
         if (SingletonCoprocessorService.class.isAssignableFrom(c)) {
           this.regionServerServices.registerService(((SingletonCoprocessorService) impl).getService());
           break;
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/RegionServerServices.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/RegionServerServices.java
index d331840..59426ee 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/RegionServerServices.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/RegionServerServices.java
@@ -19,7 +19,9 @@
 package org.apache.hadoop.hbase.regionserver;
 
 import org.apache.hadoop.hbase.classification.InterfaceAudience;
+import org.apache.hadoop.hbase.classification.InterfaceStability;
 import org.apache.hadoop.fs.FileSystem;
+import org.apache.hadoop.hbase.HBaseInterfaceAudience;
 import org.apache.hadoop.hbase.HRegionInfo;
 import org.apache.hadoop.hbase.catalog.CatalogTracker;
 import org.apache.hadoop.hbase.executor.ExecutorService;
@@ -39,7 +41,8 @@ import java.util.concurrent.ConcurrentMap;
 /**
  * Services provided by {@link HRegionServer}
  */
-@InterfaceAudience.Private
+@InterfaceAudience.LimitedPrivate(HBaseInterfaceAudience.COPROC)
+@InterfaceStability.Evolving
 public interface RegionServerServices
     extends OnlineRegions, FavoredNodesForRegion, PriorityFunction {
   /**
@@ -80,7 +83,7 @@ public interface RegionServerServices
    * @throws KeeperException
    * @throws IOException
    */
-  void postOpenDeployTasks(final HRegion r, final CatalogTracker ct)
+  void postOpenDeployTasks(final Region r, final CatalogTracker ct)
   throws KeeperException, IOException;
 
   /**
@@ -127,7 +130,7 @@ public interface RegionServerServices
   /**
    * @return set of recovering regions on the hosting region server
    */
-  Map<String, HRegion> getRecoveringRegions();
+  Map<String, Region> getRecoveringRegions();
 
   /**
    * Only required for "old" log replay; if it's removed, remove this.
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/RegionSplitPolicy.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/RegionSplitPolicy.java
index 77611da..1d8f567 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/RegionSplitPolicy.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/RegionSplitPolicy.java
@@ -18,7 +18,7 @@
 package org.apache.hadoop.hbase.regionserver;
 
 import java.io.IOException;
-import java.util.Map;
+import java.util.Collection;
 
 import org.apache.hadoop.hbase.classification.InterfaceAudience;
 import org.apache.hadoop.conf.Configuration;
@@ -74,11 +74,10 @@ public abstract class RegionSplitPolicy extends Configured {
     if (explicitSplitPoint != null) {
       return explicitSplitPoint;
     }
-    Map<byte[], Store> stores = region.getStores();
-
+    Collection<Store> stores = region.getStores();
     byte[] splitPointFromLargestStore = null;
     long largestStoreSize = 0;
-    for (Store s : stores.values()) {
+    for (Store s : stores) {
       byte[] splitPoint = s.getSplitPoint();
       long storeSize = s.getSize();
       if (splitPoint != null && largestStoreSize < storeSize) {
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/RowProcessor.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/RowProcessor.java
index cfe42e4..f7620e8 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/RowProcessor.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/RowProcessor.java
@@ -33,7 +33,7 @@ import com.google.protobuf.Message;
 
 /**
  * Defines the procedure to atomically perform multiple scans and mutations
- * on a HRegion.
+ * on a region.
  *
  * This is invoked by HRegion#processRowsWithLocks().
  * This class performs scans and generates mutations and WAL edits.
@@ -70,7 +70,7 @@ public interface RowProcessor<S extends Message, T extends Message> {
   boolean readOnly();
 
   /**
-   * HRegion handles the locks and MVCC and invokes this method properly.
+   * Region handles the locks and MVCC and invokes this method properly.
    *
    * You should override this to create your own RowProcessor.
    *
@@ -79,22 +79,22 @@ public interface RowProcessor<S extends Message, T extends Message> {
    * we advance MVCC after releasing the locks for optimization purpose.
    *
    * @param now the current system millisecond
-   * @param region the HRegion
+   * @param region the region
    * @param mutations the output mutations to apply to memstore
    * @param walEdit the output WAL edits to apply to write ahead log
    */
   void process(long now,
-               HRegion region,
+               Region region,
                List<Mutation> mutations,
                WALEdit walEdit) throws IOException;
 
   /**
    * The hook to be executed before process().
    *
-   * @param region the HRegion
+   * @param region the region
    * @param walEdit the output WAL edits to apply to write ahead log
    */
-  void preProcess(HRegion region, WALEdit walEdit) throws IOException;
+  void preProcess(Region region, WALEdit walEdit) throws IOException;
 
   /**
    * The hook to be executed after the process() but before applying the Mutations to region. Also
@@ -103,25 +103,25 @@ public interface RowProcessor<S extends Message, T extends Message> {
    * @param walEdit the output WAL edits to apply to write ahead log
    * @throws IOException
    */
-  void preBatchMutate(HRegion region, WALEdit walEdit) throws IOException;
+  void preBatchMutate(Region region, WALEdit walEdit) throws IOException;
 
   /**
    * The hook to be executed after the process() and applying the Mutations to region. The
-   * difference of this one with {@link #postProcess(HRegion, WALEdit, boolean)} is this hook will
+   * difference of this one with {@link #postProcess(Region, WALEdit, boolean)} is this hook will
    * be executed before the mvcc transaction completion.
    * @param region
    * @throws IOException
    */
-  void postBatchMutate(HRegion region) throws IOException;
+  void postBatchMutate(Region region) throws IOException;
 
   /**
    * The hook to be executed after process() and applying the Mutations to region.
    *
-   * @param region the HRegion
+   * @param region the region
    * @param walEdit the output WAL edits to apply to write ahead log
    * @param success true if batch operation is successful otherwise false.
    */
-  void postProcess(HRegion region, WALEdit walEdit, boolean success) throws IOException;
+  void postProcess(Region region, WALEdit walEdit, boolean success) throws IOException;
 
   /**
    * @return The cluster ids that have the change.
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/SplitLogWorker.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/SplitLogWorker.java
index 4635c88..571edca 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/SplitLogWorker.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/SplitLogWorker.java
@@ -35,7 +35,6 @@ import org.apache.hadoop.hbase.classification.InterfaceAudience;
 import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.fs.FileSystem;
 import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.hbase.HConstants;
 import org.apache.hadoop.hbase.NotServingRegionException;
 import org.apache.hadoop.hbase.ServerName;
 import org.apache.hadoop.hbase.SplitLogCounters;
@@ -261,7 +260,7 @@ public class SplitLogWorker extends ZooKeeperListener implements Runnable {
             taskReadyLock.wait(checkInterval);
             if (this.server != null) {
               // check to see if we have stale recovering regions in our internal memory state
-              Map<String, HRegion> recoveringRegions = this.server.getRecoveringRegions();
+              Map<String, Region> recoveringRegions = this.server.getRecoveringRegions();
               if (!recoveringRegions.isEmpty()) {
                 // Make a local copy to prevent ConcurrentModificationException when other threads
                 // modify recoveringRegions
@@ -270,7 +269,7 @@ public class SplitLogWorker extends ZooKeeperListener implements Runnable {
                   String nodePath = ZKUtil.joinZNode(this.watcher.recoveringRegionsZNode, region);
                   try {
                     if (ZKUtil.checkExists(this.watcher, nodePath) == -1) {
-                      HRegion r = recoveringRegions.remove(region);
+                      HRegion r = (HRegion)recoveringRegions.remove(region);
                       if (r != null) {
                         r.setRecovering(false);
                       }
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/SplitRequest.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/SplitRequest.java
index b6a305f..a60ca74 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/SplitRequest.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/SplitRequest.java
@@ -43,9 +43,9 @@ class SplitRequest implements Runnable {
   private final HRegionServer server;
   private TableLock tableLock;
 
-  SplitRequest(HRegion region, byte[] midKey, HRegionServer hrs) {
+  SplitRequest(Region region, byte[] midKey, HRegionServer hrs) {
     Preconditions.checkNotNull(hrs);
-    this.parent = region;
+    this.parent = (HRegion)region;
     this.midKey = midKey;
     this.server = hrs;
   }
@@ -70,7 +70,7 @@ class SplitRequest implements Runnable {
       //acquire a shared read lock on the table, so that table schema modifications
       //do not happen concurrently
       tableLock = server.getTableLockManager().readLock(parent.getTableDesc().getTableName()
-          , "SPLIT_REGION:" + parent.getRegionNameAsString());
+          , "SPLIT_REGION:" + parent.getRegionInfo().getRegionNameAsString());
       try {
         tableLock.acquire();
       } catch (IOException ex) {
@@ -88,22 +88,22 @@ class SplitRequest implements Runnable {
         if (this.server.isStopping() || this.server.isStopped()) {
           LOG.info(
               "Skip rollback/cleanup of failed split of "
-                  + parent.getRegionNameAsString() + " because server is"
+                  + parent.getRegionInfo().getRegionNameAsString() + " because server is"
                   + (this.server.isStopping() ? " stopping" : " stopped"), e);
           return;
         }
         try {
           LOG.info("Running rollback/cleanup of failed split of " +
-            parent.getRegionNameAsString() + "; " + e.getMessage(), e);
+            parent.getRegionInfo().getRegionNameAsString() + "; " + e.getMessage(), e);
           if (st.rollback(this.server, this.server)) {
             LOG.info("Successful rollback of failed split of " +
-              parent.getRegionNameAsString());
+              parent.getRegionInfo().getRegionNameAsString());
           } else {
             this.server.abort("Abort; we got an error after point-of-no-return");
           }
         } catch (RuntimeException ee) {
           String msg = "Failed rollback of failed split of " +
-            parent.getRegionNameAsString() + " -- aborting server";
+            parent.getRegionInfo().getRegionNameAsString() + " -- aborting server";
           // If failed rollback, kill this server to avoid having a hole in table.
           LOG.info(msg, ee);
           this.server.abort(msg + " -- Cause: " + ee.getMessage());
@@ -133,7 +133,7 @@ class SplitRequest implements Runnable {
         server.getMetrics().incrSplitSuccess();
         // Log success
         LOG.info("Region split, hbase:meta updated, and report to master. Parent="
-            + parent.getRegionNameAsString() + ", new regions: "
+            + parent.getRegionInfo().getRegionNameAsString() + ", new regions: "
             + st.getFirstDaughter().getRegionNameAsString() + ", "
             + st.getSecondDaughter().getRegionNameAsString() + ". Split took "
             + StringUtils.formatTimeDiff(EnvironmentEdgeManager.currentTimeMillis(), startTime));
@@ -149,9 +149,9 @@ class SplitRequest implements Runnable {
         this.tableLock.release();
       } catch (IOException ex) {
         LOG.error("Could not release the table lock (something is really wrong). " 
-           + "Aborting this server to avoid holding the lock forever.");
+          + "Aborting this server to avoid holding the lock forever.");
         this.server.abort("Abort; we got an error when releasing the table lock "
-                         + "on " + parent.getRegionNameAsString());
+          + "on " + parent.getRegionInfo().getRegionNameAsString());
       }
     }
   }
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/SplitTransaction.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/SplitTransaction.java
index 4bfc092..5047796 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/SplitTransaction.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/SplitTransaction.java
@@ -213,11 +213,11 @@ public class SplitTransaction {
    * @param r Region to split
    * @param splitrow Row to split around
    */
-  public SplitTransaction(final HRegion r, final byte [] splitrow) {
-    this.parent = r;
+  public SplitTransaction(final Region r, final byte [] splitrow) {
+    this.parent = (HRegion)r;
     this.splitrow = splitrow;
     this.journal.add(new JournalEntry(JournalEntryType.STARTED));
-    this.useZKForAssignment = ConfigUtil.useZKForAssignment(r.getBaseConf());
+    this.useZKForAssignment = ConfigUtil.useZKForAssignment(((HRegion)r).getBaseConf());
   }
 
   /**
@@ -230,7 +230,6 @@ public class SplitTransaction {
     // Split key can be null if this region is unsplittable; i.e. has refs.
     if (this.splitrow == null) return false;
     HRegionInfo hri = this.parent.getRegionInfo();
-    parent.prepareToSplit();
     // Check splitrow.
     byte [] startKey = hri.getStartKey();
     byte [] endKey = hri.getEndKey();
@@ -311,7 +310,7 @@ public class SplitTransaction {
       if (this.parent.getCoprocessorHost().
           preSplitBeforePONR(this.splitrow, metaEntries)) {
         throw new IOException("Coprocessor bypassing region "
-            + this.parent.getRegionNameAsString() + " split.");
+            + parent.getRegionInfo().getRegionNameAsString() + " split.");
       }
       try {
         for (Mutation p : metaEntries) {
@@ -377,13 +376,13 @@ public class SplitTransaction {
           parent.getRegionInfo(), server.getServerName(), hri_a, hri_b);
       } catch (KeeperException e) {
         throw new IOException("Failed creating PENDING_SPLIT znode on " +
-          this.parent.getRegionNameAsString(), e);
+          parent.getRegionInfo().getRegionNameAsString(), e);
       }
     } else if (services != null && !useZKForAssignment) {
       if (!services.reportRegionStateTransition(TransitionCode.READY_TO_SPLIT,
         parent.getRegionInfo(), hri_a, hri_b)) {
         throw new IOException("Failed to get ok from master to split "
-            + parent.getRegionNameAsString());
+            + parent.getRegionInfo().getRegionNameAsString());
       }
     }
     this.journal.add(new JournalEntry(JournalEntryType.SET_SPLITTING_IN_ZK));
@@ -653,7 +652,7 @@ public class SplitTransaction {
         Thread.currentThread().interrupt();
       }
       throw new IOException("Failed getting SPLITTING znode on "
-        + parent.getRegionNameAsString(), e);
+        + parent.getRegionInfo().getRegionNameAsString(), e);
     }
   }
 
@@ -965,7 +964,7 @@ public class SplitTransaction {
           this.parent.initialize();
         } catch (IOException e) {
           LOG.error("Failed rollbacking CLOSED_PARENT_REGION of region " +
-            this.parent.getRegionNameAsString(), e);
+            parent.getRegionInfo().getRegionNameAsString(), e);
           throw new RuntimeException(e);
         }
         break;
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/handler/CloseRegionHandler.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/handler/CloseRegionHandler.java
index 3f79553..633f2e6 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/handler/CloseRegionHandler.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/handler/CloseRegionHandler.java
@@ -128,7 +128,7 @@ public class CloseRegionHandler extends EventHandler {
       LOG.debug("Processing close of " + name);
       String encodedRegionName = regionInfo.getEncodedName();
       // Check that this region is being served here
-      HRegion region = this.rsServices.getFromOnlineRegions(encodedRegionName);
+      HRegion region = (HRegion)rsServices.getFromOnlineRegions(encodedRegionName);
       if (region == null) {
         LOG.warn("Received CLOSE for region " + name + " but currently not serving - ignoring");
         if (zk){
@@ -179,7 +179,7 @@ public class CloseRegionHandler extends EventHandler {
         }
       }
       // Done!  Region is closed on this RS
-      LOG.debug("Closed " + region.getRegionNameAsString());
+      LOG.debug("Closed " + region.getRegionInfo().getRegionNameAsString());
     } finally {
       this.rsServices.getRegionsInTransitionInRS().
           remove(this.regionInfo.getEncodedNameAsBytes());
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/snapshot/FlushSnapshotSubprocedure.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/snapshot/FlushSnapshotSubprocedure.java
index 710479c..edb4918 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/snapshot/FlushSnapshotSubprocedure.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/snapshot/FlushSnapshotSubprocedure.java
@@ -30,6 +30,8 @@ import org.apache.hadoop.hbase.procedure.ProcedureMember;
 import org.apache.hadoop.hbase.procedure.Subprocedure;
 import org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.SnapshotDescription;
 import org.apache.hadoop.hbase.regionserver.HRegion;
+import org.apache.hadoop.hbase.regionserver.Region;
+import org.apache.hadoop.hbase.regionserver.Region.Operation;
 import org.apache.hadoop.hbase.regionserver.snapshot.RegionServerSnapshotManager.SnapshotSubprocedurePool;
 import org.apache.hadoop.hbase.snapshot.ClientSnapshotDescriptionUtils;
 
@@ -45,14 +47,14 @@ import org.apache.hadoop.hbase.snapshot.ClientSnapshotDescriptionUtils;
 public class FlushSnapshotSubprocedure extends Subprocedure {
   private static final Log LOG = LogFactory.getLog(FlushSnapshotSubprocedure.class);
 
-  private final List<HRegion> regions;
+  private final List<Region> regions;
   private final SnapshotDescription snapshot;
   private final SnapshotSubprocedurePool taskManager;
   private boolean snapshotSkipFlush = false;
 
   public FlushSnapshotSubprocedure(ProcedureMember member,
       ForeignExceptionDispatcher errorListener, long wakeFrequency, long timeout,
-      List<HRegion> regions, SnapshotDescription snapshot,
+      List<Region> regions, SnapshotDescription snapshot,
       SnapshotSubprocedurePool taskManager) {
     super(member, snapshot.getName(), errorListener, wakeFrequency, timeout);
     this.snapshot = snapshot;
@@ -69,8 +71,9 @@ public class FlushSnapshotSubprocedure extends Subprocedure {
    */
   private class RegionSnapshotTask implements Callable<Void> {
     HRegion region;
-    RegionSnapshotTask(HRegion region) {
-      this.region = region;
+
+    RegionSnapshotTask(Region region) {
+      this.region = (HRegion)region;
     }
 
     @Override
@@ -81,7 +84,7 @@ public class FlushSnapshotSubprocedure extends Subprocedure {
       // an interleaving such that globally regions are missing, so we still need the verification
       // step.
       LOG.debug("Starting region operation on " + region);
-      region.startRegionOperation();
+      region.startRegionOperation(Operation.ANY);
       try {
         if (snapshotSkipFlush) {
         /*
@@ -126,7 +129,7 @@ public class FlushSnapshotSubprocedure extends Subprocedure {
     }
 
     // Add all hfiles already existing in region.
-    for (HRegion region : regions) {
+    for (Region region : regions) {
       // submit one task per region for parallelize by region.
       taskManager.submitTask(new RegionSnapshotTask(region));
       monitor.rethrowException();
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/snapshot/RegionServerSnapshotManager.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/snapshot/RegionServerSnapshotManager.java
index f887821..33e68e9 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/snapshot/RegionServerSnapshotManager.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/snapshot/RegionServerSnapshotManager.java
@@ -48,8 +48,8 @@ import org.apache.hadoop.hbase.procedure.Subprocedure;
 import org.apache.hadoop.hbase.procedure.SubprocedureFactory;
 import org.apache.hadoop.hbase.procedure.ZKProcedureMemberRpcs;
 import org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.SnapshotDescription;
-import org.apache.hadoop.hbase.regionserver.HRegion;
 import org.apache.hadoop.hbase.regionserver.HRegionServer;
+import org.apache.hadoop.hbase.regionserver.Region;
 import org.apache.hadoop.hbase.regionserver.RegionServerServices;
 import org.apache.hadoop.hbase.snapshot.SnapshotCreationException;
 import org.apache.hadoop.hbase.zookeeper.ZooKeeperWatcher;
@@ -160,7 +160,7 @@ public class RegionServerSnapshotManager extends RegionServerProcedureManager {
 
     // check to see if this server is hosting any regions for the snapshots
     // check to see if we have regions for the snapshot
-    List<HRegion> involvedRegions;
+    List<Region> involvedRegions;
     try {
       involvedRegions = getRegionsToSnapshot(snapshot);
     } catch (IOException e1) {
@@ -220,7 +220,7 @@ public class RegionServerSnapshotManager extends RegionServerProcedureManager {
    *         the given snapshot.
    * @throws IOException
    */
-  private List<HRegion> getRegionsToSnapshot(SnapshotDescription snapshot) throws IOException {
+  private List<Region> getRegionsToSnapshot(SnapshotDescription snapshot) throws IOException {
     return rss.getOnlineRegions(TableName.valueOf(snapshot.getTable()));
   }
 
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/security/access/AccessControlLists.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/security/access/AccessControlLists.java
index b2205aa..b01ab0e 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/security/access/AccessControlLists.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/security/access/AccessControlLists.java
@@ -39,6 +39,7 @@ import org.apache.hadoop.hbase.Cell;
 import org.apache.hadoop.hbase.CellUtil;
 import org.apache.hadoop.hbase.HColumnDescriptor;
 import org.apache.hadoop.hbase.HConstants;
+import org.apache.hadoop.hbase.HRegionInfo;
 import org.apache.hadoop.hbase.HTableDescriptor;
 import org.apache.hadoop.hbase.NamespaceDescriptor;
 import org.apache.hadoop.hbase.TableName;
@@ -61,6 +62,7 @@ import org.apache.hadoop.hbase.protobuf.generated.AccessControlProtos;
 import org.apache.hadoop.hbase.regionserver.BloomType;
 import org.apache.hadoop.hbase.regionserver.HRegion;
 import org.apache.hadoop.hbase.regionserver.InternalScanner;
+import org.apache.hadoop.hbase.regionserver.Region;
 import org.apache.hadoop.hbase.security.User;
 import org.apache.hadoop.hbase.util.Bytes;
 import org.apache.hadoop.hbase.util.Pair;
@@ -331,8 +333,8 @@ public class AccessControlLists {
    * Returns {@code true} if the given region is part of the {@code _acl_}
    * metadata table.
    */
-  static boolean isAclRegion(HRegion region) {
-    return ACL_TABLE_NAME.equals(region.getTableDesc().getTableName());
+  static boolean isAclRegion(HRegionInfo regionInfo) {
+    return ACL_TABLE_NAME.equals(regionInfo.getTable());
   }
 
   /**
@@ -351,10 +353,10 @@ public class AccessControlLists {
    * @throws IOException
    */
   static Map<byte[], ListMultimap<String,TablePermission>> loadAll(
-      HRegion aclRegion)
+      Region aclRegion)
     throws IOException {
 
-    if (!isAclRegion(aclRegion)) {
+    if (!isAclRegion(aclRegion.getRegionInfo())) {
       throw new IOException("Can only load permissions from "+ACL_TABLE_NAME);
     }
 
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/security/access/AccessController.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/security/access/AccessController.java
index 2c0b05f..9447b9b 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/security/access/AccessController.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/security/access/AccessController.java
@@ -66,7 +66,6 @@ import org.apache.hadoop.hbase.coprocessor.CoprocessorException;
 import org.apache.hadoop.hbase.coprocessor.CoprocessorService;
 import org.apache.hadoop.hbase.coprocessor.EndpointObserver;
 import org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment;
-import org.apache.hadoop.hbase.coprocessor.MasterObserver;
 import org.apache.hadoop.hbase.coprocessor.ObserverContext;
 import org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment;
 import org.apache.hadoop.hbase.coprocessor.RegionServerCoprocessorEnvironment;
@@ -86,9 +85,9 @@ import org.apache.hadoop.hbase.protobuf.generated.AdminProtos.WALEntry;
 import org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.SnapshotDescription;
 import org.apache.hadoop.hbase.protobuf.generated.SecureBulkLoadProtos.CleanupBulkLoadRequest;
 import org.apache.hadoop.hbase.protobuf.generated.SecureBulkLoadProtos.PrepareBulkLoadRequest;
-import org.apache.hadoop.hbase.regionserver.HRegion;
 import org.apache.hadoop.hbase.regionserver.InternalScanner;
 import org.apache.hadoop.hbase.regionserver.MiniBatchOperationInProgress;
+import org.apache.hadoop.hbase.regionserver.Region;
 import org.apache.hadoop.hbase.regionserver.RegionScanner;
 import org.apache.hadoop.hbase.regionserver.ScanType;
 import org.apache.hadoop.hbase.regionserver.Store;
@@ -197,7 +196,7 @@ public class AccessController extends BaseMasterAndRegionObserver
   // This boolean having relevance only in the Master.
   private volatile boolean aclTabAvailable = false;
 
-  public HRegion getRegion() {
+  public Region getRegion() {
     return regionEnv != null ? regionEnv.getRegion() : null;
   }
 
@@ -206,10 +205,9 @@ public class AccessController extends BaseMasterAndRegionObserver
   }
 
   void initialize(RegionCoprocessorEnvironment e) throws IOException {
-    final HRegion region = e.getRegion();
     Configuration conf = e.getConfiguration();
     Map<byte[], ListMultimap<String,TablePermission>> tables =
-        AccessControlLists.loadAll(region);
+        AccessControlLists.loadAll(e.getRegion());
     // For each table, write out the table's permissions to the respective
     // znode for that table.
     for (Map.Entry<byte[], ListMultimap<String,TablePermission>> t:
@@ -274,7 +272,7 @@ public class AccessController extends BaseMasterAndRegionObserver
   AuthResult permissionGranted(String request, User user, Action permRequest,
       RegionCoprocessorEnvironment e,
       Map<byte [], ? extends Collection<?>> families) {
-    HRegionInfo hri = e.getRegion().getRegionInfo();
+    HRegionInfo hri = e.getRegionInfo();
     TableName tableName = hri.getTable();
 
     // 1. All users need read access to hbase:meta table.
@@ -547,7 +545,7 @@ public class AccessController extends BaseMasterAndRegionObserver
       RegionCoprocessorEnvironment env,
       Map<byte[], ? extends Collection<byte[]>> familyMap)
     throws IOException {
-    HRegionInfo hri = env.getRegion().getRegionInfo();
+    HRegionInfo hri = env.getRegionInfo();
     TableName tableName = hri.getTable();
 
     if (user == null) {
@@ -701,7 +699,7 @@ public class AccessController extends BaseMasterAndRegionObserver
         familyMap1.put(new SimpleByteRange(entry.getKey()), (List<Cell>) entry.getValue());
       }
     }
-    RegionScanner scanner = getRegion(e).getScanner(new Scan(get));
+    RegionScanner scanner = e.getRegion().getScanner(new Scan(get));
     List<Cell> cells = Lists.newArrayList();
     Cell prevCell = null;
     ByteRange curFam = new SimpleByteRange();
@@ -754,7 +752,7 @@ public class AccessController extends BaseMasterAndRegionObserver
           foundColumn = true;
           for (Action action: actions) {
             // Are there permissions for this user for the cell?
-            if (!authManager.authorize(user, getTableName(e), cell, action)) {
+            if (!authManager.authorize(user, e.getRegionInfo().getTable(), cell, action)) {
               // We can stop if the cell ACL denies access
               return false;
             }
@@ -1217,11 +1215,10 @@ public class AccessController extends BaseMasterAndRegionObserver
   public void preOpen(ObserverContext<RegionCoprocessorEnvironment> e)
       throws IOException {
     RegionCoprocessorEnvironment env = e.getEnvironment();
-    final HRegion region = env.getRegion();
-    if (region == null) {
-      LOG.error("NULL region from RegionCoprocessorEnvironment in preOpen()");
+    HRegionInfo regionInfo = env.getRegionInfo();
+    if (regionInfo == null) {
+      LOG.error("NULL regionInfo from RegionCoprocessorEnvironment in preOpen()");
     } else {
-      HRegionInfo regionInfo = region.getRegionInfo();
       if (regionInfo.getTable().isSystemTable()) {
         isSystemOrSuperUser(regionEnv.getConfiguration());
       } else {
@@ -1233,14 +1230,15 @@ public class AccessController extends BaseMasterAndRegionObserver
   @Override
   public void postOpen(ObserverContext<RegionCoprocessorEnvironment> c) {
     RegionCoprocessorEnvironment env = c.getEnvironment();
-    final HRegion region = env.getRegion();
-    if (region == null) {
-      LOG.error("NULL region from RegionCoprocessorEnvironment in postOpen()");
+    HRegionInfo regionInfo = env.getRegionInfo();
+    if (regionInfo == null) {
+      LOG.error("NULL regionInfo from RegionCoprocessorEnvironment in postOpen()");
       return;
     }
-    if (AccessControlLists.isAclRegion(region)) {
+    if (AccessControlLists.isAclRegion(regionInfo)) {
       aclRegion = true;
       // When this region is under recovering state, initialize will be handled by postLogReplay
+      Region region = env.getRegion();
       if (!region.isRecovering()) {
         try {
           initialize(env);
@@ -1270,27 +1268,29 @@ public class AccessController extends BaseMasterAndRegionObserver
 
   @Override
   public void preFlush(ObserverContext<RegionCoprocessorEnvironment> e) throws IOException {
-    requirePermission("flush", getTableName(e.getEnvironment()), null, null, Action.ADMIN,
-        Action.CREATE);
+    requirePermission("flush", e.getEnvironment().getRegionInfo().getTable(), null, null,
+      Action.ADMIN, Action.CREATE);
   }
 
   @Override
   public void preSplit(ObserverContext<RegionCoprocessorEnvironment> e) throws IOException {
-    requirePermission("split", getTableName(e.getEnvironment()), null, null, Action.ADMIN);
+    requirePermission("split", e.getEnvironment().getRegionInfo().getTable(), null, null,
+      Action.ADMIN);
   }
 
   @Override
   public void preSplit(ObserverContext<RegionCoprocessorEnvironment> e,
       byte[] splitRow) throws IOException {
-    requirePermission("split", getTableName(e.getEnvironment()), null, null, Action.ADMIN);
+    requirePermission("split", e.getEnvironment().getRegionInfo().getTable(), null, null,
+      Action.ADMIN);
   }
 
   @Override
   public InternalScanner preCompact(ObserverContext<RegionCoprocessorEnvironment> e,
       final Store store, final InternalScanner scanner, final ScanType scanType)
           throws IOException {
-    requirePermission("compact", getTableName(e.getEnvironment()), null, null, Action.ADMIN,
-        Action.CREATE);
+    requirePermission("compact", e.getEnvironment().getRegionInfo().getTable(), null, null,
+      Action.ADMIN, Action.CREATE);
     return scanner;
   }
 
@@ -1337,10 +1337,9 @@ public class AccessController extends BaseMasterAndRegionObserver
       throw new RuntimeException("Unhandled operation " + opType);
     }
     AuthResult authResult = permissionGranted(opType, user, env, families, Action.READ);
-    HRegion region = getRegion(env);
-    TableName table = getTableName(region);
+    TableName table = c.getEnvironment().getRegionInfo().getTable();
     Map<ByteRange, Integer> cfVsMaxVersions = Maps.newHashMap();
-    for (HColumnDescriptor hcd : region.getTableDesc().getFamilies()) {
+    for (HColumnDescriptor hcd : c.getEnvironment().getRegion().getTableDesc().getFamilies()) {
       cfVsMaxVersions.put(new SimpleByteRange(hcd.getName()), hcd.getMaxVersions());
     }
     if (!authResult.isAllowed()) {
@@ -1971,7 +1970,7 @@ public class AccessController extends BaseMasterAndRegionObserver
     if (shouldCheckExecPermission && !(service instanceof AccessControlService)) {
       requirePermission("invoke(" + service.getDescriptorForType().getName() + "." +
         methodName + ")",
-        getTableName(ctx.getEnvironment()), null, null,
+        ctx.getEnvironment().getRegionInfo().getTable(), null, null,
         Action.EXEC);
     }
     return request;
@@ -2193,26 +2192,6 @@ public class AccessController extends BaseMasterAndRegionObserver
     return AccessControlProtos.AccessControlService.newReflectiveService(this);
   }
 
-  private HRegion getRegion(RegionCoprocessorEnvironment e) {
-    return e.getRegion();
-  }
-
-  private TableName getTableName(RegionCoprocessorEnvironment e) {
-    HRegion region = e.getRegion();
-    if (region != null) {
-      return getTableName(region);
-    }
-    return null;
-  }
-
-  private TableName getTableName(HRegion region) {
-    HRegionInfo regionInfo = region.getRegionInfo();
-    if (regionInfo != null) {
-      return regionInfo.getTable();
-    }
-    return null;
-  }
-
   @Override
   public void preClose(ObserverContext<RegionCoprocessorEnvironment> e, boolean abortRequested)
       throws IOException {
@@ -2275,31 +2254,31 @@ public class AccessController extends BaseMasterAndRegionObserver
   }
 
   @Override
-  public void preMerge(ObserverContext<RegionServerCoprocessorEnvironment> ctx, HRegion regionA,
-      HRegion regionB) throws IOException {
+  public void preMerge(ObserverContext<RegionServerCoprocessorEnvironment> ctx, Region regionA,
+      Region regionB) throws IOException {
     requirePermission("mergeRegions", regionA.getTableDesc().getTableName(), null, null,
       Action.ADMIN);
   }
 
   @Override
-  public void postMerge(ObserverContext<RegionServerCoprocessorEnvironment> c, HRegion regionA,
-      HRegion regionB, HRegion mergedRegion) throws IOException { }
+  public void postMerge(ObserverContext<RegionServerCoprocessorEnvironment> c, Region regionA,
+      Region regionB, Region mergedRegion) throws IOException { }
 
   @Override
   public void preMergeCommit(ObserverContext<RegionServerCoprocessorEnvironment> ctx,
-      HRegion regionA, HRegion regionB, List<Mutation> metaEntries) throws IOException { }
+      Region regionA, Region regionB, List<Mutation> metaEntries) throws IOException { }
 
   @Override
   public void postMergeCommit(ObserverContext<RegionServerCoprocessorEnvironment> ctx,
-      HRegion regionA, HRegion regionB, HRegion mergedRegion) throws IOException { }
+      Region regionA, Region regionB, Region mergedRegion) throws IOException { }
 
   @Override
   public void preRollBackMerge(ObserverContext<RegionServerCoprocessorEnvironment> ctx,
-      HRegion regionA, HRegion regionB) throws IOException { }
+      Region regionA, Region regionB) throws IOException { }
 
   @Override
   public void postRollBackMerge(ObserverContext<RegionServerCoprocessorEnvironment> ctx,
-      HRegion regionA, HRegion regionB) throws IOException { }
+      Region regionA, Region regionB) throws IOException { }
 
   @Override
   public void preRollWALWriterRequest(ObserverContext<RegionServerCoprocessorEnvironment> ctx)
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/security/access/SecureBulkLoadEndpoint.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/security/access/SecureBulkLoadEndpoint.java
index ebf6a0f..f127c59 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/security/access/SecureBulkLoadEndpoint.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/security/access/SecureBulkLoadEndpoint.java
@@ -18,9 +18,11 @@
 
 package org.apache.hadoop.hbase.security.access;
 
+import com.google.common.collect.Lists;
 import com.google.protobuf.RpcCallback;
 import com.google.protobuf.RpcController;
 import com.google.protobuf.Service;
+
 import org.apache.commons.logging.Log;
 import org.apache.commons.logging.LogFactory;
 import org.apache.hadoop.hbase.classification.InterfaceAudience;
@@ -50,6 +52,8 @@ import org.apache.hadoop.hbase.protobuf.generated.SecureBulkLoadProtos.CleanupBu
 import org.apache.hadoop.hbase.protobuf.generated.SecureBulkLoadProtos.SecureBulkLoadHFilesRequest;
 import org.apache.hadoop.hbase.protobuf.generated.SecureBulkLoadProtos.SecureBulkLoadHFilesResponse;
 import org.apache.hadoop.hbase.regionserver.HRegion;
+import org.apache.hadoop.hbase.regionserver.Region;
+import org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost;
 import org.apache.hadoop.hbase.security.SecureBulkLoadUtil;
 import org.apache.hadoop.hbase.security.User;
 import org.apache.hadoop.hbase.security.UserProvider;
@@ -235,11 +239,15 @@ public class SecureBulkLoadEndpoint extends SecureBulkLoadService
       return;
     }
 
-    HRegion region = env.getRegion();
+    Region region = env.getRegion();
+    // XXX: Layering violation. Eventually the SecureBulkLoadEndpoint will be folded into
+    // core so this isn't a big deal.
+    RegionCoprocessorHost cpHost = region.getCoprocessorHost();
+
     boolean bypass = false;
-    if (region.getCoprocessorHost() != null) {
+    if (cpHost != null) {
       try {
-        bypass = region.getCoprocessorHost().preBulkLoadHFile(familyPaths);
+        bypass = cpHost.preBulkLoadHFile(familyPaths);
       } catch (IOException e) {
         ResponseConverter.setControllerException(controller, e);
         done.run(SecureBulkLoadHFilesResponse.newBuilder().setLoaded(false).build());
@@ -295,9 +303,9 @@ public class SecureBulkLoadEndpoint extends SecureBulkLoadService
         }
       });
     }
-    if (region.getCoprocessorHost() != null) {
+    if (cpHost != null) {
       try {
-        loaded = region.getCoprocessorHost().postBulkLoadHFile(familyPaths, loaded);
+        loaded = cpHost.postBulkLoadHFile(familyPaths, loaded);
       } catch (IOException e) {
         ResponseConverter.setControllerException(controller, e);
         done.run(SecureBulkLoadHFilesResponse.newBuilder().setLoaded(false).build());
@@ -308,9 +316,14 @@ public class SecureBulkLoadEndpoint extends SecureBulkLoadService
   }
 
   private List<BulkLoadObserver> getBulkLoadObservers() {
-    List<BulkLoadObserver> coprocessorList =
-              this.env.getRegion().getCoprocessorHost().findCoprocessors(BulkLoadObserver.class);
-
+    // XXX: Layering violation. Eventually the SecureBulkLoadEndpoint will be folded into
+    // core so this isn't a big deal.
+    Region region = env.getRegion();
+    RegionCoprocessorHost cpHost = region.getCoprocessorHost();
+    List<BulkLoadObserver> coprocessorList = Lists.newArrayList();
+    if (cpHost != null) {
+      coprocessorList.addAll(cpHost.findCoprocessors(BulkLoadObserver.class));
+    }
     return coprocessorList;
   }
 
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/security/visibility/DefaultVisibilityLabelServiceImpl.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/security/visibility/DefaultVisibilityLabelServiceImpl.java
index 9ff6254..b1b5da5 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/security/visibility/DefaultVisibilityLabelServiceImpl.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/security/visibility/DefaultVisibilityLabelServiceImpl.java
@@ -43,6 +43,7 @@ import org.apache.commons.logging.LogFactory;
 import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.hbase.Cell;
 import org.apache.hadoop.hbase.CellUtil;
+import org.apache.hadoop.hbase.HConstants;
 import org.apache.hadoop.hbase.HConstants.OperationStatusCode;
 import org.apache.hadoop.hbase.Tag;
 import org.apache.hadoop.hbase.TagType;
@@ -54,8 +55,8 @@ import org.apache.hadoop.hbase.client.Scan;
 import org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment;
 import org.apache.hadoop.hbase.filter.Filter;
 import org.apache.hadoop.hbase.io.util.StreamUtils;
-import org.apache.hadoop.hbase.regionserver.HRegion;
 import org.apache.hadoop.hbase.regionserver.OperationStatus;
+import org.apache.hadoop.hbase.regionserver.Region;
 import org.apache.hadoop.hbase.regionserver.RegionScanner;
 import org.apache.hadoop.hbase.security.User;
 import org.apache.hadoop.hbase.security.access.AccessControlLists;
@@ -75,7 +76,7 @@ public class DefaultVisibilityLabelServiceImpl implements VisibilityLabelService
 
   private volatile int ordinalCounter = -1;
   private Configuration conf;
-  private HRegion labelsRegion;
+  private Region labelsRegion;
   private VisibilityLabelsCache labelsCache;
   private List<ScanLabelGenerator> scanLabelGenerators;
   private List<String> superUsers;
@@ -195,7 +196,7 @@ public class DefaultVisibilityLabelServiceImpl implements VisibilityLabelService
     return new Pair<Map<String, Integer>, Map<String, List<Integer>>>(labels, userAuths);
   }
 
-  protected void addSystemLabel(HRegion region, Map<String, Integer> labels,
+  protected void addSystemLabel(Region region, Map<String, Integer> labels,
       Map<String, List<Integer>> userAuths) throws IOException {
     if (!labels.containsKey(SYSTEM_LABEL)) {
       Put p = new Put(Bytes.toBytes(SYSTEM_LABEL_ORDINAL));
@@ -306,7 +307,7 @@ public class DefaultVisibilityLabelServiceImpl implements VisibilityLabelService
   private boolean mutateLabelsRegion(List<Mutation> mutations, OperationStatus[] finalOpStatus)
       throws IOException {
     OperationStatus[] opStatus = this.labelsRegion.batchMutate(mutations
-        .toArray(new Mutation[mutations.size()]));
+        .toArray(new Mutation[mutations.size()]), HConstants.NO_NONCE, HConstants.NO_NONCE);
     int i = 0;
     boolean updateZk = false;
     for (OperationStatus status : opStatus) {
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/security/visibility/VisibilityController.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/security/visibility/VisibilityController.java
index bf1e9d7..d5c92bb 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/security/visibility/VisibilityController.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/security/visibility/VisibilityController.java
@@ -92,7 +92,6 @@ import org.apache.hadoop.hbase.protobuf.generated.VisibilityLabelsProtos.Visibil
 import org.apache.hadoop.hbase.regionserver.BloomType;
 import org.apache.hadoop.hbase.regionserver.DeleteTracker;
 import org.apache.hadoop.hbase.regionserver.DisabledRegionSplitPolicy;
-import org.apache.hadoop.hbase.regionserver.HRegion;
 import org.apache.hadoop.hbase.regionserver.InternalScanner;
 import org.apache.hadoop.hbase.regionserver.MiniBatchOperationInProgress;
 import org.apache.hadoop.hbase.regionserver.OperationStatus;
@@ -244,7 +243,7 @@ public class VisibilityController extends BaseMasterAndRegionObserver implements
   @Override
   public void postOpen(ObserverContext<RegionCoprocessorEnvironment> e) {
     // Read the entire labels table and populate the zk
-    if (e.getEnvironment().getRegion().getRegionInfo().getTable().equals(LABELS_TABLE_NAME)) {
+    if (e.getEnvironment().getRegionInfo().getTable().equals(LABELS_TABLE_NAME)) {
       this.labelsRegion = true;
       this.acOn = CoprocessorHost.getLoadedCoprocessors().contains(AccessController.class.getName());
       // Defer the init of VisibilityLabelService on labels region until it is in recovering state.
@@ -279,7 +278,7 @@ public class VisibilityController extends BaseMasterAndRegionObserver implements
   @Override
   public void preBatchMutate(ObserverContext<RegionCoprocessorEnvironment> c,
       MiniBatchOperationInProgress<Mutation> miniBatchOp) throws IOException {
-    if (c.getEnvironment().getRegion().getRegionInfo().getTable().isSystemTable()) {
+    if (c.getEnvironment().getRegionInfo().getTable().isSystemTable()) {
       return;
     }
     // TODO this can be made as a global LRU cache at HRS level?
@@ -523,7 +522,6 @@ public class VisibilityController extends BaseMasterAndRegionObserver implements
     if (!initialized) {
       throw new VisibilityControllerNotReadyException("VisibilityController not yet initialized!");
     }
-    HRegion region = e.getEnvironment().getRegion();
     Authorizations authorizations = null;
     try {
       authorizations = scan.getAuthorizations();
@@ -534,14 +532,14 @@ public class VisibilityController extends BaseMasterAndRegionObserver implements
       // No Authorizations present for this scan/Get!
       // In case of system tables other than "labels" just scan with out visibility check and
       // filtering. Checking visibility labels for META and NAMESPACE table is not needed.
-      TableName table = region.getRegionInfo().getTable();
+      TableName table = e.getEnvironment().getRegionInfo().getTable();
       if (table.isSystemTable() && !table.equals(LABELS_TABLE_NAME)) {
         return s;
       }
     }
 
-    Filter visibilityLabelFilter = VisibilityUtils.createVisibilityLabelFilter(region,
-        authorizations);
+    Filter visibilityLabelFilter = VisibilityUtils.createVisibilityLabelFilter(
+      e.getEnvironment().getRegion(), authorizations);
     if (visibilityLabelFilter != null) {
       Filter filter = scan.getFilter();
       if (filter != null) {
@@ -557,8 +555,7 @@ public class VisibilityController extends BaseMasterAndRegionObserver implements
   public DeleteTracker postInstantiateDeleteTracker(
       ObserverContext<RegionCoprocessorEnvironment> ctx, DeleteTracker delTracker)
       throws IOException {
-    HRegion region = ctx.getEnvironment().getRegion();
-    TableName table = region.getRegionInfo().getTable();
+    TableName table = ctx.getEnvironment().getRegionInfo().getTable();
     if (table.isSystemTable()) {
       return delTracker;
     }
@@ -621,7 +618,6 @@ public class VisibilityController extends BaseMasterAndRegionObserver implements
     if (!initialized) {
       throw new VisibilityControllerNotReadyException("VisibilityController not yet initialized!");
     }
-    HRegion region = e.getEnvironment().getRegion();
     Authorizations authorizations = null;
     try {
       authorizations = get.getAuthorizations();
@@ -632,7 +628,7 @@ public class VisibilityController extends BaseMasterAndRegionObserver implements
       // No Authorizations present for this scan/Get!
       // In case of system tables other than "labels" just scan with out visibility check and
       // filtering. Checking visibility labels for META and NAMESPACE table is not needed.
-      TableName table = region.getRegionInfo().getTable();
+      TableName table = e.getEnvironment().getRegionInfo().getTable();
       if (table.isSystemTable() && !table.equals(LABELS_TABLE_NAME)) {
         return;
       }
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/security/visibility/VisibilityUtils.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/security/visibility/VisibilityUtils.java
index baf2a97..da43f17 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/security/visibility/VisibilityUtils.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/security/visibility/VisibilityUtils.java
@@ -50,7 +50,7 @@ import org.apache.hadoop.hbase.protobuf.generated.VisibilityLabelsProtos.MultiUs
 import org.apache.hadoop.hbase.protobuf.generated.VisibilityLabelsProtos.UserAuthorizations;
 import org.apache.hadoop.hbase.protobuf.generated.VisibilityLabelsProtos.VisibilityLabel;
 import org.apache.hadoop.hbase.protobuf.generated.VisibilityLabelsProtos.VisibilityLabelsRequest;
-import org.apache.hadoop.hbase.regionserver.HRegion;
+import org.apache.hadoop.hbase.regionserver.Region;
 import org.apache.hadoop.hbase.security.AccessDeniedException;
 import org.apache.hadoop.hbase.security.User;
 import org.apache.hadoop.hbase.security.access.AccessControlLists;
@@ -308,7 +308,7 @@ public class VisibilityUtils {
     return false;
   }
 
-  public static Filter createVisibilityLabelFilter(HRegion region, Authorizations authorizations)
+  public static Filter createVisibilityLabelFilter(Region region, Authorizations authorizations)
       throws IOException {
     Map<ByteRange, Integer> cfVsMaxVersions = new HashMap<ByteRange, Integer>();
     for (HColumnDescriptor hcd : region.getTableDesc().getFamilies()) {
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/snapshot/SnapshotManifest.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/snapshot/SnapshotManifest.java
index 38ccf08..007dbfa 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/snapshot/SnapshotManifest.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/snapshot/SnapshotManifest.java
@@ -166,7 +166,7 @@ public class SnapshotManifest {
     // 2. iterate through all the stores in the region
     LOG.debug("Creating references for hfiles");
 
-    for (Store store : region.getStores().values()) {
+    for (Store store : region.getStores()) {
       // 2.1. build the snapshot reference for the store
       Object familyData = visitor.familyOpen(regionData, store.getFamily().getName());
       monitor.rethrowException();
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/tool/WriteSinkCoprocessor.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/tool/WriteSinkCoprocessor.java
index fff1374..5686ca4 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/tool/WriteSinkCoprocessor.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/tool/WriteSinkCoprocessor.java
@@ -60,7 +60,7 @@ public class WriteSinkCoprocessor extends BaseRegionObserver {
 
   @Override
   public void preOpen(ObserverContext<RegionCoprocessorEnvironment> e) throws IOException {
-    regionName = e.getEnvironment().getRegion().getRegionNameAsString();
+    regionName = e.getEnvironment().getRegionInfo().getRegionNameAsString();
   }
 
 
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/util/HBaseFsck.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/util/HBaseFsck.java
index 4d21043..24db1a8 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/util/HBaseFsck.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/util/HBaseFsck.java
@@ -1313,7 +1313,7 @@ public class HBaseFsck extends Configured {
         "You may need to restore the previously sidelined hbase:meta");
       return false;
     }
-    meta.batchMutate(puts.toArray(new Put[puts.size()]));
+    meta.batchMutate(puts.toArray(new Put[puts.size()]), HConstants.NO_NONCE, HConstants.NO_NONCE);
     HRegion.closeHRegion(meta);
     LOG.info("Success! hbase:meta table rebuilt.");
     LOG.info("Old hbase:meta is moved into " + backupDir);
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/util/HMerge.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/util/HMerge.java
index bfa883e..88ac50f 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/util/HMerge.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/util/HMerge.java
@@ -125,8 +125,12 @@ class HMerge {
             "HBase instance must be running to merge a normal table");
       }
       HBaseAdmin admin = new HBaseAdmin(conf);
-      if (!admin.isTableDisabled(tableName)) {
-        throw new TableNotDisabledException(tableName);
+      try {
+        if (!admin.isTableDisabled(tableName)) {
+          throw new TableNotDisabledException(tableName);
+        }
+      } finally {
+        admin.close();
       }
       new OnlineMerger(conf, fs, tableName).process();
     }
@@ -196,16 +200,17 @@ class HMerge {
         if ((currentSize + nextSize) <= (maxFilesize / 2)) {
           // We merge two adjacent regions if their total size is less than
           // one half of the desired maximum size
-          LOG.info("Merging regions " + currentRegion.getRegionNameAsString() +
-            " and " + nextRegion.getRegionNameAsString());
+          LOG.info("Merging regions " + currentRegion.getRegionInfo().getRegionNameAsString() +
+            " and " + nextRegion.getRegionInfo().getRegionNameAsString());
           HRegion mergedRegion =
             HRegion.mergeAdjacent(currentRegion, nextRegion);
-          updateMeta(currentRegion.getRegionName(), nextRegion.getRegionName(),
-              mergedRegion);
+          updateMeta(currentRegion.getRegionInfo().getRegionName(),
+              nextRegion.getRegionInfo().getRegionName(), mergedRegion);
           break;
         }
-        LOG.info("not merging regions " + Bytes.toStringBinary(currentRegion.getRegionName())
-            + " and " + Bytes.toStringBinary(nextRegion.getRegionName()));
+        LOG.info("Not merging regions "
+          + Bytes.toStringBinary(currentRegion.getRegionInfo().getRegionName())
+          + " and " + Bytes.toStringBinary(nextRegion.getRegionInfo().getRegionName()));
         currentRegion.close();
         currentRegion = nextRegion;
         currentSize = nextSize;
@@ -331,7 +336,7 @@ class HMerge {
 
       if(LOG.isDebugEnabled()) {
         LOG.debug("updated columns in row: "
-            + Bytes.toStringBinary(newRegion.getRegionName()));
+            + Bytes.toStringBinary(newRegion.getRegionInfo().getRegionName()));
       }
     }
   }
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/util/Merge.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/util/Merge.java
index a120449..e6e3ac6 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/util/Merge.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/util/Merge.java
@@ -173,11 +173,11 @@ public class Merge extends Configured implements Tool {
   throws IOException {
     if (info1 == null) {
       throw new IOException("Could not find " + Bytes.toStringBinary(region1) + " in " +
-          Bytes.toStringBinary(meta.getRegionName()));
+          Bytes.toStringBinary(meta.getRegionInfo().getRegionName()));
     }
     if (info2 == null) {
       throw new IOException("Could not find " + Bytes.toStringBinary(region2) + " in " +
-          Bytes.toStringBinary(meta.getRegionName()));
+          Bytes.toStringBinary(meta.getRegionInfo().getRegionName()));
     }
     HRegion merged = null;
     HLog log = utils.getLog();
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/zookeeper/RecoveringRegionWatcher.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/zookeeper/RecoveringRegionWatcher.java
index a07bd2f..efef82b 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/zookeeper/RecoveringRegionWatcher.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/zookeeper/RecoveringRegionWatcher.java
@@ -23,6 +23,7 @@ import org.apache.commons.logging.LogFactory;
 import org.apache.hadoop.hbase.classification.InterfaceAudience;
 import org.apache.hadoop.hbase.regionserver.HRegion;
 import org.apache.hadoop.hbase.regionserver.HRegionServer;
+import org.apache.hadoop.hbase.regionserver.Region;
 import org.apache.zookeeper.KeeperException;
 
 /**
@@ -58,9 +59,9 @@ public class RecoveringRegionWatcher extends ZooKeeperListener {
     }
 
     String regionName = path.substring(parentPath.length() + 1);
-    HRegion region = this.server.getRecoveringRegions().remove(regionName);
+    Region region = this.server.getRecoveringRegions().remove(regionName);
     if (region != null) {
-      region.setRecovering(false);
+      ((HRegion)region).setRecovering(false);
     }
 
     LOG.info(path + " deleted; " + regionName + " recovered.");
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/HBaseTestingUtility.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/HBaseTestingUtility.java
index 6b02205..895d272 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/HBaseTestingUtility.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/HBaseTestingUtility.java
@@ -82,6 +82,7 @@ import org.apache.hadoop.hbase.regionserver.HRegion;
 import org.apache.hadoop.hbase.regionserver.HRegionServer;
 import org.apache.hadoop.hbase.regionserver.HStore;
 import org.apache.hadoop.hbase.regionserver.InternalScanner;
+import org.apache.hadoop.hbase.regionserver.Region;
 import org.apache.hadoop.hbase.regionserver.RegionServerServices;
 import org.apache.hadoop.hbase.regionserver.wal.HLog;
 import org.apache.hadoop.hbase.security.User;
@@ -3080,7 +3081,7 @@ public class HBaseTestingUtility extends HBaseCommonTestingUtility {
    * @throws NodeExistsException
    */
   public static ZooKeeperWatcher createAndForceNodeToOpenedState(
-      HBaseTestingUtility TEST_UTIL, HRegion region,
+      HBaseTestingUtility TEST_UTIL, Region region,
       ServerName serverName) throws ZooKeeperConnectionException,
       IOException, KeeperException, NodeExistsException {
     ZooKeeperWatcher zkw = getZooKeeperWatcher(TEST_UTIL);
@@ -3414,10 +3415,10 @@ public class HBaseTestingUtility extends HBaseCommonTestingUtility {
           if (server.equals(rs.getServerName())) {
             continue;
           }
-          Collection<HRegion> hrs = rs.getOnlineRegionsLocalContext();
-          for (HRegion r: hrs) {
+          Collection<Region> hrs = rs.getOnlineRegionsLocalContext();
+          for (Region r: hrs) {
             assertTrue("Region should not be double assigned",
-              r.getRegionId() != hri.getRegionId());
+              r.getRegionInfo().getRegionId() != hri.getRegionId());
           }
         }
         return; // good, we are happy
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/MiniHBaseCluster.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/MiniHBaseCluster.java
index 55db7ea..f351b81 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/MiniHBaseCluster.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/MiniHBaseCluster.java
@@ -36,6 +36,7 @@ import org.apache.hadoop.hbase.protobuf.generated.ClientProtos.ClientService;
 import org.apache.hadoop.hbase.protobuf.generated.RegionServerStatusProtos.RegionServerStartupResponse;
 import org.apache.hadoop.hbase.regionserver.HRegion;
 import org.apache.hadoop.hbase.regionserver.HRegionServer;
+import org.apache.hadoop.hbase.regionserver.Region;
 import org.apache.hadoop.hbase.security.User;
 import org.apache.hadoop.hbase.test.MetricsAssertHelper;
 import org.apache.hadoop.hbase.util.JVMClusterUtil;
@@ -514,8 +515,8 @@ public class MiniHBaseCluster extends HBaseCluster {
   public void flushcache() throws IOException {
     for (JVMClusterUtil.RegionServerThread t:
         this.hbaseCluster.getRegionServers()) {
-      for(HRegion r: t.getRegionServer().getOnlineRegionsLocalContext()) {
-        r.flushcache();
+      for (Region r: t.getRegionServer().getOnlineRegionsLocalContext()) {
+        ((HRegion)r).flushcache();
       }
     }
   }
@@ -527,9 +528,9 @@ public class MiniHBaseCluster extends HBaseCluster {
   public void flushcache(TableName tableName) throws IOException {
     for (JVMClusterUtil.RegionServerThread t:
         this.hbaseCluster.getRegionServers()) {
-      for(HRegion r: t.getRegionServer().getOnlineRegionsLocalContext()) {
+      for (Region r: t.getRegionServer().getOnlineRegionsLocalContext()) {
         if(r.getTableDesc().getTableName().equals(tableName)) {
-          r.flushcache();
+          ((HRegion)r).flushcache();
         }
       }
     }
@@ -542,8 +543,8 @@ public class MiniHBaseCluster extends HBaseCluster {
   public void compact(boolean major) throws IOException {
     for (JVMClusterUtil.RegionServerThread t:
         this.hbaseCluster.getRegionServers()) {
-      for(HRegion r: t.getRegionServer().getOnlineRegionsLocalContext()) {
-        r.compactStores(major);
+      for (Region r: t.getRegionServer().getOnlineRegionsLocalContext()) {
+        ((HRegion)r).compactStores(major);
       }
     }
   }
@@ -555,9 +556,9 @@ public class MiniHBaseCluster extends HBaseCluster {
   public void compact(TableName tableName, boolean major) throws IOException {
     for (JVMClusterUtil.RegionServerThread t:
         this.hbaseCluster.getRegionServers()) {
-      for(HRegion r: t.getRegionServer().getOnlineRegionsLocalContext()) {
+      for (Region r: t.getRegionServer().getOnlineRegionsLocalContext()) {
         if(r.getTableDesc().getTableName().equals(tableName)) {
-          r.compactStores(major);
+          ((HRegion)r).compactStores(major);
         }
       }
     }
@@ -594,9 +595,9 @@ public class MiniHBaseCluster extends HBaseCluster {
     List<HRegion> ret = new ArrayList<HRegion>();
     for (JVMClusterUtil.RegionServerThread rst : getRegionServerThreads()) {
       HRegionServer hrs = rst.getRegionServer();
-      for (HRegion region : hrs.getOnlineRegionsLocalContext()) {
+      for (Region region : hrs.getOnlineRegionsLocalContext()) {
         if (region.getTableDesc().getTableName().equals(tableName)) {
-          ret.add(region);
+          ret.add((HRegion)region);
         }
       }
     }
@@ -622,8 +623,7 @@ public class MiniHBaseCluster extends HBaseCluster {
     int count = 0;
     for (JVMClusterUtil.RegionServerThread rst: getRegionServerThreads()) {
       HRegionServer hrs = rst.getRegionServer();
-      HRegion metaRegion =
-        hrs.getOnlineRegion(regionName);
+      Region metaRegion = hrs.getOnlineRegion(regionName);
       if (metaRegion != null) {
         index = count;
         break;
@@ -675,19 +675,18 @@ public class MiniHBaseCluster extends HBaseCluster {
   }
 
   public List<HRegion> findRegionsForTable(TableName tableName) {
-    ArrayList<HRegion> ret = new ArrayList<HRegion>();
+    List<HRegion> ret = new ArrayList<HRegion>();
     for (JVMClusterUtil.RegionServerThread rst : getRegionServerThreads()) {
       HRegionServer hrs = rst.getRegionServer();
-      for (HRegion region : hrs.getOnlineRegions(tableName)) {
+      for (Region region : hrs.getOnlineRegions(tableName)) {
         if (region.getTableDesc().getTableName().equals(tableName)) {
-          ret.add(region);
+          ret.add((HRegion)region);
         }
       }
     }
     return ret;
   }
 
-
   protected int getRegionServerIndex(ServerName serverName) {
     //we have a small number of region servers, this should be fine for now.
     List<RegionServerThread> servers = getRegionServerThreads();
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/MockRegionServerServices.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/MockRegionServerServices.java
index f90f51e..2108795 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/MockRegionServerServices.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/MockRegionServerServices.java
@@ -39,9 +39,9 @@ import org.apache.hadoop.hbase.protobuf.generated.RPCProtos;
 import org.apache.hadoop.hbase.protobuf.generated.RegionServerStatusProtos.RegionStateTransition.TransitionCode;
 import org.apache.hadoop.hbase.regionserver.CompactionRequestor;
 import org.apache.hadoop.hbase.regionserver.FlushRequester;
-import org.apache.hadoop.hbase.regionserver.HRegion;
 import org.apache.hadoop.hbase.regionserver.HeapMemoryManager;
 import org.apache.hadoop.hbase.regionserver.Leases;
+import org.apache.hadoop.hbase.regionserver.Region;
 import org.apache.hadoop.hbase.regionserver.RegionServerAccounting;
 import org.apache.hadoop.hbase.regionserver.RegionServerServices;
 import org.apache.hadoop.hbase.regionserver.ServerNonceManager;
@@ -54,7 +54,7 @@ import org.apache.zookeeper.KeeperException;
  * Basic mock region server services.  Should only be instantiated by HBaseTestingUtility.b
  */
 class MockRegionServerServices implements RegionServerServices {
-  private final Map<String, HRegion> regions = new HashMap<String, HRegion>();
+  private final Map<String, Region> regions = new HashMap<String, Region>();
   private boolean stopping = false;
   private final ConcurrentSkipListMap<byte[], Boolean> rit =
     new ConcurrentSkipListMap<byte[], Boolean>(Bytes.BYTES_COMPARATOR);
@@ -78,26 +78,26 @@ class MockRegionServerServices implements RegionServerServices {
   }
 
   @Override
-  public boolean removeFromOnlineRegions(HRegion r, ServerName destination) {
+  public boolean removeFromOnlineRegions(Region r, ServerName destination) {
     return this.regions.remove(r.getRegionInfo().getEncodedName()) != null;
   }
 
   @Override
-  public HRegion getFromOnlineRegions(String encodedRegionName) {
+  public Region getFromOnlineRegions(String encodedRegionName) {
     return this.regions.get(encodedRegionName);
   }
 
-  public List<HRegion> getOnlineRegions(TableName tableName) throws IOException {
+  public List<Region> getOnlineRegions(TableName tableName) throws IOException {
     return null;
   }
 
   @Override
-  public void addToOnlineRegions(HRegion r) {
+  public void addToOnlineRegions(Region r) {
     this.regions.put(r.getRegionInfo().getEncodedName(), r);
   }
 
   @Override
-  public void postOpenDeployTasks(HRegion r, CatalogTracker ct)
+  public void postOpenDeployTasks(Region r, CatalogTracker ct)
       throws KeeperException, IOException {
     addToOnlineRegions(r);
   }
@@ -216,7 +216,7 @@ class MockRegionServerServices implements RegionServerServices {
   }
 
   @Override
-  public Map<String, HRegion> getRecoveringRegions() {
+  public Map<String, Region> getRecoveringRegions() {
     // TODO Auto-generated method stub
     return null;
   }
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/TestGlobalMemStoreSize.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/TestGlobalMemStoreSize.java
index 90420fd..f00ceeb 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/TestGlobalMemStoreSize.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/TestGlobalMemStoreSize.java
@@ -23,6 +23,7 @@ import static org.junit.Assert.*;
 import java.io.IOException;
 import java.util.List;
 import java.util.ArrayList;
+
 import org.apache.commons.logging.Log;
 import org.apache.commons.logging.LogFactory;
 import org.apache.hadoop.conf.Configuration;
@@ -30,6 +31,7 @@ import org.apache.hadoop.hbase.client.HTable;
 import org.apache.hadoop.hbase.protobuf.ProtobufUtil;
 import org.apache.hadoop.hbase.regionserver.HRegionServer;
 import org.apache.hadoop.hbase.regionserver.HRegion;
+import org.apache.hadoop.hbase.regionserver.Region;
 import org.apache.hadoop.hbase.testclassification.MediumTests;
 import org.apache.hadoop.hbase.util.Bytes;
 import org.apache.hadoop.hbase.util.JVMClusterUtil;
@@ -85,7 +87,7 @@ public class TestGlobalMemStoreSize {
       for (HRegionInfo regionInfo : ProtobufUtil.getOnlineRegions(server)) {
         globalMemStoreSize += 
           server.getFromOnlineRegions(regionInfo.getEncodedName()).
-          getMemstoreSize().get();
+          getMemstoreSize();
       }
       assertEquals(server.getRegionServerAccounting().getGlobalMemstoreSize(),
         globalMemStoreSize);
@@ -98,7 +100,7 @@ public class TestGlobalMemStoreSize {
         ", size=" + server.getRegionServerAccounting().getGlobalMemstoreSize());
 
       for (HRegionInfo regionInfo : ProtobufUtil.getOnlineRegions(server)) {
-        HRegion r = server.getFromOnlineRegions(regionInfo.getEncodedName());
+        Region r = server.getFromOnlineRegions(regionInfo.getEncodedName());
         flush(r, server);
       }
       LOG.info("Post flush on " + server.getServerName());
@@ -113,14 +115,14 @@ public class TestGlobalMemStoreSize {
         // If size > 0, see if its because the meta region got edits while
         // our test was running....
         for (HRegionInfo regionInfo : ProtobufUtil.getOnlineRegions(server)) {
-          HRegion r = server.getFromOnlineRegions(regionInfo.getEncodedName());
-          long l = r.getMemstoreSize().longValue();
+          Region r = server.getFromOnlineRegions(regionInfo.getEncodedName());
+          long l = r.getMemstoreSize();
           if (l > 0) {
             // Only meta could have edits at this stage.  Give it another flush
             // clear them.
             assertTrue(regionInfo.isMetaRegion());
             LOG.info(r.toString() + " " + l + ", reflushing");
-            r.flushcache();
+            flush(r, server);
           }
         }
       }
@@ -138,10 +140,10 @@ public class TestGlobalMemStoreSize {
    * @param server
    * @throws IOException
    */
-  private void flush(final HRegion r, final HRegionServer server)
+  private void flush(final Region r, final HRegionServer server)
   throws IOException {
     LOG.info("Flush " + r.toString() + " on " + server.getServerName() +
-      ", " +  r.flushcache() + ", size=" +
+      ", " +  ((HRegion)r).flushcache() + ", size=" +
       server.getRegionServerAccounting().getGlobalMemstoreSize());
   }
 
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/TestIOFencing.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/TestIOFencing.java
index 6fe1a2b..442db17 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/TestIOFencing.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/TestIOFencing.java
@@ -39,6 +39,7 @@ import org.apache.hadoop.hbase.regionserver.ConstantSizeRegionSplitPolicy;
 import org.apache.hadoop.hbase.regionserver.HRegion;
 import org.apache.hadoop.hbase.regionserver.HRegionServer;
 import org.apache.hadoop.hbase.regionserver.HStore;
+import org.apache.hadoop.hbase.regionserver.Region;
 import org.apache.hadoop.hbase.regionserver.RegionServerServices;
 import org.apache.hadoop.hbase.regionserver.Store;
 import org.apache.hadoop.hbase.regionserver.StoreFile;
@@ -280,13 +281,14 @@ public class TestIOFencing {
       long startWaitTime = System.currentTimeMillis();
       while (compactingRegion.getLastFlushTime() <= lastFlushTime ||
           compactingRegion.countStoreFiles() <= 1) {
-        LOG.info("Waiting for the region to flush " + compactingRegion.getRegionNameAsString());
+        LOG.info("Waiting for the region to flush " +
+          compactingRegion.getRegionInfo().getRegionNameAsString());
         Thread.sleep(1000);
         assertTrue("Timed out waiting for the region to flush",
           System.currentTimeMillis() - startWaitTime < 30000);
       }
       assertTrue(compactingRegion.countStoreFiles() > 1);
-      final byte REGION_NAME[] = compactingRegion.getRegionName();
+      final byte REGION_NAME[] = compactingRegion.getRegionInfo().getRegionName();
       LOG.info("Asking for compaction");
       admin.majorCompact(TABLE_NAME.getName());
       LOG.info("Waiting for compaction to be about to start");
@@ -304,7 +306,7 @@ public class TestIOFencing {
       Waiter.waitFor(c, 60000, new Waiter.Predicate<Exception>() {
         @Override
         public boolean evaluate() throws Exception {
-          HRegion newRegion = newServer.getOnlineRegion(REGION_NAME);
+          Region newRegion = newServer.getOnlineRegion(REGION_NAME);
           return newRegion != null && !newRegion.isRecovering();
         }
       });
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/backup/TestHFileArchiving.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/backup/TestHFileArchiving.java
index e29c713..8fb6d53 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/backup/TestHFileArchiving.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/backup/TestHFileArchiving.java
@@ -44,6 +44,7 @@ import org.apache.hadoop.hbase.regionserver.ConstantSizeRegionSplitPolicy;
 import org.apache.hadoop.hbase.regionserver.HRegion;
 import org.apache.hadoop.hbase.regionserver.HRegionFileSystem;
 import org.apache.hadoop.hbase.regionserver.HRegionServer;
+import org.apache.hadoop.hbase.regionserver.Region;
 import org.apache.hadoop.hbase.util.Bytes;
 import org.apache.hadoop.hbase.util.FSUtils;
 import org.apache.hadoop.hbase.util.HFileArchiveTestingUtil;
@@ -231,10 +232,10 @@ public class TestHFileArchiving {
     UTIL.loadRegion(region, TEST_FAM);
 
     // get the hfiles in the region
-    List<HRegion> regions = hrs.getOnlineRegions(TABLE_NAME);
+    List<Region> regions = hrs.getOnlineRegions(TABLE_NAME);
     assertEquals("More that 1 region for test table.", 1, regions.size());
 
-    region = regions.get(0);
+    region = (HRegion)regions.get(0);
     // wait for all the compactions to complete
     region.waitForFlushesAndCompactions();
 
@@ -310,10 +311,10 @@ public class TestHFileArchiving {
     UTIL.loadRegion(region, TEST_FAM);
 
     // get the hfiles in the region
-    List<HRegion> regions = hrs.getOnlineRegions(TABLE_NAME);
+    List<Region> regions = hrs.getOnlineRegions(TABLE_NAME);
     assertEquals("More that 1 region for test table.", 1, regions.size());
 
-    region = regions.get(0);
+    region = (HRegion)regions.get(0);
     // wait for all the compactions to complete
     region.waitForFlushesAndCompactions();
 
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestAdmin2.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestAdmin2.java
index 5710a96..5e74fd1 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestAdmin2.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestAdmin2.java
@@ -57,6 +57,7 @@ import org.apache.hadoop.hbase.master.HMaster;
 import org.apache.hadoop.hbase.protobuf.ProtobufUtil;
 import org.apache.hadoop.hbase.regionserver.HRegion;
 import org.apache.hadoop.hbase.regionserver.HRegionServer;
+import org.apache.hadoop.hbase.regionserver.Region;
 import org.apache.hadoop.hbase.regionserver.wal.HLogUtilsForTests;
 import org.apache.hadoop.hbase.util.Bytes;
 import org.apache.hadoop.hbase.util.Pair;
@@ -534,10 +535,9 @@ public class TestAdmin2 {
 
     // flush all regions
 
-    List<HRegion> regions = new ArrayList<HRegion>(regionServer
-        .getOnlineRegionsLocalContext());
-    for (HRegion r : regions) {
-      r.flushcache();
+    List<Region> regions = new ArrayList<Region>(regionServer.getOnlineRegionsLocalContext());
+    for (Region r : regions) {
+      ((HRegion)r).flushcache();
     }
     admin.rollHLogWriter(regionServer.getServerName().getServerName());
     int count = HLogUtilsForTests.getNumRolledLogFiles(regionServer.getWAL());
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestClientPushback.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestClientPushback.java
index 266adfe..c013461 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestClientPushback.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestClientPushback.java
@@ -34,8 +34,8 @@ import org.apache.hadoop.hbase.client.backoff.ClientBackoffPolicy;
 import org.apache.hadoop.hbase.client.backoff.ExponentialClientBackoffPolicy;
 import org.apache.hadoop.hbase.client.backoff.ServerStatistics;
 import org.apache.hadoop.hbase.client.coprocessor.Batch;
-import org.apache.hadoop.hbase.regionserver.HRegion;
 import org.apache.hadoop.hbase.regionserver.HRegionServer;
+import org.apache.hadoop.hbase.regionserver.Region;
 import org.apache.hadoop.hbase.testclassification.MediumTests;
 import org.apache.hadoop.hbase.util.Bytes;
 import org.apache.hadoop.hbase.util.EnvironmentEdgeManager;
@@ -96,7 +96,7 @@ public class TestClientPushback {
     HTable table = (HTable) conn.getTable(tablename);
 
     HRegionServer rs = UTIL.getHBaseCluster().getRegionServer(0);
-    HRegion region = rs.getOnlineRegions(tablename).get(0);
+    Region region = rs.getOnlineRegions(tablename).get(0);
 
     LOG.debug("Writing some data to "+tablename);
     // write some data
@@ -106,7 +106,8 @@ public class TestClientPushback {
     table.flushCommits();
 
     // get the current load on RS. Hopefully memstore isn't flushed since we wrote the the data
-    int load = (int)((region.addAndGetGlobalMemstoreSize(0) * 100) / flushSizeBytes);
+    int load = (int)((rs.getRegionServerAccounting().getGlobalMemstoreSize() * 100) /
+        flushSizeBytes);
     LOG.debug("Done writing some data to "+tablename);
 
     // get the stats for the region hosting our table
@@ -118,7 +119,7 @@ public class TestClientPushback {
     assertNotNull( "No stats configured for the client!", stats);
     // get the names so we can query the stats
     ServerName server = rs.getServerName();
-    byte[] regionName = region.getRegionName();
+    byte[] regionName = region.getRegionInfo().getRegionName();
 
     // check to see we found some load on the memstore
     ServerStatistics serverStats = stats.getServerStatsForTesting(server);
@@ -129,8 +130,8 @@ public class TestClientPushback {
     // check that the load reported produces a nonzero delay
     long backoffTime = backoffPolicy.getBackoffTime(server, regionName, serverStats);
     assertNotEquals("Reported load does not produce a backoff", backoffTime, 0);
-    LOG.debug("Backoff calculated for " + region.getRegionNameAsString() + " @ " + server +
-      " is " + backoffTime);
+    LOG.debug("Backoff calculated for " + region.getRegionInfo().getRegionNameAsString() + " @ " +
+      server + " is " + backoffTime);
 
     // Reach into the connection and submit work directly to AsyncProcess so we can
     // monitor how long the submission was delayed via a callback
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestFromClientSide.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestFromClientSide.java
index 7eae85a..34ddc3f 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestFromClientSide.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestFromClientSide.java
@@ -92,6 +92,7 @@ import org.apache.hadoop.hbase.protobuf.generated.MultiRowMutationProtos.MultiRo
 import org.apache.hadoop.hbase.regionserver.HRegion;
 import org.apache.hadoop.hbase.regionserver.HRegionServer;
 import org.apache.hadoop.hbase.regionserver.NoSuchColumnFamilyException;
+import org.apache.hadoop.hbase.regionserver.Region;
 import org.apache.hadoop.hbase.regionserver.Store;
 import org.apache.hadoop.hbase.util.Bytes;
 import org.apache.hadoop.hbase.util.EnvironmentEdgeManager;
@@ -4360,8 +4361,8 @@ public class TestFromClientSide {
     // in Store.rowAtOrBeforeFromStoreFile
     table.setAutoFlush(true);
     String regionName = table.getRegionLocations().firstKey().getEncodedName();
-    HRegion region =
-        TEST_UTIL.getRSForFirstRegionInTable(tableAname).getFromOnlineRegions(regionName);
+    Region region = TEST_UTIL.getRSForFirstRegionInTable(tableAname)
+        .getFromOnlineRegions(regionName);
     Put put1 = new Put(firstRow);
     Put put2 = new Put(secondRow);
     Put put3 = new Put(thirdRow);
@@ -4379,7 +4380,7 @@ public class TestFromClientSide {
     table.put(put2);
     table.put(put3);
     table.put(put4);
-    region.flushcache();
+    ((HRegion)region).flushcache();
     Result result = null;
 
     // Test before first that null is returned
@@ -5080,9 +5081,9 @@ public class TestFromClientSide {
     HTable table = TEST_UTIL.createTable(tableName, new byte [][] {FAMILY});
     // get the block cache and region
     String regionName = table.getRegionLocations().firstKey().getEncodedName();
-    HRegion region = TEST_UTIL.getRSForFirstRegionInTable(
-        tableName).getFromOnlineRegions(regionName);
-    Store store = region.getStores().values().iterator().next();
+    Region region = TEST_UTIL.getRSForFirstRegionInTable(tableName)
+        .getFromOnlineRegions(regionName);
+    Store store = region.getStores().iterator().next();
     CacheConfig cacheConf = store.getCacheConfig();
     cacheConf.setCacheDataOnWrite(true);
     cacheConf.setEvictOnClose(true);
@@ -5117,7 +5118,7 @@ public class TestFromClientSide {
     assertEquals(startBlockMiss, cache.getStats().getMissCount());
     // flush the data
     System.out.println("Flushing cache");
-    region.flushcache();
+    ((HRegion)region).flushcache();
     // expect one more block in cache, no change in hits/misses
     long expectedBlockCount = startBlockCount + 1;
     long expectedBlockHits = startBlockHits;
@@ -5144,7 +5145,7 @@ public class TestFromClientSide {
     assertEquals(expectedBlockMiss, cache.getStats().getMissCount());
     // flush, one new block
     System.out.println("Flushing cache");
-    region.flushcache();
+    ((HRegion)region).flushcache();
     assertEquals(++expectedBlockCount, cache.getBlockCount());
     assertEquals(expectedBlockHits, cache.getStats().getHitCount());
     assertEquals(expectedBlockMiss, cache.getStats().getMissCount());
@@ -5152,7 +5153,7 @@ public class TestFromClientSide {
     System.out.println("Compacting");
     assertEquals(2, store.getStorefilesCount());
     store.triggerMajorCompaction();
-    region.compactStores();
+    ((HRegion)region).compactStores();
     waitForStoreFileCount(store, 1, 10000); // wait 10 seconds max
     assertEquals(1, store.getStorefilesCount());
     expectedBlockCount -= 2; // evicted two blocks, cached none
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestHCM.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestHCM.java
index 0ab204d..65340a3 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestHCM.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestHCM.java
@@ -58,8 +58,8 @@ import org.apache.hadoop.hbase.exceptions.RegionMovedException;
 import org.apache.hadoop.hbase.filter.Filter;
 import org.apache.hadoop.hbase.filter.FilterBase;
 import org.apache.hadoop.hbase.master.HMaster;
-import org.apache.hadoop.hbase.regionserver.HRegion;
 import org.apache.hadoop.hbase.regionserver.HRegionServer;
+import org.apache.hadoop.hbase.regionserver.Region;
 import org.apache.hadoop.hbase.regionserver.RegionServerStoppedException;
 import org.apache.hadoop.hbase.util.Bytes;
 import org.apache.hadoop.hbase.util.EnvironmentEdgeManager;
@@ -795,9 +795,9 @@ public class TestHCM {
     ServerName destServerName = destServer.getServerName();
 
     //find another row in the cur server that is less than ROW_X
-    List<HRegion> regions = curServer.getOnlineRegions(TABLE_NAME3);
+    List<Region> regions = curServer.getOnlineRegions(TABLE_NAME3);
     byte[] otherRow = null;
-    for (HRegion region : regions) {
+    for (Region region : regions) {
       if (!region.getRegionInfo().getEncodedName().equals(toMove.getRegionInfo().getEncodedName())
           && Bytes.BYTES_COMPARATOR.compare(region.getRegionInfo().getStartKey(), ROW_X) < 0) {
         otherRow = region.getRegionInfo().getStartKey();
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/coprocessor/ColumnAggregationEndpointNullResponse.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/coprocessor/ColumnAggregationEndpointNullResponse.java
index f91b8a6..4315946 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/coprocessor/ColumnAggregationEndpointNullResponse.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/coprocessor/ColumnAggregationEndpointNullResponse.java
@@ -33,8 +33,8 @@ import org.apache.hadoop.hbase.coprocessor.protobuf.generated.ColumnAggregationW
 import org.apache.hadoop.hbase.coprocessor.protobuf.generated.ColumnAggregationWithNullResponseProtos.SumRequest;
 import org.apache.hadoop.hbase.coprocessor.protobuf.generated.ColumnAggregationWithNullResponseProtos.SumResponse;
 import org.apache.hadoop.hbase.protobuf.ResponseConverter;
-import org.apache.hadoop.hbase.regionserver.HRegion;
 import org.apache.hadoop.hbase.regionserver.InternalScanner;
+import org.apache.hadoop.hbase.regionserver.Region;
 import org.apache.hadoop.hbase.util.Bytes;
 
 import com.google.protobuf.RpcCallback;
@@ -86,9 +86,9 @@ implements Coprocessor, CoprocessorService  {
     int sumResult = 0;
     InternalScanner scanner = null;
     try {
-      HRegion region = this.env.getRegion();
+      Region region = this.env.getRegion();
       // for the last region in the table, return null to test null handling
-      if (Bytes.equals(region.getEndKey(), HConstants.EMPTY_END_ROW)) {
+      if (Bytes.equals(region.getRegionInfo().getEndKey(), HConstants.EMPTY_END_ROW)) {
         done.run(null);
         return;
       }
@@ -122,6 +122,6 @@ implements Coprocessor, CoprocessorService  {
     }
     done.run(SumResponse.newBuilder().setSum(sumResult).build());
     LOG.info("Returning sum " + sumResult + " for region " +
-        Bytes.toStringBinary(env.getRegion().getRegionName()));
+        Bytes.toStringBinary(env.getRegion().getRegionInfo().getRegionName()));
   }
 }
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/coprocessor/ColumnAggregationEndpointWithErrors.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/coprocessor/ColumnAggregationEndpointWithErrors.java
index a16fc19..54289ef 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/coprocessor/ColumnAggregationEndpointWithErrors.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/coprocessor/ColumnAggregationEndpointWithErrors.java
@@ -34,8 +34,8 @@ import org.apache.hadoop.hbase.coprocessor.protobuf.generated.ColumnAggregationW
 import org.apache.hadoop.hbase.coprocessor.protobuf.generated.ColumnAggregationWithErrorsProtos.SumRequest;
 import org.apache.hadoop.hbase.coprocessor.protobuf.generated.ColumnAggregationWithErrorsProtos.SumResponse;
 import org.apache.hadoop.hbase.protobuf.ResponseConverter;
-import org.apache.hadoop.hbase.regionserver.HRegion;
 import org.apache.hadoop.hbase.regionserver.InternalScanner;
+import org.apache.hadoop.hbase.regionserver.Region;
 import org.apache.hadoop.hbase.util.Bytes;
 
 import com.google.protobuf.RpcCallback;
@@ -87,9 +87,9 @@ implements Coprocessor, CoprocessorService  {
     int sumResult = 0;
     InternalScanner scanner = null;
     try {
-      HRegion region = this.env.getRegion();
+      Region region = this.env.getRegion();
       // throw an exception for requests to the last region in the table, to test error handling
-      if (Bytes.equals(region.getEndKey(), HConstants.EMPTY_END_ROW)) {
+      if (Bytes.equals(region.getRegionInfo().getEndKey(), HConstants.EMPTY_END_ROW)) {
         throw new DoNotRetryIOException("An expected exception");
       }
       scanner = region.getScanner(scan);
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/coprocessor/SimpleRegionObserver.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/coprocessor/SimpleRegionObserver.java
index bf53518..ea85b0f 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/coprocessor/SimpleRegionObserver.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/coprocessor/SimpleRegionObserver.java
@@ -54,8 +54,8 @@ import org.apache.hadoop.hbase.filter.CompareFilter.CompareOp;
 import org.apache.hadoop.hbase.io.FSDataInputStreamWrapper;
 import org.apache.hadoop.hbase.io.Reference;
 import org.apache.hadoop.hbase.io.hfile.CacheConfig;
-import org.apache.hadoop.hbase.regionserver.HRegion;
-import org.apache.hadoop.hbase.regionserver.HRegion.Operation;
+import org.apache.hadoop.hbase.regionserver.Region;
+import org.apache.hadoop.hbase.regionserver.Region.Operation;
 import org.apache.hadoop.hbase.regionserver.InternalScanner;
 import org.apache.hadoop.hbase.regionserver.KeyValueScanner;
 import org.apache.hadoop.hbase.regionserver.Leases;
@@ -150,8 +150,8 @@ public class SimpleRegionObserver extends BaseRegionObserver {
     // from external packages
     RegionCoprocessorEnvironment re = (RegionCoprocessorEnvironment)e;
     Leases leases = re.getRegionServerServices().getLeases();
-    leases.createLease(re.getRegion().getRegionNameAsString(), 2000, null);
-    leases.cancelLease(re.getRegion().getRegionNameAsString());
+    leases.createLease(re.getRegionInfo().getRegionNameAsString(), 2000, null);
+    leases.cancelLease(re.getRegionInfo().getRegionNameAsString());
   }
 
   @Override
@@ -228,7 +228,7 @@ public class SimpleRegionObserver extends BaseRegionObserver {
   }
   
   @Override
-  public void postSplit(ObserverContext<RegionCoprocessorEnvironment> c, HRegion l, HRegion r) {
+  public void postSplit(ObserverContext<RegionCoprocessorEnvironment> c, Region l, Region r) {
     ctPostSplit.incrementAndGet();
   }
 
@@ -345,8 +345,7 @@ public class SimpleRegionObserver extends BaseRegionObserver {
     assertNotNull(e.getRegion());
     assertNotNull(get);
     assertNotNull(results);
-    if (e.getRegion().getTableDesc().getTableName().equals(
-        TestRegionObserverInterface.TEST_TABLE)) {
+    if (e.getRegionInfo().getTable().equals(TestRegionObserverInterface.TEST_TABLE)) {
       boolean foundA = false;
       boolean foundB = false;
       boolean foundC = false;
@@ -377,26 +376,22 @@ public class SimpleRegionObserver extends BaseRegionObserver {
     assertNotNull(e);
     assertNotNull(e.getRegion());
     assertNotNull(familyMap);
-    if (e.getRegion().getTableDesc().getTableName().equals(
-        TestRegionObserverInterface.TEST_TABLE)) {
+    if (e.getRegionInfo().getTable().equals(TestRegionObserverInterface.TEST_TABLE)) {
       List<Cell> cells = familyMap.get(TestRegionObserverInterface.A);
       assertNotNull(cells);
       assertNotNull(cells.get(0));
       KeyValue kv = (KeyValue)cells.get(0);
-      assertTrue(Bytes.equals(kv.getQualifier(),
-          TestRegionObserverInterface.A));
+      assertTrue(Bytes.equals(kv.getQualifier(), TestRegionObserverInterface.A));
       cells = familyMap.get(TestRegionObserverInterface.B);
       assertNotNull(cells);
       assertNotNull(cells.get(0));
       kv = (KeyValue)cells.get(0);
-      assertTrue(Bytes.equals(kv.getQualifier(),
-          TestRegionObserverInterface.B));
+      assertTrue(Bytes.equals(kv.getQualifier(), TestRegionObserverInterface.B));
       cells = familyMap.get(TestRegionObserverInterface.C);
       assertNotNull(cells);
       assertNotNull(cells.get(0));
       kv = (KeyValue)cells.get(0);
-      assertTrue(Bytes.equals(kv.getQualifier(),
-          TestRegionObserverInterface.C));
+      assertTrue(Bytes.equals(kv.getQualifier(), TestRegionObserverInterface.C));
     }
     ctPrePut.incrementAndGet();
   }
@@ -411,8 +406,7 @@ public class SimpleRegionObserver extends BaseRegionObserver {
     assertNotNull(e.getRegion());
     assertNotNull(familyMap);
     List<Cell> cells = familyMap.get(TestRegionObserverInterface.A);
-    if (e.getRegion().getTableDesc().getTableName().equals(
-        TestRegionObserverInterface.TEST_TABLE)) {
+    if (e.getRegionInfo().getTable().equals(TestRegionObserverInterface.TEST_TABLE)) {
       assertNotNull(cells);
       assertNotNull(cells.get(0));
       // KeyValue v1 expectation.  Cast for now until we go all Cell all the time. TODO
@@ -629,8 +623,7 @@ public class SimpleRegionObserver extends BaseRegionObserver {
     RegionCoprocessorEnvironment e = ctx.getEnvironment();
     assertNotNull(e);
     assertNotNull(e.getRegion());
-    if (e.getRegion().getTableDesc().getTableName().equals(
-        TestRegionObserverInterface.TEST_TABLE)) {
+    if (e.getRegionInfo().getTable().equals(TestRegionObserverInterface.TEST_TABLE)) {
       assertNotNull(familyPaths);
       assertEquals(1,familyPaths.size());
       assertArrayEquals(familyPaths.get(0).getFirst(), TestRegionObserverInterface.A);
@@ -647,8 +640,7 @@ public class SimpleRegionObserver extends BaseRegionObserver {
     RegionCoprocessorEnvironment e = ctx.getEnvironment();
     assertNotNull(e);
     assertNotNull(e.getRegion());
-    if (e.getRegion().getTableDesc().getTableName().equals(
-        TestRegionObserverInterface.TEST_TABLE)) {
+    if (e.getRegionInfo().getTable().equals(TestRegionObserverInterface.TEST_TABLE)) {
       assertNotNull(familyPaths);
       assertEquals(1,familyPaths.size());
       assertArrayEquals(familyPaths.get(0).getFirst(), TestRegionObserverInterface.A);
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/coprocessor/TestClassLoading.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/coprocessor/TestClassLoading.java
index 8f1a6f1..c4afeb4 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/coprocessor/TestClassLoading.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/coprocessor/TestClassLoading.java
@@ -20,11 +20,10 @@ package org.apache.hadoop.hbase.coprocessor;
 
 import org.apache.commons.logging.Log;
 import org.apache.commons.logging.LogFactory;
-
 import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.hbase.*;
 import org.apache.hadoop.hbase.client.HBaseAdmin;
-import org.apache.hadoop.hbase.regionserver.HRegion;
+import org.apache.hadoop.hbase.regionserver.Region;
 import org.apache.hadoop.hbase.regionserver.TestServerCustomProtocol;
 import org.apache.hadoop.hbase.testclassification.MediumTests;
 import org.apache.hadoop.hbase.util.ClassLoaderTestHelper;
@@ -162,12 +161,11 @@ public class TestClassLoading {
     // verify that the coprocessors were loaded
     boolean foundTableRegion=false;
     boolean found1 = true, found2 = true, found2_k1 = true, found2_k2 = true, found2_k3 = true;
-    Map<HRegion, Set<ClassLoader>> regionsActiveClassLoaders =
-        new HashMap<HRegion, Set<ClassLoader>>();
+    Map<Region, Set<ClassLoader>> regionsActiveClassLoaders =
+        new HashMap<Region, Set<ClassLoader>>();
     MiniHBaseCluster hbase = TEST_UTIL.getHBaseCluster();
-    for (HRegion region:
-        hbase.getRegionServer(0).getOnlineRegionsLocalContext()) {
-      if (region.getRegionNameAsString().startsWith(tableName)) {
+    for (Region region: hbase.getRegionServer(0).getOnlineRegionsLocalContext()) {
+      if (region.getRegionInfo().getRegionNameAsString().startsWith(tableName)) {
         foundTableRegion = true;
         CoprocessorEnvironment env;
         env = region.getCoprocessorHost().findCoprocessorEnvironment(cpName1);
@@ -205,7 +203,7 @@ public class TestClassLoading {
     //check if region active classloaders are shared across all RS regions
     Set<ClassLoader> externalClassLoaders = new HashSet<ClassLoader>(
       CoprocessorClassLoader.getAllCached());
-    for (Map.Entry<HRegion, Set<ClassLoader>> regionCP : regionsActiveClassLoaders.entrySet()) {
+    for (Map.Entry<Region, Set<ClassLoader>> regionCP : regionsActiveClassLoaders.entrySet()) {
       assertTrue("Some CP classloaders for region " + regionCP.getKey() + " are not cached."
         + " ClassLoader Cache:" + externalClassLoaders
         + " Region ClassLoaders:" + regionCP.getValue(),
@@ -234,9 +232,8 @@ public class TestClassLoading {
     // verify that the coprocessor was loaded
     boolean found = false;
     MiniHBaseCluster hbase = TEST_UTIL.getHBaseCluster();
-    for (HRegion region:
-        hbase.getRegionServer(0).getOnlineRegionsLocalContext()) {
-      if (region.getRegionNameAsString().startsWith(cpName3)) {
+    for (Region region: hbase.getRegionServer(0).getOnlineRegionsLocalContext()) {
+      if (region.getRegionInfo().getRegionNameAsString().startsWith(cpName3)) {
         found = (region.getCoprocessorHost().findCoprocessor(cpName3) != null);
       }
     }
@@ -260,9 +257,8 @@ public class TestClassLoading {
     // verify that the coprocessor was loaded correctly
     boolean found = false;
     MiniHBaseCluster hbase = TEST_UTIL.getHBaseCluster();
-    for (HRegion region:
-        hbase.getRegionServer(0).getOnlineRegionsLocalContext()) {
-      if (region.getRegionNameAsString().startsWith(cpName4)) {
+    for (Region region: hbase.getRegionServer(0).getOnlineRegionsLocalContext()) {
+      if (region.getRegionInfo().getRegionNameAsString().startsWith(cpName4)) {
         Coprocessor cp = region.getCoprocessorHost().findCoprocessor(cpName4);
         if (cp != null) {
           found = true;
@@ -332,9 +328,8 @@ public class TestClassLoading {
         found6_k4 = false;
 
     MiniHBaseCluster hbase = TEST_UTIL.getHBaseCluster();
-    for (HRegion region:
-        hbase.getRegionServer(0).getOnlineRegionsLocalContext()) {
-      if (region.getRegionNameAsString().startsWith(tableName)) {
+    for (Region region: hbase.getRegionServer(0).getOnlineRegionsLocalContext()) {
+      if (region.getRegionInfo().getRegionNameAsString().startsWith(tableName)) {
         found_1 = found_1 ||
             (region.getCoprocessorHost().findCoprocessor(cpName1) != null);
         found_2 = found_2 ||
@@ -421,9 +416,8 @@ public class TestClassLoading {
     boolean found1 = false, found2 = false, found2_k1 = false,
         found2_k2 = false, found2_k3 = false;
     MiniHBaseCluster hbase = TEST_UTIL.getHBaseCluster();
-    for (HRegion region:
-        hbase.getRegionServer(0).getOnlineRegionsLocalContext()) {
-      if (region.getRegionNameAsString().startsWith(tableName)) {
+    for (Region region: hbase.getRegionServer(0).getOnlineRegionsLocalContext()) {
+      if (region.getRegionInfo().getRegionNameAsString().startsWith(tableName)) {
         CoprocessorEnvironment env;
         env = region.getCoprocessorHost().findCoprocessorEnvironment(cpName1);
         if (env != null) {
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/coprocessor/TestCoprocessorEndpoint.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/coprocessor/TestCoprocessorEndpoint.java
index 3c78390..bfbbf8f 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/coprocessor/TestCoprocessorEndpoint.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/coprocessor/TestCoprocessorEndpoint.java
@@ -69,7 +69,7 @@ public class TestCoprocessorEndpoint {
   private static final Log LOG = LogFactory.getLog(TestCoprocessorEndpoint.class);
 
   private static final TableName TEST_TABLE =
-      TableName.valueOf("TestCoprocessorEndpoint");
+      TableName.valueOf("TestTable");
   private static final byte[] TEST_FAMILY = Bytes.toBytes("TestFamily");
   private static final byte[] TEST_QUALIFIER = Bytes.toBytes("TestQualifier");
   private static byte[] ROW = Bytes.toBytes("testRow");
@@ -91,12 +91,12 @@ public class TestCoprocessorEndpoint {
     conf.setStrings(CoprocessorHost.MASTER_COPROCESSOR_CONF_KEY,
         ProtobufCoprocessorService.class.getName());
     util.startMiniCluster(2);
-
-    HBaseAdmin admin = util.getHBaseAdmin();
+    HBaseAdmin admin = new HBaseAdmin(conf);
     HTableDescriptor desc = new HTableDescriptor(TEST_TABLE);
     desc.addFamily(new HColumnDescriptor(TEST_FAMILY));
     admin.createTable(desc, new byte[][]{ROWS[rowSeperator1], ROWS[rowSeperator2]});
     util.waitUntilAllRegionsAssigned(TEST_TABLE);
+    admin.close();
 
     HTable table = new HTable(conf, TEST_TABLE);
     for (int i = 0; i < ROWSIZE; i++) {
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/coprocessor/TestCoprocessorInterface.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/coprocessor/TestCoprocessorInterface.java
index 9ebda14..2693297 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/coprocessor/TestCoprocessorInterface.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/coprocessor/TestCoprocessorInterface.java
@@ -54,6 +54,7 @@ import org.apache.hadoop.hbase.client.Get;
 import org.apache.hadoop.hbase.client.Scan;
 import org.apache.hadoop.hbase.regionserver.HRegion;
 import org.apache.hadoop.hbase.regionserver.InternalScanner;
+import org.apache.hadoop.hbase.regionserver.Region;
 import org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost;
 import org.apache.hadoop.hbase.regionserver.RegionScanner;
 import org.apache.hadoop.hbase.regionserver.ScanType;
@@ -213,7 +214,7 @@ public class TestCoprocessorInterface {
       preSplitWithSplitRowCalled = true;
     }
     @Override
-    public void postSplit(ObserverContext<RegionCoprocessorEnvironment> e, HRegion l, HRegion r) {
+    public void postSplit(ObserverContext<RegionCoprocessorEnvironment> e, Region l, Region r) {
       postSplitCalled = true;
     }
 
@@ -318,7 +319,7 @@ public class TestCoprocessorInterface {
     // now have all Environments fail
     for (int i = 0; i < regions.length; i++) {
       try {
-        byte [] r = regions[i].getStartKey();
+        byte [] r = regions[i].getRegionInfo().getStartKey();
         if (r == null || r.length <= 0) {
           // Its the start row.  Can't ask for null.  Ask for minimal key instead.
           r = new byte [] {0};
@@ -379,7 +380,8 @@ public class TestCoprocessorInterface {
 
     // HBASE-4197
     Scan s = new Scan();
-    RegionScanner scanner = regions[0].getCoprocessorHost().postScannerOpen(s, regions[0].getScanner(s));
+    RegionScanner scanner = regions[0].getCoprocessorHost().postScannerOpen(s,
+      regions[0].getScanner(s));
     assertTrue(scanner instanceof CustomScanner);
     // this would throw an exception before HBASE-4197
     scanner.next(new ArrayList<Cell>());
@@ -394,8 +396,7 @@ public class TestCoprocessorInterface {
 
     for (int i = 0; i < regions.length; i++) {
       HRegion.closeHRegion(regions[i]);
-      c = region.getCoprocessorHost()
-            .findCoprocessor(CoprocessorImpl.class.getName());
+      c = region.getCoprocessorHost().findCoprocessor(CoprocessorImpl.class.getName());
       assertTrue("Coprocessor not started", ((CoprocessorImpl)c).wasStarted());
       assertTrue("Coprocessor not stopped", ((CoprocessorImpl)c).wasStopped());
       assertTrue(((CoprocessorImpl)c).wasOpened());
@@ -500,12 +501,12 @@ public class TestCoprocessorInterface {
         i++;
       }
     } catch (IOException ioe) {
-      LOG.info("Split transaction of " + r.getRegionNameAsString() +
+      LOG.info("Split transaction of " + r.getRegionInfo().getRegionNameAsString() +
           " failed:" + ioe.getMessage());
       assertTrue(false);
     } catch (RuntimeException e) {
       LOG.info("Failed rollback of failed split of " +
-          r.getRegionNameAsString() + e.getMessage());
+          r.getRegionInfo().getRegionNameAsString() + e.getMessage());
     }
 
     assertTrue(i == 2);
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/coprocessor/TestCoprocessorTableEndpoint.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/coprocessor/TestCoprocessorTableEndpoint.java
deleted file mode 100644
index 5c91d2a..0000000
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/coprocessor/TestCoprocessorTableEndpoint.java
+++ /dev/null
@@ -1,181 +0,0 @@
-/*
- *
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.hadoop.hbase.coprocessor;
-
-import static org.junit.Assert.assertEquals;
-
-import java.io.IOException;
-import java.util.Map;
-
-import org.apache.hadoop.hbase.client.HBaseAdmin;
-import org.apache.hadoop.hbase.client.HTable;
-import org.apache.hadoop.hbase.testclassification.MediumTests;
-import org.apache.hadoop.hbase.util.ByteStringer;
-import org.apache.hadoop.hbase.HBaseTestingUtility;
-import org.apache.hadoop.hbase.HColumnDescriptor;
-import org.apache.hadoop.hbase.HTableDescriptor;
-import org.apache.hadoop.hbase.TableName;
-import org.apache.hadoop.hbase.client.Put;
-import org.apache.hadoop.hbase.client.coprocessor.Batch;
-import org.apache.hadoop.hbase.coprocessor.protobuf.generated.ColumnAggregationProtos;
-import org.apache.hadoop.hbase.ipc.BlockingRpcCallback;
-import org.apache.hadoop.hbase.util.Bytes;
-import org.junit.AfterClass;
-import org.junit.BeforeClass;
-import org.junit.Test;
-import org.junit.experimental.categories.Category;
-
-import com.google.protobuf.ServiceException;
-
-@Category(MediumTests.class)
-public class TestCoprocessorTableEndpoint {
-
-  private static final byte[] TEST_FAMILY = Bytes.toBytes("TestFamily");
-  private static final byte[] TEST_QUALIFIER = Bytes.toBytes("TestQualifier");
-  private static final byte[] ROW = Bytes.toBytes("testRow");
-  private static final int ROWSIZE = 20;
-  private static final int rowSeperator1 = 5;
-  private static final int rowSeperator2 = 12;
-  private static final byte[][] ROWS = makeN(ROW, ROWSIZE);
-
-  private static final HBaseTestingUtility TEST_UTIL = new HBaseTestingUtility();
-
-  @BeforeClass
-  public static void setupBeforeClass() throws Exception {
-    TEST_UTIL.startMiniCluster(2);
-  }
-
-  @AfterClass
-  public static void tearDownAfterClass() throws Exception {
-    TEST_UTIL.shutdownMiniCluster();
-  }
-
-  @Test
-  public void testCoprocessorTableEndpoint() throws Throwable {    
-    final TableName tableName = TableName.valueOf("testCoprocessorTableEndpoint");
-
-    HTableDescriptor desc = new HTableDescriptor(tableName);
-    desc.addFamily(new HColumnDescriptor(TEST_FAMILY));
-    desc.addCoprocessor(org.apache.hadoop.hbase.coprocessor.ColumnAggregationEndpoint.class.getName());
-
-    createTable(desc);
-    verifyTable(tableName);
-  }
-
-  @Test
-  public void testDynamicCoprocessorTableEndpoint() throws Throwable {    
-    final TableName tableName = TableName.valueOf("testDynamicCoprocessorTableEndpoint");
-
-    HTableDescriptor desc = new HTableDescriptor(tableName);
-    desc.addFamily(new HColumnDescriptor(TEST_FAMILY));
-
-    createTable(desc);
-
-    desc.addCoprocessor(org.apache.hadoop.hbase.coprocessor.ColumnAggregationEndpoint.class.getName());
-    updateTable(desc);
-
-    verifyTable(tableName);
-  }
-
-  private static byte[][] makeN(byte[] base, int n) {
-    byte[][] ret = new byte[n][];
-    for (int i = 0; i < n; i++) {
-      ret[i] = Bytes.add(base, Bytes.toBytes(String.format("%02d", i)));
-    }
-    return ret;
-  }
-
-  private static Map<byte [], Long> sum(final HTable table, final byte [] family,
-    final byte [] qualifier, final byte [] start, final byte [] end)
-      throws ServiceException, Throwable {
-  return table.coprocessorService(ColumnAggregationProtos.ColumnAggregationService.class,
-      start, end,
-    new Batch.Call<ColumnAggregationProtos.ColumnAggregationService, Long>() {
-      @Override
-      public Long call(ColumnAggregationProtos.ColumnAggregationService instance)
-      throws IOException {
-        BlockingRpcCallback<ColumnAggregationProtos.SumResponse> rpcCallback =
-            new BlockingRpcCallback<ColumnAggregationProtos.SumResponse>();
-        ColumnAggregationProtos.SumRequest.Builder builder =
-          ColumnAggregationProtos.SumRequest.newBuilder();
-        builder.setFamily(ByteStringer.wrap(family));
-        if (qualifier != null && qualifier.length > 0) {
-          builder.setQualifier(ByteStringer.wrap(qualifier));
-        }
-        instance.sum(null, builder.build(), rpcCallback);
-        return rpcCallback.get().getSum();
-      }
-    });
-  }
-
-  private static final void createTable(HTableDescriptor desc) throws Exception {
-    HBaseAdmin admin = TEST_UTIL.getHBaseAdmin();
-    admin.createTable(desc, new byte[][]{ROWS[rowSeperator1], ROWS[rowSeperator2]});
-    TEST_UTIL.waitUntilAllRegionsAssigned(desc.getTableName());
-    HTable table = new HTable(TEST_UTIL.getConfiguration(), desc.getTableName());
-    try {
-      for (int i = 0; i < ROWSIZE; i++) {
-        Put put = new Put(ROWS[i]);
-        put.add(TEST_FAMILY, TEST_QUALIFIER, Bytes.toBytes(i));
-        table.put(put);
-      }
-    } finally {
-      table.close();    
-    }
-  }
-
-  private static void updateTable(HTableDescriptor desc) throws Exception {
-    HBaseAdmin admin = TEST_UTIL.getHBaseAdmin();
-    admin.disableTable(desc.getTableName());
-    admin.modifyTable(desc.getTableName(), desc);
-    admin.enableTable(desc.getTableName());
-  }
-
-  private static final void verifyTable(TableName tableName) throws Throwable {
-    HTable table = new HTable(TEST_UTIL.getConfiguration(), tableName);
-    try {
-      Map<byte[], Long> results = sum(table, TEST_FAMILY, TEST_QUALIFIER, ROWS[0],
-        ROWS[ROWS.length-1]);
-      int sumResult = 0;
-      int expectedResult = 0;
-      for (Map.Entry<byte[], Long> e : results.entrySet()) {
-        sumResult += e.getValue();
-      }
-      for (int i = 0; i < ROWSIZE; i++) {
-        expectedResult += i;
-      }
-      assertEquals("Invalid result", expectedResult, sumResult);
-
-      // scan: for region 2 and region 3
-      results.clear();
-      results = sum(table, TEST_FAMILY, TEST_QUALIFIER, ROWS[rowSeperator1], ROWS[ROWS.length-1]);
-      sumResult = 0;
-      expectedResult = 0;
-      for (Map.Entry<byte[], Long> e : results.entrySet()) {
-        sumResult += e.getValue();
-      }
-      for (int i = rowSeperator1; i < ROWSIZE; i++) {
-        expectedResult += i;
-      }
-      assertEquals("Invalid result", expectedResult, sumResult);
-    } finally {
-      table.close();
-    }
-  }
-}
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/coprocessor/TestRegionObserverScannerOpenHook.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/coprocessor/TestRegionObserverScannerOpenHook.java
index 19bde79..e14e388 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/coprocessor/TestRegionObserverScannerOpenHook.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/coprocessor/TestRegionObserverScannerOpenHook.java
@@ -53,6 +53,7 @@ import org.apache.hadoop.hbase.regionserver.HRegionServer;
 import org.apache.hadoop.hbase.regionserver.HStore;
 import org.apache.hadoop.hbase.regionserver.InternalScanner;
 import org.apache.hadoop.hbase.regionserver.KeyValueScanner;
+import org.apache.hadoop.hbase.regionserver.Region;
 import org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost;
 import org.apache.hadoop.hbase.regionserver.RegionServerServices;
 import org.apache.hadoop.hbase.regionserver.ScanType;
@@ -269,10 +270,10 @@ public class TestRegionObserverScannerOpenHook {
     table.flushCommits();
 
     HRegionServer rs = UTIL.getRSForFirstRegionInTable(desc.getTableName());
-    List<HRegion> regions = rs.getOnlineRegions(desc.getTableName());
+    List<Region> regions = rs.getOnlineRegions(desc.getTableName());
     assertEquals("More than 1 region serving test table with 1 row", 1, regions.size());
-    HRegion region = regions.get(0);
-    admin.flush(region.getRegionName());
+    Region region = regions.get(0);
+    admin.flush(region.getRegionInfo().getRegionName());
     CountDownLatch latch = ((CompactionCompletionNotifyingRegion)region)
         .getCompactionStateChangeLatch();
     
@@ -281,7 +282,7 @@ public class TestRegionObserverScannerOpenHook {
     put.add(A, A, A);
     table.put(put);
     table.flushCommits();
-    admin.flush(region.getRegionName());
+    admin.flush(region.getRegionInfo().getRegionName());
 
     // run a compaction, which normally would should get rid of the data
     // wait for the compaction checker to complete
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/coprocessor/TestRegionServerObserver.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/coprocessor/TestRegionServerObserver.java
index 7330167..49bab15 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/coprocessor/TestRegionServerObserver.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/coprocessor/TestRegionServerObserver.java
@@ -40,6 +40,7 @@ import org.apache.hadoop.hbase.client.HBaseAdmin;
 import org.apache.hadoop.hbase.client.Mutation;
 import org.apache.hadoop.hbase.regionserver.HRegion;
 import org.apache.hadoop.hbase.regionserver.HRegionServer;
+import org.apache.hadoop.hbase.regionserver.Region;
 import org.apache.hadoop.hbase.regionserver.RegionMergeTransaction;
 import org.apache.hadoop.hbase.regionserver.RegionServerCoprocessorHost;
 import org.apache.hadoop.hbase.util.Bytes;
@@ -88,7 +89,7 @@ public class TestRegionServerObserver {
       desc.addFamily(new HColumnDescriptor(FAM));
       admin.createTable(desc, new byte[][] { Bytes.toBytes("row") });
       assertFalse(regionServerObserver.wasRegionMergeCalled());
-      List<HRegion> regions = regionServer.getOnlineRegions(TableName.valueOf(TABLENAME));
+      List<Region> regions = regionServer.getOnlineRegions(TableName.valueOf(TABLENAME));
       admin.mergeRegions(regions.get(0).getRegionInfo().getEncodedNameAsBytes(), regions.get(1)
           .getRegionInfo().getEncodedNameAsBytes(), true);
       int regionsCount = regionServer.getOnlineRegions(TableName.valueOf(TABLENAME)).size();
@@ -133,20 +134,21 @@ public class TestRegionServerObserver {
     }
 
     @Override
-    public void preMerge(ObserverContext<RegionServerCoprocessorEnvironment> ctx, HRegion regionA,
-        HRegion regionB) throws IOException {
+    public void preMerge(ObserverContext<RegionServerCoprocessorEnvironment> ctx, Region regionA,
+        Region regionB) throws IOException {
       preMergeCalled = true;
     }
 
     @Override
     public void preMergeCommit(ObserverContext<RegionServerCoprocessorEnvironment> ctx,
-        HRegion regionA, HRegion regionB, List<Mutation> metaEntries) throws IOException {
+        Region regionA, Region regionB, List<Mutation> metaEntries) throws IOException {
       preMergeBeforePONRCalled = true;
       RegionServerCoprocessorEnvironment environment = ctx.getEnvironment();
       HRegionServer rs = (HRegionServer) environment.getRegionServerServices();
-      List<HRegion> onlineRegions =
+      List<Region> onlineRegions =
           rs.getOnlineRegions(TableName.valueOf("testRegionServerObserver_2"));
-      rmt = new RegionMergeTransaction(onlineRegions.get(0), onlineRegions.get(1), true);
+      rmt = new RegionMergeTransaction((HRegion)onlineRegions.get(0),
+        (HRegion)onlineRegions.get(1), true);
       if (!rmt.prepare(rs)) {
         LOG.error("Prepare for the region merge of table "
             + onlineRegions.get(0).getTableDesc().getNameAsString()
@@ -162,7 +164,7 @@ public class TestRegionServerObserver {
 
     @Override
     public void postMergeCommit(ObserverContext<RegionServerCoprocessorEnvironment> ctx,
-        HRegion regionA, HRegion regionB, HRegion mr) throws IOException {
+        Region regionA, Region regionB, Region mr) throws IOException {
       preMergeAfterPONRCalled = true;
       RegionServerCoprocessorEnvironment environment = ctx.getEnvironment();
       HRegionServer rs = (HRegionServer) environment.getRegionServerServices();
@@ -171,19 +173,19 @@ public class TestRegionServerObserver {
 
     @Override
     public void preRollBackMerge(ObserverContext<RegionServerCoprocessorEnvironment> ctx,
-        HRegion regionA, HRegion regionB) throws IOException {
+        Region regionA, Region regionB) throws IOException {
       preRollBackMergeCalled = true;
     }
 
     @Override
     public void postRollBackMerge(ObserverContext<RegionServerCoprocessorEnvironment> ctx,
-        HRegion regionA, HRegion regionB) throws IOException {
+        Region regionA, Region regionB) throws IOException {
       postRollBackMergeCalled = true;
     }
 
     @Override
-    public void postMerge(ObserverContext<RegionServerCoprocessorEnvironment> c, HRegion regionA,
-        HRegion regionB, HRegion mergedRegion) throws IOException {
+    public void postMerge(ObserverContext<RegionServerCoprocessorEnvironment> c, Region regionA,
+        Region regionB, Region mergedRegion) throws IOException {
       postMergeCalled = true;
     }
 
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/coprocessor/TestRowProcessorEndpoint.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/coprocessor/TestRowProcessorEndpoint.java
index e2af220..3da584e 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/coprocessor/TestRowProcessorEndpoint.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/coprocessor/TestRowProcessorEndpoint.java
@@ -63,8 +63,8 @@ import org.apache.hadoop.hbase.protobuf.generated.RowProcessorProtos.ProcessRequ
 import org.apache.hadoop.hbase.protobuf.generated.RowProcessorProtos.ProcessResponse;
 import org.apache.hadoop.hbase.protobuf.generated.RowProcessorProtos.RowProcessorService;
 import org.apache.hadoop.hbase.regionserver.BaseRowProcessor;
-import org.apache.hadoop.hbase.regionserver.HRegion;
 import org.apache.hadoop.hbase.regionserver.InternalScanner;
+import org.apache.hadoop.hbase.regionserver.Region;
 import org.apache.hadoop.hbase.regionserver.wal.WALEdit;
 import org.apache.hadoop.hbase.util.Bytes;
 import org.junit.AfterClass;
@@ -329,7 +329,7 @@ public class TestRowProcessorEndpoint {
       }
 
       @Override
-      public void process(long now, HRegion region,
+      public void process(long now, Region region,
           List<Mutation> mutations, WALEdit walEdit) throws IOException {
         // Scan current counter
         List<Cell> kvs = new ArrayList<Cell>();
@@ -413,7 +413,7 @@ public class TestRowProcessorEndpoint {
       }
 
       @Override
-      public void process(long now, HRegion region,
+      public void process(long now, Region region,
           List<Mutation> mutations, WALEdit walEdit) throws IOException {
         List<Cell> kvs = new ArrayList<Cell>();
         { // First scan to get friends of the person
@@ -497,7 +497,7 @@ public class TestRowProcessorEndpoint {
       }
 
       @Override
-      public void process(long now, HRegion region,
+      public void process(long now, Region region,
           List<Mutation> mutations, WALEdit walEdit) throws IOException {
 
         // Override the time to avoid race-condition in the unit test caused by
@@ -591,7 +591,7 @@ public class TestRowProcessorEndpoint {
       }
 
       @Override
-      public void process(long now, HRegion region,
+      public void process(long now, Region region,
           List<Mutation> mutations, WALEdit walEdit) throws IOException {
         try {
           // Sleep for a long time so it timeout
@@ -625,7 +625,7 @@ public class TestRowProcessorEndpoint {
     }
 
     public static void doScan(
-        HRegion region, Scan scan, List<Cell> result) throws IOException {
+        Region region, Scan scan, List<Cell> result) throws IOException {
       InternalScanner scanner = null;
       try {
         scan.setIsolationLevel(IsolationLevel.READ_UNCOMMITTED);
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/mapreduce/TestImportTSVWithOperationAttributes.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/mapreduce/TestImportTSVWithOperationAttributes.java
index 6210730..0b9453e 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/mapreduce/TestImportTSVWithOperationAttributes.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/mapreduce/TestImportTSVWithOperationAttributes.java
@@ -39,7 +39,6 @@ import org.apache.hadoop.hbase.HBaseTestingUtility;
 import org.apache.hadoop.hbase.HConstants;
 import org.apache.hadoop.hbase.testclassification.LargeTests;
 import org.apache.hadoop.hbase.client.Durability;
-import org.apache.hadoop.hbase.client.HBaseAdmin;
 import org.apache.hadoop.hbase.client.HTable;
 import org.apache.hadoop.hbase.client.Put;
 import org.apache.hadoop.hbase.client.Result;
@@ -48,7 +47,7 @@ import org.apache.hadoop.hbase.client.Scan;
 import org.apache.hadoop.hbase.coprocessor.BaseRegionObserver;
 import org.apache.hadoop.hbase.coprocessor.ObserverContext;
 import org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment;
-import org.apache.hadoop.hbase.regionserver.HRegion;
+import org.apache.hadoop.hbase.regionserver.Region;
 import org.apache.hadoop.hbase.regionserver.wal.WALEdit;
 import org.apache.hadoop.hbase.util.Bytes;
 import org.apache.hadoop.util.Tool;
@@ -96,7 +95,6 @@ public class TestImportTSVWithOperationAttributes implements Configurable {
     conf.set("hbase.coprocessor.master.classes", OperationAttributesTestController.class.getName());
     conf.set("hbase.coprocessor.region.classes", OperationAttributesTestController.class.getName());
     util.startMiniCluster();
-    HBaseAdmin admin = new HBaseAdmin(util.getConfiguration());
     util.startMiniMapReduceCluster();
   }
 
@@ -242,11 +240,11 @@ public class TestImportTSVWithOperationAttributes implements Configurable {
     @Override
     public void prePut(ObserverContext<RegionCoprocessorEnvironment> e, Put put, WALEdit edit,
         Durability durability) throws IOException {
-      HRegion region = e.getEnvironment().getRegion();
+      Region region = e.getEnvironment().getRegion();
       if (!region.getRegionInfo().isMetaTable()
           && !region.getRegionInfo().getTable().isSystemTable()) {
         if (put.getAttribute(TEST_ATR_KEY) != null) {
-          LOG.debug("allow any put to happen " + region.getRegionNameAsString());
+          LOG.debug("allow any put to happen " + region.getRegionInfo().getRegionNameAsString());
         } else {
           e.bypass();
         }
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/mapreduce/TestImportTSVWithTTLs.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/mapreduce/TestImportTSVWithTTLs.java
index bbb266f..1f76002 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/mapreduce/TestImportTSVWithTTLs.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/mapreduce/TestImportTSVWithTTLs.java
@@ -39,7 +39,7 @@ import org.apache.hadoop.hbase.client.Put;
 import org.apache.hadoop.hbase.coprocessor.BaseRegionObserver;
 import org.apache.hadoop.hbase.coprocessor.ObserverContext;
 import org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment;
-import org.apache.hadoop.hbase.regionserver.HRegion;
+import org.apache.hadoop.hbase.regionserver.Region;
 import org.apache.hadoop.hbase.regionserver.wal.WALEdit;
 import org.apache.hadoop.hbase.testclassification.LargeTests;
 import org.apache.hadoop.hbase.util.Bytes;
@@ -157,7 +157,7 @@ public class TestImportTSVWithTTLs implements Configurable {
     @Override
     public void prePut(ObserverContext<RegionCoprocessorEnvironment> e, Put put, WALEdit edit,
         Durability durability) throws IOException {
-      HRegion region = e.getEnvironment().getRegion();
+      Region region = e.getEnvironment().getRegion();
       if (!region.getRegionInfo().isMetaTable()
           && !region.getRegionInfo().getTable().isSystemTable()) {
         // The put carries the TTL attribute
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/master/MockRegionServer.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/master/MockRegionServer.java
index c09a82d..13dde01 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/master/MockRegionServer.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/master/MockRegionServer.java
@@ -28,6 +28,7 @@ import java.util.TreeMap;
 import java.util.concurrent.ConcurrentSkipListMap;
 
 import com.google.protobuf.Message;
+
 import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.fs.FileSystem;
 import org.apache.hadoop.hbase.CellScannable;
@@ -90,9 +91,9 @@ import org.apache.hadoop.hbase.protobuf.generated.RPCProtos;
 import org.apache.hadoop.hbase.protobuf.generated.RegionServerStatusProtos.RegionStateTransition.TransitionCode;
 import org.apache.hadoop.hbase.regionserver.CompactionRequestor;
 import org.apache.hadoop.hbase.regionserver.FlushRequester;
-import org.apache.hadoop.hbase.regionserver.HRegion;
 import org.apache.hadoop.hbase.regionserver.HeapMemoryManager;
 import org.apache.hadoop.hbase.regionserver.Leases;
+import org.apache.hadoop.hbase.regionserver.Region;
 import org.apache.hadoop.hbase.regionserver.RegionServerAccounting;
 import org.apache.hadoop.hbase.regionserver.RegionServerServices;
 import org.apache.hadoop.hbase.regionserver.ServerNonceManager;
@@ -248,18 +249,18 @@ ClientProtos.ClientService.BlockingInterface, RegionServerServices {
   }
 
   @Override
-  public void addToOnlineRegions(HRegion r) {
+  public void addToOnlineRegions(Region r) {
     // TODO Auto-generated method stub
   }
 
   @Override
-  public boolean removeFromOnlineRegions(HRegion r, ServerName destination) {
+  public boolean removeFromOnlineRegions(Region r, ServerName destination) {
     // TODO Auto-generated method stub
     return false;
   }
 
   @Override
-  public HRegion getFromOnlineRegions(String encodedRegionName) {
+  public Region getFromOnlineRegions(String encodedRegionName) {
     // TODO Auto-generated method stub
     return null;
   }
@@ -313,7 +314,7 @@ ClientProtos.ClientService.BlockingInterface, RegionServerServices {
   }
 
   @Override
-  public void postOpenDeployTasks(HRegion r, CatalogTracker ct)
+  public void postOpenDeployTasks(Region r, CatalogTracker ct)
       throws KeeperException, IOException {
     // TODO Auto-generated method stub
   }
@@ -504,7 +505,7 @@ ClientProtos.ClientService.BlockingInterface, RegionServerServices {
   }
 
   @Override
-  public List<HRegion> getOnlineRegions(TableName tableName) throws IOException {
+  public List<Region> getOnlineRegions(TableName tableName) throws IOException {
     // TODO Auto-generated method stub
     return null;
   }
@@ -545,7 +546,7 @@ ClientProtos.ClientService.BlockingInterface, RegionServerServices {
   }
 
   @Override
-  public Map<String, HRegion> getRecoveringRegions() {
+  public Map<String, Region> getRecoveringRegions() {
     // TODO Auto-generated method stub
     return null;
   }
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/master/TestAssignmentListener.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/master/TestAssignmentListener.java
index 36a8265..38c49d0 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/master/TestAssignmentListener.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/master/TestAssignmentListener.java
@@ -36,13 +36,12 @@ import org.apache.hadoop.hbase.client.HBaseAdmin;
 import org.apache.hadoop.hbase.client.HTable;
 import org.apache.hadoop.hbase.client.Put;
 import org.apache.hadoop.hbase.regionserver.HRegion;
+import org.apache.hadoop.hbase.regionserver.Region;
 import org.apache.hadoop.hbase.util.Bytes;
 import org.apache.hadoop.hbase.util.JVMClusterUtil;
-
 import org.junit.AfterClass;
 import org.junit.BeforeClass;
 import org.junit.Test;
-
 import org.junit.experimental.categories.Category;
 
 @Category(MediumTests.class)
@@ -227,8 +226,8 @@ public class TestAssignmentListener {
         admin.majorCompact(TABLE_NAME_STR);
         mergeable = 0;
         for (JVMClusterUtil.RegionServerThread regionThread: miniCluster.getRegionServerThreads()) {
-          for (HRegion region: regionThread.getRegionServer().getOnlineRegions(TABLE_NAME)) {
-            mergeable += region.isMergeable() ? 1 : 0;
+          for (Region region: regionThread.getRegionServer().getOnlineRegions(TABLE_NAME)) {
+            mergeable += ((HRegion)region).isMergeable() ? 1 : 0;
           }
         }
       }
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/master/TestDistributedLogSplitting.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/master/TestDistributedLogSplitting.java
index 6df70d3..25d3f27 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/master/TestDistributedLogSplitting.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/master/TestDistributedLogSplitting.java
@@ -26,6 +26,7 @@ import static org.apache.hadoop.hbase.SplitLogCounters.tot_wkr_task_done;
 import static org.apache.hadoop.hbase.SplitLogCounters.tot_wkr_task_err;
 import static org.apache.hadoop.hbase.SplitLogCounters.tot_wkr_task_resigned;
 import static org.junit.Assert.*;
+
 import java.io.IOException;
 import java.util.ArrayList;
 import java.util.Arrays;
@@ -81,6 +82,7 @@ import org.apache.hadoop.hbase.protobuf.ProtobufUtil;
 import org.apache.hadoop.hbase.protobuf.generated.AdminProtos.GetRegionInfoResponse.CompactionState;
 import org.apache.hadoop.hbase.regionserver.HRegion;
 import org.apache.hadoop.hbase.regionserver.HRegionServer;
+import org.apache.hadoop.hbase.regionserver.Region;
 import org.apache.hadoop.hbase.regionserver.wal.HLog;
 import org.apache.hadoop.hbase.regionserver.wal.HLogFactory;
 import org.apache.hadoop.hbase.regionserver.wal.HLogUtil;
@@ -1407,7 +1409,7 @@ public class TestDistributedLogSplitting {
         }
         LOG.debug("adding data to rs = " + rst.getName() +
             " region = "+ hri.getRegionNameAsString());
-        HRegion region = hrs.getOnlineRegion(hri.getRegionName());
+        Region region = hrs.getOnlineRegion(hri.getRegionName());
         assertTrue(region != null);
         putData(region, hri.getStartKey(), nrows, Bytes.toBytes("q"), family);
       }
@@ -1496,7 +1498,7 @@ public class TestDistributedLogSplitting {
     master.assignmentManager.waitUntilNoRegionsInTransition(60000);
   }
 
-  private void putData(HRegion region, byte[] startRow, int numRows, byte [] qf,
+  private void putData(Region region, byte[] startRow, int numRows, byte [] qf,
       byte [] ...families)
   throws IOException {
     for(int i = 0; i < numRows; i++) {
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/master/TestGetLastFlushedSequenceId.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/master/TestGetLastFlushedSequenceId.java
index 4409259..c869f3e 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/master/TestGetLastFlushedSequenceId.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/master/TestGetLastFlushedSequenceId.java
@@ -31,8 +31,8 @@ import org.apache.hadoop.hbase.NamespaceDescriptor;
 import org.apache.hadoop.hbase.TableName;
 import org.apache.hadoop.hbase.client.HTable;
 import org.apache.hadoop.hbase.client.Put;
-import org.apache.hadoop.hbase.regionserver.HRegion;
 import org.apache.hadoop.hbase.regionserver.HRegionServer;
+import org.apache.hadoop.hbase.regionserver.Region;
 import org.apache.hadoop.hbase.testclassification.MediumTests;
 import org.apache.hadoop.hbase.util.Bytes;
 import org.apache.hadoop.hbase.util.JVMClusterUtil;
@@ -76,10 +76,10 @@ public class TestGetLastFlushedSequenceId {
     table.flushCommits();
     MiniHBaseCluster cluster = testUtil.getMiniHBaseCluster();
     List<JVMClusterUtil.RegionServerThread> rsts = cluster.getRegionServerThreads();
-    HRegion region = null;
+    Region region = null;
     for (int i = 0; i < cluster.getRegionServerThreads().size(); i++) {
       HRegionServer hrs = rsts.get(i).getRegionServer();
-      for (HRegion r : hrs.getOnlineRegions(tableName)) {
+      for (Region r : hrs.getOnlineRegions(tableName)) {
         region = r;
         break;
       }
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/master/TestMasterFailover.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/master/TestMasterFailover.java
index 83ad29d..a56d506 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/master/TestMasterFailover.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/master/TestMasterFailover.java
@@ -57,6 +57,7 @@ import org.apache.hadoop.hbase.protobuf.ProtobufUtil;
 import org.apache.hadoop.hbase.protobuf.RequestConverter;
 import org.apache.hadoop.hbase.regionserver.HRegion;
 import org.apache.hadoop.hbase.regionserver.HRegionServer;
+import org.apache.hadoop.hbase.regionserver.Region;
 import org.apache.hadoop.hbase.regionserver.RegionMergeTransaction;
 import org.apache.hadoop.hbase.regionserver.RegionServerStoppedException;
 import org.apache.hadoop.hbase.util.Bytes;
@@ -950,7 +951,7 @@ public class TestMasterFailover {
     List<RegionServerThread> regionServerThreads =
       cluster.getRegionServerThreads();
     int count = -1;
-    HRegion metaRegion = null;
+    Region metaRegion = null;
     for (RegionServerThread regionServerThread : regionServerThreads) {
       HRegionServer regionServer = regionServerThread.getRegionServer();
       metaRegion = regionServer.getOnlineRegion(HRegionInfo.FIRST_META_REGIONINFO.getRegionName());
@@ -963,8 +964,7 @@ public class TestMasterFailover {
     TEST_UTIL.shutdownMiniHBaseCluster();
 
     // Create a ZKW to use in the test
-    ZooKeeperWatcher zkw =
-      HBaseTestingUtility.createAndForceNodeToOpenedState(TEST_UTIL,
+    ZooKeeperWatcher zkw = HBaseTestingUtility.createAndForceNodeToOpenedState(TEST_UTIL,
           metaRegion, regionServer.getServerName());
 
     LOG.info("Staring cluster for second time");
@@ -1359,9 +1359,9 @@ public class TestMasterFailover {
     // region server should expire (how it can be verified?)
     MetaRegionTracker.setMetaLocation(activeMaster.getZooKeeper(),
       rs.getServerName(), State.PENDING_OPEN);
-    HRegion meta = rs.getFromOnlineRegions(HRegionInfo.FIRST_META_REGIONINFO.getEncodedName());
+    Region meta = rs.getFromOnlineRegions(HRegionInfo.FIRST_META_REGIONINFO.getEncodedName());
     rs.removeFromOnlineRegions(meta, null);
-    meta.close();
+    ((HRegion)meta).close();
 
     log("Aborting master");
     activeMaster.stop("test-kill");
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/master/TestMasterStatusServlet.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/master/TestMasterStatusServlet.java
index 3d2f7ff..d628639 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/master/TestMasterStatusServlet.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/master/TestMasterStatusServlet.java
@@ -96,7 +96,6 @@ public class TestMasterStatusServlet {
     ZooKeeperWatcher zkw = Mockito.mock(ZooKeeperWatcher.class);
     Mockito.doReturn("fakequorum").when(zkw).getQuorum();
     Mockito.doReturn(zkw).when(master).getZooKeeperWatcher();
-    Mockito.doReturn(zkw).when(master).getZooKeeper();
 
     // Fake MasterAddressTracker
     MasterAddressTracker tracker = Mockito.mock(MasterAddressTracker.class);
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/master/TestOpenedRegionHandler.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/master/TestOpenedRegionHandler.java
index 13766dc..d386d39 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/master/TestOpenedRegionHandler.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/master/TestOpenedRegionHandler.java
@@ -35,6 +35,7 @@ import org.apache.hadoop.hbase.executor.EventType;
 import org.apache.hadoop.hbase.master.handler.OpenedRegionHandler;
 import org.apache.hadoop.hbase.regionserver.HRegion;
 import org.apache.hadoop.hbase.regionserver.HRegionServer;
+import org.apache.hadoop.hbase.regionserver.Region;
 import org.apache.hadoop.hbase.testclassification.MediumTests;
 import org.apache.hadoop.hbase.util.Bytes;
 import org.apache.hadoop.hbase.util.MockServer;
@@ -188,6 +189,7 @@ public class TestOpenedRegionHandler {
 
     return cluster;
   }
+
   private void abortMaster(MiniHBaseCluster cluster) {
     // Stop the master
     log("Aborting master");
@@ -195,20 +197,21 @@ public class TestOpenedRegionHandler {
     cluster.waitOnMaster(0);
     log("Master has aborted");
   }
+
   private HRegion getRegionBeingServed(MiniHBaseCluster cluster,
       HRegionServer regionServer) {
-    Collection<HRegion> onlineRegionsLocalContext = regionServer
-        .getOnlineRegionsLocalContext();
-    Iterator<HRegion> iterator = onlineRegionsLocalContext.iterator();
-    HRegion region = null;
+    Collection<Region> onlineRegionsLocalContext = regionServer.getOnlineRegionsLocalContext();
+    Iterator<Region> iterator = onlineRegionsLocalContext.iterator();
+    Region region = null;
     while (iterator.hasNext()) {
       region = iterator.next();
       if (!region.getRegionInfo().isMetaTable()) {
         break;
       }
     }
-    return region;
+    return (HRegion)region;
   }
+  
   private void log(String msg) {
     LOG.debug("\n\nTRR: " + msg + "\n");
   }
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/master/TestRegionPlacement.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/master/TestRegionPlacement.java
index 63e0c65..67f8283 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/master/TestRegionPlacement.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/master/TestRegionPlacement.java
@@ -60,8 +60,8 @@ import org.apache.hadoop.hbase.master.balancer.FavoredNodeLoadBalancer;
 import org.apache.hadoop.hbase.master.balancer.FavoredNodesPlan;
 import org.apache.hadoop.hbase.master.balancer.FavoredNodesPlan.Position;
 import org.apache.hadoop.hbase.master.balancer.LoadBalancerFactory;
-import org.apache.hadoop.hbase.regionserver.HRegion;
 import org.apache.hadoop.hbase.regionserver.HRegionServer;
+import org.apache.hadoop.hbase.regionserver.Region;
 import org.apache.hadoop.hbase.util.Bytes;
 import org.apache.hadoop.hbase.util.Pair;
 import org.apache.zookeeper.KeeperException;
@@ -301,10 +301,10 @@ public class TestRegionPlacement {
       // kill a random non-meta server carrying at least one region
       killIndex = random.nextInt(servers.length);
       serverToKill = TEST_UTIL.getHBaseCluster().getRegionServer(killIndex).getServerName();
-      Collection<HRegion> regs =
+      Collection<Region> regs =
           TEST_UTIL.getHBaseCluster().getRegionServer(killIndex).getOnlineRegionsLocalContext();
       isNamespaceServer = false;
-      for (HRegion r : regs) {
+      for (Region r : regs) {
         if (r.getRegionInfo().getTable().getNamespaceAsString()
             .equals(NamespaceDescriptor.SYSTEM_NAMESPACE_NAME_STR)) {
           isNamespaceServer = true;
@@ -549,8 +549,7 @@ public class TestRegionPlacement {
     MiniHBaseCluster cluster = TEST_UTIL.getHBaseCluster();
     for (int i = 0; i < SLAVES; i++) {
       HRegionServer rs = cluster.getRegionServer(i);
-      for (HRegion region: rs.getOnlineRegions(
-          TableName.valueOf("testRegionAssignment"))) {
+      for (Region region: rs.getOnlineRegions(TableName.valueOf("testRegionAssignment"))) {
         InetSocketAddress[] favoredSocketAddress = rs.getFavoredNodesForRegion(
             region.getRegionInfo().getEncodedName());
         List<ServerName> favoredServerList = plan.getAssignmentMap().get(region.getRegionInfo());
@@ -579,7 +578,7 @@ public class TestRegionPlacement {
             assertNotNull(addrFromPlan);
             assertTrue("Region server " + rs.getServerName().getHostAndPort()
                 + " has the " + positions[j] +
-                " for region " + region.getRegionNameAsString() + " is " +
+                " for region " + region.getRegionInfo().getRegionNameAsString() + " is " +
                 addrFromRS + " which is inconsistent with the plan "
                 + addrFromPlan, addrFromRS.equals(addrFromPlan));
           }
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/master/TestTableLockManager.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/master/TestTableLockManager.java
index ee84674..481e445 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/master/TestTableLockManager.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/master/TestTableLockManager.java
@@ -371,7 +371,7 @@ public class TestTableLockManager {
         try {
           HRegion region = TEST_UTIL.getSplittableRegion(tableName, -1);
           if (region != null) {
-            byte[] regionName = region.getRegionName();
+            byte[] regionName = region.getRegionInfo().getRegionName();
             admin.flush(regionName);
             admin.compact(regionName);
             admin.split(regionName);
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/NoOpScanPolicyObserver.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/NoOpScanPolicyObserver.java
index 36493cd..d5e0221 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/NoOpScanPolicyObserver.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/NoOpScanPolicyObserver.java
@@ -74,7 +74,7 @@ public class NoOpScanPolicyObserver extends BaseRegionObserver {
   public KeyValueScanner preStoreScannerOpen(final ObserverContext<RegionCoprocessorEnvironment> c,
       Store store, final Scan scan, final NavigableSet<byte[]> targetCols, KeyValueScanner s)
       throws IOException {
-    HRegion r = c.getEnvironment().getRegion();
+    Region r = c.getEnvironment().getRegion();
     return scan.isReversed() ? new ReversedStoreScanner(store,
         store.getScanInfo(), scan, targetCols, r.getReadpoint(scan
             .getIsolationLevel())) : new StoreScanner(store,
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestAtomicOperation.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestAtomicOperation.java
index ac9c8a2..22551a8 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestAtomicOperation.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestAtomicOperation.java
@@ -70,6 +70,8 @@ import org.junit.Test;
 import org.junit.experimental.categories.Category;
 import org.junit.rules.TestName;
 
+import com.google.common.base.Preconditions;
+
 /**
  * Testing of HRegion.incrementColumnValue, HRegion.increment,
  * and HRegion.append
@@ -124,11 +126,11 @@ public class TestAtomicOperation {
     a.setReturnResults(false);
     a.add(fam1, qual1, Bytes.toBytes(v1));
     a.add(fam1, qual2, Bytes.toBytes(v2));
-    assertNull(region.append(a));
+    assertNull(region.append(a, HConstants.NO_NONCE, HConstants.NO_NONCE));
     a = new Append(row);
     a.add(fam1, qual1, Bytes.toBytes(v2));
     a.add(fam1, qual2, Bytes.toBytes(v1));
-    Result result = region.append(a);
+    Result result = region.append(a, HConstants.NO_NONCE, HConstants.NO_NONCE);
     assertEquals(0, Bytes.compareTo(Bytes.toBytes(v1+v2), result.getValue(fam1, qual1)));
     assertEquals(0, Bytes.compareTo(Bytes.toBytes(v2+v1), result.getValue(fam1, qual2)));
   }
@@ -237,7 +239,7 @@ public class TestAtomicOperation {
           inc.addColumn(fam1, qual2, amount*2);
           inc.addColumn(fam2, qual3, amount*3);
           inc.setDurability(Durability.ASYNC_WAL);
-          region.increment(inc);
+          region.increment(inc, HConstants.NO_NONCE, HConstants.NO_NONCE);
 
           // verify: Make sure we only see completed increments
           Get g = new Get(row);
@@ -275,7 +277,7 @@ public class TestAtomicOperation {
               a.add(fam1, qual2, val);
               a.add(fam2, qual3, val);
               a.setDurability(Durability.ASYNC_WAL);
-              region.append(a);
+              region.append(a, HConstants.NO_NONCE, HConstants.NO_NONCE);
 
               Get g = new Get(row);
               Result result = region.get(g);
@@ -459,7 +461,7 @@ public class TestAtomicOperation {
                 p.add(fam1, qual1, value2);
                 mrm.add(p);
               }
-              region.mutateRowsWithLocks(mrm, rowsToLock);
+              region.mutateRowsWithLocks(mrm, rowsToLock, HConstants.NO_NONCE, HConstants.NO_NONCE);
               op ^= true;
               // check: should always see exactly one column
               Scan s = new Scan(row);
@@ -545,7 +547,7 @@ public class TestAtomicOperation {
     put.add(Bytes.toBytes(family), Bytes.toBytes("q1"), Bytes.toBytes("10"));
     puts[0] = put;
     
-    region.batchMutate(puts);
+    region.batchMutate(puts, HConstants.NO_NONCE, HConstants.NO_NONCE);
     MultithreadedTestUtil.TestContext ctx =
       new MultithreadedTestUtil.TestContext(conf);
     ctx.addThread(new PutThread(ctx, region));
@@ -578,7 +580,7 @@ public class TestAtomicOperation {
       put.add(Bytes.toBytes(family), Bytes.toBytes("q1"), Bytes.toBytes("50"));
       puts[0] = put;
       testStep = TestStep.PUT_STARTED;
-      region.batchMutate(puts);
+      region.batchMutate(puts, HConstants.NO_NONCE, HConstants.NO_NONCE);
     }
   }
 
@@ -619,10 +621,13 @@ public class TestAtomicOperation {
       return new WrappedRowLock(super.getRowLockInternal(row, waitForLock));
     }
     
-    public class WrappedRowLock extends RowLock {
+    public class WrappedRowLock extends RowLockImpl {
 
       private WrappedRowLock(RowLock rowLock) {
-        super(rowLock.context);
+        super();
+        Preconditions.checkArgument(rowLock instanceof RowLockImpl,
+          "rowLock must be a RowLockImpl");
+        setContext(((RowLockImpl)rowLock).getContext());
       }
 
       @Override
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestCompaction.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestCompaction.java
index db315b7..8b77a7c 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestCompaction.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestCompaction.java
@@ -316,7 +316,7 @@ public class TestCompaction {
     CountDownLatch latch = new CountDownLatch(numStores);
     // create some store files and setup requests for each store on which we want to do a
     // compaction
-    for (Store store : r.getStores().values()) {
+    for (Store store : r.getStores()) {
       createStoreFile(r, store.getColumnFamilyName());
       createStoreFile(r, store.getColumnFamilyName());
       createStoreFile(r, store.getColumnFamilyName());
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestCompactionState.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestCompactionState.java
index 211b1b4..dd8eceb 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestCompactionState.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestCompactionState.java
@@ -88,7 +88,7 @@ public class TestCompactionState {
     HTable ht = null;
     try {
       ht = TEST_UTIL.createTable(table, family);
-      HBaseAdmin admin = new HBaseAdmin(TEST_UTIL.getConfiguration());
+      HBaseAdmin admin = TEST_UTIL.getHBaseAdmin();
       try {
         admin.compact(table, fakecf);
       } catch (IOException ioe) {
@@ -133,11 +133,11 @@ public class TestCompactionState {
       ht = TEST_UTIL.createTable(table, families);
       loadData(ht, families, 3000, flushes);
       HRegionServer rs = TEST_UTIL.getMiniHBaseCluster().getRegionServer(0);
-      List<HRegion> regions = rs.getOnlineRegions(table);
+      List<Region> regions = rs.getOnlineRegions(table);
       int countBefore = countStoreFilesInFamilies(regions, families);
       int countBeforeSingleFamily = countStoreFilesInFamily(regions, family);
       assertTrue(countBefore > 0); // there should be some data files
-      HBaseAdmin admin = new HBaseAdmin(TEST_UTIL.getConfiguration());
+      HBaseAdmin admin = TEST_UTIL.getHBaseAdmin();
       if (expectedState == CompactionState.MINOR) {
         if (singleFamily) {
           admin.compact(table.getName(), family);
@@ -163,8 +163,8 @@ public class TestCompactionState {
       // Now, should have the right compaction state,
       // otherwise, the compaction should have already been done
       if (expectedState != state) {
-        for (HRegion region: regions) {
-          state = region.getCompactionState();
+        for (Region region: regions) {
+          state = ((HRegion)region).getCompactionState();
           assertEquals(CompactionState.NONE, state);
         }
       } else {
@@ -201,13 +201,13 @@ public class TestCompactionState {
   }
 
   private static int countStoreFilesInFamily(
-      List<HRegion> regions, final byte[] family) {
+      List<Region> regions, final byte[] family) {
     return countStoreFilesInFamilies(regions, new byte[][]{family});
   }
 
-  private static int countStoreFilesInFamilies(List<HRegion> regions, final byte[][] families) {
+  private static int countStoreFilesInFamilies(List<Region> regions, final byte[][] families) {
     int count = 0;
-    for (HRegion region: regions) {
+    for (Region region: regions) {
       count += region.getStoreFileList(families).size();
     }
     return count;
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestEncryptionKeyRotation.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestEncryptionKeyRotation.java
index 32f0ad9..c13cc0a 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestEncryptionKeyRotation.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestEncryptionKeyRotation.java
@@ -195,9 +195,9 @@ public class TestEncryptionKeyRotation {
 
   private static List<Path> findStorefilePaths(TableName tableName) throws Exception {
     List<Path> paths = new ArrayList<Path>();
-    for (HRegion region:
+    for (Region region:
         TEST_UTIL.getRSForFirstRegionInTable(tableName).getOnlineRegions(tableName)) {
-      for (Store store: region.getStores().values()) {
+      for (Store store: region.getStores()) {
         for (StoreFile storefile: store.getStorefiles()) {
           paths.add(storefile.getPath());
         }
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestEncryptionRandomKeying.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestEncryptionRandomKeying.java
index 850709d..c146e0b 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestEncryptionRandomKeying.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestEncryptionRandomKeying.java
@@ -52,9 +52,9 @@ public class TestEncryptionRandomKeying {
 
   private static List<Path> findStorefilePaths(byte[] tableName) throws Exception {
     List<Path> paths = new ArrayList<Path>();
-    for (HRegion region:
+    for (Region region:
         TEST_UTIL.getRSForFirstRegionInTable(tableName).getOnlineRegions(htd.getTableName())) {
-      for (Store store: region.getStores().values()) {
+      for (Store store: region.getStores()) {
         for (StoreFile storefile: store.getStorefiles()) {
           paths.add(storefile.getPath());
         }
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestEndToEndSplitTransaction.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestEndToEndSplitTransaction.java
index d401cc9..391aebf 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestEndToEndSplitTransaction.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestEndToEndSplitTransaction.java
@@ -99,12 +99,10 @@ public class TestEndToEndSplitTransaction {
     byte []firstRow = Bytes.toBytes("aaa");
     byte []splitRow = Bytes.toBytes("lll");
     byte []lastRow = Bytes.toBytes("zzz");
-    HConnection con = HConnectionManager
-        .getConnection(TEST_UTIL.getConfiguration());
+    HConnection con = HConnectionManager.getConnection(TEST_UTIL.getConfiguration());
     // this will also cache the region
-    byte[] regionName = con.locateRegion(tableName, splitRow).getRegionInfo()
-        .getRegionName();
-    HRegion region = server.getRegion(regionName);
+    byte[] regionName = con.locateRegion(tableName, splitRow).getRegionInfo().getRegionName();
+    Region region = server.getRegion(regionName);
     SplitTransaction split = new SplitTransaction(region, splitRow);
     split.useZKForAssignment = ConfigUtil.useZKForAssignment(conf);
     split.prepare();
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestGetClosestAtOrBefore.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestGetClosestAtOrBefore.java
index 3aaa4b8..3c57a06 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestGetClosestAtOrBefore.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestGetClosestAtOrBefore.java
@@ -161,7 +161,7 @@ public class TestGetClosestAtOrBefore extends HBaseTestCase {
         tableb, tofindBytes,
       HConstants.NINES, false);
     LOG.info("find=" + new String(metaKey));
-    Result r = mr.getClosestRowBefore(metaKey);
+    Result r = mr.getClosestRowBefore(metaKey, HConstants.CATALOG_FAMILY);
     if (answer == -1) {
       assertNull(r);
       return null;
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestHRegion.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestHRegion.java
index 1ee9d06..1eb6cac 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestHRegion.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestHRegion.java
@@ -118,7 +118,7 @@ import org.apache.hadoop.hbase.monitoring.TaskMonitor;
 import org.apache.hadoop.hbase.protobuf.ProtobufUtil;
 import org.apache.hadoop.hbase.protobuf.generated.WALProtos.CompactionDescriptor;
 import org.apache.hadoop.hbase.regionserver.HRegion.RegionScannerImpl;
-import org.apache.hadoop.hbase.regionserver.HRegion.RowLock;
+import org.apache.hadoop.hbase.regionserver.Region.RowLock;
 import org.apache.hadoop.hbase.regionserver.TestStore.FaultyFileSystem;
 import org.apache.hadoop.hbase.regionserver.wal.FaultyHLog;
 import org.apache.hadoop.hbase.regionserver.wal.HLog;
@@ -237,7 +237,7 @@ public class TestHRegion {
     region.put(put);
     // Close with something in memstore and something in the snapshot.  Make sure all is cleared.
     region.close();
-    assertEquals(0, region.getMemstoreSize().get());
+    assertEquals(0, region.getMemstoreSize());
     HRegion.closeHRegion(region);
   }
 
@@ -328,13 +328,13 @@ public class TestHRegion {
         try {
           // Initialize region
           region = initHRegion(tableName, name.getMethodName(), conf, COLUMN_FAMILY_BYTES);
-          long size = region.getMemstoreSize().get();
+          long size = region.getMemstoreSize();
           Assert.assertEquals(0, size);
           // Put one item into memstore.  Measure the size of one item in memstore.
           Put p1 = new Put(row);
           p1.add(new KeyValue(row, COLUMN_FAMILY_BYTES, qual1, 1, (byte[])null));
           region.put(p1);
-          final long sizeOfOnePut = region.getMemstoreSize().get();
+          final long sizeOfOnePut = region.getMemstoreSize();
           // Fail a flush which means the current memstore will hang out as memstore 'snapshot'.
           try {
             LOG.info("Flushing");
@@ -346,20 +346,20 @@ public class TestHRegion {
           // Make it so all writes succeed from here on out
           ffs.fault.set(false);
           // Check sizes.  Should still be the one entry.
-          Assert.assertEquals(sizeOfOnePut, region.getMemstoreSize().get());
+          Assert.assertEquals(sizeOfOnePut, region.getMemstoreSize());
           // Now add two entries so that on this next flush that fails, we can see if we
           // subtract the right amount, the snapshot size only.
           Put p2 = new Put(row);
           p2.add(new KeyValue(row, COLUMN_FAMILY_BYTES, qual2, 2, (byte[])null));
           p2.add(new KeyValue(row, COLUMN_FAMILY_BYTES, qual3, 3, (byte[])null));
           region.put(p2);
-          Assert.assertEquals(sizeOfOnePut * 3, region.getMemstoreSize().get());
+          Assert.assertEquals(sizeOfOnePut * 3, region.getMemstoreSize());
           // Do a successful flush.  It will clear the snapshot only.  Thats how flushes work.
           // If already a snapshot, we clear it else we move the memstore to be snapshot and flush
           // it
           region.flushcache();
           // Make sure our memory accounting is right.
-          Assert.assertEquals(sizeOfOnePut * 2, region.getMemstoreSize().get());
+          Assert.assertEquals(sizeOfOnePut * 2, region.getMemstoreSize());
         } finally {
           HRegion.closeHRegion(region);
         }
@@ -479,7 +479,7 @@ public class TestHRegion {
       }
       MonitoredTask status = TaskMonitor.get().createStatus(method);
       Map<byte[], Long> maxSeqIdInStores = new TreeMap<byte[], Long>(Bytes.BYTES_COMPARATOR);
-      for (Store store : region.getStores().values()) {
+      for (Store store : region.getStores()) {
         maxSeqIdInStores.put(store.getColumnFamilyName().getBytes(), minSeqId - 1);
       }
       long seqId = region.replayRecoveredEditsIfAny(regiondir, maxSeqIdInStores, null, status);
@@ -530,7 +530,7 @@ public class TestHRegion {
       long recoverSeqId = 1030;
       MonitoredTask status = TaskMonitor.get().createStatus(method);
       Map<byte[], Long> maxSeqIdInStores = new TreeMap<byte[], Long>(Bytes.BYTES_COMPARATOR);
-      for (Store store : region.getStores().values()) {
+      for (Store store : region.getStores()) {
         maxSeqIdInStores.put(store.getColumnFamilyName().getBytes(), recoverSeqId - 1);
       }
       long seqId = region.replayRecoveredEditsIfAny(regiondir, maxSeqIdInStores, null, status);
@@ -573,7 +573,7 @@ public class TestHRegion {
       dos.close();
 
       Map<byte[], Long> maxSeqIdInStores = new TreeMap<byte[], Long>(Bytes.BYTES_COMPARATOR);
-      for (Store store : region.getStores().values()) {
+      for (Store store : region.getStores()) {
         maxSeqIdInStores.put(store.getColumnFamilyName().getBytes(), minSeqId);
       }
       long seqId = region.replayRecoveredEditsIfAny(regiondir, maxSeqIdInStores, null, null);
@@ -594,9 +594,8 @@ public class TestHRegion {
       Path regiondir = region.getRegionFileSystem().getRegionDir();
       FileSystem fs = region.getRegionFileSystem().getFileSystem();
       byte[] regionName = region.getRegionInfo().getEncodedNameAsBytes();
-
-      assertEquals(0, region.getStoreFileList(
-        region.getStores().keySet().toArray(new byte[0][])).size());
+      byte[][] columns = region.getTableDesc().getFamiliesKeys().toArray(new byte[0][]);
+      assertEquals(0, region.getStoreFileList(columns));
 
       Path recoveredEditsDir = HLogUtil.getRegionDirRecoveredEditsDir(regiondir);
 
@@ -632,15 +631,14 @@ public class TestHRegion {
       long recoverSeqId = 1030;
       Map<byte[], Long> maxSeqIdInStores = new TreeMap<byte[], Long>(Bytes.BYTES_COMPARATOR);
       MonitoredTask status = TaskMonitor.get().createStatus(method);
-      for (Store store : region.getStores().values()) {
+      for (Store store : region.getStores()) {
         maxSeqIdInStores.put(store.getColumnFamilyName().getBytes(), recoverSeqId - 1);
       }
       long seqId = region.replayRecoveredEditsIfAny(regiondir, maxSeqIdInStores, null, status);
       assertEquals(maxSeqId, seqId);
 
       // assert that the files are flushed
-      assertEquals(1, region.getStoreFileList(
-        region.getStores().keySet().toArray(new byte[0][])).size());
+      assertEquals(1, region.getStoreFileList(columns));
 
     } finally {
       HRegion.closeHRegion(this.region);
@@ -884,7 +882,7 @@ public class TestHRegion {
     append.add(Bytes.toBytes("somefamily"), Bytes.toBytes("somequalifier"),
         Bytes.toBytes("somevalue"));
     try {
-      region.append(append);
+      region.append(append, HConstants.NO_NONCE, HConstants.NO_NONCE);
     } catch (IOException e) {
       exceptionCaught = true;
     } finally {
@@ -903,7 +901,7 @@ public class TestHRegion {
     inc.setDurability(Durability.SKIP_WAL);
     inc.addColumn(Bytes.toBytes("somefamily"), Bytes.toBytes("somequalifier"), 1L);
     try {
-      region.increment(inc);
+      region.increment(inc, HConstants.NO_NONCE, HConstants.NO_NONCE);
     } catch (IOException e) {
       exceptionCaught = true;
     } finally {
@@ -1026,7 +1024,8 @@ public class TestHRegion {
         puts[i].add(cf, qual, val);
       }
 
-      OperationStatus[] codes = this.region.batchMutate(puts);
+      OperationStatus[] codes = this.region.batchMutate(puts, HConstants.NO_NONCE,
+        HConstants.NO_NONCE);
       assertEquals(10, codes.length);
       for (int i = 0; i < 10; i++) {
         assertEquals(OperationStatusCode.SUCCESS, codes[i].getOperationStatusCode());
@@ -1035,7 +1034,7 @@ public class TestHRegion {
 
       LOG.info("Next a batch put with one invalid family");
       puts[5].add(Bytes.toBytes("BAD_CF"), qual, val);
-      codes = this.region.batchMutate(puts);
+      codes = this.region.batchMutate(puts, HConstants.NO_NONCE, HConstants.NO_NONCE);
       assertEquals(10, codes.length);
       for (int i = 0; i < 10; i++) {
         assertEquals((i == 5) ? OperationStatusCode.BAD_FAMILY : OperationStatusCode.SUCCESS,
@@ -1077,7 +1076,7 @@ public class TestHRegion {
       TestThread putter = new TestThread(ctx) {
         @Override
         public void doWork() throws IOException {
-          retFromThread.set(region.batchMutate(puts));
+          retFromThread.set(region.batchMutate(puts, HConstants.NO_NONCE, HConstants.NO_NONCE));
         }
       };
       LOG.info("...starting put thread while holding locks");
@@ -1173,7 +1172,8 @@ public class TestHRegion {
         puts[i].add(cf, qual, val);
       }
 
-      OperationStatus[] codes = this.region.batchMutate(puts);
+      OperationStatus[] codes = this.region.batchMutate(puts, HConstants.NO_NONCE,
+        HConstants.NO_NONCE);
       assertEquals(10, codes.length);
       for (int i = 0; i < 10; i++) {
         assertEquals(OperationStatusCode.SANITY_CHECK_FAILURE, codes[i].getOperationStatusCode());
@@ -2159,15 +2159,16 @@ public class TestHRegion {
       result = st.execute(null, null);
     } catch (IOException ioe) {
       try {
-        LOG.info("Running rollback of failed split of " + parent.getRegionNameAsString() + "; "
-            + ioe.getMessage());
+        LOG.info("Running rollback of failed split of " +
+          parent.getRegionInfo().getRegionNameAsString() + "; " + ioe.getMessage());
         st.rollback(null, null);
-        LOG.info("Successful rollback of failed split of " + parent.getRegionNameAsString());
+        LOG.info("Successful rollback of failed split of " +
+          parent.getRegionInfo().getRegionNameAsString());
         return null;
       } catch (RuntimeException e) {
         // If failed rollback, kill this server to avoid having a hole in table.
-        LOG.info("Failed rollback of failed split of " + parent.getRegionNameAsString()
-            + " -- aborting server", e);
+        LOG.info("Failed rollback of failed split of " +
+          parent.getRegionInfo().getRegionNameAsString() + " -- aborting server", e);
       }
     }
     finally {
@@ -3056,7 +3057,8 @@ public class TestHRegion {
           if (midkeys[i] != null) {
             rs = splitRegion(regions[i], midkeys[i]);
             for (int j = 0; j < rs.length; j++) {
-              sortedMap.put(Bytes.toString(rs[j].getRegionName()), HRegion.openHRegion(rs[j], null));
+              sortedMap.put(Bytes.toString(rs[j].getRegionInfo().getRegionName()),
+                HRegion.openHRegion(rs[j], null));
             }
           }
         }
@@ -3976,7 +3978,7 @@ public class TestHRegion {
         inc.addColumn(family, qualifier, ONE);
         count++;
         try {
-          region.increment(inc);
+          region.increment(inc, HConstants.NO_NONCE, HConstants.NO_NONCE);
         } catch (IOException e) {
           e.printStackTrace();
           break;
@@ -4063,7 +4065,7 @@ public class TestHRegion {
         app.add(family, qualifier, CHAR);
         count++;
         try {
-          region.append(app);
+          region.append(app, HConstants.NO_NONCE, HConstants.NO_NONCE);
         } catch (IOException e) {
           e.printStackTrace();
           break;
@@ -5343,7 +5345,7 @@ public class TestHRegion {
       // Increment with a TTL of 5 seconds
       Increment incr = new Increment(row).addColumn(fam1, q1, 1L);
       incr.setTTL(5000);
-      region.increment(incr); // 2
+      region.increment(incr, HConstants.NO_NONCE, HConstants.NO_NONCE); // 2
 
       // New value should be 2
       r = region.get(new Get(row));
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestKeepDeletes.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestKeepDeletes.java
index abf8adf..e9da727 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestKeepDeletes.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestKeepDeletes.java
@@ -938,7 +938,7 @@ public class TestKeepDeletes {
     Scan s = new Scan();
     s.setRaw(true);
     // use max versions from the store(s)
-    s.setMaxVersions(region.getStores().values().iterator().next().getScanInfo().getMaxVersions());
+    s.setMaxVersions(region.getStores().iterator().next().getScanInfo().getMaxVersions());
     InternalScanner scan = region.getScanner(s);
     List<Cell> kvs = new ArrayList<Cell>();
     int res = 0;
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestMajorCompaction.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestMajorCompaction.java
index 76c1b15..fad18b6 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestMajorCompaction.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestMajorCompaction.java
@@ -174,8 +174,8 @@ public class TestMajorCompaction {
       throws Exception {
     Map<HStore, HFileDataBlockEncoder> replaceBlockCache =
         new HashMap<HStore, HFileDataBlockEncoder>();
-    for (Entry<byte[], Store> pair : r.getStores().entrySet()) {
-      HStore store = (HStore) pair.getValue();
+    for (Store s: r.getStores()) {
+      HStore store = (HStore)s;
       HFileDataBlockEncoder blockEncoder = store.getDataBlockEncoder();
       replaceBlockCache.put(store, blockEncoder);
       final DataBlockEncoding inCache = DataBlockEncoding.PREFIX;
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestMemStore.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestMemStore.java
index 39c6cfb..cbda801 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestMemStore.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestMemStore.java
@@ -25,7 +25,6 @@ import java.rmi.UnexpectedException;
 import java.util.ArrayList;
 import java.util.Arrays;
 import java.util.List;
-import java.util.Map;
 import java.util.concurrent.atomic.AtomicReference;
 
 import junit.framework.TestCase;
@@ -909,10 +908,10 @@ public class TestMemStore extends TestCase {
       HBaseTestingUtility hbaseUtility = HBaseTestingUtility.createLocalHTU(conf);
       HRegion region = hbaseUtility.createTestRegion("foobar", new HColumnDescriptor("foo"));
 
-      Map<byte[], Store> stores = region.getStores();
+      List<Store> stores = region.getStores();
       assertTrue(stores.size() == 1);
 
-      Store s = stores.entrySet().iterator().next().getValue();
+      Store s = stores.get(0);
       edge.setCurrentTimeMillis(1234);
       s.add(KeyValueTestUtil.create("r", "f", "q", 100, "v"));
       edge.setCurrentTimeMillis(1234 + 100);
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestParallelPut.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestParallelPut.java
index f2bc3c7..16f9fda 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestParallelPut.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestParallelPut.java
@@ -30,6 +30,7 @@ import org.apache.hadoop.hbase.Cell;
 import org.apache.hadoop.hbase.CellUtil;
 import org.apache.hadoop.hbase.HBaseTestingUtility;
 import org.apache.hadoop.hbase.HColumnDescriptor;
+import org.apache.hadoop.hbase.HConstants;
 import org.apache.hadoop.hbase.HConstants.OperationStatusCode;
 import org.apache.hadoop.hbase.HRegionInfo;
 import org.apache.hadoop.hbase.HTableDescriptor;
@@ -208,7 +209,7 @@ public class TestParallelPut {
         put.add(fam1, qual1, value);
         in[0] = put;
         try {
-          OperationStatus[] ret = region.batchMutate(in);
+          OperationStatus[] ret = region.batchMutate(in, HConstants.NO_NONCE, HConstants.NO_NONCE);
           assertEquals(1, ret.length);
           assertEquals(OperationStatusCode.SUCCESS, ret[0].getOperationStatusCode());
           assertGet(rowkey, fam1, qual1, value);
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestRegionFavoredNodes.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestRegionFavoredNodes.java
index 46a4062..0669a58 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestRegionFavoredNodes.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestRegionFavoredNodes.java
@@ -110,8 +110,8 @@ public class TestRegionFavoredNodes {
     // them as favored nodes through the HRegion.
     for (int i = 0; i < REGION_SERVERS; i++) {
       HRegionServer server = TEST_UTIL.getHBaseCluster().getRegionServer(i);
-      List<HRegion> regions = server.getOnlineRegions(TABLE_NAME);
-      for (HRegion region : regions) {
+      List<Region> regions = server.getOnlineRegions(TABLE_NAME);
+      for (Region region : regions) {
         List<org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.ServerName>favoredNodes =
             new ArrayList<org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.ServerName>(3);
         String encodedRegionName = region.getRegionInfo().getEncodedName();
@@ -138,8 +138,8 @@ public class TestRegionFavoredNodes {
     // they are consistent with the favored nodes for that region.
     for (int i = 0; i < REGION_SERVERS; i++) {
       HRegionServer server = TEST_UTIL.getHBaseCluster().getRegionServer(i);
-      List<HRegion> regions = server.getOnlineRegions(TABLE_NAME);
-      for (HRegion region : regions) {
+      List<Region> regions = server.getOnlineRegions(TABLE_NAME);
+      for (Region region : regions) {
         List<String> files = region.getStoreFileList(new byte[][]{COLUMN_FAMILY});
         for (String file : files) {
           FileStatus status = TEST_UTIL.getDFSCluster().getFileSystem().
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestRegionMergeTransaction.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestRegionMergeTransaction.java
index 2654f04..b7f1368 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestRegionMergeTransaction.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestRegionMergeTransaction.java
@@ -118,9 +118,9 @@ public class TestRegionMergeTransaction {
         false);
     RegionMergeTransaction spyMT = Mockito.spy(mt);
     doReturn(false).when(spyMT).hasMergeQualifierInMeta(null,
-        region_a.getRegionName());
+        region_a.getRegionInfo().getRegionName());
     doReturn(false).when(spyMT).hasMergeQualifierInMeta(null,
-        region_b.getRegionName());
+        region_b.getRegionInfo().getRegionName());
     assertTrue(spyMT.prepare(null));
     return spyMT;
   }
@@ -157,9 +157,9 @@ public class TestRegionMergeTransaction {
         true);
     RegionMergeTransaction spyMT = Mockito.spy(mt);
     doReturn(false).when(spyMT).hasMergeQualifierInMeta(null,
-        region_a.getRegionName());
+        region_a.getRegionInfo().getRegionName());
     doReturn(false).when(spyMT).hasMergeQualifierInMeta(null,
-        region_c.getRegionName());
+        region_c.getRegionInfo().getRegionName());
     assertTrue("Since focible is true, should merge two regions even if they are not adjacent",
         spyMT.prepare(null));
   }
@@ -199,9 +199,9 @@ public class TestRegionMergeTransaction {
         false);
     RegionMergeTransaction spyMT = Mockito.spy(mt);
     doReturn(true).when(spyMT).hasMergeQualifierInMeta(null,
-        region_a.getRegionName());
+        region_a.getRegionInfo().getRegionName());
     doReturn(true).when(spyMT).hasMergeQualifierInMeta(null,
-        region_b.getRegionName());
+        region_b.getRegionInfo().getRegionName());
     assertFalse(spyMT.prepare(null));
   }
 
@@ -230,10 +230,10 @@ public class TestRegionMergeTransaction {
     // to be under the merged region dirs.
     assertEquals(0, this.fs.listStatus(mt.getMergesDir()).length);
     // Check merged region have correct key span.
-    assertTrue(Bytes.equals(this.region_a.getStartKey(),
-        mergedRegion.getStartKey()));
-    assertTrue(Bytes.equals(this.region_b.getEndKey(),
-        mergedRegion.getEndKey()));
+    assertTrue(Bytes.equals(this.region_a.getRegionInfo().getStartKey(),
+        mergedRegion.getRegionInfo().getStartKey()));
+    assertTrue(Bytes.equals(this.region_b.getRegionInfo().getEndKey(),
+        mergedRegion.getRegionInfo().getEndKey()));
     // Count rows. merged region are already open
     try {
       int mergedRegionRowCount = countRows(mergedRegion);
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestRegionSplitPolicy.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestRegionSplitPolicy.java
index d906160..11925ff 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestRegionSplitPolicy.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestRegionSplitPolicy.java
@@ -70,7 +70,7 @@ public class TestRegionSplitPolicy {
     // Now make it so the mock region has a RegionServerService that will
     // return 'online regions'.
     RegionServerServices rss = Mockito.mock(RegionServerServices.class);
-    final List<HRegion> regions = new ArrayList<HRegion>();
+    final List<Region> regions = new ArrayList<Region>();
     Mockito.when(rss.getOnlineRegions(TABLENAME)).thenReturn(regions);
     Mockito.when(mockRegion.getRegionServerServices()).thenReturn(rss);
     // Set max size for this 'table'.
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestResettingCounters.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestResettingCounters.java
index cb2a3ba..26406f5 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestResettingCounters.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestResettingCounters.java
@@ -83,14 +83,14 @@ public class TestResettingCounters {
       }
 
       // increment odd qualifiers 5 times and flush
-      for (int i=0;i<5;i++) region.increment(odd);
+      for (int i=0;i<5;i++) region.increment(odd, HConstants.NO_NONCE, HConstants.NO_NONCE);
       region.flushcache();
 
       // increment even qualifiers 5 times
-      for (int i=0;i<5;i++) region.increment(even);
+      for (int i=0;i<5;i++) region.increment(even, HConstants.NO_NONCE, HConstants.NO_NONCE);
 
       // increment all qualifiers, should have value=6 for all
-      Result result = region.increment(all);
+      Result result = region.increment(all, HConstants.NO_NONCE, HConstants.NO_NONCE);
       assertEquals(numQualifiers, result.size());
       Cell [] kvs = result.rawCells();
       for (int i=0;i<kvs.length;i++) {
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestSplitTransaction.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestSplitTransaction.java
index 88a708e..97bb4ca 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestSplitTransaction.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestSplitTransaction.java
@@ -229,10 +229,12 @@ public class TestSplitTransaction {
     // to be under the daughter region dirs.
     assertEquals(0, this.fs.listStatus(this.parent.getRegionFileSystem().getSplitsDir()).length);
     // Check daughters have correct key span.
-    assertTrue(Bytes.equals(this.parent.getStartKey(), daughters.getFirst().getStartKey()));
-    assertTrue(Bytes.equals(GOOD_SPLIT_ROW, daughters.getFirst().getEndKey()));
-    assertTrue(Bytes.equals(daughters.getSecond().getStartKey(), GOOD_SPLIT_ROW));
-    assertTrue(Bytes.equals(this.parent.getEndKey(), daughters.getSecond().getEndKey()));
+    assertTrue(Bytes.equals(this.parent.getRegionInfo().getStartKey(),
+      daughters.getFirst().getRegionInfo().getStartKey()));
+    assertTrue(Bytes.equals(GOOD_SPLIT_ROW, daughters.getFirst().getRegionInfo().getEndKey()));
+    assertTrue(Bytes.equals(daughters.getSecond().getRegionInfo().getStartKey(), GOOD_SPLIT_ROW));
+    assertTrue(Bytes.equals(this.parent.getRegionInfo().getEndKey(),
+      daughters.getSecond().getRegionInfo().getEndKey()));
     // Count rows. daughters are already open
     int daughtersRowCount = 0;
     for (HRegion openRegion: daughters) {
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestSplitTransactionOnCluster.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestSplitTransactionOnCluster.java
index 5254b77..0718ca2 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestSplitTransactionOnCluster.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestSplitTransactionOnCluster.java
@@ -191,7 +191,8 @@ public class TestSplitTransactionOnCluster {
       HTable t = createTableAndWait(tableName.getName(), Bytes.toBytes("cf"));
       final List<HRegion> regions = cluster.getRegions(tableName);
       HRegionInfo hri = getAndCheckSingleTableRegion(regions);
-      int regionServerIndex = cluster.getServerWith(regions.get(0).getRegionName());
+      int regionServerIndex =
+          cluster.getServerWith(regions.get(0).getRegionInfo().getRegionName());
       final HRegionServer regionServer = cluster.getRegionServer(regionServerIndex);
       insertData(tableName.getName(), admin, t);
       t.close();
@@ -247,7 +248,7 @@ public class TestSplitTransactionOnCluster {
       assertFalse("region still in transition", rit.containsKey(
           rit.containsKey(hri.getTable())));
 
-      List<HRegion> onlineRegions = regionServer.getOnlineRegions(tableName);
+      List<Region> onlineRegions = regionServer.getOnlineRegions(tableName);
       // Region server side split is successful.
       assertEquals("The parent region should be splitted", 2, onlineRegions.size());
       //Should be present in RIT
@@ -291,7 +292,7 @@ public class TestSplitTransactionOnCluster {
         Coprocessor.PRIORITY_USER, region.getBaseConf());
 
       // split async
-      this.admin.split(region.getRegionName(), new byte[] {42});
+      this.admin.split(region.getRegionInfo().getRegionName(), new byte[] {42});
 
       // we have to wait until the SPLITTING state is seen by the master
       FailingSplitRegionObserver observer = (FailingSplitRegionObserver) region
@@ -850,7 +851,8 @@ public class TestSplitTransactionOnCluster {
     List<HRegion> regions = null;
     try {
       regions = cluster.getRegions(tableName);
-      int regionServerIndex = cluster.getServerWith(regions.get(0).getRegionName());
+      int regionServerIndex =
+          cluster.getServerWith(regions.get(0).getRegionInfo().getRegionName());
       HRegionServer regionServer = cluster.getRegionServer(regionServerIndex);
       insertData(tableName.getName(), admin, t);
       // Turn off balancer so it doesn't cut in and mess up our placements.
@@ -924,7 +926,7 @@ public class TestSplitTransactionOnCluster {
     List<HRegion> regions = cluster.getRegions(tableName);
     HRegionInfo hri = getAndCheckSingleTableRegion(regions);
     ensureTableRegionNotOnSameServerAsMeta(admin, hri);
-    int regionServerIndex = cluster.getServerWith(regions.get(0).getRegionName());
+    int regionServerIndex = cluster.getServerWith(regions.get(0).getRegionInfo().getRegionName());
     HRegionServer regionServer = cluster.getRegionServer(regionServerIndex);
     // Turn off balancer so it doesn't cut in and mess up our placements.
     this.admin.setBalancerRunning(false, true);
@@ -1019,7 +1021,7 @@ public class TestSplitTransactionOnCluster {
       fail("Each table should have at least one region.");
     }
     ServerName serverName =
-        cluster.getServerHoldingRegion(firstTableRegions.get(0).getRegionName());
+        cluster.getServerHoldingRegion(firstTableRegions.get(0).getRegionInfo().getRegionName());
     admin.move(secondTableRegions.get(0).getRegionInfo().getEncodedNameAsBytes(),
       Bytes.toBytes(serverName.getServerName()));
     HTable table1 = null;
@@ -1059,7 +1061,8 @@ public class TestSplitTransactionOnCluster {
       List<HRegion> regions = awaitTableRegions(tableName);
       assertTrue("Table not online", cluster.getRegions(tableName).size() != 0);
 
-      int regionServerIndex = cluster.getServerWith(regions.get(0).getRegionName());
+      int regionServerIndex =
+          cluster.getServerWith(regions.get(0).getRegionInfo().getRegionName());
       HRegionServer regionServer = cluster.getRegionServer(regionServerIndex);
       final HRegion region = findSplittableRegion(regions);
       assertTrue("not able to find a splittable region", region != null);
@@ -1148,9 +1151,9 @@ public class TestSplitTransactionOnCluster {
       }
       admin.flush(desc.getTableName().toString());
       List<HRegion> regions = cluster.getRegions(desc.getTableName());
-      int serverWith = cluster.getServerWith(regions.get(0).getRegionName());
+      int serverWith = cluster.getServerWith(regions.get(0).getRegionInfo().getRegionName());
       HRegionServer regionServer = cluster.getRegionServer(serverWith);
-      cluster.getServerWith(regions.get(0).getRegionName());
+      cluster.getServerWith(regions.get(0).getRegionInfo().getRegionName());
       SplitTransaction st = new SplitTransaction(regions.get(0), Bytes.toBytes("r3"));
       st.prepare();
       st.stepsBeforePONR(regionServer, regionServer, false);
@@ -1465,12 +1468,12 @@ public class TestSplitTransactionOnCluster {
         byte[] splitKey, List<Mutation> metaEntries) throws IOException {
       RegionCoprocessorEnvironment environment = ctx.getEnvironment();
       HRegionServer rs = (HRegionServer) environment.getRegionServerServices();
-      List<HRegion> onlineRegions =
+      List<Region> onlineRegions =
           rs.getOnlineRegions(TableName.valueOf("testSplitHooksBeforeAndAfterPONR_2"));
-      HRegion region = onlineRegions.get(0);
-      for (HRegion r : onlineRegions) {
+      HRegion region = (HRegion)onlineRegions.get(0);
+      for (Region r : onlineRegions) {
         if (r.getRegionInfo().containsRow(splitKey)) {
-          region = r;
+          region = (HRegion)r;
           break;
         }
       }
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/compactions/TestCompactionWithThroughputController.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/compactions/TestCompactionWithThroughputController.java
index 586f363..21279dd 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/compactions/TestCompactionWithThroughputController.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/compactions/TestCompactionWithThroughputController.java
@@ -39,9 +39,9 @@ import org.apache.hadoop.hbase.client.HTable;
 import org.apache.hadoop.hbase.client.HTableInterface;
 import org.apache.hadoop.hbase.client.Put;
 import org.apache.hadoop.hbase.regionserver.DefaultStoreEngine;
-import org.apache.hadoop.hbase.regionserver.HRegion;
 import org.apache.hadoop.hbase.regionserver.HRegionServer;
 import org.apache.hadoop.hbase.regionserver.HStore;
+import org.apache.hadoop.hbase.regionserver.Region;
 import org.apache.hadoop.hbase.regionserver.Store;
 import org.apache.hadoop.hbase.regionserver.StoreEngine;
 import org.apache.hadoop.hbase.regionserver.StripeStoreConfig;
@@ -72,8 +72,8 @@ public class TestCompactionWithThroughputController {
     List<JVMClusterUtil.RegionServerThread> rsts = cluster.getRegionServerThreads();
     for (int i = 0; i < cluster.getRegionServerThreads().size(); i++) {
       HRegionServer hrs = rsts.get(i).getRegionServer();
-      for (HRegion region : hrs.getOnlineRegions(tableName)) {
-        return region.getStores().values().iterator().next();
+      for (Region region : hrs.getOnlineRegions(tableName)) {
+        return region.getStores().iterator().next();
       }
     }
     return null;
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/wal/TestDurability.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/wal/TestDurability.java
index 7aba563..cf53440 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/wal/TestDurability.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/wal/TestDurability.java
@@ -27,6 +27,7 @@ import org.apache.hadoop.fs.FileSystem;
 import org.apache.hadoop.fs.Path;
 import org.apache.hadoop.hbase.HBaseTestingUtility;
 import org.apache.hadoop.hbase.HColumnDescriptor;
+import org.apache.hadoop.hbase.HConstants;
 import org.apache.hadoop.hbase.HRegionInfo;
 import org.apache.hadoop.hbase.HTableDescriptor;
 import org.apache.hadoop.hbase.testclassification.MediumTests;
@@ -147,7 +148,7 @@ public class TestDurability {
     // col1: amount = 1, 1 write back to WAL
     Increment inc1 = new Increment(row1);
     inc1.addColumn(FAMILY, col1, 1);
-    Result res = region.increment(inc1);
+    Result res = region.increment(inc1, HConstants.NO_NONCE, HConstants.NO_NONCE);
     assertEquals(1, res.size());
     assertEquals(1, Bytes.toLong(res.getValue(FAMILY, col1)));
     verifyHLogCount(wal, 1);
@@ -155,7 +156,7 @@ public class TestDurability {
     // col1: amount = 0, 0 write back to WAL
     inc1 = new Increment(row1);
     inc1.addColumn(FAMILY, col1, 0);
-    res = region.increment(inc1);
+    res = region.increment(inc1, HConstants.NO_NONCE, HConstants.NO_NONCE);
     assertEquals(1, res.size());
     assertEquals(1, Bytes.toLong(res.getValue(FAMILY, col1)));
     verifyHLogCount(wal, 1);
@@ -166,7 +167,7 @@ public class TestDurability {
     inc1.addColumn(FAMILY, col1, 0);
     inc1.addColumn(FAMILY, col2, 0);
     inc1.addColumn(FAMILY, col3, 0);
-    res = region.increment(inc1);
+    res = region.increment(inc1, HConstants.NO_NONCE, HConstants.NO_NONCE);
     assertEquals(3, res.size());
     assertEquals(1, Bytes.toLong(res.getValue(FAMILY, col1)));
     assertEquals(0, Bytes.toLong(res.getValue(FAMILY, col2)));
@@ -179,7 +180,7 @@ public class TestDurability {
     inc1.addColumn(FAMILY, col1, 5);
     inc1.addColumn(FAMILY, col2, 4);
     inc1.addColumn(FAMILY, col3, 3);
-    res = region.increment(inc1);
+    res = region.increment(inc1, HConstants.NO_NONCE, HConstants.NO_NONCE);
     assertEquals(3, res.size());
     assertEquals(6, Bytes.toLong(res.getValue(FAMILY, col1)));
     assertEquals(4, Bytes.toLong(res.getValue(FAMILY, col2)));
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/wal/TestHLogFiltering.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/wal/TestHLogFiltering.java
index db281bc..175a4f6 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/wal/TestHLogFiltering.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/wal/TestHLogFiltering.java
@@ -38,8 +38,8 @@ import org.apache.hadoop.hbase.master.HMaster;
 import org.apache.hadoop.hbase.protobuf.RequestConverter;
 import org.apache.hadoop.hbase.protobuf.generated.AdminProtos.FlushRegionRequest;
 import org.apache.hadoop.hbase.protobuf.generated.RegionServerStatusProtos.GetLastFlushedSequenceIdRequest;
-import org.apache.hadoop.hbase.regionserver.HRegion;
 import org.apache.hadoop.hbase.regionserver.HRegionServer;
+import org.apache.hadoop.hbase.regionserver.Region;
 import org.apache.hadoop.hbase.util.Bytes;
 import org.junit.After;
 import org.junit.Before;
@@ -133,8 +133,8 @@ public class TestHLogFiltering {
   private List<byte[]> getRegionsByServer(int rsId) throws IOException {
     List<byte[]> regionNames = Lists.newArrayList();
     HRegionServer hrs = getRegionServer(rsId);
-    for (HRegion r : hrs.getOnlineRegions(TABLE_NAME)) {
-      regionNames.add(r.getRegionName());
+    for (Region r : hrs.getOnlineRegions(TABLE_NAME)) {
+      regionNames.add(r.getRegionInfo().getRegionName());
     }
     return regionNames;
   }
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/wal/TestLogRolling.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/wal/TestLogRolling.java
index d69d095..1060f12 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/wal/TestLogRolling.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/wal/TestLogRolling.java
@@ -58,6 +58,7 @@ import org.apache.hadoop.hbase.fs.HFileSystem;
 import org.apache.hadoop.hbase.ipc.RpcClient;
 import org.apache.hadoop.hbase.regionserver.HRegion;
 import org.apache.hadoop.hbase.regionserver.HRegionServer;
+import org.apache.hadoop.hbase.regionserver.Region;
 import org.apache.hadoop.hbase.regionserver.Store;
 import org.apache.hadoop.hbase.util.Bytes;
 import org.apache.hadoop.hbase.util.FSUtils;
@@ -66,7 +67,6 @@ import org.apache.hadoop.hbase.util.Threads;
 import org.apache.hadoop.hdfs.MiniDFSCluster;
 import org.apache.hadoop.hdfs.protocol.DatanodeInfo;
 import org.apache.hadoop.hdfs.server.datanode.DataNode;
-
 import org.junit.After;
 import org.junit.Assert;
 import org.junit.Before;
@@ -211,11 +211,9 @@ public class TestLogRolling  {
       LOG.info("after writing there are " + ((FSHLog) log).getNumRolledLogFiles() + " log files");
 
       // flush all regions
-
-      List<HRegion> regions =
-        new ArrayList<HRegion>(server.getOnlineRegionsLocalContext());
-      for (HRegion r: regions) {
-        r.flushcache();
+      List<Region> regions = new ArrayList<Region>(server.getOnlineRegionsLocalContext());
+      for (Region r: regions) {
+        ((HRegion)r).flushcache();
       }
 
       // Now roll the log
@@ -573,10 +571,9 @@ public class TestLogRolling  {
     assertTrue(loggedRows.contains("row1005"));
 
     // flush all regions
-    List<HRegion> regions =
-        new ArrayList<HRegion>(server.getOnlineRegionsLocalContext());
-    for (HRegion r: regions) {
-      r.flushcache();
+    List<Region> regions = new ArrayList<Region>(server.getOnlineRegionsLocalContext());
+    for (Region r: regions) {
+      ((HRegion)r).flushcache();
     }
 
     ResultScanner scanner = table.getScanner(new Scan());
@@ -615,7 +612,7 @@ public class TestLogRolling  {
     server = TEST_UTIL.getRSForFirstRegionInTable(Bytes.toBytes(tableName));
     this.log = server.getWAL();
     FSHLog fshLog = (FSHLog)log;
-    HRegion region = server.getOnlineRegions(table2.getName()).get(0);
+    HRegion region = (HRegion)server.getOnlineRegions(table2.getName()).get(0);
     Store s = region.getStore(HConstants.CATALOG_FAMILY);
 
     //have to flush namespace to ensure it doesn't affect wall tests
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/wal/TestWALReplay.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/wal/TestWALReplay.java
index 2f59b25..a296cc7 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/wal/TestWALReplay.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/wal/TestWALReplay.java
@@ -66,6 +66,7 @@ import org.apache.hadoop.hbase.regionserver.DefaultStoreFlusher;
 import org.apache.hadoop.hbase.regionserver.FlushRequester;
 import org.apache.hadoop.hbase.regionserver.HRegion;
 import org.apache.hadoop.hbase.regionserver.HRegionServer;
+import org.apache.hadoop.hbase.regionserver.Region;
 import org.apache.hadoop.hbase.regionserver.RegionScanner;
 import org.apache.hadoop.hbase.regionserver.RegionServerServices;
 import org.apache.hadoop.hbase.regionserver.Store;
@@ -184,7 +185,7 @@ public class TestWALReplay {
     // move region to another regionserver
     HRegion destRegion = regions.get(0);
     int originServerNum = hbaseCluster
-        .getServerWith(destRegion.getRegionName());
+        .getServerWith(destRegion.getRegionInfo().getRegionName());
     assertTrue("Please start more than 1 regionserver", hbaseCluster
         .getRegionServerThreads().size() > 1);
     int destServerNum = 0;
@@ -208,13 +209,14 @@ public class TestWALReplay {
     assertEquals(0, count);
 
     // flush region and make major compaction
-    destServer.getOnlineRegion(destRegion.getRegionName()).flushcache();
+    HRegion region = (HRegion)
+        destServer.getOnlineRegion(destRegion.getRegionInfo().getRegionName());
+    region.flushcache();
     // wait to complete major compaction
-    for (Store store : destServer.getOnlineRegion(destRegion.getRegionName())
-        .getStores().values()) {
+    for (Store store : region.getStores()) {
       store.triggerMajorCompaction();
     }
-    destServer.getOnlineRegion(destRegion.getRegionName()).compactStores();
+    region.compactStores();
 
     // move region to origin regionserver
     moveRegionAndWait(destRegion, originServer);
@@ -339,7 +341,7 @@ public class TestWALReplay {
         Bytes.toBytes("z"), 10);
     List <Pair<byte[],String>>  hfs= new ArrayList<Pair<byte[],String>>(1);
     hfs.add(Pair.newPair(family, f.toString()));
-    region.bulkLoadHFiles(hfs, true);
+    region.bulkLoadHFiles(hfs, true, null);
 
     // Add an edit so something in the WAL
     byte [] row = tableName.getName();
@@ -412,7 +414,7 @@ public class TestWALReplay {
           Bytes.toBytes(i + "50"), 10);
       hfs.add(Pair.newPair(family, f.toString()));
     }
-    region.bulkLoadHFiles(hfs, true);
+    region.bulkLoadHFiles(hfs, true, null);
     final int rowsInsertedCount = 31;
     assertEquals(rowsInsertedCount, getScannedCount(region.getScanner(new Scan())));
 
@@ -931,7 +933,7 @@ public class TestWALReplay {
     private HRegion r;
 
     @Override
-    public void requestFlush(HRegion region) {
+    public void requestFlush(Region region) {
       try {
         r.flushcache();
       } catch (IOException e) {
@@ -940,7 +942,7 @@ public class TestWALReplay {
     }
 
     @Override
-    public void requestDelayedFlush(HRegion region, long when) {
+    public void requestDelayedFlush(Region region, long when) {
       // TODO Auto-generated method stub
       
     }
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/security/access/SecureTestUtil.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/security/access/SecureTestUtil.java
index 5d979bd..663ca22 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/security/access/SecureTestUtil.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/security/access/SecureTestUtil.java
@@ -46,6 +46,8 @@ import org.apache.hadoop.hbase.protobuf.ProtobufUtil;
 import org.apache.hadoop.hbase.protobuf.generated.AccessControlProtos.AccessControlService;
 import org.apache.hadoop.hbase.protobuf.generated.AccessControlProtos.CheckPermissionsRequest;
 import org.apache.hadoop.hbase.regionserver.HRegion;
+import org.apache.hadoop.hbase.regionserver.Region;
+import org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost;
 import org.apache.hadoop.hbase.security.AccessDeniedException;
 import org.apache.hadoop.hbase.security.User;
 import org.apache.hadoop.hbase.util.JVMClusterUtil.RegionServerThread;
@@ -264,9 +266,9 @@ public class SecureTestUtil {
   private static List<AccessController> getAccessControllers(MiniHBaseCluster cluster) {
     List<AccessController> result = Lists.newArrayList();
     for (RegionServerThread t: cluster.getLiveRegionServerThreads()) {
-      for (HRegion region: t.getRegionServer().getOnlineRegionsLocalContext()) {
+      for (Region region: t.getRegionServer().getOnlineRegionsLocalContext()) {
         Coprocessor cp = region.getCoprocessorHost()
-          .findCoprocessor(AccessController.class.getName());
+            .findCoprocessor(AccessController.class.getName());
         if (cp != null) {
           result.add((AccessController)cp);
         }
@@ -299,7 +301,7 @@ public class SecureTestUtil {
         for (Map.Entry<AccessController,Long> e: mtimes.entrySet()) {
           if (!oldMTimes.containsKey(e.getKey())) {
             LOG.error("Snapshot of AccessController state does not include instance on region " +
-              e.getKey().getRegion().getRegionNameAsString());
+              e.getKey().getRegion().getRegionInfo().getRegionNameAsString());
             // Error out the predicate, we will try again
             return false;
           }
@@ -307,8 +309,8 @@ public class SecureTestUtil {
           long now = e.getValue();
           if (now <= old) {
             LOG.info("AccessController on region " +
-              e.getKey().getRegion().getRegionNameAsString() + " has not updated: mtime=" +
-              now);
+              e.getKey().getRegion().getRegionInfo().getRegionNameAsString() +
+              " has not updated: mtime=" + now);
             return false;
           }
         }
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/security/access/TestAccessController.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/security/access/TestAccessController.java
index 646a0a6..e8cf693 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/security/access/TestAccessController.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/security/access/TestAccessController.java
@@ -91,6 +91,7 @@ import org.apache.hadoop.hbase.protobuf.generated.AccessControlProtos.AccessCont
 import org.apache.hadoop.hbase.protobuf.generated.AccessControlProtos.CheckPermissionsRequest;
 import org.apache.hadoop.hbase.regionserver.HRegion;
 import org.apache.hadoop.hbase.regionserver.HRegionServer;
+import org.apache.hadoop.hbase.regionserver.Region;
 import org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost;
 import org.apache.hadoop.hbase.regionserver.RegionServerCoprocessorHost;
 import org.apache.hadoop.hbase.regionserver.ScanType;
@@ -626,8 +627,8 @@ public class TestAccessController extends SecureTestUtil {
 
   @Test
   public void testMergeRegions() throws Exception {
-
-    final List<HRegion> regions = TEST_UTIL.getHBaseCluster().findRegionsForTable(TEST_TABLE.getTableName());
+    final List<HRegion> regions = TEST_UTIL.getHBaseCluster()
+        .findRegionsForTable(TEST_TABLE.getTableName());
 
     AccessTestAction action = new AccessTestAction() {
       @Override
@@ -2267,9 +2268,8 @@ public class TestAccessController extends SecureTestUtil {
     for (JVMClusterUtil.RegionServerThread thread:
         TEST_UTIL.getMiniHBaseCluster().getRegionServerThreads()) {
       HRegionServer rs = thread.getRegionServer();
-      for (HRegion region: rs.getOnlineRegions(TEST_TABLE.getTableName())) {
-        region.getCoprocessorHost().load(PingCoprocessor.class,
-          Coprocessor.PRIORITY_USER, conf);
+      for (Region region: rs.getOnlineRegions(TEST_TABLE.getTableName())) {
+        region.getCoprocessorHost().load(PingCoprocessor.class, Coprocessor.PRIORITY_USER, conf);
       }
     }
 
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/security/visibility/ExpAsStringVisibilityLabelServiceImpl.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/security/visibility/ExpAsStringVisibilityLabelServiceImpl.java
index c1c690b..b0bd801 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/security/visibility/ExpAsStringVisibilityLabelServiceImpl.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/security/visibility/ExpAsStringVisibilityLabelServiceImpl.java
@@ -47,8 +47,8 @@ import org.apache.hadoop.hbase.client.HTable;
 import org.apache.hadoop.hbase.client.Put;
 import org.apache.hadoop.hbase.client.Result;
 import org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment;
-import org.apache.hadoop.hbase.regionserver.HRegion;
 import org.apache.hadoop.hbase.regionserver.OperationStatus;
+import org.apache.hadoop.hbase.regionserver.Region;
 import org.apache.hadoop.hbase.security.User;
 import org.apache.hadoop.hbase.security.access.AccessControlLists;
 import org.apache.hadoop.hbase.security.visibility.expression.ExpressionNode;
@@ -78,7 +78,7 @@ public class ExpAsStringVisibilityLabelServiceImpl implements VisibilityLabelSer
   private final ExpressionParser expressionParser = new ExpressionParser();
   private final ExpressionExpander expressionExpander = new ExpressionExpander();
   private Configuration conf;
-  private HRegion labelsRegion;
+  private Region labelsRegion;
   private List<ScanLabelGenerator> scanLabelGenerators;
   private List<String> superUsers;
   private List<String> superGroups;
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/security/visibility/TestVisibilityLabels.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/security/visibility/TestVisibilityLabels.java
index d84a2ac..58e08b8 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/security/visibility/TestVisibilityLabels.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/security/visibility/TestVisibilityLabels.java
@@ -54,8 +54,8 @@ import org.apache.hadoop.hbase.protobuf.generated.ClientProtos.RegionActionResul
 import org.apache.hadoop.hbase.protobuf.generated.VisibilityLabelsProtos.GetAuthsResponse;
 import org.apache.hadoop.hbase.protobuf.generated.VisibilityLabelsProtos.VisibilityLabelsResponse;
 import org.apache.hadoop.hbase.regionserver.BloomType;
-import org.apache.hadoop.hbase.regionserver.HRegion;
 import org.apache.hadoop.hbase.regionserver.HRegionServer;
+import org.apache.hadoop.hbase.regionserver.Region;
 import org.apache.hadoop.hbase.security.User;
 import org.apache.hadoop.hbase.util.Bytes;
 import org.apache.hadoop.hbase.util.JVMClusterUtil.RegionServerThread;
@@ -327,7 +327,7 @@ public abstract class TestVisibilityLabels {
         List<RegionServerThread> regionServerThreads = TEST_UTIL.getHBaseCluster()
             .getRegionServerThreads();
         for (RegionServerThread rsThread : regionServerThreads) {
-          List<HRegion> onlineRegions = rsThread.getRegionServer().getOnlineRegions(
+          List<Region> onlineRegions = rsThread.getRegionServer().getOnlineRegions(
               LABELS_TABLE_NAME);
           if (onlineRegions.size() > 0) {
             rsThread.getRegionServer().abort("Aborting ");
@@ -361,7 +361,7 @@ public abstract class TestVisibilityLabels {
     for (RegionServerThread rsThread : regionServerThreads) {
       while (true) {
         if (!rsThread.getRegionServer().isAborted()) {
-          List<HRegion> onlineRegions = rsThread.getRegionServer().getOnlineRegions(
+          List<Region> onlineRegions = rsThread.getRegionServer().getOnlineRegions(
               LABELS_TABLE_NAME);
           if (onlineRegions.size() > 0) {
             break;
@@ -429,7 +429,7 @@ public abstract class TestVisibilityLabels {
       } catch (InterruptedException e) {
       }
     }
-    HRegion labelsTableRegion = regionServer.getOnlineRegions(LABELS_TABLE_NAME).get(0);
+    Region labelsTableRegion = regionServer.getOnlineRegions(LABELS_TABLE_NAME).get(0);
     while (labelsTableRegion.isRecovering()) {
       try {
         Thread.sleep(10);
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/snapshot/SnapshotTestingUtils.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/snapshot/SnapshotTestingUtils.java
index 072e044..926db5a 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/snapshot/SnapshotTestingUtils.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/snapshot/SnapshotTestingUtils.java
@@ -63,6 +63,7 @@ import org.apache.hadoop.hbase.protobuf.generated.MasterProtos.IsSnapshotDoneRes
 import org.apache.hadoop.hbase.regionserver.HRegion;
 import org.apache.hadoop.hbase.regionserver.HRegionFileSystem;
 import org.apache.hadoop.hbase.regionserver.HRegionServer;
+import org.apache.hadoop.hbase.regionserver.Region;
 import org.apache.hadoop.hbase.snapshot.SnapshotReferenceUtil;
 import org.apache.hadoop.hbase.util.Bytes;
 import org.apache.hadoop.hbase.util.FSTableDescriptors;
@@ -627,9 +628,9 @@ public class SnapshotTestingUtils {
                                             final TableName tableName)
       throws IOException, InterruptedException {
     HRegionServer rs = util.getRSForFirstRegionInTable(tableName);
-    List<HRegion> onlineRegions = rs.getOnlineRegions(tableName);
-    for (HRegion region : onlineRegions) {
-      region.waitForFlushesAndCompactions();
+    List<Region> onlineRegions = rs.getOnlineRegions(tableName);
+    for (Region region : onlineRegions) {
+      ((HRegion)region).waitForFlushesAndCompactions();
     }
     // Wait up to 60 seconds for a table to be available.
     final HBaseAdmin hBaseAdmin = util.getHBaseAdmin();
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/util/TestHBaseFsck.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/util/TestHBaseFsck.java
index e233302..f2d288c 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/util/TestHBaseFsck.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/util/TestHBaseFsck.java
@@ -2437,9 +2437,9 @@ public class TestHBaseFsck {
       }
       TEST_UTIL.getHBaseAdmin().flush(desc.getTableName().toString());
       List<HRegion> regions = cluster.getRegions(desc.getTableName());
-      int serverWith = cluster.getServerWith(regions.get(0).getRegionName());
+      int serverWith = cluster.getServerWith(regions.get(0).getRegionInfo().getRegionName());
       HRegionServer regionServer = cluster.getRegionServer(serverWith);
-      cluster.getServerWith(regions.get(0).getRegionName());
+      cluster.getServerWith(regions.get(0).getRegionInfo().getRegionName());
       SplitTransaction st = new SplitTransaction(regions.get(0), Bytes.toBytes("r3"));
       st.prepare();
       st.stepsBeforePONR(regionServer, regionServer, false);
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/util/TestHBaseFsckEncryption.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/util/TestHBaseFsckEncryption.java
index beaa6f7..4af88ff 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/util/TestHBaseFsckEncryption.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/util/TestHBaseFsckEncryption.java
@@ -43,14 +43,13 @@ import org.apache.hadoop.hbase.io.crypto.KeyProviderForTesting;
 import org.apache.hadoop.hbase.io.crypto.aes.AES;
 import org.apache.hadoop.hbase.io.hfile.CacheConfig;
 import org.apache.hadoop.hbase.io.hfile.HFile;
-import org.apache.hadoop.hbase.regionserver.HRegion;
+import org.apache.hadoop.hbase.regionserver.Region;
 import org.apache.hadoop.hbase.regionserver.Store;
 import org.apache.hadoop.hbase.regionserver.StoreFile;
 import org.apache.hadoop.hbase.security.EncryptionUtil;
 import org.apache.hadoop.hbase.security.User;
 import org.apache.hadoop.hbase.util.hbck.HFileCorruptionChecker;
 import org.apache.hadoop.hbase.util.hbck.HbckTestingUtil;
-
 import org.junit.After;
 import org.junit.Before;
 import org.junit.Test;
@@ -140,9 +139,9 @@ public class TestHBaseFsckEncryption {
 
   private List<Path> findStorefilePaths(byte[] tableName) throws Exception {
     List<Path> paths = new ArrayList<Path>();
-    for (HRegion region:
+    for (Region region:
         TEST_UTIL.getRSForFirstRegionInTable(tableName).getOnlineRegions(htd.getTableName())) {
-      for (Store store: region.getStores().values()) {
+      for (Store store: region.getStores()) {
         for (StoreFile storefile: store.getStorefiles()) {
           paths.add(storefile.getPath());
         }
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/util/TestMergeTable.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/util/TestMergeTable.java
index 09db479..2e00281 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/util/TestMergeTable.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/util/TestMergeTable.java
@@ -140,7 +140,7 @@ public class TestMergeTable {
   throws IOException {
     HRegionInfo hri = new HRegionInfo(desc.getTableName(), startKey, endKey);
     HRegion region = HRegion.createHRegion(hri, rootdir, UTIL.getConfiguration(), desc);
-    LOG.info("Created region " + region.getRegionNameAsString());
+    LOG.info("Created region " + region.getRegionInfo().getRegionNameAsString());
     for(int i = firstRow; i < firstRow + nrows; i++) {
       Put put = new Put(Bytes.toBytes("row_" + String.format("%1$05d", i)));
       put.setDurability(Durability.SKIP_WAL);
diff --git a/hbase-shell/src/main/ruby/hbase/admin.rb b/hbase-shell/src/main/ruby/hbase/admin.rb
index dad66c6..3dfe865 100644
--- a/hbase-shell/src/main/ruby/hbase/admin.rb
+++ b/hbase-shell/src/main/ruby/hbase/admin.rb
@@ -77,9 +77,9 @@ module Hbase
     end
 
     #----------------------------------------------------------------------------------------------
-    # Requests a regionserver's WAL roll
+    # Requests a regionserver's HLog roll
     def hlog_roll(server_name)
-      @admin.rollWALWriter(ServerName.valueOf(server_name))
+      @admin.rollHLogWriter(server_name)
     end
 
     #----------------------------------------------------------------------------------------------
