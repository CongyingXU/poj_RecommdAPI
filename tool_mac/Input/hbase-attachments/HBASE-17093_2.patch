diff --git a/hbase-client/src/main/java/org/apache/hadoop/hbase/client/SecureBulkLoadClient.java b/hbase-client/src/main/java/org/apache/hadoop/hbase/client/SecureBulkLoadClient.java
index c8d9738..1e43602 100644
--- a/hbase-client/src/main/java/org/apache/hadoop/hbase/client/SecureBulkLoadClient.java
+++ b/hbase-client/src/main/java/org/apache/hadoop/hbase/client/SecureBulkLoadClient.java
@@ -107,10 +107,10 @@ public class SecureBulkLoadClient {
    * @param assignSeqNum
    * @param userToken
    * @param bulkToken
-   * @return true if all are loaded
+   * @return List of FamilyPath for loaded hfiles
    * @throws IOException
    */
-  public boolean secureBulkLoadHFiles(final ClientService.BlockingInterface client,
+  public List<FamilyPath> secureBulkLoadHFiles(final ClientService.BlockingInterface client,
       final List<Pair<byte[], String>> familyPaths,
       final byte[] regionName, boolean assignSeqNum,
       final Token<?> userToken, final String bulkToken) throws IOException {
@@ -128,10 +128,10 @@ public class SecureBulkLoadClient {
    * @param userToken
    * @param bulkToken
    * @param copyFiles
-   * @return true if all are loaded
+   * @return List of FamilyPath for loaded hfiles
    * @throws IOException
    */
-  public boolean secureBulkLoadHFiles(final ClientService.BlockingInterface client,
+  public List<FamilyPath> secureBulkLoadHFiles(final ClientService.BlockingInterface client,
       final List<Pair<byte[], String>> familyPaths,
       final byte[] regionName, boolean assignSeqNum,
       final Token<?> userToken, final String bulkToken, boolean copyFiles) throws IOException {
@@ -141,7 +141,7 @@ public class SecureBulkLoadClient {
 
     try {
       BulkLoadHFileResponse response = client.bulkLoadHFile(null, request);
-      return response.getLoaded();
+      return ProtobufUtil.toFamilyPath(response.getFamilyPathList());
     } catch (Exception se) {
       throw ProtobufUtil.handleRemoteException(se);
     }
diff --git a/hbase-client/src/main/java/org/apache/hadoop/hbase/shaded/protobuf/ProtobufUtil.java b/hbase-client/src/main/java/org/apache/hadoop/hbase/shaded/protobuf/ProtobufUtil.java
index 2758c26..94dad30 100644
--- a/hbase-client/src/main/java/org/apache/hadoop/hbase/shaded/protobuf/ProtobufUtil.java
+++ b/hbase-client/src/main/java/org/apache/hadoop/hbase/shaded/protobuf/ProtobufUtil.java
@@ -65,6 +65,7 @@ import org.apache.hadoop.hbase.client.CompactionState;
 import org.apache.hadoop.hbase.client.Consistency;
 import org.apache.hadoop.hbase.client.Delete;
 import org.apache.hadoop.hbase.client.Durability;
+import org.apache.hadoop.hbase.client.FamilyPath;
 import org.apache.hadoop.hbase.client.Get;
 import org.apache.hadoop.hbase.client.Increment;
 import org.apache.hadoop.hbase.client.Mutation;
@@ -467,6 +468,14 @@ public final class ProtobufUtil {
     }
   }
 
+  public static List<FamilyPath> toFamilyPath(List<ClientProtos.FamilyPath> protoLst) {
+    List<FamilyPath> lst = new ArrayList<>();
+    for (ClientProtos.FamilyPath fp : protoLst) {
+      lst.add(new FamilyPath(fp.getFamily().toByteArray(), new Path(fp.getPath())));
+    }
+    return lst;
+  }
+
   /**
    * Convert a protocol buffer Get to a client Get
    *
diff --git a/hbase-client/src/main/java/org/apache/hadoop/hbase/shaded/protobuf/RequestConverter.java b/hbase-client/src/main/java/org/apache/hadoop/hbase/shaded/protobuf/RequestConverter.java
index abd1563..331be94 100644
--- a/hbase-client/src/main/java/org/apache/hadoop/hbase/shaded/protobuf/RequestConverter.java
+++ b/hbase-client/src/main/java/org/apache/hadoop/hbase/shaded/protobuf/RequestConverter.java
@@ -556,10 +556,10 @@ public final class RequestConverter {
             .setService(userToken.getService().toString()).build();
     }
 
-    List<ClientProtos.BulkLoadHFileRequest.FamilyPath> protoFamilyPaths =
-        new ArrayList<ClientProtos.BulkLoadHFileRequest.FamilyPath>(familyPaths.size());
+    List<ClientProtos.FamilyPath> protoFamilyPaths =
+        new ArrayList<ClientProtos.FamilyPath>(familyPaths.size());
     for(Pair<byte[], String> el: familyPaths) {
-      protoFamilyPaths.add(ClientProtos.BulkLoadHFileRequest.FamilyPath.newBuilder()
+      protoFamilyPaths.add(ClientProtos.FamilyPath.newBuilder()
         .setFamily(UnsafeByteOperations.unsafeWrap(el.getFirst()))
         .setPath(el.getSecond()).build());
     }
diff --git a/hbase-endpoint/src/main/java/org/apache/hadoop/hbase/security/access/SecureBulkLoadEndpoint.java b/hbase-endpoint/src/main/java/org/apache/hadoop/hbase/security/access/SecureBulkLoadEndpoint.java
index 1849d90..a416355 100644
--- a/hbase-endpoint/src/main/java/org/apache/hadoop/hbase/security/access/SecureBulkLoadEndpoint.java
+++ b/hbase-endpoint/src/main/java/org/apache/hadoop/hbase/security/access/SecureBulkLoadEndpoint.java
@@ -19,12 +19,16 @@
 package org.apache.hadoop.hbase.security.access;
 
 import java.io.IOException;
+import java.util.List;
+import java.util.Map;
 
 import org.apache.commons.logging.Log;
 import org.apache.commons.logging.LogFactory;
+import org.apache.hadoop.fs.Path;
 import org.apache.hadoop.hbase.Coprocessor;
 import org.apache.hadoop.hbase.CoprocessorEnvironment;
 import org.apache.hadoop.hbase.classification.InterfaceAudience;
+import org.apache.hadoop.hbase.client.FamilyPath;
 import org.apache.hadoop.hbase.coprocessor.CoprocessorService;
 import org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment;
 import org.apache.hadoop.hbase.ipc.CoprocessorRpcUtils;
@@ -40,6 +44,7 @@ import org.apache.hadoop.hbase.protobuf.generated.SecureBulkLoadProtos.SecureBul
 import org.apache.hadoop.hbase.protobuf.generated.SecureBulkLoadProtos.SecureBulkLoadHFilesResponse;
 import org.apache.hadoop.hbase.protobuf.generated.SecureBulkLoadProtos.SecureBulkLoadService;
 import org.apache.hadoop.hbase.regionserver.SecureBulkLoadManager;
+import org.apache.hadoop.hbase.shaded.com.google.protobuf.UnsafeByteOperations;
 
 import com.google.protobuf.RpcCallback;
 import com.google.protobuf.RpcController;
@@ -128,12 +133,14 @@ public class SecureBulkLoadEndpoint extends SecureBulkLoadService
   public void secureBulkLoadHFiles(RpcController controller, SecureBulkLoadHFilesRequest request,
       RpcCallback<SecureBulkLoadHFilesResponse> done) {
     boolean loaded = false;
+    Map<byte[], List<Path>> map = null;
     try {
       SecureBulkLoadManager secureBulkLoadManager =
           this.env.getRegionServerServices().getSecureBulkLoadManager();
       BulkLoadHFileRequest bulkLoadHFileRequest = ConvertSecureBulkLoadHFilesRequest(request);
-      loaded = secureBulkLoadManager.secureBulkLoadHFiles(this.env.getRegion(),
+      map = secureBulkLoadManager.secureBulkLoadHFiles(this.env.getRegion(),
           convert(bulkLoadHFileRequest));
+      loaded = map != null && !map.isEmpty();
     } catch (IOException e) {
       CoprocessorRpcUtils.setControllerException(controller, e);
     }
diff --git a/hbase-protocol-shaded/src/main/java/org/apache/hadoop/hbase/shaded/protobuf/generated/ClientProtos.java b/hbase-protocol-shaded/src/main/java/org/apache/hadoop/hbase/shaded/protobuf/generated/ClientProtos.java
index bfd196e..f949a24 100644
--- a/hbase-protocol-shaded/src/main/java/org/apache/hadoop/hbase/shaded/protobuf/generated/ClientProtos.java
+++ b/hbase-protocol-shaded/src/main/java/org/apache/hadoop/hbase/shaded/protobuf/generated/ClientProtos.java
@@ -21482,91 +21482,32 @@ public final class ClientProtos {
 
   }
 
-  public interface BulkLoadHFileRequestOrBuilder extends
-      // @@protoc_insertion_point(interface_extends:hbase.pb.BulkLoadHFileRequest)
+  public interface FamilyPathOrBuilder extends
+      // @@protoc_insertion_point(interface_extends:hbase.pb.FamilyPath)
       org.apache.hadoop.hbase.shaded.com.google.protobuf.MessageOrBuilder {
 
     /**
-     * <code>required .hbase.pb.RegionSpecifier region = 1;</code>
-     */
-    boolean hasRegion();
-    /**
-     * <code>required .hbase.pb.RegionSpecifier region = 1;</code>
-     */
-    org.apache.hadoop.hbase.shaded.protobuf.generated.HBaseProtos.RegionSpecifier getRegion();
-    /**
-     * <code>required .hbase.pb.RegionSpecifier region = 1;</code>
-     */
-    org.apache.hadoop.hbase.shaded.protobuf.generated.HBaseProtos.RegionSpecifierOrBuilder getRegionOrBuilder();
-
-    /**
-     * <code>repeated .hbase.pb.BulkLoadHFileRequest.FamilyPath family_path = 2;</code>
-     */
-    java.util.List<org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos.BulkLoadHFileRequest.FamilyPath> 
-        getFamilyPathList();
-    /**
-     * <code>repeated .hbase.pb.BulkLoadHFileRequest.FamilyPath family_path = 2;</code>
-     */
-    org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos.BulkLoadHFileRequest.FamilyPath getFamilyPath(int index);
-    /**
-     * <code>repeated .hbase.pb.BulkLoadHFileRequest.FamilyPath family_path = 2;</code>
-     */
-    int getFamilyPathCount();
-    /**
-     * <code>repeated .hbase.pb.BulkLoadHFileRequest.FamilyPath family_path = 2;</code>
-     */
-    java.util.List<? extends org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos.BulkLoadHFileRequest.FamilyPathOrBuilder> 
-        getFamilyPathOrBuilderList();
-    /**
-     * <code>repeated .hbase.pb.BulkLoadHFileRequest.FamilyPath family_path = 2;</code>
-     */
-    org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos.BulkLoadHFileRequest.FamilyPathOrBuilder getFamilyPathOrBuilder(
-        int index);
-
-    /**
-     * <code>optional bool assign_seq_num = 3;</code>
-     */
-    boolean hasAssignSeqNum();
-    /**
-     * <code>optional bool assign_seq_num = 3;</code>
-     */
-    boolean getAssignSeqNum();
-
-    /**
-     * <code>optional .hbase.pb.DelegationToken fs_token = 4;</code>
-     */
-    boolean hasFsToken();
-    /**
-     * <code>optional .hbase.pb.DelegationToken fs_token = 4;</code>
+     * <code>required bytes family = 1;</code>
      */
-    org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos.DelegationToken getFsToken();
+    boolean hasFamily();
     /**
-     * <code>optional .hbase.pb.DelegationToken fs_token = 4;</code>
+     * <code>required bytes family = 1;</code>
      */
-    org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos.DelegationTokenOrBuilder getFsTokenOrBuilder();
+    org.apache.hadoop.hbase.shaded.com.google.protobuf.ByteString getFamily();
 
     /**
-     * <code>optional string bulk_token = 5;</code>
+     * <code>required string path = 2;</code>
      */
-    boolean hasBulkToken();
+    boolean hasPath();
     /**
-     * <code>optional string bulk_token = 5;</code>
+     * <code>required string path = 2;</code>
      */
-    java.lang.String getBulkToken();
+    java.lang.String getPath();
     /**
-     * <code>optional string bulk_token = 5;</code>
+     * <code>required string path = 2;</code>
      */
     org.apache.hadoop.hbase.shaded.com.google.protobuf.ByteString
-        getBulkTokenBytes();
-
-    /**
-     * <code>optional bool copy_file = 6 [default = false];</code>
-     */
-    boolean hasCopyFile();
-    /**
-     * <code>optional bool copy_file = 6 [default = false];</code>
-     */
-    boolean getCopyFile();
+        getPathBytes();
   }
   /**
    * <pre>
@@ -21575,21 +21516,19 @@ public final class ClientProtos {
    * into an open region.
    * </pre>
    *
-   * Protobuf type {@code hbase.pb.BulkLoadHFileRequest}
+   * Protobuf type {@code hbase.pb.FamilyPath}
    */
-  public  static final class BulkLoadHFileRequest extends
+  public  static final class FamilyPath extends
       org.apache.hadoop.hbase.shaded.com.google.protobuf.GeneratedMessageV3 implements
-      // @@protoc_insertion_point(message_implements:hbase.pb.BulkLoadHFileRequest)
-      BulkLoadHFileRequestOrBuilder {
-    // Use BulkLoadHFileRequest.newBuilder() to construct.
-    private BulkLoadHFileRequest(org.apache.hadoop.hbase.shaded.com.google.protobuf.GeneratedMessageV3.Builder<?> builder) {
+      // @@protoc_insertion_point(message_implements:hbase.pb.FamilyPath)
+      FamilyPathOrBuilder {
+    // Use FamilyPath.newBuilder() to construct.
+    private FamilyPath(org.apache.hadoop.hbase.shaded.com.google.protobuf.GeneratedMessageV3.Builder<?> builder) {
       super(builder);
     }
-    private BulkLoadHFileRequest() {
-      familyPath_ = java.util.Collections.emptyList();
-      assignSeqNum_ = false;
-      bulkToken_ = "";
-      copyFile_ = false;
+    private FamilyPath() {
+      family_ = org.apache.hadoop.hbase.shaded.com.google.protobuf.ByteString.EMPTY;
+      path_ = "";
     }
 
     @java.lang.Override
@@ -21597,7 +21536,7 @@ public final class ClientProtos {
     getUnknownFields() {
       return this.unknownFields;
     }
-    private BulkLoadHFileRequest(
+    private FamilyPath(
         org.apache.hadoop.hbase.shaded.com.google.protobuf.CodedInputStream input,
         org.apache.hadoop.hbase.shaded.com.google.protobuf.ExtensionRegistryLite extensionRegistry)
         throws org.apache.hadoop.hbase.shaded.com.google.protobuf.InvalidProtocolBufferException {
@@ -21621,54 +21560,14 @@ public final class ClientProtos {
               break;
             }
             case 10: {
-              org.apache.hadoop.hbase.shaded.protobuf.generated.HBaseProtos.RegionSpecifier.Builder subBuilder = null;
-              if (((bitField0_ & 0x00000001) == 0x00000001)) {
-                subBuilder = region_.toBuilder();
-              }
-              region_ = input.readMessage(org.apache.hadoop.hbase.shaded.protobuf.generated.HBaseProtos.RegionSpecifier.PARSER, extensionRegistry);
-              if (subBuilder != null) {
-                subBuilder.mergeFrom(region_);
-                region_ = subBuilder.buildPartial();
-              }
               bitField0_ |= 0x00000001;
+              family_ = input.readBytes();
               break;
             }
             case 18: {
-              if (!((mutable_bitField0_ & 0x00000002) == 0x00000002)) {
-                familyPath_ = new java.util.ArrayList<org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos.BulkLoadHFileRequest.FamilyPath>();
-                mutable_bitField0_ |= 0x00000002;
-              }
-              familyPath_.add(
-                  input.readMessage(org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos.BulkLoadHFileRequest.FamilyPath.PARSER, extensionRegistry));
-              break;
-            }
-            case 24: {
-              bitField0_ |= 0x00000002;
-              assignSeqNum_ = input.readBool();
-              break;
-            }
-            case 34: {
-              org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos.DelegationToken.Builder subBuilder = null;
-              if (((bitField0_ & 0x00000004) == 0x00000004)) {
-                subBuilder = fsToken_.toBuilder();
-              }
-              fsToken_ = input.readMessage(org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos.DelegationToken.PARSER, extensionRegistry);
-              if (subBuilder != null) {
-                subBuilder.mergeFrom(fsToken_);
-                fsToken_ = subBuilder.buildPartial();
-              }
-              bitField0_ |= 0x00000004;
-              break;
-            }
-            case 42: {
               org.apache.hadoop.hbase.shaded.com.google.protobuf.ByteString bs = input.readBytes();
-              bitField0_ |= 0x00000008;
-              bulkToken_ = bs;
-              break;
-            }
-            case 48: {
-              bitField0_ |= 0x00000010;
-              copyFile_ = input.readBool();
+              bitField0_ |= 0x00000002;
+              path_ = bs;
               break;
             }
           }
@@ -21679,671 +21578,778 @@ public final class ClientProtos {
         throw new org.apache.hadoop.hbase.shaded.com.google.protobuf.InvalidProtocolBufferException(
             e).setUnfinishedMessage(this);
       } finally {
-        if (((mutable_bitField0_ & 0x00000002) == 0x00000002)) {
-          familyPath_ = java.util.Collections.unmodifiableList(familyPath_);
-        }
         this.unknownFields = unknownFields.build();
         makeExtensionsImmutable();
       }
     }
     public static final org.apache.hadoop.hbase.shaded.com.google.protobuf.Descriptors.Descriptor
         getDescriptor() {
-      return org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos.internal_static_hbase_pb_BulkLoadHFileRequest_descriptor;
+      return org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos.internal_static_hbase_pb_FamilyPath_descriptor;
     }
 
     protected org.apache.hadoop.hbase.shaded.com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
         internalGetFieldAccessorTable() {
-      return org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos.internal_static_hbase_pb_BulkLoadHFileRequest_fieldAccessorTable
+      return org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos.internal_static_hbase_pb_FamilyPath_fieldAccessorTable
           .ensureFieldAccessorsInitialized(
-              org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos.BulkLoadHFileRequest.class, org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos.BulkLoadHFileRequest.Builder.class);
+              org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos.FamilyPath.class, org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos.FamilyPath.Builder.class);
     }
 
-    public interface FamilyPathOrBuilder extends
-        // @@protoc_insertion_point(interface_extends:hbase.pb.BulkLoadHFileRequest.FamilyPath)
-        org.apache.hadoop.hbase.shaded.com.google.protobuf.MessageOrBuilder {
-
-      /**
-       * <code>required bytes family = 1;</code>
-       */
-      boolean hasFamily();
-      /**
-       * <code>required bytes family = 1;</code>
-       */
-      org.apache.hadoop.hbase.shaded.com.google.protobuf.ByteString getFamily();
+    private int bitField0_;
+    public static final int FAMILY_FIELD_NUMBER = 1;
+    private org.apache.hadoop.hbase.shaded.com.google.protobuf.ByteString family_;
+    /**
+     * <code>required bytes family = 1;</code>
+     */
+    public boolean hasFamily() {
+      return ((bitField0_ & 0x00000001) == 0x00000001);
+    }
+    /**
+     * <code>required bytes family = 1;</code>
+     */
+    public org.apache.hadoop.hbase.shaded.com.google.protobuf.ByteString getFamily() {
+      return family_;
+    }
 
-      /**
-       * <code>required string path = 2;</code>
-       */
-      boolean hasPath();
-      /**
-       * <code>required string path = 2;</code>
-       */
-      java.lang.String getPath();
-      /**
-       * <code>required string path = 2;</code>
-       */
-      org.apache.hadoop.hbase.shaded.com.google.protobuf.ByteString
-          getPathBytes();
+    public static final int PATH_FIELD_NUMBER = 2;
+    private volatile java.lang.Object path_;
+    /**
+     * <code>required string path = 2;</code>
+     */
+    public boolean hasPath() {
+      return ((bitField0_ & 0x00000002) == 0x00000002);
     }
     /**
-     * Protobuf type {@code hbase.pb.BulkLoadHFileRequest.FamilyPath}
+     * <code>required string path = 2;</code>
      */
-    public  static final class FamilyPath extends
-        org.apache.hadoop.hbase.shaded.com.google.protobuf.GeneratedMessageV3 implements
-        // @@protoc_insertion_point(message_implements:hbase.pb.BulkLoadHFileRequest.FamilyPath)
-        FamilyPathOrBuilder {
-      // Use FamilyPath.newBuilder() to construct.
-      private FamilyPath(org.apache.hadoop.hbase.shaded.com.google.protobuf.GeneratedMessageV3.Builder<?> builder) {
-        super(builder);
+    public java.lang.String getPath() {
+      java.lang.Object ref = path_;
+      if (ref instanceof java.lang.String) {
+        return (java.lang.String) ref;
+      } else {
+        org.apache.hadoop.hbase.shaded.com.google.protobuf.ByteString bs = 
+            (org.apache.hadoop.hbase.shaded.com.google.protobuf.ByteString) ref;
+        java.lang.String s = bs.toStringUtf8();
+        if (bs.isValidUtf8()) {
+          path_ = s;
+        }
+        return s;
       }
-      private FamilyPath() {
-        family_ = org.apache.hadoop.hbase.shaded.com.google.protobuf.ByteString.EMPTY;
-        path_ = "";
+    }
+    /**
+     * <code>required string path = 2;</code>
+     */
+    public org.apache.hadoop.hbase.shaded.com.google.protobuf.ByteString
+        getPathBytes() {
+      java.lang.Object ref = path_;
+      if (ref instanceof java.lang.String) {
+        org.apache.hadoop.hbase.shaded.com.google.protobuf.ByteString b = 
+            org.apache.hadoop.hbase.shaded.com.google.protobuf.ByteString.copyFromUtf8(
+                (java.lang.String) ref);
+        path_ = b;
+        return b;
+      } else {
+        return (org.apache.hadoop.hbase.shaded.com.google.protobuf.ByteString) ref;
       }
+    }
 
-      @java.lang.Override
-      public final org.apache.hadoop.hbase.shaded.com.google.protobuf.UnknownFieldSet
-      getUnknownFields() {
-        return this.unknownFields;
-      }
-      private FamilyPath(
-          org.apache.hadoop.hbase.shaded.com.google.protobuf.CodedInputStream input,
-          org.apache.hadoop.hbase.shaded.com.google.protobuf.ExtensionRegistryLite extensionRegistry)
-          throws org.apache.hadoop.hbase.shaded.com.google.protobuf.InvalidProtocolBufferException {
-        this();
-        int mutable_bitField0_ = 0;
-        org.apache.hadoop.hbase.shaded.com.google.protobuf.UnknownFieldSet.Builder unknownFields =
-            org.apache.hadoop.hbase.shaded.com.google.protobuf.UnknownFieldSet.newBuilder();
-        try {
-          boolean done = false;
-          while (!done) {
-            int tag = input.readTag();
-            switch (tag) {
-              case 0:
-                done = true;
-                break;
-              default: {
-                if (!parseUnknownField(input, unknownFields,
-                                       extensionRegistry, tag)) {
-                  done = true;
-                }
-                break;
-              }
-              case 10: {
-                bitField0_ |= 0x00000001;
-                family_ = input.readBytes();
-                break;
-              }
-              case 18: {
-                org.apache.hadoop.hbase.shaded.com.google.protobuf.ByteString bs = input.readBytes();
-                bitField0_ |= 0x00000002;
-                path_ = bs;
-                break;
-              }
-            }
-          }
-        } catch (org.apache.hadoop.hbase.shaded.com.google.protobuf.InvalidProtocolBufferException e) {
-          throw e.setUnfinishedMessage(this);
-        } catch (java.io.IOException e) {
-          throw new org.apache.hadoop.hbase.shaded.com.google.protobuf.InvalidProtocolBufferException(
-              e).setUnfinishedMessage(this);
-        } finally {
-          this.unknownFields = unknownFields.build();
-          makeExtensionsImmutable();
-        }
+    private byte memoizedIsInitialized = -1;
+    public final boolean isInitialized() {
+      byte isInitialized = memoizedIsInitialized;
+      if (isInitialized == 1) return true;
+      if (isInitialized == 0) return false;
+
+      if (!hasFamily()) {
+        memoizedIsInitialized = 0;
+        return false;
       }
-      public static final org.apache.hadoop.hbase.shaded.com.google.protobuf.Descriptors.Descriptor
-          getDescriptor() {
-        return org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos.internal_static_hbase_pb_BulkLoadHFileRequest_FamilyPath_descriptor;
+      if (!hasPath()) {
+        memoizedIsInitialized = 0;
+        return false;
       }
+      memoizedIsInitialized = 1;
+      return true;
+    }
 
-      protected org.apache.hadoop.hbase.shaded.com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
-          internalGetFieldAccessorTable() {
-        return org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos.internal_static_hbase_pb_BulkLoadHFileRequest_FamilyPath_fieldAccessorTable
-            .ensureFieldAccessorsInitialized(
-                org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos.BulkLoadHFileRequest.FamilyPath.class, org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos.BulkLoadHFileRequest.FamilyPath.Builder.class);
+    public void writeTo(org.apache.hadoop.hbase.shaded.com.google.protobuf.CodedOutputStream output)
+                        throws java.io.IOException {
+      if (((bitField0_ & 0x00000001) == 0x00000001)) {
+        output.writeBytes(1, family_);
       }
+      if (((bitField0_ & 0x00000002) == 0x00000002)) {
+        org.apache.hadoop.hbase.shaded.com.google.protobuf.GeneratedMessageV3.writeString(output, 2, path_);
+      }
+      unknownFields.writeTo(output);
+    }
 
-      private int bitField0_;
-      public static final int FAMILY_FIELD_NUMBER = 1;
-      private org.apache.hadoop.hbase.shaded.com.google.protobuf.ByteString family_;
-      /**
-       * <code>required bytes family = 1;</code>
-       */
-      public boolean hasFamily() {
-        return ((bitField0_ & 0x00000001) == 0x00000001);
+    public int getSerializedSize() {
+      int size = memoizedSize;
+      if (size != -1) return size;
+
+      size = 0;
+      if (((bitField0_ & 0x00000001) == 0x00000001)) {
+        size += org.apache.hadoop.hbase.shaded.com.google.protobuf.CodedOutputStream
+          .computeBytesSize(1, family_);
       }
-      /**
-       * <code>required bytes family = 1;</code>
-       */
-      public org.apache.hadoop.hbase.shaded.com.google.protobuf.ByteString getFamily() {
-        return family_;
+      if (((bitField0_ & 0x00000002) == 0x00000002)) {
+        size += org.apache.hadoop.hbase.shaded.com.google.protobuf.GeneratedMessageV3.computeStringSize(2, path_);
       }
+      size += unknownFields.getSerializedSize();
+      memoizedSize = size;
+      return size;
+    }
 
-      public static final int PATH_FIELD_NUMBER = 2;
-      private volatile java.lang.Object path_;
-      /**
-       * <code>required string path = 2;</code>
-       */
-      public boolean hasPath() {
-        return ((bitField0_ & 0x00000002) == 0x00000002);
+    private static final long serialVersionUID = 0L;
+    @java.lang.Override
+    public boolean equals(final java.lang.Object obj) {
+      if (obj == this) {
+       return true;
       }
-      /**
-       * <code>required string path = 2;</code>
-       */
-      public java.lang.String getPath() {
-        java.lang.Object ref = path_;
-        if (ref instanceof java.lang.String) {
-          return (java.lang.String) ref;
-        } else {
-          org.apache.hadoop.hbase.shaded.com.google.protobuf.ByteString bs = 
-              (org.apache.hadoop.hbase.shaded.com.google.protobuf.ByteString) ref;
-          java.lang.String s = bs.toStringUtf8();
-          if (bs.isValidUtf8()) {
-            path_ = s;
-          }
-          return s;
-        }
+      if (!(obj instanceof org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos.FamilyPath)) {
+        return super.equals(obj);
       }
-      /**
-       * <code>required string path = 2;</code>
-       */
-      public org.apache.hadoop.hbase.shaded.com.google.protobuf.ByteString
-          getPathBytes() {
-        java.lang.Object ref = path_;
-        if (ref instanceof java.lang.String) {
-          org.apache.hadoop.hbase.shaded.com.google.protobuf.ByteString b = 
-              org.apache.hadoop.hbase.shaded.com.google.protobuf.ByteString.copyFromUtf8(
-                  (java.lang.String) ref);
-          path_ = b;
-          return b;
-        } else {
-          return (org.apache.hadoop.hbase.shaded.com.google.protobuf.ByteString) ref;
-        }
+      org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos.FamilyPath other = (org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos.FamilyPath) obj;
+
+      boolean result = true;
+      result = result && (hasFamily() == other.hasFamily());
+      if (hasFamily()) {
+        result = result && getFamily()
+            .equals(other.getFamily());
+      }
+      result = result && (hasPath() == other.hasPath());
+      if (hasPath()) {
+        result = result && getPath()
+            .equals(other.getPath());
       }
+      result = result && unknownFields.equals(other.unknownFields);
+      return result;
+    }
 
-      private byte memoizedIsInitialized = -1;
-      public final boolean isInitialized() {
-        byte isInitialized = memoizedIsInitialized;
-        if (isInitialized == 1) return true;
-        if (isInitialized == 0) return false;
+    @java.lang.Override
+    public int hashCode() {
+      if (memoizedHashCode != 0) {
+        return memoizedHashCode;
+      }
+      int hash = 41;
+      hash = (19 * hash) + getDescriptorForType().hashCode();
+      if (hasFamily()) {
+        hash = (37 * hash) + FAMILY_FIELD_NUMBER;
+        hash = (53 * hash) + getFamily().hashCode();
+      }
+      if (hasPath()) {
+        hash = (37 * hash) + PATH_FIELD_NUMBER;
+        hash = (53 * hash) + getPath().hashCode();
+      }
+      hash = (29 * hash) + unknownFields.hashCode();
+      memoizedHashCode = hash;
+      return hash;
+    }
 
-        if (!hasFamily()) {
-          memoizedIsInitialized = 0;
-          return false;
-        }
-        if (!hasPath()) {
-          memoizedIsInitialized = 0;
-          return false;
-        }
-        memoizedIsInitialized = 1;
-        return true;
+    public static org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos.FamilyPath parseFrom(
+        org.apache.hadoop.hbase.shaded.com.google.protobuf.ByteString data)
+        throws org.apache.hadoop.hbase.shaded.com.google.protobuf.InvalidProtocolBufferException {
+      return PARSER.parseFrom(data);
+    }
+    public static org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos.FamilyPath parseFrom(
+        org.apache.hadoop.hbase.shaded.com.google.protobuf.ByteString data,
+        org.apache.hadoop.hbase.shaded.com.google.protobuf.ExtensionRegistryLite extensionRegistry)
+        throws org.apache.hadoop.hbase.shaded.com.google.protobuf.InvalidProtocolBufferException {
+      return PARSER.parseFrom(data, extensionRegistry);
+    }
+    public static org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos.FamilyPath parseFrom(byte[] data)
+        throws org.apache.hadoop.hbase.shaded.com.google.protobuf.InvalidProtocolBufferException {
+      return PARSER.parseFrom(data);
+    }
+    public static org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos.FamilyPath parseFrom(
+        byte[] data,
+        org.apache.hadoop.hbase.shaded.com.google.protobuf.ExtensionRegistryLite extensionRegistry)
+        throws org.apache.hadoop.hbase.shaded.com.google.protobuf.InvalidProtocolBufferException {
+      return PARSER.parseFrom(data, extensionRegistry);
+    }
+    public static org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos.FamilyPath parseFrom(java.io.InputStream input)
+        throws java.io.IOException {
+      return org.apache.hadoop.hbase.shaded.com.google.protobuf.GeneratedMessageV3
+          .parseWithIOException(PARSER, input);
+    }
+    public static org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos.FamilyPath parseFrom(
+        java.io.InputStream input,
+        org.apache.hadoop.hbase.shaded.com.google.protobuf.ExtensionRegistryLite extensionRegistry)
+        throws java.io.IOException {
+      return org.apache.hadoop.hbase.shaded.com.google.protobuf.GeneratedMessageV3
+          .parseWithIOException(PARSER, input, extensionRegistry);
+    }
+    public static org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos.FamilyPath parseDelimitedFrom(java.io.InputStream input)
+        throws java.io.IOException {
+      return org.apache.hadoop.hbase.shaded.com.google.protobuf.GeneratedMessageV3
+          .parseDelimitedWithIOException(PARSER, input);
+    }
+    public static org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos.FamilyPath parseDelimitedFrom(
+        java.io.InputStream input,
+        org.apache.hadoop.hbase.shaded.com.google.protobuf.ExtensionRegistryLite extensionRegistry)
+        throws java.io.IOException {
+      return org.apache.hadoop.hbase.shaded.com.google.protobuf.GeneratedMessageV3
+          .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
+    }
+    public static org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos.FamilyPath parseFrom(
+        org.apache.hadoop.hbase.shaded.com.google.protobuf.CodedInputStream input)
+        throws java.io.IOException {
+      return org.apache.hadoop.hbase.shaded.com.google.protobuf.GeneratedMessageV3
+          .parseWithIOException(PARSER, input);
+    }
+    public static org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos.FamilyPath parseFrom(
+        org.apache.hadoop.hbase.shaded.com.google.protobuf.CodedInputStream input,
+        org.apache.hadoop.hbase.shaded.com.google.protobuf.ExtensionRegistryLite extensionRegistry)
+        throws java.io.IOException {
+      return org.apache.hadoop.hbase.shaded.com.google.protobuf.GeneratedMessageV3
+          .parseWithIOException(PARSER, input, extensionRegistry);
+    }
+
+    public Builder newBuilderForType() { return newBuilder(); }
+    public static Builder newBuilder() {
+      return DEFAULT_INSTANCE.toBuilder();
+    }
+    public static Builder newBuilder(org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos.FamilyPath prototype) {
+      return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
+    }
+    public Builder toBuilder() {
+      return this == DEFAULT_INSTANCE
+          ? new Builder() : new Builder().mergeFrom(this);
+    }
+
+    @java.lang.Override
+    protected Builder newBuilderForType(
+        org.apache.hadoop.hbase.shaded.com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
+      Builder builder = new Builder(parent);
+      return builder;
+    }
+    /**
+     * <pre>
+     **
+     * Atomically bulk load multiple HFiles (say from different column families)
+     * into an open region.
+     * </pre>
+     *
+     * Protobuf type {@code hbase.pb.FamilyPath}
+     */
+    public static final class Builder extends
+        org.apache.hadoop.hbase.shaded.com.google.protobuf.GeneratedMessageV3.Builder<Builder> implements
+        // @@protoc_insertion_point(builder_implements:hbase.pb.FamilyPath)
+        org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos.FamilyPathOrBuilder {
+      public static final org.apache.hadoop.hbase.shaded.com.google.protobuf.Descriptors.Descriptor
+          getDescriptor() {
+        return org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos.internal_static_hbase_pb_FamilyPath_descriptor;
       }
 
-      public void writeTo(org.apache.hadoop.hbase.shaded.com.google.protobuf.CodedOutputStream output)
-                          throws java.io.IOException {
-        if (((bitField0_ & 0x00000001) == 0x00000001)) {
-          output.writeBytes(1, family_);
-        }
-        if (((bitField0_ & 0x00000002) == 0x00000002)) {
-          org.apache.hadoop.hbase.shaded.com.google.protobuf.GeneratedMessageV3.writeString(output, 2, path_);
-        }
-        unknownFields.writeTo(output);
+      protected org.apache.hadoop.hbase.shaded.com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
+          internalGetFieldAccessorTable() {
+        return org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos.internal_static_hbase_pb_FamilyPath_fieldAccessorTable
+            .ensureFieldAccessorsInitialized(
+                org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos.FamilyPath.class, org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos.FamilyPath.Builder.class);
       }
 
-      public int getSerializedSize() {
-        int size = memoizedSize;
-        if (size != -1) return size;
+      // Construct using org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos.FamilyPath.newBuilder()
+      private Builder() {
+        maybeForceBuilderInitialization();
+      }
 
-        size = 0;
-        if (((bitField0_ & 0x00000001) == 0x00000001)) {
-          size += org.apache.hadoop.hbase.shaded.com.google.protobuf.CodedOutputStream
-            .computeBytesSize(1, family_);
-        }
-        if (((bitField0_ & 0x00000002) == 0x00000002)) {
-          size += org.apache.hadoop.hbase.shaded.com.google.protobuf.GeneratedMessageV3.computeStringSize(2, path_);
+      private Builder(
+          org.apache.hadoop.hbase.shaded.com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
+        super(parent);
+        maybeForceBuilderInitialization();
+      }
+      private void maybeForceBuilderInitialization() {
+        if (org.apache.hadoop.hbase.shaded.com.google.protobuf.GeneratedMessageV3
+                .alwaysUseFieldBuilders) {
         }
-        size += unknownFields.getSerializedSize();
-        memoizedSize = size;
-        return size;
+      }
+      public Builder clear() {
+        super.clear();
+        family_ = org.apache.hadoop.hbase.shaded.com.google.protobuf.ByteString.EMPTY;
+        bitField0_ = (bitField0_ & ~0x00000001);
+        path_ = "";
+        bitField0_ = (bitField0_ & ~0x00000002);
+        return this;
       }
 
-      private static final long serialVersionUID = 0L;
-      @java.lang.Override
-      public boolean equals(final java.lang.Object obj) {
-        if (obj == this) {
-         return true;
-        }
-        if (!(obj instanceof org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos.BulkLoadHFileRequest.FamilyPath)) {
-          return super.equals(obj);
-        }
-        org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos.BulkLoadHFileRequest.FamilyPath other = (org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos.BulkLoadHFileRequest.FamilyPath) obj;
+      public org.apache.hadoop.hbase.shaded.com.google.protobuf.Descriptors.Descriptor
+          getDescriptorForType() {
+        return org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos.internal_static_hbase_pb_FamilyPath_descriptor;
+      }
 
-        boolean result = true;
-        result = result && (hasFamily() == other.hasFamily());
-        if (hasFamily()) {
-          result = result && getFamily()
-              .equals(other.getFamily());
-        }
-        result = result && (hasPath() == other.hasPath());
-        if (hasPath()) {
-          result = result && getPath()
-              .equals(other.getPath());
+      public org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos.FamilyPath getDefaultInstanceForType() {
+        return org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos.FamilyPath.getDefaultInstance();
+      }
+
+      public org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos.FamilyPath build() {
+        org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos.FamilyPath result = buildPartial();
+        if (!result.isInitialized()) {
+          throw newUninitializedMessageException(result);
         }
-        result = result && unknownFields.equals(other.unknownFields);
         return result;
       }
 
-      @java.lang.Override
-      public int hashCode() {
-        if (memoizedHashCode != 0) {
-          return memoizedHashCode;
-        }
-        int hash = 41;
-        hash = (19 * hash) + getDescriptorForType().hashCode();
-        if (hasFamily()) {
-          hash = (37 * hash) + FAMILY_FIELD_NUMBER;
-          hash = (53 * hash) + getFamily().hashCode();
+      public org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos.FamilyPath buildPartial() {
+        org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos.FamilyPath result = new org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos.FamilyPath(this);
+        int from_bitField0_ = bitField0_;
+        int to_bitField0_ = 0;
+        if (((from_bitField0_ & 0x00000001) == 0x00000001)) {
+          to_bitField0_ |= 0x00000001;
         }
-        if (hasPath()) {
-          hash = (37 * hash) + PATH_FIELD_NUMBER;
-          hash = (53 * hash) + getPath().hashCode();
+        result.family_ = family_;
+        if (((from_bitField0_ & 0x00000002) == 0x00000002)) {
+          to_bitField0_ |= 0x00000002;
         }
-        hash = (29 * hash) + unknownFields.hashCode();
-        memoizedHashCode = hash;
-        return hash;
+        result.path_ = path_;
+        result.bitField0_ = to_bitField0_;
+        onBuilt();
+        return result;
       }
 
-      public static org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos.BulkLoadHFileRequest.FamilyPath parseFrom(
-          org.apache.hadoop.hbase.shaded.com.google.protobuf.ByteString data)
-          throws org.apache.hadoop.hbase.shaded.com.google.protobuf.InvalidProtocolBufferException {
-        return PARSER.parseFrom(data);
+      public Builder clone() {
+        return (Builder) super.clone();
       }
-      public static org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos.BulkLoadHFileRequest.FamilyPath parseFrom(
-          org.apache.hadoop.hbase.shaded.com.google.protobuf.ByteString data,
-          org.apache.hadoop.hbase.shaded.com.google.protobuf.ExtensionRegistryLite extensionRegistry)
-          throws org.apache.hadoop.hbase.shaded.com.google.protobuf.InvalidProtocolBufferException {
-        return PARSER.parseFrom(data, extensionRegistry);
+      public Builder setField(
+          org.apache.hadoop.hbase.shaded.com.google.protobuf.Descriptors.FieldDescriptor field,
+          Object value) {
+        return (Builder) super.setField(field, value);
       }
-      public static org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos.BulkLoadHFileRequest.FamilyPath parseFrom(byte[] data)
-          throws org.apache.hadoop.hbase.shaded.com.google.protobuf.InvalidProtocolBufferException {
-        return PARSER.parseFrom(data);
+      public Builder clearField(
+          org.apache.hadoop.hbase.shaded.com.google.protobuf.Descriptors.FieldDescriptor field) {
+        return (Builder) super.clearField(field);
       }
-      public static org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos.BulkLoadHFileRequest.FamilyPath parseFrom(
-          byte[] data,
-          org.apache.hadoop.hbase.shaded.com.google.protobuf.ExtensionRegistryLite extensionRegistry)
-          throws org.apache.hadoop.hbase.shaded.com.google.protobuf.InvalidProtocolBufferException {
-        return PARSER.parseFrom(data, extensionRegistry);
+      public Builder clearOneof(
+          org.apache.hadoop.hbase.shaded.com.google.protobuf.Descriptors.OneofDescriptor oneof) {
+        return (Builder) super.clearOneof(oneof);
       }
-      public static org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos.BulkLoadHFileRequest.FamilyPath parseFrom(java.io.InputStream input)
-          throws java.io.IOException {
-        return org.apache.hadoop.hbase.shaded.com.google.protobuf.GeneratedMessageV3
-            .parseWithIOException(PARSER, input);
+      public Builder setRepeatedField(
+          org.apache.hadoop.hbase.shaded.com.google.protobuf.Descriptors.FieldDescriptor field,
+          int index, Object value) {
+        return (Builder) super.setRepeatedField(field, index, value);
       }
-      public static org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos.BulkLoadHFileRequest.FamilyPath parseFrom(
-          java.io.InputStream input,
-          org.apache.hadoop.hbase.shaded.com.google.protobuf.ExtensionRegistryLite extensionRegistry)
-          throws java.io.IOException {
-        return org.apache.hadoop.hbase.shaded.com.google.protobuf.GeneratedMessageV3
-            .parseWithIOException(PARSER, input, extensionRegistry);
+      public Builder addRepeatedField(
+          org.apache.hadoop.hbase.shaded.com.google.protobuf.Descriptors.FieldDescriptor field,
+          Object value) {
+        return (Builder) super.addRepeatedField(field, value);
       }
-      public static org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos.BulkLoadHFileRequest.FamilyPath parseDelimitedFrom(java.io.InputStream input)
-          throws java.io.IOException {
-        return org.apache.hadoop.hbase.shaded.com.google.protobuf.GeneratedMessageV3
-            .parseDelimitedWithIOException(PARSER, input);
+      public Builder mergeFrom(org.apache.hadoop.hbase.shaded.com.google.protobuf.Message other) {
+        if (other instanceof org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos.FamilyPath) {
+          return mergeFrom((org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos.FamilyPath)other);
+        } else {
+          super.mergeFrom(other);
+          return this;
+        }
       }
-      public static org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos.BulkLoadHFileRequest.FamilyPath parseDelimitedFrom(
-          java.io.InputStream input,
-          org.apache.hadoop.hbase.shaded.com.google.protobuf.ExtensionRegistryLite extensionRegistry)
-          throws java.io.IOException {
-        return org.apache.hadoop.hbase.shaded.com.google.protobuf.GeneratedMessageV3
-            .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
+
+      public Builder mergeFrom(org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos.FamilyPath other) {
+        if (other == org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos.FamilyPath.getDefaultInstance()) return this;
+        if (other.hasFamily()) {
+          setFamily(other.getFamily());
+        }
+        if (other.hasPath()) {
+          bitField0_ |= 0x00000002;
+          path_ = other.path_;
+          onChanged();
+        }
+        this.mergeUnknownFields(other.unknownFields);
+        onChanged();
+        return this;
       }
-      public static org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos.BulkLoadHFileRequest.FamilyPath parseFrom(
-          org.apache.hadoop.hbase.shaded.com.google.protobuf.CodedInputStream input)
-          throws java.io.IOException {
-        return org.apache.hadoop.hbase.shaded.com.google.protobuf.GeneratedMessageV3
-            .parseWithIOException(PARSER, input);
+
+      public final boolean isInitialized() {
+        if (!hasFamily()) {
+          return false;
+        }
+        if (!hasPath()) {
+          return false;
+        }
+        return true;
       }
-      public static org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos.BulkLoadHFileRequest.FamilyPath parseFrom(
+
+      public Builder mergeFrom(
           org.apache.hadoop.hbase.shaded.com.google.protobuf.CodedInputStream input,
           org.apache.hadoop.hbase.shaded.com.google.protobuf.ExtensionRegistryLite extensionRegistry)
           throws java.io.IOException {
-        return org.apache.hadoop.hbase.shaded.com.google.protobuf.GeneratedMessageV3
-            .parseWithIOException(PARSER, input, extensionRegistry);
+        org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos.FamilyPath parsedMessage = null;
+        try {
+          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
+        } catch (org.apache.hadoop.hbase.shaded.com.google.protobuf.InvalidProtocolBufferException e) {
+          parsedMessage = (org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos.FamilyPath) e.getUnfinishedMessage();
+          throw e.unwrapIOException();
+        } finally {
+          if (parsedMessage != null) {
+            mergeFrom(parsedMessage);
+          }
+        }
+        return this;
       }
+      private int bitField0_;
 
-      public Builder newBuilderForType() { return newBuilder(); }
-      public static Builder newBuilder() {
-        return DEFAULT_INSTANCE.toBuilder();
+      private org.apache.hadoop.hbase.shaded.com.google.protobuf.ByteString family_ = org.apache.hadoop.hbase.shaded.com.google.protobuf.ByteString.EMPTY;
+      /**
+       * <code>required bytes family = 1;</code>
+       */
+      public boolean hasFamily() {
+        return ((bitField0_ & 0x00000001) == 0x00000001);
       }
-      public static Builder newBuilder(org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos.BulkLoadHFileRequest.FamilyPath prototype) {
-        return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
+      /**
+       * <code>required bytes family = 1;</code>
+       */
+      public org.apache.hadoop.hbase.shaded.com.google.protobuf.ByteString getFamily() {
+        return family_;
       }
-      public Builder toBuilder() {
-        return this == DEFAULT_INSTANCE
-            ? new Builder() : new Builder().mergeFrom(this);
+      /**
+       * <code>required bytes family = 1;</code>
+       */
+      public Builder setFamily(org.apache.hadoop.hbase.shaded.com.google.protobuf.ByteString value) {
+        if (value == null) {
+    throw new NullPointerException();
+  }
+  bitField0_ |= 0x00000001;
+        family_ = value;
+        onChanged();
+        return this;
+      }
+      /**
+       * <code>required bytes family = 1;</code>
+       */
+      public Builder clearFamily() {
+        bitField0_ = (bitField0_ & ~0x00000001);
+        family_ = getDefaultInstance().getFamily();
+        onChanged();
+        return this;
       }
 
-      @java.lang.Override
-      protected Builder newBuilderForType(
-          org.apache.hadoop.hbase.shaded.com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
-        Builder builder = new Builder(parent);
-        return builder;
+      private java.lang.Object path_ = "";
+      /**
+       * <code>required string path = 2;</code>
+       */
+      public boolean hasPath() {
+        return ((bitField0_ & 0x00000002) == 0x00000002);
       }
       /**
-       * Protobuf type {@code hbase.pb.BulkLoadHFileRequest.FamilyPath}
+       * <code>required string path = 2;</code>
        */
-      public static final class Builder extends
-          org.apache.hadoop.hbase.shaded.com.google.protobuf.GeneratedMessageV3.Builder<Builder> implements
-          // @@protoc_insertion_point(builder_implements:hbase.pb.BulkLoadHFileRequest.FamilyPath)
-          org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos.BulkLoadHFileRequest.FamilyPathOrBuilder {
-        public static final org.apache.hadoop.hbase.shaded.com.google.protobuf.Descriptors.Descriptor
-            getDescriptor() {
-          return org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos.internal_static_hbase_pb_BulkLoadHFileRequest_FamilyPath_descriptor;
+      public java.lang.String getPath() {
+        java.lang.Object ref = path_;
+        if (!(ref instanceof java.lang.String)) {
+          org.apache.hadoop.hbase.shaded.com.google.protobuf.ByteString bs =
+              (org.apache.hadoop.hbase.shaded.com.google.protobuf.ByteString) ref;
+          java.lang.String s = bs.toStringUtf8();
+          if (bs.isValidUtf8()) {
+            path_ = s;
+          }
+          return s;
+        } else {
+          return (java.lang.String) ref;
         }
-
-        protected org.apache.hadoop.hbase.shaded.com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
-            internalGetFieldAccessorTable() {
-          return org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos.internal_static_hbase_pb_BulkLoadHFileRequest_FamilyPath_fieldAccessorTable
-              .ensureFieldAccessorsInitialized(
-                  org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos.BulkLoadHFileRequest.FamilyPath.class, org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos.BulkLoadHFileRequest.FamilyPath.Builder.class);
+      }
+      /**
+       * <code>required string path = 2;</code>
+       */
+      public org.apache.hadoop.hbase.shaded.com.google.protobuf.ByteString
+          getPathBytes() {
+        java.lang.Object ref = path_;
+        if (ref instanceof String) {
+          org.apache.hadoop.hbase.shaded.com.google.protobuf.ByteString b = 
+              org.apache.hadoop.hbase.shaded.com.google.protobuf.ByteString.copyFromUtf8(
+                  (java.lang.String) ref);
+          path_ = b;
+          return b;
+        } else {
+          return (org.apache.hadoop.hbase.shaded.com.google.protobuf.ByteString) ref;
         }
+      }
+      /**
+       * <code>required string path = 2;</code>
+       */
+      public Builder setPath(
+          java.lang.String value) {
+        if (value == null) {
+    throw new NullPointerException();
+  }
+  bitField0_ |= 0x00000002;
+        path_ = value;
+        onChanged();
+        return this;
+      }
+      /**
+       * <code>required string path = 2;</code>
+       */
+      public Builder clearPath() {
+        bitField0_ = (bitField0_ & ~0x00000002);
+        path_ = getDefaultInstance().getPath();
+        onChanged();
+        return this;
+      }
+      /**
+       * <code>required string path = 2;</code>
+       */
+      public Builder setPathBytes(
+          org.apache.hadoop.hbase.shaded.com.google.protobuf.ByteString value) {
+        if (value == null) {
+    throw new NullPointerException();
+  }
+  bitField0_ |= 0x00000002;
+        path_ = value;
+        onChanged();
+        return this;
+      }
+      public final Builder setUnknownFields(
+          final org.apache.hadoop.hbase.shaded.com.google.protobuf.UnknownFieldSet unknownFields) {
+        return super.setUnknownFields(unknownFields);
+      }
 
-        // Construct using org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos.BulkLoadHFileRequest.FamilyPath.newBuilder()
-        private Builder() {
-          maybeForceBuilderInitialization();
-        }
+      public final Builder mergeUnknownFields(
+          final org.apache.hadoop.hbase.shaded.com.google.protobuf.UnknownFieldSet unknownFields) {
+        return super.mergeUnknownFields(unknownFields);
+      }
 
-        private Builder(
-            org.apache.hadoop.hbase.shaded.com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
-          super(parent);
-          maybeForceBuilderInitialization();
-        }
-        private void maybeForceBuilderInitialization() {
-          if (org.apache.hadoop.hbase.shaded.com.google.protobuf.GeneratedMessageV3
-                  .alwaysUseFieldBuilders) {
-          }
-        }
-        public Builder clear() {
-          super.clear();
-          family_ = org.apache.hadoop.hbase.shaded.com.google.protobuf.ByteString.EMPTY;
-          bitField0_ = (bitField0_ & ~0x00000001);
-          path_ = "";
-          bitField0_ = (bitField0_ & ~0x00000002);
-          return this;
-        }
 
-        public org.apache.hadoop.hbase.shaded.com.google.protobuf.Descriptors.Descriptor
-            getDescriptorForType() {
-          return org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos.internal_static_hbase_pb_BulkLoadHFileRequest_FamilyPath_descriptor;
-        }
+      // @@protoc_insertion_point(builder_scope:hbase.pb.FamilyPath)
+    }
 
-        public org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos.BulkLoadHFileRequest.FamilyPath getDefaultInstanceForType() {
-          return org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos.BulkLoadHFileRequest.FamilyPath.getDefaultInstance();
-        }
+    // @@protoc_insertion_point(class_scope:hbase.pb.FamilyPath)
+    private static final org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos.FamilyPath DEFAULT_INSTANCE;
+    static {
+      DEFAULT_INSTANCE = new org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos.FamilyPath();
+    }
 
-        public org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos.BulkLoadHFileRequest.FamilyPath build() {
-          org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos.BulkLoadHFileRequest.FamilyPath result = buildPartial();
-          if (!result.isInitialized()) {
-            throw newUninitializedMessageException(result);
-          }
-          return result;
-        }
+    public static org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos.FamilyPath getDefaultInstance() {
+      return DEFAULT_INSTANCE;
+    }
 
-        public org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos.BulkLoadHFileRequest.FamilyPath buildPartial() {
-          org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos.BulkLoadHFileRequest.FamilyPath result = new org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos.BulkLoadHFileRequest.FamilyPath(this);
-          int from_bitField0_ = bitField0_;
-          int to_bitField0_ = 0;
-          if (((from_bitField0_ & 0x00000001) == 0x00000001)) {
-            to_bitField0_ |= 0x00000001;
-          }
-          result.family_ = family_;
-          if (((from_bitField0_ & 0x00000002) == 0x00000002)) {
-            to_bitField0_ |= 0x00000002;
-          }
-          result.path_ = path_;
-          result.bitField0_ = to_bitField0_;
-          onBuilt();
-          return result;
-        }
+    @java.lang.Deprecated public static final org.apache.hadoop.hbase.shaded.com.google.protobuf.Parser<FamilyPath>
+        PARSER = new org.apache.hadoop.hbase.shaded.com.google.protobuf.AbstractParser<FamilyPath>() {
+      public FamilyPath parsePartialFrom(
+          org.apache.hadoop.hbase.shaded.com.google.protobuf.CodedInputStream input,
+          org.apache.hadoop.hbase.shaded.com.google.protobuf.ExtensionRegistryLite extensionRegistry)
+          throws org.apache.hadoop.hbase.shaded.com.google.protobuf.InvalidProtocolBufferException {
+          return new FamilyPath(input, extensionRegistry);
+      }
+    };
 
-        public Builder clone() {
-          return (Builder) super.clone();
-        }
-        public Builder setField(
-            org.apache.hadoop.hbase.shaded.com.google.protobuf.Descriptors.FieldDescriptor field,
-            Object value) {
-          return (Builder) super.setField(field, value);
-        }
-        public Builder clearField(
-            org.apache.hadoop.hbase.shaded.com.google.protobuf.Descriptors.FieldDescriptor field) {
-          return (Builder) super.clearField(field);
-        }
-        public Builder clearOneof(
-            org.apache.hadoop.hbase.shaded.com.google.protobuf.Descriptors.OneofDescriptor oneof) {
-          return (Builder) super.clearOneof(oneof);
-        }
-        public Builder setRepeatedField(
-            org.apache.hadoop.hbase.shaded.com.google.protobuf.Descriptors.FieldDescriptor field,
-            int index, Object value) {
-          return (Builder) super.setRepeatedField(field, index, value);
-        }
-        public Builder addRepeatedField(
-            org.apache.hadoop.hbase.shaded.com.google.protobuf.Descriptors.FieldDescriptor field,
-            Object value) {
-          return (Builder) super.addRepeatedField(field, value);
-        }
-        public Builder mergeFrom(org.apache.hadoop.hbase.shaded.com.google.protobuf.Message other) {
-          if (other instanceof org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos.BulkLoadHFileRequest.FamilyPath) {
-            return mergeFrom((org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos.BulkLoadHFileRequest.FamilyPath)other);
-          } else {
-            super.mergeFrom(other);
-            return this;
-          }
-        }
+    public static org.apache.hadoop.hbase.shaded.com.google.protobuf.Parser<FamilyPath> parser() {
+      return PARSER;
+    }
 
-        public Builder mergeFrom(org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos.BulkLoadHFileRequest.FamilyPath other) {
-          if (other == org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos.BulkLoadHFileRequest.FamilyPath.getDefaultInstance()) return this;
-          if (other.hasFamily()) {
-            setFamily(other.getFamily());
-          }
-          if (other.hasPath()) {
-            bitField0_ |= 0x00000002;
-            path_ = other.path_;
-            onChanged();
-          }
-          this.mergeUnknownFields(other.unknownFields);
-          onChanged();
-          return this;
-        }
+    @java.lang.Override
+    public org.apache.hadoop.hbase.shaded.com.google.protobuf.Parser<FamilyPath> getParserForType() {
+      return PARSER;
+    }
 
-        public final boolean isInitialized() {
-          if (!hasFamily()) {
-            return false;
-          }
-          if (!hasPath()) {
-            return false;
-          }
-          return true;
-        }
+    public org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos.FamilyPath getDefaultInstanceForType() {
+      return DEFAULT_INSTANCE;
+    }
 
-        public Builder mergeFrom(
-            org.apache.hadoop.hbase.shaded.com.google.protobuf.CodedInputStream input,
-            org.apache.hadoop.hbase.shaded.com.google.protobuf.ExtensionRegistryLite extensionRegistry)
-            throws java.io.IOException {
-          org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos.BulkLoadHFileRequest.FamilyPath parsedMessage = null;
-          try {
-            parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
-          } catch (org.apache.hadoop.hbase.shaded.com.google.protobuf.InvalidProtocolBufferException e) {
-            parsedMessage = (org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos.BulkLoadHFileRequest.FamilyPath) e.getUnfinishedMessage();
-            throw e.unwrapIOException();
-          } finally {
-            if (parsedMessage != null) {
-              mergeFrom(parsedMessage);
+  }
+
+  public interface BulkLoadHFileRequestOrBuilder extends
+      // @@protoc_insertion_point(interface_extends:hbase.pb.BulkLoadHFileRequest)
+      org.apache.hadoop.hbase.shaded.com.google.protobuf.MessageOrBuilder {
+
+    /**
+     * <code>required .hbase.pb.RegionSpecifier region = 1;</code>
+     */
+    boolean hasRegion();
+    /**
+     * <code>required .hbase.pb.RegionSpecifier region = 1;</code>
+     */
+    org.apache.hadoop.hbase.shaded.protobuf.generated.HBaseProtos.RegionSpecifier getRegion();
+    /**
+     * <code>required .hbase.pb.RegionSpecifier region = 1;</code>
+     */
+    org.apache.hadoop.hbase.shaded.protobuf.generated.HBaseProtos.RegionSpecifierOrBuilder getRegionOrBuilder();
+
+    /**
+     * <code>repeated .hbase.pb.FamilyPath family_path = 2;</code>
+     */
+    java.util.List<org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos.FamilyPath> 
+        getFamilyPathList();
+    /**
+     * <code>repeated .hbase.pb.FamilyPath family_path = 2;</code>
+     */
+    org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos.FamilyPath getFamilyPath(int index);
+    /**
+     * <code>repeated .hbase.pb.FamilyPath family_path = 2;</code>
+     */
+    int getFamilyPathCount();
+    /**
+     * <code>repeated .hbase.pb.FamilyPath family_path = 2;</code>
+     */
+    java.util.List<? extends org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos.FamilyPathOrBuilder> 
+        getFamilyPathOrBuilderList();
+    /**
+     * <code>repeated .hbase.pb.FamilyPath family_path = 2;</code>
+     */
+    org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos.FamilyPathOrBuilder getFamilyPathOrBuilder(
+        int index);
+
+    /**
+     * <code>optional bool assign_seq_num = 3;</code>
+     */
+    boolean hasAssignSeqNum();
+    /**
+     * <code>optional bool assign_seq_num = 3;</code>
+     */
+    boolean getAssignSeqNum();
+
+    /**
+     * <code>optional .hbase.pb.DelegationToken fs_token = 4;</code>
+     */
+    boolean hasFsToken();
+    /**
+     * <code>optional .hbase.pb.DelegationToken fs_token = 4;</code>
+     */
+    org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos.DelegationToken getFsToken();
+    /**
+     * <code>optional .hbase.pb.DelegationToken fs_token = 4;</code>
+     */
+    org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos.DelegationTokenOrBuilder getFsTokenOrBuilder();
+
+    /**
+     * <code>optional string bulk_token = 5;</code>
+     */
+    boolean hasBulkToken();
+    /**
+     * <code>optional string bulk_token = 5;</code>
+     */
+    java.lang.String getBulkToken();
+    /**
+     * <code>optional string bulk_token = 5;</code>
+     */
+    org.apache.hadoop.hbase.shaded.com.google.protobuf.ByteString
+        getBulkTokenBytes();
+
+    /**
+     * <code>optional bool copy_file = 6 [default = false];</code>
+     */
+    boolean hasCopyFile();
+    /**
+     * <code>optional bool copy_file = 6 [default = false];</code>
+     */
+    boolean getCopyFile();
+  }
+  /**
+   * Protobuf type {@code hbase.pb.BulkLoadHFileRequest}
+   */
+  public  static final class BulkLoadHFileRequest extends
+      org.apache.hadoop.hbase.shaded.com.google.protobuf.GeneratedMessageV3 implements
+      // @@protoc_insertion_point(message_implements:hbase.pb.BulkLoadHFileRequest)
+      BulkLoadHFileRequestOrBuilder {
+    // Use BulkLoadHFileRequest.newBuilder() to construct.
+    private BulkLoadHFileRequest(org.apache.hadoop.hbase.shaded.com.google.protobuf.GeneratedMessageV3.Builder<?> builder) {
+      super(builder);
+    }
+    private BulkLoadHFileRequest() {
+      familyPath_ = java.util.Collections.emptyList();
+      assignSeqNum_ = false;
+      bulkToken_ = "";
+      copyFile_ = false;
+    }
+
+    @java.lang.Override
+    public final org.apache.hadoop.hbase.shaded.com.google.protobuf.UnknownFieldSet
+    getUnknownFields() {
+      return this.unknownFields;
+    }
+    private BulkLoadHFileRequest(
+        org.apache.hadoop.hbase.shaded.com.google.protobuf.CodedInputStream input,
+        org.apache.hadoop.hbase.shaded.com.google.protobuf.ExtensionRegistryLite extensionRegistry)
+        throws org.apache.hadoop.hbase.shaded.com.google.protobuf.InvalidProtocolBufferException {
+      this();
+      int mutable_bitField0_ = 0;
+      org.apache.hadoop.hbase.shaded.com.google.protobuf.UnknownFieldSet.Builder unknownFields =
+          org.apache.hadoop.hbase.shaded.com.google.protobuf.UnknownFieldSet.newBuilder();
+      try {
+        boolean done = false;
+        while (!done) {
+          int tag = input.readTag();
+          switch (tag) {
+            case 0:
+              done = true;
+              break;
+            default: {
+              if (!parseUnknownField(input, unknownFields,
+                                     extensionRegistry, tag)) {
+                done = true;
+              }
+              break;
+            }
+            case 10: {
+              org.apache.hadoop.hbase.shaded.protobuf.generated.HBaseProtos.RegionSpecifier.Builder subBuilder = null;
+              if (((bitField0_ & 0x00000001) == 0x00000001)) {
+                subBuilder = region_.toBuilder();
+              }
+              region_ = input.readMessage(org.apache.hadoop.hbase.shaded.protobuf.generated.HBaseProtos.RegionSpecifier.PARSER, extensionRegistry);
+              if (subBuilder != null) {
+                subBuilder.mergeFrom(region_);
+                region_ = subBuilder.buildPartial();
+              }
+              bitField0_ |= 0x00000001;
+              break;
+            }
+            case 18: {
+              if (!((mutable_bitField0_ & 0x00000002) == 0x00000002)) {
+                familyPath_ = new java.util.ArrayList<org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos.FamilyPath>();
+                mutable_bitField0_ |= 0x00000002;
+              }
+              familyPath_.add(
+                  input.readMessage(org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos.FamilyPath.PARSER, extensionRegistry));
+              break;
+            }
+            case 24: {
+              bitField0_ |= 0x00000002;
+              assignSeqNum_ = input.readBool();
+              break;
+            }
+            case 34: {
+              org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos.DelegationToken.Builder subBuilder = null;
+              if (((bitField0_ & 0x00000004) == 0x00000004)) {
+                subBuilder = fsToken_.toBuilder();
+              }
+              fsToken_ = input.readMessage(org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos.DelegationToken.PARSER, extensionRegistry);
+              if (subBuilder != null) {
+                subBuilder.mergeFrom(fsToken_);
+                fsToken_ = subBuilder.buildPartial();
+              }
+              bitField0_ |= 0x00000004;
+              break;
             }
-          }
-          return this;
-        }
-        private int bitField0_;
-
-        private org.apache.hadoop.hbase.shaded.com.google.protobuf.ByteString family_ = org.apache.hadoop.hbase.shaded.com.google.protobuf.ByteString.EMPTY;
-        /**
-         * <code>required bytes family = 1;</code>
-         */
-        public boolean hasFamily() {
-          return ((bitField0_ & 0x00000001) == 0x00000001);
-        }
-        /**
-         * <code>required bytes family = 1;</code>
-         */
-        public org.apache.hadoop.hbase.shaded.com.google.protobuf.ByteString getFamily() {
-          return family_;
-        }
-        /**
-         * <code>required bytes family = 1;</code>
-         */
-        public Builder setFamily(org.apache.hadoop.hbase.shaded.com.google.protobuf.ByteString value) {
-          if (value == null) {
-    throw new NullPointerException();
-  }
-  bitField0_ |= 0x00000001;
-          family_ = value;
-          onChanged();
-          return this;
-        }
-        /**
-         * <code>required bytes family = 1;</code>
-         */
-        public Builder clearFamily() {
-          bitField0_ = (bitField0_ & ~0x00000001);
-          family_ = getDefaultInstance().getFamily();
-          onChanged();
-          return this;
-        }
-
-        private java.lang.Object path_ = "";
-        /**
-         * <code>required string path = 2;</code>
-         */
-        public boolean hasPath() {
-          return ((bitField0_ & 0x00000002) == 0x00000002);
-        }
-        /**
-         * <code>required string path = 2;</code>
-         */
-        public java.lang.String getPath() {
-          java.lang.Object ref = path_;
-          if (!(ref instanceof java.lang.String)) {
-            org.apache.hadoop.hbase.shaded.com.google.protobuf.ByteString bs =
-                (org.apache.hadoop.hbase.shaded.com.google.protobuf.ByteString) ref;
-            java.lang.String s = bs.toStringUtf8();
-            if (bs.isValidUtf8()) {
-              path_ = s;
+            case 42: {
+              org.apache.hadoop.hbase.shaded.com.google.protobuf.ByteString bs = input.readBytes();
+              bitField0_ |= 0x00000008;
+              bulkToken_ = bs;
+              break;
+            }
+            case 48: {
+              bitField0_ |= 0x00000010;
+              copyFile_ = input.readBool();
+              break;
             }
-            return s;
-          } else {
-            return (java.lang.String) ref;
-          }
-        }
-        /**
-         * <code>required string path = 2;</code>
-         */
-        public org.apache.hadoop.hbase.shaded.com.google.protobuf.ByteString
-            getPathBytes() {
-          java.lang.Object ref = path_;
-          if (ref instanceof String) {
-            org.apache.hadoop.hbase.shaded.com.google.protobuf.ByteString b = 
-                org.apache.hadoop.hbase.shaded.com.google.protobuf.ByteString.copyFromUtf8(
-                    (java.lang.String) ref);
-            path_ = b;
-            return b;
-          } else {
-            return (org.apache.hadoop.hbase.shaded.com.google.protobuf.ByteString) ref;
           }
         }
-        /**
-         * <code>required string path = 2;</code>
-         */
-        public Builder setPath(
-            java.lang.String value) {
-          if (value == null) {
-    throw new NullPointerException();
-  }
-  bitField0_ |= 0x00000002;
-          path_ = value;
-          onChanged();
-          return this;
-        }
-        /**
-         * <code>required string path = 2;</code>
-         */
-        public Builder clearPath() {
-          bitField0_ = (bitField0_ & ~0x00000002);
-          path_ = getDefaultInstance().getPath();
-          onChanged();
-          return this;
-        }
-        /**
-         * <code>required string path = 2;</code>
-         */
-        public Builder setPathBytes(
-            org.apache.hadoop.hbase.shaded.com.google.protobuf.ByteString value) {
-          if (value == null) {
-    throw new NullPointerException();
-  }
-  bitField0_ |= 0x00000002;
-          path_ = value;
-          onChanged();
-          return this;
-        }
-        public final Builder setUnknownFields(
-            final org.apache.hadoop.hbase.shaded.com.google.protobuf.UnknownFieldSet unknownFields) {
-          return super.setUnknownFields(unknownFields);
-        }
-
-        public final Builder mergeUnknownFields(
-            final org.apache.hadoop.hbase.shaded.com.google.protobuf.UnknownFieldSet unknownFields) {
-          return super.mergeUnknownFields(unknownFields);
-        }
-
-
-        // @@protoc_insertion_point(builder_scope:hbase.pb.BulkLoadHFileRequest.FamilyPath)
-      }
-
-      // @@protoc_insertion_point(class_scope:hbase.pb.BulkLoadHFileRequest.FamilyPath)
-      private static final org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos.BulkLoadHFileRequest.FamilyPath DEFAULT_INSTANCE;
-      static {
-        DEFAULT_INSTANCE = new org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos.BulkLoadHFileRequest.FamilyPath();
-      }
-
-      public static org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos.BulkLoadHFileRequest.FamilyPath getDefaultInstance() {
-        return DEFAULT_INSTANCE;
-      }
-
-      @java.lang.Deprecated public static final org.apache.hadoop.hbase.shaded.com.google.protobuf.Parser<FamilyPath>
-          PARSER = new org.apache.hadoop.hbase.shaded.com.google.protobuf.AbstractParser<FamilyPath>() {
-        public FamilyPath parsePartialFrom(
-            org.apache.hadoop.hbase.shaded.com.google.protobuf.CodedInputStream input,
-            org.apache.hadoop.hbase.shaded.com.google.protobuf.ExtensionRegistryLite extensionRegistry)
-            throws org.apache.hadoop.hbase.shaded.com.google.protobuf.InvalidProtocolBufferException {
-            return new FamilyPath(input, extensionRegistry);
+      } catch (org.apache.hadoop.hbase.shaded.com.google.protobuf.InvalidProtocolBufferException e) {
+        throw e.setUnfinishedMessage(this);
+      } catch (java.io.IOException e) {
+        throw new org.apache.hadoop.hbase.shaded.com.google.protobuf.InvalidProtocolBufferException(
+            e).setUnfinishedMessage(this);
+      } finally {
+        if (((mutable_bitField0_ & 0x00000002) == 0x00000002)) {
+          familyPath_ = java.util.Collections.unmodifiableList(familyPath_);
         }
-      };
-
-      public static org.apache.hadoop.hbase.shaded.com.google.protobuf.Parser<FamilyPath> parser() {
-        return PARSER;
-      }
-
-      @java.lang.Override
-      public org.apache.hadoop.hbase.shaded.com.google.protobuf.Parser<FamilyPath> getParserForType() {
-        return PARSER;
-      }
-
-      public org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos.BulkLoadHFileRequest.FamilyPath getDefaultInstanceForType() {
-        return DEFAULT_INSTANCE;
+        this.unknownFields = unknownFields.build();
+        makeExtensionsImmutable();
       }
+    }
+    public static final org.apache.hadoop.hbase.shaded.com.google.protobuf.Descriptors.Descriptor
+        getDescriptor() {
+      return org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos.internal_static_hbase_pb_BulkLoadHFileRequest_descriptor;
+    }
 
+    protected org.apache.hadoop.hbase.shaded.com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
+        internalGetFieldAccessorTable() {
+      return org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos.internal_static_hbase_pb_BulkLoadHFileRequest_fieldAccessorTable
+          .ensureFieldAccessorsInitialized(
+              org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos.BulkLoadHFileRequest.class, org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos.BulkLoadHFileRequest.Builder.class);
     }
 
     private int bitField0_;
@@ -22369,36 +22375,36 @@ public final class ClientProtos {
     }
 
     public static final int FAMILY_PATH_FIELD_NUMBER = 2;
-    private java.util.List<org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos.BulkLoadHFileRequest.FamilyPath> familyPath_;
+    private java.util.List<org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos.FamilyPath> familyPath_;
     /**
-     * <code>repeated .hbase.pb.BulkLoadHFileRequest.FamilyPath family_path = 2;</code>
+     * <code>repeated .hbase.pb.FamilyPath family_path = 2;</code>
      */
-    public java.util.List<org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos.BulkLoadHFileRequest.FamilyPath> getFamilyPathList() {
+    public java.util.List<org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos.FamilyPath> getFamilyPathList() {
       return familyPath_;
     }
     /**
-     * <code>repeated .hbase.pb.BulkLoadHFileRequest.FamilyPath family_path = 2;</code>
+     * <code>repeated .hbase.pb.FamilyPath family_path = 2;</code>
      */
-    public java.util.List<? extends org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos.BulkLoadHFileRequest.FamilyPathOrBuilder> 
+    public java.util.List<? extends org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos.FamilyPathOrBuilder> 
         getFamilyPathOrBuilderList() {
       return familyPath_;
     }
     /**
-     * <code>repeated .hbase.pb.BulkLoadHFileRequest.FamilyPath family_path = 2;</code>
+     * <code>repeated .hbase.pb.FamilyPath family_path = 2;</code>
      */
     public int getFamilyPathCount() {
       return familyPath_.size();
     }
     /**
-     * <code>repeated .hbase.pb.BulkLoadHFileRequest.FamilyPath family_path = 2;</code>
+     * <code>repeated .hbase.pb.FamilyPath family_path = 2;</code>
      */
-    public org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos.BulkLoadHFileRequest.FamilyPath getFamilyPath(int index) {
+    public org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos.FamilyPath getFamilyPath(int index) {
       return familyPath_.get(index);
     }
     /**
-     * <code>repeated .hbase.pb.BulkLoadHFileRequest.FamilyPath family_path = 2;</code>
+     * <code>repeated .hbase.pb.FamilyPath family_path = 2;</code>
      */
-    public org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos.BulkLoadHFileRequest.FamilyPathOrBuilder getFamilyPathOrBuilder(
+    public org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos.FamilyPathOrBuilder getFamilyPathOrBuilder(
         int index) {
       return familyPath_.get(index);
     }
@@ -22735,12 +22741,6 @@ public final class ClientProtos {
       return builder;
     }
     /**
-     * <pre>
-     **
-     * Atomically bulk load multiple HFiles (say from different column families)
-     * into an open region.
-     * </pre>
-     *
      * Protobuf type {@code hbase.pb.BulkLoadHFileRequest}
      */
     public static final class Builder extends
@@ -23106,22 +23106,22 @@ public final class ClientProtos {
         return regionBuilder_;
       }
 
-      private java.util.List<org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos.BulkLoadHFileRequest.FamilyPath> familyPath_ =
+      private java.util.List<org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos.FamilyPath> familyPath_ =
         java.util.Collections.emptyList();
       private void ensureFamilyPathIsMutable() {
         if (!((bitField0_ & 0x00000002) == 0x00000002)) {
-          familyPath_ = new java.util.ArrayList<org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos.BulkLoadHFileRequest.FamilyPath>(familyPath_);
+          familyPath_ = new java.util.ArrayList<org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos.FamilyPath>(familyPath_);
           bitField0_ |= 0x00000002;
          }
       }
 
       private org.apache.hadoop.hbase.shaded.com.google.protobuf.RepeatedFieldBuilderV3<
-          org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos.BulkLoadHFileRequest.FamilyPath, org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos.BulkLoadHFileRequest.FamilyPath.Builder, org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos.BulkLoadHFileRequest.FamilyPathOrBuilder> familyPathBuilder_;
+          org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos.FamilyPath, org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos.FamilyPath.Builder, org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos.FamilyPathOrBuilder> familyPathBuilder_;
 
       /**
-       * <code>repeated .hbase.pb.BulkLoadHFileRequest.FamilyPath family_path = 2;</code>
+       * <code>repeated .hbase.pb.FamilyPath family_path = 2;</code>
        */
-      public java.util.List<org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos.BulkLoadHFileRequest.FamilyPath> getFamilyPathList() {
+      public java.util.List<org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos.FamilyPath> getFamilyPathList() {
         if (familyPathBuilder_ == null) {
           return java.util.Collections.unmodifiableList(familyPath_);
         } else {
@@ -23129,7 +23129,7 @@ public final class ClientProtos {
         }
       }
       /**
-       * <code>repeated .hbase.pb.BulkLoadHFileRequest.FamilyPath family_path = 2;</code>
+       * <code>repeated .hbase.pb.FamilyPath family_path = 2;</code>
        */
       public int getFamilyPathCount() {
         if (familyPathBuilder_ == null) {
@@ -23139,9 +23139,9 @@ public final class ClientProtos {
         }
       }
       /**
-       * <code>repeated .hbase.pb.BulkLoadHFileRequest.FamilyPath family_path = 2;</code>
+       * <code>repeated .hbase.pb.FamilyPath family_path = 2;</code>
        */
-      public org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos.BulkLoadHFileRequest.FamilyPath getFamilyPath(int index) {
+      public org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos.FamilyPath getFamilyPath(int index) {
         if (familyPathBuilder_ == null) {
           return familyPath_.get(index);
         } else {
@@ -23149,10 +23149,10 @@ public final class ClientProtos {
         }
       }
       /**
-       * <code>repeated .hbase.pb.BulkLoadHFileRequest.FamilyPath family_path = 2;</code>
+       * <code>repeated .hbase.pb.FamilyPath family_path = 2;</code>
        */
       public Builder setFamilyPath(
-          int index, org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos.BulkLoadHFileRequest.FamilyPath value) {
+          int index, org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos.FamilyPath value) {
         if (familyPathBuilder_ == null) {
           if (value == null) {
             throw new NullPointerException();
@@ -23166,10 +23166,10 @@ public final class ClientProtos {
         return this;
       }
       /**
-       * <code>repeated .hbase.pb.BulkLoadHFileRequest.FamilyPath family_path = 2;</code>
+       * <code>repeated .hbase.pb.FamilyPath family_path = 2;</code>
        */
       public Builder setFamilyPath(
-          int index, org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos.BulkLoadHFileRequest.FamilyPath.Builder builderForValue) {
+          int index, org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos.FamilyPath.Builder builderForValue) {
         if (familyPathBuilder_ == null) {
           ensureFamilyPathIsMutable();
           familyPath_.set(index, builderForValue.build());
@@ -23180,9 +23180,9 @@ public final class ClientProtos {
         return this;
       }
       /**
-       * <code>repeated .hbase.pb.BulkLoadHFileRequest.FamilyPath family_path = 2;</code>
+       * <code>repeated .hbase.pb.FamilyPath family_path = 2;</code>
        */
-      public Builder addFamilyPath(org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos.BulkLoadHFileRequest.FamilyPath value) {
+      public Builder addFamilyPath(org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos.FamilyPath value) {
         if (familyPathBuilder_ == null) {
           if (value == null) {
             throw new NullPointerException();
@@ -23196,10 +23196,10 @@ public final class ClientProtos {
         return this;
       }
       /**
-       * <code>repeated .hbase.pb.BulkLoadHFileRequest.FamilyPath family_path = 2;</code>
+       * <code>repeated .hbase.pb.FamilyPath family_path = 2;</code>
        */
       public Builder addFamilyPath(
-          int index, org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos.BulkLoadHFileRequest.FamilyPath value) {
+          int index, org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos.FamilyPath value) {
         if (familyPathBuilder_ == null) {
           if (value == null) {
             throw new NullPointerException();
@@ -23213,10 +23213,10 @@ public final class ClientProtos {
         return this;
       }
       /**
-       * <code>repeated .hbase.pb.BulkLoadHFileRequest.FamilyPath family_path = 2;</code>
+       * <code>repeated .hbase.pb.FamilyPath family_path = 2;</code>
        */
       public Builder addFamilyPath(
-          org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos.BulkLoadHFileRequest.FamilyPath.Builder builderForValue) {
+          org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos.FamilyPath.Builder builderForValue) {
         if (familyPathBuilder_ == null) {
           ensureFamilyPathIsMutable();
           familyPath_.add(builderForValue.build());
@@ -23227,10 +23227,10 @@ public final class ClientProtos {
         return this;
       }
       /**
-       * <code>repeated .hbase.pb.BulkLoadHFileRequest.FamilyPath family_path = 2;</code>
+       * <code>repeated .hbase.pb.FamilyPath family_path = 2;</code>
        */
       public Builder addFamilyPath(
-          int index, org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos.BulkLoadHFileRequest.FamilyPath.Builder builderForValue) {
+          int index, org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos.FamilyPath.Builder builderForValue) {
         if (familyPathBuilder_ == null) {
           ensureFamilyPathIsMutable();
           familyPath_.add(index, builderForValue.build());
@@ -23241,10 +23241,10 @@ public final class ClientProtos {
         return this;
       }
       /**
-       * <code>repeated .hbase.pb.BulkLoadHFileRequest.FamilyPath family_path = 2;</code>
+       * <code>repeated .hbase.pb.FamilyPath family_path = 2;</code>
        */
       public Builder addAllFamilyPath(
-          java.lang.Iterable<? extends org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos.BulkLoadHFileRequest.FamilyPath> values) {
+          java.lang.Iterable<? extends org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos.FamilyPath> values) {
         if (familyPathBuilder_ == null) {
           ensureFamilyPathIsMutable();
           org.apache.hadoop.hbase.shaded.com.google.protobuf.AbstractMessageLite.Builder.addAll(
@@ -23256,7 +23256,7 @@ public final class ClientProtos {
         return this;
       }
       /**
-       * <code>repeated .hbase.pb.BulkLoadHFileRequest.FamilyPath family_path = 2;</code>
+       * <code>repeated .hbase.pb.FamilyPath family_path = 2;</code>
        */
       public Builder clearFamilyPath() {
         if (familyPathBuilder_ == null) {
@@ -23269,7 +23269,7 @@ public final class ClientProtos {
         return this;
       }
       /**
-       * <code>repeated .hbase.pb.BulkLoadHFileRequest.FamilyPath family_path = 2;</code>
+       * <code>repeated .hbase.pb.FamilyPath family_path = 2;</code>
        */
       public Builder removeFamilyPath(int index) {
         if (familyPathBuilder_ == null) {
@@ -23282,16 +23282,16 @@ public final class ClientProtos {
         return this;
       }
       /**
-       * <code>repeated .hbase.pb.BulkLoadHFileRequest.FamilyPath family_path = 2;</code>
+       * <code>repeated .hbase.pb.FamilyPath family_path = 2;</code>
        */
-      public org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos.BulkLoadHFileRequest.FamilyPath.Builder getFamilyPathBuilder(
+      public org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos.FamilyPath.Builder getFamilyPathBuilder(
           int index) {
         return getFamilyPathFieldBuilder().getBuilder(index);
       }
       /**
-       * <code>repeated .hbase.pb.BulkLoadHFileRequest.FamilyPath family_path = 2;</code>
+       * <code>repeated .hbase.pb.FamilyPath family_path = 2;</code>
        */
-      public org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos.BulkLoadHFileRequest.FamilyPathOrBuilder getFamilyPathOrBuilder(
+      public org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos.FamilyPathOrBuilder getFamilyPathOrBuilder(
           int index) {
         if (familyPathBuilder_ == null) {
           return familyPath_.get(index);  } else {
@@ -23299,9 +23299,9 @@ public final class ClientProtos {
         }
       }
       /**
-       * <code>repeated .hbase.pb.BulkLoadHFileRequest.FamilyPath family_path = 2;</code>
+       * <code>repeated .hbase.pb.FamilyPath family_path = 2;</code>
        */
-      public java.util.List<? extends org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos.BulkLoadHFileRequest.FamilyPathOrBuilder> 
+      public java.util.List<? extends org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos.FamilyPathOrBuilder> 
            getFamilyPathOrBuilderList() {
         if (familyPathBuilder_ != null) {
           return familyPathBuilder_.getMessageOrBuilderList();
@@ -23310,33 +23310,33 @@ public final class ClientProtos {
         }
       }
       /**
-       * <code>repeated .hbase.pb.BulkLoadHFileRequest.FamilyPath family_path = 2;</code>
+       * <code>repeated .hbase.pb.FamilyPath family_path = 2;</code>
        */
-      public org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos.BulkLoadHFileRequest.FamilyPath.Builder addFamilyPathBuilder() {
+      public org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos.FamilyPath.Builder addFamilyPathBuilder() {
         return getFamilyPathFieldBuilder().addBuilder(
-            org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos.BulkLoadHFileRequest.FamilyPath.getDefaultInstance());
+            org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos.FamilyPath.getDefaultInstance());
       }
       /**
-       * <code>repeated .hbase.pb.BulkLoadHFileRequest.FamilyPath family_path = 2;</code>
+       * <code>repeated .hbase.pb.FamilyPath family_path = 2;</code>
        */
-      public org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos.BulkLoadHFileRequest.FamilyPath.Builder addFamilyPathBuilder(
+      public org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos.FamilyPath.Builder addFamilyPathBuilder(
           int index) {
         return getFamilyPathFieldBuilder().addBuilder(
-            index, org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos.BulkLoadHFileRequest.FamilyPath.getDefaultInstance());
+            index, org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos.FamilyPath.getDefaultInstance());
       }
       /**
-       * <code>repeated .hbase.pb.BulkLoadHFileRequest.FamilyPath family_path = 2;</code>
+       * <code>repeated .hbase.pb.FamilyPath family_path = 2;</code>
        */
-      public java.util.List<org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos.BulkLoadHFileRequest.FamilyPath.Builder> 
+      public java.util.List<org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos.FamilyPath.Builder> 
            getFamilyPathBuilderList() {
         return getFamilyPathFieldBuilder().getBuilderList();
       }
       private org.apache.hadoop.hbase.shaded.com.google.protobuf.RepeatedFieldBuilderV3<
-          org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos.BulkLoadHFileRequest.FamilyPath, org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos.BulkLoadHFileRequest.FamilyPath.Builder, org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos.BulkLoadHFileRequest.FamilyPathOrBuilder> 
+          org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos.FamilyPath, org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos.FamilyPath.Builder, org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos.FamilyPathOrBuilder> 
           getFamilyPathFieldBuilder() {
         if (familyPathBuilder_ == null) {
           familyPathBuilder_ = new org.apache.hadoop.hbase.shaded.com.google.protobuf.RepeatedFieldBuilderV3<
-              org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos.BulkLoadHFileRequest.FamilyPath, org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos.BulkLoadHFileRequest.FamilyPath.Builder, org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos.BulkLoadHFileRequest.FamilyPathOrBuilder>(
+              org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos.FamilyPath, org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos.FamilyPath.Builder, org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos.FamilyPathOrBuilder>(
                   familyPath_,
                   ((bitField0_ & 0x00000002) == 0x00000002),
                   getParentForChildren(),
@@ -23664,6 +23664,30 @@ public final class ClientProtos {
      * <code>required bool loaded = 1;</code>
      */
     boolean getLoaded();
+
+    /**
+     * <code>repeated .hbase.pb.FamilyPath family_path = 2;</code>
+     */
+    java.util.List<org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos.FamilyPath> 
+        getFamilyPathList();
+    /**
+     * <code>repeated .hbase.pb.FamilyPath family_path = 2;</code>
+     */
+    org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos.FamilyPath getFamilyPath(int index);
+    /**
+     * <code>repeated .hbase.pb.FamilyPath family_path = 2;</code>
+     */
+    int getFamilyPathCount();
+    /**
+     * <code>repeated .hbase.pb.FamilyPath family_path = 2;</code>
+     */
+    java.util.List<? extends org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos.FamilyPathOrBuilder> 
+        getFamilyPathOrBuilderList();
+    /**
+     * <code>repeated .hbase.pb.FamilyPath family_path = 2;</code>
+     */
+    org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos.FamilyPathOrBuilder getFamilyPathOrBuilder(
+        int index);
   }
   /**
    * Protobuf type {@code hbase.pb.BulkLoadHFileResponse}
@@ -23678,6 +23702,7 @@ public final class ClientProtos {
     }
     private BulkLoadHFileResponse() {
       loaded_ = false;
+      familyPath_ = java.util.Collections.emptyList();
     }
 
     @java.lang.Override
@@ -23713,6 +23738,15 @@ public final class ClientProtos {
               loaded_ = input.readBool();
               break;
             }
+            case 18: {
+              if (!((mutable_bitField0_ & 0x00000002) == 0x00000002)) {
+                familyPath_ = new java.util.ArrayList<org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos.FamilyPath>();
+                mutable_bitField0_ |= 0x00000002;
+              }
+              familyPath_.add(
+                  input.readMessage(org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos.FamilyPath.PARSER, extensionRegistry));
+              break;
+            }
           }
         }
       } catch (org.apache.hadoop.hbase.shaded.com.google.protobuf.InvalidProtocolBufferException e) {
@@ -23721,6 +23755,9 @@ public final class ClientProtos {
         throw new org.apache.hadoop.hbase.shaded.com.google.protobuf.InvalidProtocolBufferException(
             e).setUnfinishedMessage(this);
       } finally {
+        if (((mutable_bitField0_ & 0x00000002) == 0x00000002)) {
+          familyPath_ = java.util.Collections.unmodifiableList(familyPath_);
+        }
         this.unknownFields = unknownFields.build();
         makeExtensionsImmutable();
       }
@@ -23753,6 +23790,41 @@ public final class ClientProtos {
       return loaded_;
     }
 
+    public static final int FAMILY_PATH_FIELD_NUMBER = 2;
+    private java.util.List<org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos.FamilyPath> familyPath_;
+    /**
+     * <code>repeated .hbase.pb.FamilyPath family_path = 2;</code>
+     */
+    public java.util.List<org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos.FamilyPath> getFamilyPathList() {
+      return familyPath_;
+    }
+    /**
+     * <code>repeated .hbase.pb.FamilyPath family_path = 2;</code>
+     */
+    public java.util.List<? extends org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos.FamilyPathOrBuilder> 
+        getFamilyPathOrBuilderList() {
+      return familyPath_;
+    }
+    /**
+     * <code>repeated .hbase.pb.FamilyPath family_path = 2;</code>
+     */
+    public int getFamilyPathCount() {
+      return familyPath_.size();
+    }
+    /**
+     * <code>repeated .hbase.pb.FamilyPath family_path = 2;</code>
+     */
+    public org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos.FamilyPath getFamilyPath(int index) {
+      return familyPath_.get(index);
+    }
+    /**
+     * <code>repeated .hbase.pb.FamilyPath family_path = 2;</code>
+     */
+    public org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos.FamilyPathOrBuilder getFamilyPathOrBuilder(
+        int index) {
+      return familyPath_.get(index);
+    }
+
     private byte memoizedIsInitialized = -1;
     public final boolean isInitialized() {
       byte isInitialized = memoizedIsInitialized;
@@ -23763,6 +23835,12 @@ public final class ClientProtos {
         memoizedIsInitialized = 0;
         return false;
       }
+      for (int i = 0; i < getFamilyPathCount(); i++) {
+        if (!getFamilyPath(i).isInitialized()) {
+          memoizedIsInitialized = 0;
+          return false;
+        }
+      }
       memoizedIsInitialized = 1;
       return true;
     }
@@ -23772,6 +23850,9 @@ public final class ClientProtos {
       if (((bitField0_ & 0x00000001) == 0x00000001)) {
         output.writeBool(1, loaded_);
       }
+      for (int i = 0; i < familyPath_.size(); i++) {
+        output.writeMessage(2, familyPath_.get(i));
+      }
       unknownFields.writeTo(output);
     }
 
@@ -23784,6 +23865,10 @@ public final class ClientProtos {
         size += org.apache.hadoop.hbase.shaded.com.google.protobuf.CodedOutputStream
           .computeBoolSize(1, loaded_);
       }
+      for (int i = 0; i < familyPath_.size(); i++) {
+        size += org.apache.hadoop.hbase.shaded.com.google.protobuf.CodedOutputStream
+          .computeMessageSize(2, familyPath_.get(i));
+      }
       size += unknownFields.getSerializedSize();
       memoizedSize = size;
       return size;
@@ -23806,6 +23891,8 @@ public final class ClientProtos {
         result = result && (getLoaded()
             == other.getLoaded());
       }
+      result = result && getFamilyPathList()
+          .equals(other.getFamilyPathList());
       result = result && unknownFields.equals(other.unknownFields);
       return result;
     }
@@ -23822,6 +23909,10 @@ public final class ClientProtos {
         hash = (53 * hash) + org.apache.hadoop.hbase.shaded.com.google.protobuf.Internal.hashBoolean(
             getLoaded());
       }
+      if (getFamilyPathCount() > 0) {
+        hash = (37 * hash) + FAMILY_PATH_FIELD_NUMBER;
+        hash = (53 * hash) + getFamilyPathList().hashCode();
+      }
       hash = (29 * hash) + unknownFields.hashCode();
       memoizedHashCode = hash;
       return hash;
@@ -23936,12 +24027,19 @@ public final class ClientProtos {
       private void maybeForceBuilderInitialization() {
         if (org.apache.hadoop.hbase.shaded.com.google.protobuf.GeneratedMessageV3
                 .alwaysUseFieldBuilders) {
+          getFamilyPathFieldBuilder();
         }
       }
       public Builder clear() {
         super.clear();
         loaded_ = false;
         bitField0_ = (bitField0_ & ~0x00000001);
+        if (familyPathBuilder_ == null) {
+          familyPath_ = java.util.Collections.emptyList();
+          bitField0_ = (bitField0_ & ~0x00000002);
+        } else {
+          familyPathBuilder_.clear();
+        }
         return this;
       }
 
@@ -23970,6 +24068,15 @@ public final class ClientProtos {
           to_bitField0_ |= 0x00000001;
         }
         result.loaded_ = loaded_;
+        if (familyPathBuilder_ == null) {
+          if (((bitField0_ & 0x00000002) == 0x00000002)) {
+            familyPath_ = java.util.Collections.unmodifiableList(familyPath_);
+            bitField0_ = (bitField0_ & ~0x00000002);
+          }
+          result.familyPath_ = familyPath_;
+        } else {
+          result.familyPath_ = familyPathBuilder_.build();
+        }
         result.bitField0_ = to_bitField0_;
         onBuilt();
         return result;
@@ -24015,6 +24122,32 @@ public final class ClientProtos {
         if (other.hasLoaded()) {
           setLoaded(other.getLoaded());
         }
+        if (familyPathBuilder_ == null) {
+          if (!other.familyPath_.isEmpty()) {
+            if (familyPath_.isEmpty()) {
+              familyPath_ = other.familyPath_;
+              bitField0_ = (bitField0_ & ~0x00000002);
+            } else {
+              ensureFamilyPathIsMutable();
+              familyPath_.addAll(other.familyPath_);
+            }
+            onChanged();
+          }
+        } else {
+          if (!other.familyPath_.isEmpty()) {
+            if (familyPathBuilder_.isEmpty()) {
+              familyPathBuilder_.dispose();
+              familyPathBuilder_ = null;
+              familyPath_ = other.familyPath_;
+              bitField0_ = (bitField0_ & ~0x00000002);
+              familyPathBuilder_ = 
+                org.apache.hadoop.hbase.shaded.com.google.protobuf.GeneratedMessageV3.alwaysUseFieldBuilders ?
+                   getFamilyPathFieldBuilder() : null;
+            } else {
+              familyPathBuilder_.addAllMessages(other.familyPath_);
+            }
+          }
+        }
         this.mergeUnknownFields(other.unknownFields);
         onChanged();
         return this;
@@ -24024,6 +24157,11 @@ public final class ClientProtos {
         if (!hasLoaded()) {
           return false;
         }
+        for (int i = 0; i < getFamilyPathCount(); i++) {
+          if (!getFamilyPath(i).isInitialized()) {
+            return false;
+          }
+        }
         return true;
       }
 
@@ -24077,6 +24215,246 @@ public final class ClientProtos {
         onChanged();
         return this;
       }
+
+      private java.util.List<org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos.FamilyPath> familyPath_ =
+        java.util.Collections.emptyList();
+      private void ensureFamilyPathIsMutable() {
+        if (!((bitField0_ & 0x00000002) == 0x00000002)) {
+          familyPath_ = new java.util.ArrayList<org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos.FamilyPath>(familyPath_);
+          bitField0_ |= 0x00000002;
+         }
+      }
+
+      private org.apache.hadoop.hbase.shaded.com.google.protobuf.RepeatedFieldBuilderV3<
+          org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos.FamilyPath, org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos.FamilyPath.Builder, org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos.FamilyPathOrBuilder> familyPathBuilder_;
+
+      /**
+       * <code>repeated .hbase.pb.FamilyPath family_path = 2;</code>
+       */
+      public java.util.List<org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos.FamilyPath> getFamilyPathList() {
+        if (familyPathBuilder_ == null) {
+          return java.util.Collections.unmodifiableList(familyPath_);
+        } else {
+          return familyPathBuilder_.getMessageList();
+        }
+      }
+      /**
+       * <code>repeated .hbase.pb.FamilyPath family_path = 2;</code>
+       */
+      public int getFamilyPathCount() {
+        if (familyPathBuilder_ == null) {
+          return familyPath_.size();
+        } else {
+          return familyPathBuilder_.getCount();
+        }
+      }
+      /**
+       * <code>repeated .hbase.pb.FamilyPath family_path = 2;</code>
+       */
+      public org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos.FamilyPath getFamilyPath(int index) {
+        if (familyPathBuilder_ == null) {
+          return familyPath_.get(index);
+        } else {
+          return familyPathBuilder_.getMessage(index);
+        }
+      }
+      /**
+       * <code>repeated .hbase.pb.FamilyPath family_path = 2;</code>
+       */
+      public Builder setFamilyPath(
+          int index, org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos.FamilyPath value) {
+        if (familyPathBuilder_ == null) {
+          if (value == null) {
+            throw new NullPointerException();
+          }
+          ensureFamilyPathIsMutable();
+          familyPath_.set(index, value);
+          onChanged();
+        } else {
+          familyPathBuilder_.setMessage(index, value);
+        }
+        return this;
+      }
+      /**
+       * <code>repeated .hbase.pb.FamilyPath family_path = 2;</code>
+       */
+      public Builder setFamilyPath(
+          int index, org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos.FamilyPath.Builder builderForValue) {
+        if (familyPathBuilder_ == null) {
+          ensureFamilyPathIsMutable();
+          familyPath_.set(index, builderForValue.build());
+          onChanged();
+        } else {
+          familyPathBuilder_.setMessage(index, builderForValue.build());
+        }
+        return this;
+      }
+      /**
+       * <code>repeated .hbase.pb.FamilyPath family_path = 2;</code>
+       */
+      public Builder addFamilyPath(org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos.FamilyPath value) {
+        if (familyPathBuilder_ == null) {
+          if (value == null) {
+            throw new NullPointerException();
+          }
+          ensureFamilyPathIsMutable();
+          familyPath_.add(value);
+          onChanged();
+        } else {
+          familyPathBuilder_.addMessage(value);
+        }
+        return this;
+      }
+      /**
+       * <code>repeated .hbase.pb.FamilyPath family_path = 2;</code>
+       */
+      public Builder addFamilyPath(
+          int index, org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos.FamilyPath value) {
+        if (familyPathBuilder_ == null) {
+          if (value == null) {
+            throw new NullPointerException();
+          }
+          ensureFamilyPathIsMutable();
+          familyPath_.add(index, value);
+          onChanged();
+        } else {
+          familyPathBuilder_.addMessage(index, value);
+        }
+        return this;
+      }
+      /**
+       * <code>repeated .hbase.pb.FamilyPath family_path = 2;</code>
+       */
+      public Builder addFamilyPath(
+          org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos.FamilyPath.Builder builderForValue) {
+        if (familyPathBuilder_ == null) {
+          ensureFamilyPathIsMutable();
+          familyPath_.add(builderForValue.build());
+          onChanged();
+        } else {
+          familyPathBuilder_.addMessage(builderForValue.build());
+        }
+        return this;
+      }
+      /**
+       * <code>repeated .hbase.pb.FamilyPath family_path = 2;</code>
+       */
+      public Builder addFamilyPath(
+          int index, org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos.FamilyPath.Builder builderForValue) {
+        if (familyPathBuilder_ == null) {
+          ensureFamilyPathIsMutable();
+          familyPath_.add(index, builderForValue.build());
+          onChanged();
+        } else {
+          familyPathBuilder_.addMessage(index, builderForValue.build());
+        }
+        return this;
+      }
+      /**
+       * <code>repeated .hbase.pb.FamilyPath family_path = 2;</code>
+       */
+      public Builder addAllFamilyPath(
+          java.lang.Iterable<? extends org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos.FamilyPath> values) {
+        if (familyPathBuilder_ == null) {
+          ensureFamilyPathIsMutable();
+          org.apache.hadoop.hbase.shaded.com.google.protobuf.AbstractMessageLite.Builder.addAll(
+              values, familyPath_);
+          onChanged();
+        } else {
+          familyPathBuilder_.addAllMessages(values);
+        }
+        return this;
+      }
+      /**
+       * <code>repeated .hbase.pb.FamilyPath family_path = 2;</code>
+       */
+      public Builder clearFamilyPath() {
+        if (familyPathBuilder_ == null) {
+          familyPath_ = java.util.Collections.emptyList();
+          bitField0_ = (bitField0_ & ~0x00000002);
+          onChanged();
+        } else {
+          familyPathBuilder_.clear();
+        }
+        return this;
+      }
+      /**
+       * <code>repeated .hbase.pb.FamilyPath family_path = 2;</code>
+       */
+      public Builder removeFamilyPath(int index) {
+        if (familyPathBuilder_ == null) {
+          ensureFamilyPathIsMutable();
+          familyPath_.remove(index);
+          onChanged();
+        } else {
+          familyPathBuilder_.remove(index);
+        }
+        return this;
+      }
+      /**
+       * <code>repeated .hbase.pb.FamilyPath family_path = 2;</code>
+       */
+      public org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos.FamilyPath.Builder getFamilyPathBuilder(
+          int index) {
+        return getFamilyPathFieldBuilder().getBuilder(index);
+      }
+      /**
+       * <code>repeated .hbase.pb.FamilyPath family_path = 2;</code>
+       */
+      public org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos.FamilyPathOrBuilder getFamilyPathOrBuilder(
+          int index) {
+        if (familyPathBuilder_ == null) {
+          return familyPath_.get(index);  } else {
+          return familyPathBuilder_.getMessageOrBuilder(index);
+        }
+      }
+      /**
+       * <code>repeated .hbase.pb.FamilyPath family_path = 2;</code>
+       */
+      public java.util.List<? extends org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos.FamilyPathOrBuilder> 
+           getFamilyPathOrBuilderList() {
+        if (familyPathBuilder_ != null) {
+          return familyPathBuilder_.getMessageOrBuilderList();
+        } else {
+          return java.util.Collections.unmodifiableList(familyPath_);
+        }
+      }
+      /**
+       * <code>repeated .hbase.pb.FamilyPath family_path = 2;</code>
+       */
+      public org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos.FamilyPath.Builder addFamilyPathBuilder() {
+        return getFamilyPathFieldBuilder().addBuilder(
+            org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos.FamilyPath.getDefaultInstance());
+      }
+      /**
+       * <code>repeated .hbase.pb.FamilyPath family_path = 2;</code>
+       */
+      public org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos.FamilyPath.Builder addFamilyPathBuilder(
+          int index) {
+        return getFamilyPathFieldBuilder().addBuilder(
+            index, org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos.FamilyPath.getDefaultInstance());
+      }
+      /**
+       * <code>repeated .hbase.pb.FamilyPath family_path = 2;</code>
+       */
+      public java.util.List<org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos.FamilyPath.Builder> 
+           getFamilyPathBuilderList() {
+        return getFamilyPathFieldBuilder().getBuilderList();
+      }
+      private org.apache.hadoop.hbase.shaded.com.google.protobuf.RepeatedFieldBuilderV3<
+          org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos.FamilyPath, org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos.FamilyPath.Builder, org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos.FamilyPathOrBuilder> 
+          getFamilyPathFieldBuilder() {
+        if (familyPathBuilder_ == null) {
+          familyPathBuilder_ = new org.apache.hadoop.hbase.shaded.com.google.protobuf.RepeatedFieldBuilderV3<
+              org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos.FamilyPath, org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos.FamilyPath.Builder, org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos.FamilyPathOrBuilder>(
+                  familyPath_,
+                  ((bitField0_ & 0x00000002) == 0x00000002),
+                  getParentForChildren(),
+                  isClean());
+          familyPath_ = null;
+        }
+        return familyPathBuilder_;
+      }
       public final Builder setUnknownFields(
           final org.apache.hadoop.hbase.shaded.com.google.protobuf.UnknownFieldSet unknownFields) {
         return super.setUnknownFields(unknownFields);
@@ -40271,15 +40649,15 @@ public final class ClientProtos {
     org.apache.hadoop.hbase.shaded.com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
       internal_static_hbase_pb_ScanResponse_fieldAccessorTable;
   private static final org.apache.hadoop.hbase.shaded.com.google.protobuf.Descriptors.Descriptor
-    internal_static_hbase_pb_BulkLoadHFileRequest_descriptor;
+    internal_static_hbase_pb_FamilyPath_descriptor;
   private static final 
     org.apache.hadoop.hbase.shaded.com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
-      internal_static_hbase_pb_BulkLoadHFileRequest_fieldAccessorTable;
+      internal_static_hbase_pb_FamilyPath_fieldAccessorTable;
   private static final org.apache.hadoop.hbase.shaded.com.google.protobuf.Descriptors.Descriptor
-    internal_static_hbase_pb_BulkLoadHFileRequest_FamilyPath_descriptor;
+    internal_static_hbase_pb_BulkLoadHFileRequest_descriptor;
   private static final 
     org.apache.hadoop.hbase.shaded.com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
-      internal_static_hbase_pb_BulkLoadHFileRequest_FamilyPath_fieldAccessorTable;
+      internal_static_hbase_pb_BulkLoadHFileRequest_fieldAccessorTable;
   private static final org.apache.hadoop.hbase.shaded.com.google.protobuf.Descriptors.Descriptor
     internal_static_hbase_pb_BulkLoadHFileResponse_descriptor;
   private static final 
@@ -40463,81 +40841,82 @@ public final class ClientProtos {
       "_per_result\030\007 \003(\010\022\036\n\026more_results_in_reg" +
       "ion\030\010 \001(\010\022\031\n\021heartbeat_message\030\t \001(\010\022+\n\014" +
       "scan_metrics\030\n \001(\0132\025.hbase.pb.ScanMetric" +
-      "s\"\240\002\n\024BulkLoadHFileRequest\022)\n\006region\030\001 \002" +
-      "(\0132\031.hbase.pb.RegionSpecifier\022>\n\013family_" +
-      "path\030\002 \003(\0132).hbase.pb.BulkLoadHFileReque" +
-      "st.FamilyPath\022\026\n\016assign_seq_num\030\003 \001(\010\022+\n" +
-      "\010fs_token\030\004 \001(\0132\031.hbase.pb.DelegationTok" +
-      "en\022\022\n\nbulk_token\030\005 \001(\t\022\030\n\tcopy_file\030\006 \001(",
-      "\010:\005false\032*\n\nFamilyPath\022\016\n\006family\030\001 \002(\014\022\014" +
-      "\n\004path\030\002 \002(\t\"\'\n\025BulkLoadHFileResponse\022\016\n" +
-      "\006loaded\030\001 \002(\010\"V\n\017DelegationToken\022\022\n\niden" +
-      "tifier\030\001 \001(\014\022\020\n\010password\030\002 \001(\014\022\014\n\004kind\030\003" +
-      " \001(\t\022\017\n\007service\030\004 \001(\t\"l\n\026PrepareBulkLoad" +
-      "Request\022\'\n\ntable_name\030\001 \002(\0132\023.hbase.pb.T" +
-      "ableName\022)\n\006region\030\002 \001(\0132\031.hbase.pb.Regi" +
-      "onSpecifier\"-\n\027PrepareBulkLoadResponse\022\022" +
-      "\n\nbulk_token\030\001 \002(\t\"W\n\026CleanupBulkLoadReq" +
-      "uest\022\022\n\nbulk_token\030\001 \002(\t\022)\n\006region\030\002 \001(\013",
-      "2\031.hbase.pb.RegionSpecifier\"\031\n\027CleanupBu" +
-      "lkLoadResponse\"a\n\026CoprocessorServiceCall" +
-      "\022\013\n\003row\030\001 \002(\014\022\024\n\014service_name\030\002 \002(\t\022\023\n\013m" +
-      "ethod_name\030\003 \002(\t\022\017\n\007request\030\004 \002(\014\"B\n\030Cop" +
-      "rocessorServiceResult\022&\n\005value\030\001 \001(\0132\027.h" +
-      "base.pb.NameBytesPair\"v\n\031CoprocessorServ" +
-      "iceRequest\022)\n\006region\030\001 \002(\0132\031.hbase.pb.Re" +
-      "gionSpecifier\022.\n\004call\030\002 \002(\0132 .hbase.pb.C" +
-      "oprocessorServiceCall\"o\n\032CoprocessorServ" +
-      "iceResponse\022)\n\006region\030\001 \002(\0132\031.hbase.pb.R",
-      "egionSpecifier\022&\n\005value\030\002 \002(\0132\027.hbase.pb" +
-      ".NameBytesPair\"\226\001\n\006Action\022\r\n\005index\030\001 \001(\r" +
-      "\022)\n\010mutation\030\002 \001(\0132\027.hbase.pb.MutationPr" +
-      "oto\022\032\n\003get\030\003 \001(\0132\r.hbase.pb.Get\0226\n\014servi" +
-      "ce_call\030\004 \001(\0132 .hbase.pb.CoprocessorServ" +
-      "iceCall\"k\n\014RegionAction\022)\n\006region\030\001 \002(\0132" +
-      "\031.hbase.pb.RegionSpecifier\022\016\n\006atomic\030\002 \001" +
-      "(\010\022 \n\006action\030\003 \003(\0132\020.hbase.pb.Action\"c\n\017" +
-      "RegionLoadStats\022\027\n\014memstoreLoad\030\001 \001(\005:\0010" +
-      "\022\030\n\rheapOccupancy\030\002 \001(\005:\0010\022\035\n\022compaction",
-      "Pressure\030\003 \001(\005:\0010\"j\n\024MultiRegionLoadStat" +
-      "s\022)\n\006region\030\001 \003(\0132\031.hbase.pb.RegionSpeci" +
-      "fier\022\'\n\004stat\030\002 \003(\0132\031.hbase.pb.RegionLoad" +
-      "Stats\"\336\001\n\021ResultOrException\022\r\n\005index\030\001 \001" +
-      "(\r\022 \n\006result\030\002 \001(\0132\020.hbase.pb.Result\022*\n\t" +
-      "exception\030\003 \001(\0132\027.hbase.pb.NameBytesPair" +
-      "\022:\n\016service_result\030\004 \001(\0132\".hbase.pb.Copr" +
-      "ocessorServiceResult\0220\n\tloadStats\030\005 \001(\0132" +
-      "\031.hbase.pb.RegionLoadStatsB\002\030\001\"x\n\022Region" +
-      "ActionResult\0226\n\021resultOrException\030\001 \003(\0132",
-      "\033.hbase.pb.ResultOrException\022*\n\texceptio" +
-      "n\030\002 \001(\0132\027.hbase.pb.NameBytesPair\"x\n\014Mult" +
-      "iRequest\022,\n\014regionAction\030\001 \003(\0132\026.hbase.p" +
-      "b.RegionAction\022\022\n\nnonceGroup\030\002 \001(\004\022&\n\tco" +
-      "ndition\030\003 \001(\0132\023.hbase.pb.Condition\"\226\001\n\rM" +
-      "ultiResponse\0228\n\022regionActionResult\030\001 \003(\013" +
-      "2\034.hbase.pb.RegionActionResult\022\021\n\tproces" +
-      "sed\030\002 \001(\010\0228\n\020regionStatistics\030\003 \001(\0132\036.hb" +
-      "ase.pb.MultiRegionLoadStats*\'\n\013Consisten" +
-      "cy\022\n\n\006STRONG\020\000\022\014\n\010TIMELINE\020\0012\263\005\n\rClientS",
-      "ervice\0222\n\003Get\022\024.hbase.pb.GetRequest\032\025.hb" +
-      "ase.pb.GetResponse\022;\n\006Mutate\022\027.hbase.pb." +
-      "MutateRequest\032\030.hbase.pb.MutateResponse\022" +
-      "5\n\004Scan\022\025.hbase.pb.ScanRequest\032\026.hbase.p" +
-      "b.ScanResponse\022P\n\rBulkLoadHFile\022\036.hbase." +
-      "pb.BulkLoadHFileRequest\032\037.hbase.pb.BulkL" +
-      "oadHFileResponse\022V\n\017PrepareBulkLoad\022 .hb" +
-      "ase.pb.PrepareBulkLoadRequest\032!.hbase.pb" +
-      ".PrepareBulkLoadResponse\022V\n\017CleanupBulkL" +
-      "oad\022 .hbase.pb.CleanupBulkLoadRequest\032!.",
-      "hbase.pb.CleanupBulkLoadResponse\022X\n\013Exec" +
-      "Service\022#.hbase.pb.CoprocessorServiceReq" +
-      "uest\032$.hbase.pb.CoprocessorServiceRespon" +
-      "se\022d\n\027ExecRegionServerService\022#.hbase.pb" +
+      "s\"*\n\nFamilyPath\022\016\n\006family\030\001 \002(\014\022\014\n\004path\030" +
+      "\002 \002(\t\"\337\001\n\024BulkLoadHFileRequest\022)\n\006region" +
+      "\030\001 \002(\0132\031.hbase.pb.RegionSpecifier\022)\n\013fam" +
+      "ily_path\030\002 \003(\0132\024.hbase.pb.FamilyPath\022\026\n\016" +
+      "assign_seq_num\030\003 \001(\010\022+\n\010fs_token\030\004 \001(\0132\031" +
+      ".hbase.pb.DelegationToken\022\022\n\nbulk_token\030",
+      "\005 \001(\t\022\030\n\tcopy_file\030\006 \001(\010:\005false\"R\n\025BulkL" +
+      "oadHFileResponse\022\016\n\006loaded\030\001 \002(\010\022)\n\013fami" +
+      "ly_path\030\002 \003(\0132\024.hbase.pb.FamilyPath\"V\n\017D" +
+      "elegationToken\022\022\n\nidentifier\030\001 \001(\014\022\020\n\010pa" +
+      "ssword\030\002 \001(\014\022\014\n\004kind\030\003 \001(\t\022\017\n\007service\030\004 " +
+      "\001(\t\"l\n\026PrepareBulkLoadRequest\022\'\n\ntable_n" +
+      "ame\030\001 \002(\0132\023.hbase.pb.TableName\022)\n\006region" +
+      "\030\002 \001(\0132\031.hbase.pb.RegionSpecifier\"-\n\027Pre" +
+      "pareBulkLoadResponse\022\022\n\nbulk_token\030\001 \002(\t" +
+      "\"W\n\026CleanupBulkLoadRequest\022\022\n\nbulk_token",
+      "\030\001 \002(\t\022)\n\006region\030\002 \001(\0132\031.hbase.pb.Region" +
+      "Specifier\"\031\n\027CleanupBulkLoadResponse\"a\n\026" +
+      "CoprocessorServiceCall\022\013\n\003row\030\001 \002(\014\022\024\n\014s" +
+      "ervice_name\030\002 \002(\t\022\023\n\013method_name\030\003 \002(\t\022\017" +
+      "\n\007request\030\004 \002(\014\"B\n\030CoprocessorServiceRes" +
+      "ult\022&\n\005value\030\001 \001(\0132\027.hbase.pb.NameBytesP" +
+      "air\"v\n\031CoprocessorServiceRequest\022)\n\006regi" +
+      "on\030\001 \002(\0132\031.hbase.pb.RegionSpecifier\022.\n\004c" +
+      "all\030\002 \002(\0132 .hbase.pb.CoprocessorServiceC" +
+      "all\"o\n\032CoprocessorServiceResponse\022)\n\006reg",
+      "ion\030\001 \002(\0132\031.hbase.pb.RegionSpecifier\022&\n\005" +
+      "value\030\002 \002(\0132\027.hbase.pb.NameBytesPair\"\226\001\n" +
+      "\006Action\022\r\n\005index\030\001 \001(\r\022)\n\010mutation\030\002 \001(\013" +
+      "2\027.hbase.pb.MutationProto\022\032\n\003get\030\003 \001(\0132\r" +
+      ".hbase.pb.Get\0226\n\014service_call\030\004 \001(\0132 .hb" +
+      "ase.pb.CoprocessorServiceCall\"k\n\014RegionA" +
+      "ction\022)\n\006region\030\001 \002(\0132\031.hbase.pb.RegionS" +
+      "pecifier\022\016\n\006atomic\030\002 \001(\010\022 \n\006action\030\003 \003(\013" +
+      "2\020.hbase.pb.Action\"c\n\017RegionLoadStats\022\027\n" +
+      "\014memstoreLoad\030\001 \001(\005:\0010\022\030\n\rheapOccupancy\030",
+      "\002 \001(\005:\0010\022\035\n\022compactionPressure\030\003 \001(\005:\0010\"" +
+      "j\n\024MultiRegionLoadStats\022)\n\006region\030\001 \003(\0132" +
+      "\031.hbase.pb.RegionSpecifier\022\'\n\004stat\030\002 \003(\013" +
+      "2\031.hbase.pb.RegionLoadStats\"\336\001\n\021ResultOr" +
+      "Exception\022\r\n\005index\030\001 \001(\r\022 \n\006result\030\002 \001(\013" +
+      "2\020.hbase.pb.Result\022*\n\texception\030\003 \001(\0132\027." +
+      "hbase.pb.NameBytesPair\022:\n\016service_result" +
+      "\030\004 \001(\0132\".hbase.pb.CoprocessorServiceResu" +
+      "lt\0220\n\tloadStats\030\005 \001(\0132\031.hbase.pb.RegionL" +
+      "oadStatsB\002\030\001\"x\n\022RegionActionResult\0226\n\021re",
+      "sultOrException\030\001 \003(\0132\033.hbase.pb.ResultO" +
+      "rException\022*\n\texception\030\002 \001(\0132\027.hbase.pb" +
+      ".NameBytesPair\"x\n\014MultiRequest\022,\n\014region" +
+      "Action\030\001 \003(\0132\026.hbase.pb.RegionAction\022\022\n\n" +
+      "nonceGroup\030\002 \001(\004\022&\n\tcondition\030\003 \001(\0132\023.hb" +
+      "ase.pb.Condition\"\226\001\n\rMultiResponse\0228\n\022re" +
+      "gionActionResult\030\001 \003(\0132\034.hbase.pb.Region" +
+      "ActionResult\022\021\n\tprocessed\030\002 \001(\010\0228\n\020regio" +
+      "nStatistics\030\003 \001(\0132\036.hbase.pb.MultiRegion" +
+      "LoadStats*\'\n\013Consistency\022\n\n\006STRONG\020\000\022\014\n\010",
+      "TIMELINE\020\0012\263\005\n\rClientService\0222\n\003Get\022\024.hb" +
+      "ase.pb.GetRequest\032\025.hbase.pb.GetResponse" +
+      "\022;\n\006Mutate\022\027.hbase.pb.MutateRequest\032\030.hb" +
+      "ase.pb.MutateResponse\0225\n\004Scan\022\025.hbase.pb" +
+      ".ScanRequest\032\026.hbase.pb.ScanResponse\022P\n\r" +
+      "BulkLoadHFile\022\036.hbase.pb.BulkLoadHFileRe" +
+      "quest\032\037.hbase.pb.BulkLoadHFileResponse\022V" +
+      "\n\017PrepareBulkLoad\022 .hbase.pb.PrepareBulk" +
+      "LoadRequest\032!.hbase.pb.PrepareBulkLoadRe" +
+      "sponse\022V\n\017CleanupBulkLoad\022 .hbase.pb.Cle",
+      "anupBulkLoadRequest\032!.hbase.pb.CleanupBu" +
+      "lkLoadResponse\022X\n\013ExecService\022#.hbase.pb" +
       ".CoprocessorServiceRequest\032$.hbase.pb.Co" +
-      "processorServiceResponse\0228\n\005Multi\022\026.hbas" +
-      "e.pb.MultiRequest\032\027.hbase.pb.MultiRespon" +
-      "seBI\n1org.apache.hadoop.hbase.shaded.pro" +
-      "tobuf.generatedB\014ClientProtosH\001\210\001\001\240\001\001"
+      "processorServiceResponse\022d\n\027ExecRegionSe" +
+      "rverService\022#.hbase.pb.CoprocessorServic" +
+      "eRequest\032$.hbase.pb.CoprocessorServiceRe" +
+      "sponse\0228\n\005Multi\022\026.hbase.pb.MultiRequest\032" +
+      "\027.hbase.pb.MultiResponseBI\n1org.apache.h" +
+      "adoop.hbase.shaded.protobuf.generatedB\014C" +
+      "lientProtosH\001\210\001\001\240\001\001"
     };
     org.apache.hadoop.hbase.shaded.com.google.protobuf.Descriptors.FileDescriptor.InternalDescriptorAssigner assigner =
         new org.apache.hadoop.hbase.shaded.com.google.protobuf.Descriptors.FileDescriptor.    InternalDescriptorAssigner() {
@@ -40652,122 +41031,122 @@ public final class ClientProtos {
       org.apache.hadoop.hbase.shaded.com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
         internal_static_hbase_pb_ScanResponse_descriptor,
         new java.lang.String[] { "CellsPerResult", "ScannerId", "MoreResults", "Ttl", "Results", "Stale", "PartialFlagPerResult", "MoreResultsInRegion", "HeartbeatMessage", "ScanMetrics", });
-    internal_static_hbase_pb_BulkLoadHFileRequest_descriptor =
+    internal_static_hbase_pb_FamilyPath_descriptor =
       getDescriptor().getMessageTypes().get(14);
+    internal_static_hbase_pb_FamilyPath_fieldAccessorTable = new
+      org.apache.hadoop.hbase.shaded.com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
+        internal_static_hbase_pb_FamilyPath_descriptor,
+        new java.lang.String[] { "Family", "Path", });
+    internal_static_hbase_pb_BulkLoadHFileRequest_descriptor =
+      getDescriptor().getMessageTypes().get(15);
     internal_static_hbase_pb_BulkLoadHFileRequest_fieldAccessorTable = new
       org.apache.hadoop.hbase.shaded.com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
         internal_static_hbase_pb_BulkLoadHFileRequest_descriptor,
         new java.lang.String[] { "Region", "FamilyPath", "AssignSeqNum", "FsToken", "BulkToken", "CopyFile", });
-    internal_static_hbase_pb_BulkLoadHFileRequest_FamilyPath_descriptor =
-      internal_static_hbase_pb_BulkLoadHFileRequest_descriptor.getNestedTypes().get(0);
-    internal_static_hbase_pb_BulkLoadHFileRequest_FamilyPath_fieldAccessorTable = new
-      org.apache.hadoop.hbase.shaded.com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
-        internal_static_hbase_pb_BulkLoadHFileRequest_FamilyPath_descriptor,
-        new java.lang.String[] { "Family", "Path", });
     internal_static_hbase_pb_BulkLoadHFileResponse_descriptor =
-      getDescriptor().getMessageTypes().get(15);
+      getDescriptor().getMessageTypes().get(16);
     internal_static_hbase_pb_BulkLoadHFileResponse_fieldAccessorTable = new
       org.apache.hadoop.hbase.shaded.com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
         internal_static_hbase_pb_BulkLoadHFileResponse_descriptor,
-        new java.lang.String[] { "Loaded", });
+        new java.lang.String[] { "Loaded", "FamilyPath", });
     internal_static_hbase_pb_DelegationToken_descriptor =
-      getDescriptor().getMessageTypes().get(16);
+      getDescriptor().getMessageTypes().get(17);
     internal_static_hbase_pb_DelegationToken_fieldAccessorTable = new
       org.apache.hadoop.hbase.shaded.com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
         internal_static_hbase_pb_DelegationToken_descriptor,
         new java.lang.String[] { "Identifier", "Password", "Kind", "Service", });
     internal_static_hbase_pb_PrepareBulkLoadRequest_descriptor =
-      getDescriptor().getMessageTypes().get(17);
+      getDescriptor().getMessageTypes().get(18);
     internal_static_hbase_pb_PrepareBulkLoadRequest_fieldAccessorTable = new
       org.apache.hadoop.hbase.shaded.com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
         internal_static_hbase_pb_PrepareBulkLoadRequest_descriptor,
         new java.lang.String[] { "TableName", "Region", });
     internal_static_hbase_pb_PrepareBulkLoadResponse_descriptor =
-      getDescriptor().getMessageTypes().get(18);
+      getDescriptor().getMessageTypes().get(19);
     internal_static_hbase_pb_PrepareBulkLoadResponse_fieldAccessorTable = new
       org.apache.hadoop.hbase.shaded.com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
         internal_static_hbase_pb_PrepareBulkLoadResponse_descriptor,
         new java.lang.String[] { "BulkToken", });
     internal_static_hbase_pb_CleanupBulkLoadRequest_descriptor =
-      getDescriptor().getMessageTypes().get(19);
+      getDescriptor().getMessageTypes().get(20);
     internal_static_hbase_pb_CleanupBulkLoadRequest_fieldAccessorTable = new
       org.apache.hadoop.hbase.shaded.com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
         internal_static_hbase_pb_CleanupBulkLoadRequest_descriptor,
         new java.lang.String[] { "BulkToken", "Region", });
     internal_static_hbase_pb_CleanupBulkLoadResponse_descriptor =
-      getDescriptor().getMessageTypes().get(20);
+      getDescriptor().getMessageTypes().get(21);
     internal_static_hbase_pb_CleanupBulkLoadResponse_fieldAccessorTable = new
       org.apache.hadoop.hbase.shaded.com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
         internal_static_hbase_pb_CleanupBulkLoadResponse_descriptor,
         new java.lang.String[] { });
     internal_static_hbase_pb_CoprocessorServiceCall_descriptor =
-      getDescriptor().getMessageTypes().get(21);
+      getDescriptor().getMessageTypes().get(22);
     internal_static_hbase_pb_CoprocessorServiceCall_fieldAccessorTable = new
       org.apache.hadoop.hbase.shaded.com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
         internal_static_hbase_pb_CoprocessorServiceCall_descriptor,
         new java.lang.String[] { "Row", "ServiceName", "MethodName", "Request", });
     internal_static_hbase_pb_CoprocessorServiceResult_descriptor =
-      getDescriptor().getMessageTypes().get(22);
+      getDescriptor().getMessageTypes().get(23);
     internal_static_hbase_pb_CoprocessorServiceResult_fieldAccessorTable = new
       org.apache.hadoop.hbase.shaded.com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
         internal_static_hbase_pb_CoprocessorServiceResult_descriptor,
         new java.lang.String[] { "Value", });
     internal_static_hbase_pb_CoprocessorServiceRequest_descriptor =
-      getDescriptor().getMessageTypes().get(23);
+      getDescriptor().getMessageTypes().get(24);
     internal_static_hbase_pb_CoprocessorServiceRequest_fieldAccessorTable = new
       org.apache.hadoop.hbase.shaded.com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
         internal_static_hbase_pb_CoprocessorServiceRequest_descriptor,
         new java.lang.String[] { "Region", "Call", });
     internal_static_hbase_pb_CoprocessorServiceResponse_descriptor =
-      getDescriptor().getMessageTypes().get(24);
+      getDescriptor().getMessageTypes().get(25);
     internal_static_hbase_pb_CoprocessorServiceResponse_fieldAccessorTable = new
       org.apache.hadoop.hbase.shaded.com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
         internal_static_hbase_pb_CoprocessorServiceResponse_descriptor,
         new java.lang.String[] { "Region", "Value", });
     internal_static_hbase_pb_Action_descriptor =
-      getDescriptor().getMessageTypes().get(25);
+      getDescriptor().getMessageTypes().get(26);
     internal_static_hbase_pb_Action_fieldAccessorTable = new
       org.apache.hadoop.hbase.shaded.com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
         internal_static_hbase_pb_Action_descriptor,
         new java.lang.String[] { "Index", "Mutation", "Get", "ServiceCall", });
     internal_static_hbase_pb_RegionAction_descriptor =
-      getDescriptor().getMessageTypes().get(26);
+      getDescriptor().getMessageTypes().get(27);
     internal_static_hbase_pb_RegionAction_fieldAccessorTable = new
       org.apache.hadoop.hbase.shaded.com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
         internal_static_hbase_pb_RegionAction_descriptor,
         new java.lang.String[] { "Region", "Atomic", "Action", });
     internal_static_hbase_pb_RegionLoadStats_descriptor =
-      getDescriptor().getMessageTypes().get(27);
+      getDescriptor().getMessageTypes().get(28);
     internal_static_hbase_pb_RegionLoadStats_fieldAccessorTable = new
       org.apache.hadoop.hbase.shaded.com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
         internal_static_hbase_pb_RegionLoadStats_descriptor,
         new java.lang.String[] { "MemstoreLoad", "HeapOccupancy", "CompactionPressure", });
     internal_static_hbase_pb_MultiRegionLoadStats_descriptor =
-      getDescriptor().getMessageTypes().get(28);
+      getDescriptor().getMessageTypes().get(29);
     internal_static_hbase_pb_MultiRegionLoadStats_fieldAccessorTable = new
       org.apache.hadoop.hbase.shaded.com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
         internal_static_hbase_pb_MultiRegionLoadStats_descriptor,
         new java.lang.String[] { "Region", "Stat", });
     internal_static_hbase_pb_ResultOrException_descriptor =
-      getDescriptor().getMessageTypes().get(29);
+      getDescriptor().getMessageTypes().get(30);
     internal_static_hbase_pb_ResultOrException_fieldAccessorTable = new
       org.apache.hadoop.hbase.shaded.com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
         internal_static_hbase_pb_ResultOrException_descriptor,
         new java.lang.String[] { "Index", "Result", "Exception", "ServiceResult", "LoadStats", });
     internal_static_hbase_pb_RegionActionResult_descriptor =
-      getDescriptor().getMessageTypes().get(30);
+      getDescriptor().getMessageTypes().get(31);
     internal_static_hbase_pb_RegionActionResult_fieldAccessorTable = new
       org.apache.hadoop.hbase.shaded.com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
         internal_static_hbase_pb_RegionActionResult_descriptor,
         new java.lang.String[] { "ResultOrException", "Exception", });
     internal_static_hbase_pb_MultiRequest_descriptor =
-      getDescriptor().getMessageTypes().get(31);
+      getDescriptor().getMessageTypes().get(32);
     internal_static_hbase_pb_MultiRequest_fieldAccessorTable = new
       org.apache.hadoop.hbase.shaded.com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
         internal_static_hbase_pb_MultiRequest_descriptor,
         new java.lang.String[] { "RegionAction", "NonceGroup", "Condition", });
     internal_static_hbase_pb_MultiResponse_descriptor =
-      getDescriptor().getMessageTypes().get(32);
+      getDescriptor().getMessageTypes().get(33);
     internal_static_hbase_pb_MultiResponse_fieldAccessorTable = new
       org.apache.hadoop.hbase.shaded.com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
         internal_static_hbase_pb_MultiResponse_descriptor,
diff --git a/hbase-protocol-shaded/src/main/protobuf/Client.proto b/hbase-protocol-shaded/src/main/protobuf/Client.proto
index 2feaa26..dd0ef03 100644
--- a/hbase-protocol-shaded/src/main/protobuf/Client.proto
+++ b/hbase-protocol-shaded/src/main/protobuf/Client.proto
@@ -334,6 +334,11 @@ message ScanResponse {
  * Atomically bulk load multiple HFiles (say from different column families)
  * into an open region.
  */
+message FamilyPath {
+  required bytes family = 1;
+  required string path = 2;
+}
+
 message BulkLoadHFileRequest {
   required RegionSpecifier region = 1;
   repeated FamilyPath family_path = 2;
@@ -341,15 +346,11 @@ message BulkLoadHFileRequest {
   optional DelegationToken fs_token = 4;
   optional string bulk_token = 5;
   optional bool copy_file = 6 [default = false];
-
-  message FamilyPath {
-    required bytes family = 1;
-    required string path = 2;
-  }
 }
 
 message BulkLoadHFileResponse {
   required bool loaded = 1;
+  repeated FamilyPath family_path = 2;
 }
 
 message DelegationToken {
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/mapreduce/LoadIncrementalHFiles.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/mapreduce/LoadIncrementalHFiles.java
index 0d9aae8..5a852d3 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/mapreduce/LoadIncrementalHFiles.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/mapreduce/LoadIncrementalHFiles.java
@@ -67,6 +67,7 @@ import org.apache.hadoop.hbase.client.Admin;
 import org.apache.hadoop.hbase.client.ClientServiceCallable;
 import org.apache.hadoop.hbase.client.Connection;
 import org.apache.hadoop.hbase.client.ConnectionFactory;
+import org.apache.hadoop.hbase.client.FamilyPath;
 import org.apache.hadoop.hbase.client.RegionLocator;
 import org.apache.hadoop.hbase.client.RpcRetryingCallerFactory;
 import org.apache.hadoop.hbase.client.SecureBulkLoadClient;
@@ -92,6 +93,7 @@ import org.apache.hadoop.hbase.security.token.FsDelegationToken;
 import org.apache.hadoop.hbase.util.Bytes;
 import org.apache.hadoop.hbase.util.FSHDFSUtils;
 import org.apache.hadoop.hbase.util.Pair;
+import org.apache.hadoop.hbase.util.Triple;
 import org.apache.hadoop.util.Tool;
 import org.apache.hadoop.util.ToolRunner;
 
@@ -134,7 +136,7 @@ public class LoadIncrementalHFiles extends Configured implements Tool {
   private int nrThreads;
   private RpcControllerFactory rpcControllerFactory;
 
-  private Map<LoadQueueItem, ByteBuffer> retValue = null;
+  private Map<FamilyPath, byte[]> retValue = null;
 
   public LoadIncrementalHFiles(Configuration conf) throws Exception {
     super(conf);
@@ -266,18 +268,9 @@ public class LoadIncrementalHFiles extends Configured implements Tool {
    * region boundary, and each part is added back into the queue.
    * The import process finishes when the queue is empty.
    */
-  public static class LoadQueueItem {
-    final byte[] family;
-    final Path hfilePath;
-
+  public static class LoadQueueItem extends FamilyPath {
     public LoadQueueItem(byte[] family, Path hfilePath) {
-      this.family = family;
-      this.hfilePath = hfilePath;
-    }
-
-    @Override
-    public String toString() {
-      return "family:"+ Bytes.toString(family) + " path:" + hfilePath.toString();
+      super(family, hfilePath);
     }
   }
 
@@ -349,7 +342,7 @@ public class LoadIncrementalHFiles extends Configured implements Tool {
       err.append("Bulk load aborted with some files not yet loaded:\n");
       err.append("-------------------------------------------------\n");
       for (LoadQueueItem q : queue) {
-        err.append("  ").append(q.hfilePath).append('\n');
+        err.append("  ").append(q.getHFilePath()).append('\n');
       }
       LOG.error(err);
     }
@@ -450,7 +443,7 @@ public class LoadIncrementalHFiles extends Configured implements Tool {
     }
   }
 
-  Map<LoadQueueItem, ByteBuffer> performBulkLoad(final Admin admin, Table table,
+  Map<FamilyPath, byte[]> performBulkLoad(final Admin admin, Table table,
       RegionLocator regionLocator, Deque<LoadQueueItem> queue, ExecutorService pool,
       SecureBulkLoadClient secureClient, boolean copyFile) throws IOException {
     int count = 0;
@@ -467,7 +460,7 @@ public class LoadIncrementalHFiles extends Configured implements Tool {
     bulkToken = secureClient.prepareBulkLoad(admin.getConnection());
     Pair<Multimap<ByteBuffer, LoadQueueItem>, Set<String>> pair = null;
 
-    Map<LoadQueueItem, ByteBuffer> item2RegionMap = new HashMap<>();
+    Map<FamilyPath, byte[]> item2RegionMap = new HashMap<>();
     // Assumes that region splits can happen while this occurs.
     while (!queue.isEmpty()) {
       // need to reload split keys each iteration.
@@ -581,7 +574,7 @@ public class LoadIncrementalHFiles extends Configured implements Tool {
     Iterator<LoadQueueItem> queueIter = queue.iterator();
     while (queueIter.hasNext()) {
       LoadQueueItem lqi = queueIter.next();
-      String familyNameInHFile = Bytes.toString(lqi.family);
+      String familyNameInHFile = Bytes.toString(lqi.getFamily());
       if (!familyNames.contains(familyNameInHFile)) {
         unmatchedFamilies.add(familyNameInHFile);
       }
@@ -648,41 +641,38 @@ public class LoadIncrementalHFiles extends Configured implements Tool {
   protected void bulkLoadPhase(final Table table, final Connection conn,
       ExecutorService pool, Deque<LoadQueueItem> queue,
       final Multimap<ByteBuffer, LoadQueueItem> regionGroups, boolean copyFile,
-      Map<LoadQueueItem, ByteBuffer> item2RegionMap) throws IOException {
+      Map<FamilyPath, byte[]> item2RegionMap) throws IOException {
     // atomically bulk load the groups.
-    Set<Future<List<LoadQueueItem>>> loadingFutures = new HashSet<>();
+    Set<Future<Triple<List<LoadQueueItem>, byte[], List<FamilyPath>>>> loadingFutures =
+        new HashSet<>();
     for (Entry<ByteBuffer, ? extends Collection<LoadQueueItem>> e: regionGroups.asMap().entrySet()){
       final byte[] first = e.getKey().array();
       final Collection<LoadQueueItem> lqis =  e.getValue();
 
-      final Callable<List<LoadQueueItem>> call = new Callable<List<LoadQueueItem>>() {
+      final Callable<Triple<List<LoadQueueItem>, byte[], List<FamilyPath>>> call =
+          new Callable<Triple<List<LoadQueueItem>, byte[], List<FamilyPath>>>() {
         @Override
-        public List<LoadQueueItem> call() throws Exception {
-          List<LoadQueueItem> toRetry =
+        public Triple<List<LoadQueueItem>, byte[], List<FamilyPath>> call() throws Exception {
+          Triple<List<LoadQueueItem>, byte[], List<FamilyPath>> toRetry =
               tryAtomicRegionLoad(conn, table.getName(), first, lqis, copyFile);
           return toRetry;
         }
       };
-      if (item2RegionMap != null) {
-        for (LoadQueueItem lqi : lqis) {
-          item2RegionMap.put(lqi, e.getKey());
-        }
-      }
       loadingFutures.add(pool.submit(call));
     }
 
     // get all the results.
-    for (Future<List<LoadQueueItem>> future : loadingFutures) {
+    for (Future<Triple<List<LoadQueueItem>, byte[], List<FamilyPath>>> future : loadingFutures) {
       try {
-        List<LoadQueueItem> toRetry = future.get();
-
-        if (item2RegionMap != null) {
-          for (LoadQueueItem lqi : toRetry) {
-            item2RegionMap.remove(lqi);
-          }
-        }
+        Triple<List<LoadQueueItem>, byte[], List<FamilyPath>> triple = future.get();
+        List<LoadQueueItem> toRetry = triple.getFirst();
         // LQIs that are requeued to be regrouped.
         queue.addAll(toRetry);
+        if (item2RegionMap != null && triple.getThird() != null) {
+          for (FamilyPath lqi : triple.getThird()) {
+            item2RegionMap.put(lqi, triple.getSecond());
+          }
+        }
 
       } catch (ExecutionException e1) {
         Throwable t = e1.getCause();
@@ -707,15 +697,15 @@ public class LoadIncrementalHFiles extends Configured implements Tool {
       final Collection<LoadQueueItem> lqis =  e.getValue();
       HashMap<byte[], MutableInt> filesMap = new HashMap<>();
       for (LoadQueueItem lqi: lqis) {
-        MutableInt count = filesMap.get(lqi.family);
+        MutableInt count = filesMap.get(lqi.getFamily());
         if (count == null) {
           count = new MutableInt();
-          filesMap.put(lqi.family, count);
+          filesMap.put(lqi.getFamily(), count);
         }
         count.increment();
         if (count.intValue() > maxFilesPerRegionPerFamily) {
           LOG.error("Trying to load more than " + maxFilesPerRegionPerFamily
-            + " hfiles to family " + Bytes.toStringBinary(lqi.family)
+            + " hfiles to family " + Bytes.toStringBinary(lqi.getFamily())
             + " of region with start key "
             + Bytes.toStringBinary(e.getKey()));
           return false;
@@ -795,9 +785,9 @@ public class LoadIncrementalHFiles extends Configured implements Tool {
   protected List<LoadQueueItem> splitStoreFile(final LoadQueueItem item,
       final Table table, byte[] startKey,
       byte[] splitKey) throws IOException {
-    final Path hfilePath = item.hfilePath;
+    final Path hfilePath = item.getHFilePath();
 
-    Path tmpDir = item.hfilePath.getParent();
+    Path tmpDir = item.getHFilePath().getParent();
     if (!tmpDir.getName().equals(TMP_DIR)) {
       tmpDir = new Path(tmpDir, TMP_DIR);
     }
@@ -806,7 +796,7 @@ public class LoadIncrementalHFiles extends Configured implements Tool {
       "region. Splitting...");
 
     String uniqueName = getUniqueName();
-    HColumnDescriptor familyDesc = table.getTableDescriptor().getFamily(item.family);
+    HColumnDescriptor familyDesc = table.getTableDescriptor().getFamily(item.getFamily());
 
     Path botOut = new Path(tmpDir, uniqueName + ".bottom");
     Path topOut = new Path(tmpDir, uniqueName + ".top");
@@ -820,19 +810,19 @@ public class LoadIncrementalHFiles extends Configured implements Tool {
     // Add these back at the *front* of the queue, so there's a lower
     // chance that the region will just split again before we get there.
     List<LoadQueueItem> lqis = new ArrayList<LoadQueueItem>(2);
-    lqis.add(new LoadQueueItem(item.family, botOut));
-    lqis.add(new LoadQueueItem(item.family, topOut));
+    lqis.add(new LoadQueueItem(item.getFamily(), botOut));
+    lqis.add(new LoadQueueItem(item.getFamily(), topOut));
 
     // If the current item is already the result of previous splits,
     // we don't need it anymore. Clean up to save space.
     // It is not part of the original input files.
     try {
-      tmpDir = item.hfilePath.getParent();
+      tmpDir = item.getHFilePath().getParent();
       if (tmpDir.getName().equals(TMP_DIR)) {
-        fs.delete(item.hfilePath, false);
+        fs.delete(item.getHFilePath(), false);
       }
     } catch (IOException e) {
-      LOG.warn("Unable to delete temporary split file " + item.hfilePath);
+      LOG.warn("Unable to delete temporary split file " + item.getHFilePath());
     }
     LOG.info("Successfully split into new HFiles " + botOut + " and " + topOut);
     return lqis;
@@ -850,7 +840,7 @@ public class LoadIncrementalHFiles extends Configured implements Tool {
   protected Pair<List<LoadQueueItem>, String> groupOrSplit(
       Multimap<ByteBuffer, LoadQueueItem> regionGroups, final LoadQueueItem item, final Table table,
       final Pair<byte[][], byte[][]> startEndKeys) throws IOException {
-    final Path hfilePath = item.hfilePath;
+    final Path hfilePath = item.getHFilePath();
     // fs is the source filesystem
     if (fs == null) {
       fs = hfilePath.getFileSystem(getConf());
@@ -946,20 +936,23 @@ public class LoadIncrementalHFiles extends Configured implements Tool {
    * @return empty list if success, list of items to retry on recoverable
    *   failure
    */
-  protected List<LoadQueueItem> tryAtomicRegionLoad(final Connection conn,
+  protected Triple<List<LoadQueueItem>, byte[], List<FamilyPath>> tryAtomicRegionLoad(
+      final Connection conn,
       final TableName tableName, final byte[] first, final Collection<LoadQueueItem> lqis,
       boolean copyFile) throws IOException {
     final List<Pair<byte[], String>> famPaths = new ArrayList<>(lqis.size());
     for (LoadQueueItem lqi : lqis) {
-      if (!unmatchedFamilies.contains(Bytes.toString(lqi.family))) {
-        famPaths.add(Pair.newPair(lqi.family, lqi.hfilePath.toString()));
+      if (!unmatchedFamilies.contains(Bytes.toString(lqi.getFamily()))) {
+        famPaths.add(Pair.newPair(lqi.getFamily(), lqi.getHFilePath().toString()));
       }
     }
-    final ClientServiceCallable<byte[]> svrCallable = new ClientServiceCallable<byte[]>(conn,
+    final ClientServiceCallable<List<FamilyPath>> svrCallable =
+        new ClientServiceCallable<List<FamilyPath>>(conn,
         tableName, first, rpcControllerFactory.newController()) {
       @Override
-      protected byte[] rpcCall() throws Exception {
+      protected List<FamilyPath> rpcCall() throws Exception {
         SecureBulkLoadClient secureClient = null;
+        List<FamilyPath> files = null;
         boolean success = false;
         try {
           LOG.debug("Going to connect to server " + getLocation() + " for row "
@@ -967,10 +960,11 @@ public class LoadIncrementalHFiles extends Configured implements Tool {
           byte[] regionName = getLocation().getRegionInfo().getRegionName();
           try (Table table = conn.getTable(getTableName())) {
             secureClient = new SecureBulkLoadClient(getConf(), table);
-            success = secureClient.secureBulkLoadHFiles(getStub(), famPaths, regionName,
+            files = secureClient.secureBulkLoadHFiles(getStub(), famPaths, regionName,
                   assignSeqIds, fsDelegationToken.getUserToken(), bulkToken, copyFile);
+            success = files == null ? false : files.size() == famPaths.size();
           }
-          return success ? regionName : null;
+          return success ? files : null;
         } finally {
           //Best effort copying of files that might not have been imported
           //from the staging directory back to original location
@@ -979,7 +973,7 @@ public class LoadIncrementalHFiles extends Configured implements Tool {
             FileSystem targetFs = FileSystem.get(getConf());
          // fs is the source filesystem
             if(fs == null) {
-              fs = lqis.iterator().next().hfilePath.getFileSystem(getConf());
+              fs = lqis.iterator().next().getHFilePath().getFileSystem(getConf());
             }
             // Check to see if the source and target filesystems are the same
             // If they are the same filesystem, we will try move the files back
@@ -1012,10 +1006,10 @@ public class LoadIncrementalHFiles extends Configured implements Tool {
     try {
       List<LoadQueueItem> toRetry = new ArrayList<>();
       Configuration conf = getConf();
-      byte[] region = RpcRetryingCallerFactory.instantiate(conf,
-          null).<byte[]> newCaller()
+      List<FamilyPath> files = RpcRetryingCallerFactory.instantiate(conf,
+          null).<List<FamilyPath>> newCaller()
           .callWithRetries(svrCallable, Integer.MAX_VALUE);
-      if (region == null) {
+      if (files == null) {
         LOG.warn("Attempt to bulk load region containing "
             + Bytes.toStringBinary(first) + " into table "
             + tableName  + " with files " + lqis
@@ -1023,7 +1017,8 @@ public class LoadIncrementalHFiles extends Configured implements Tool {
         toRetry.addAll(lqis); // return lqi's to retry
       }
       // success
-      return toRetry;
+      return new Triple<List<LoadQueueItem>, byte[], List<FamilyPath>>(toRetry, first,
+          toRetry.isEmpty() ? files : null);
     } catch (IOException e) {
       LOG.error("Encountered unrecoverable error from region server, additional details: "
           + svrCallable.getExceptionMessageAdditionalDetail(), e);
@@ -1206,7 +1201,7 @@ public class LoadIncrementalHFiles extends Configured implements Tool {
     LOG.info("Table "+ tableName +" is available!!");
   }
 
-  public Map<LoadQueueItem, ByteBuffer> run(String dirPath, Map<byte[], List<Path>> map,
+  public Map<FamilyPath, byte[]> run(String dirPath, Map<byte[], List<Path>> map,
       TableName tableName) throws Exception{
     initialize();
     try (Connection connection = ConnectionFactory.createConnection(getConf());
@@ -1250,7 +1245,7 @@ public class LoadIncrementalHFiles extends Configured implements Tool {
 
     String dirPath = args[0];
     TableName tableName = TableName.valueOf(args[1]);
-    Map<LoadQueueItem, ByteBuffer> loaded = run(dirPath, null, tableName);
+    Map<FamilyPath, byte[]> loaded = run(dirPath, null, tableName);
     if (loaded == null || !loaded.isEmpty()) return 0;
     return -1;
   }
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java
index 831627b..31ebf7a 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java
@@ -5470,14 +5470,14 @@ public class HRegion implements HeapSize, PropagatingConfigurationObserver, Regi
   }
 
   @Override
-  public boolean bulkLoadHFiles(Collection<Pair<byte[], String>> familyPaths, boolean assignSeqId,
+  public Map<byte[], List<Path>> bulkLoadHFiles(Collection<Pair<byte[], String>> familyPaths, boolean assignSeqId,
       BulkLoadListener bulkLoadListener) throws IOException {
     return bulkLoadHFiles(familyPaths, assignSeqId, bulkLoadListener, false);
   }
 
   @Override
-  public boolean bulkLoadHFiles(Collection<Pair<byte[], String>> familyPaths, boolean assignSeqId,
-      BulkLoadListener bulkLoadListener, boolean copyFile) throws IOException {
+  public Map<byte[], List<Path>> bulkLoadHFiles(Collection<Pair<byte[], String>> familyPaths,
+      boolean assignSeqId, BulkLoadListener bulkLoadListener, boolean copyFile) throws IOException {
     long seqId = -1;
     Map<byte[], List<Path>> storeFiles = new TreeMap<byte[], List<Path>>(Bytes.BYTES_COMPARATOR);
     Map<String, Long> storeFilesSizes = new HashMap<String, Long>();
@@ -5532,7 +5532,7 @@ public class HRegion implements HeapSize, PropagatingConfigurationObserver, Regi
         // problem when validating
         LOG.warn("There was a recoverable bulk load failure likely due to a" +
             " split.  These (family, HFile) pairs were not loaded: " + list);
-        return isSuccessful;
+        return null;
       }
 
       // We need to assign a sequential ID that's in between two memstores in order to preserve
@@ -5626,7 +5626,7 @@ public class HRegion implements HeapSize, PropagatingConfigurationObserver, Regi
 
       closeBulkRegionOperation();
     }
-    return isSuccessful;
+    return isSuccessful ? storeFiles : null;
   }
 
   @Override
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/RSRpcServices.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/RSRpcServices.java
index b0165f0..95a3590 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/RSRpcServices.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/RSRpcServices.java
@@ -43,6 +43,7 @@ import java.util.concurrent.atomic.LongAdder;
 import org.apache.commons.logging.Log;
 import org.apache.commons.logging.LogFactory;
 import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.fs.Path;
 import org.apache.hadoop.hbase.ByteBufferedCell;
 import org.apache.hadoop.hbase.Cell;
 import org.apache.hadoop.hbase.CellScannable;
@@ -136,7 +137,7 @@ import org.apache.hadoop.hbase.shaded.protobuf.ResponseConverter;
 import org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos;
 import org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos.Action;
 import org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos.BulkLoadHFileRequest;
-import org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos.BulkLoadHFileRequest.FamilyPath;
+import org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos.FamilyPath;
 import org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos.BulkLoadHFileResponse;
 import org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos.CleanupBulkLoadRequest;
 import org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos.CleanupBulkLoadResponse;
@@ -2110,6 +2111,7 @@ public class RSRpcServices implements HBaseRPCErrorHandler,
       Region region = getRegion(request.getRegion());
       boolean bypass = false;
       boolean loaded = false;
+      Map<byte[], List<Path>> map = null;
 
       if (!request.hasBulkToken()) {
         // Old style bulk load. This will not be supported in future releases
@@ -2123,17 +2125,30 @@ public class RSRpcServices implements HBaseRPCErrorHandler,
           bypass = region.getCoprocessorHost().preBulkLoadHFile(familyPaths);
         }
         if (!bypass) {
-          loaded = region.bulkLoadHFiles(familyPaths, request.getAssignSeqNum(), null,
+          map = region.bulkLoadHFiles(familyPaths, request.getAssignSeqNum(), null,
               request.getCopyFile());
+          if (map != null) {
+            loaded = true;
+          }
         }
         if (region.getCoprocessorHost() != null) {
           loaded = region.getCoprocessorHost().postBulkLoadHFile(familyPaths, loaded);
         }
       } else {
         // secure bulk load
-        loaded = regionServer.secureBulkLoadManager.secureBulkLoadHFiles(region, request);
+        map = regionServer.secureBulkLoadManager.secureBulkLoadHFiles(region, request);
       }
       BulkLoadHFileResponse.Builder builder = BulkLoadHFileResponse.newBuilder();
+      FamilyPath.Builder fpBuilder = FamilyPath.newBuilder();
+      if (map != null) {
+        for (Map.Entry<byte[], List<Path>> entry : map.entrySet()) {
+          for (Path p : entry.getValue()) {
+            fpBuilder.clear();
+            builder.addFamilyPath(fpBuilder.setFamily(UnsafeByteOperations.unsafeWrap(entry.getKey()))
+                .setPath(p.toString()).build());
+          }
+        }
+      }
       builder.setLoaded(loaded);
       return builder.build();
     } catch (IOException ie) {
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/Region.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/Region.java
index 1b106b2..09016e8 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/Region.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/Region.java
@@ -22,6 +22,7 @@ import java.util.Collection;
 import java.util.List;
 import java.util.Map;
 
+import org.apache.hadoop.fs.Path;
 import org.apache.hadoop.hbase.Cell;
 import org.apache.hadoop.hbase.CellComparator;
 import org.apache.hadoop.hbase.HBaseInterfaceAudience;
@@ -558,11 +559,11 @@ public interface Region extends ConfigurationObserver {
    * @param bulkLoadListener Internal hooks enabling massaging/preparation of a
    * file about to be bulk loaded
    * @param assignSeqId
-   * @return true if successful, false if failed recoverably
+   * @return Map from family to List of store file paths if successful, null if failed recoverably
    * @throws IOException if failed unrecoverably.
    */
-  boolean bulkLoadHFiles(Collection<Pair<byte[], String>> familyPaths, boolean assignSeqId,
-      BulkLoadListener bulkLoadListener) throws IOException;
+  Map<byte[], List<Path>> bulkLoadHFiles(Collection<Pair<byte[], String>> familyPaths,
+      boolean assignSeqId, BulkLoadListener bulkLoadListener) throws IOException;
 
   /**
    * Attempts to atomically load a group of hfiles.  This is critical for loading
@@ -573,11 +574,11 @@ public interface Region extends ConfigurationObserver {
    * @param bulkLoadListener Internal hooks enabling massaging/preparation of a
    * file about to be bulk loaded
    * @param copyFile always copy hfiles if true
-   * @return true if successful, false if failed recoverably
+   * @return Map from family to List of store file paths if successful, null if failed recoverably
    * @throws IOException if failed unrecoverably.
    */
-  boolean bulkLoadHFiles(Collection<Pair<byte[], String>> familyPaths, boolean assignSeqId,
-      BulkLoadListener bulkLoadListener, boolean copyFile) throws IOException;
+  Map<byte[], List<Path>> bulkLoadHFiles(Collection<Pair<byte[], String>> familyPaths,
+      boolean assignSeqId, BulkLoadListener bulkLoadListener, boolean copyFile) throws IOException;
 
   ///////////////////////////////////////////////////////////////////////////
   // Coprocessors
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/SecureBulkLoadManager.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/SecureBulkLoadManager.java
index e84ca40..fb51db3 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/SecureBulkLoadManager.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/SecureBulkLoadManager.java
@@ -171,10 +171,10 @@ public class SecureBulkLoadManager {
     fs.delete(new Path(request.getBulkToken()), true);
   }
 
-  public boolean secureBulkLoadHFiles(final Region region,
+  public Map<byte[], List<Path>> secureBulkLoadHFiles(final Region region,
       final BulkLoadHFileRequest request) throws IOException {
     final List<Pair<byte[], String>> familyPaths = new ArrayList<Pair<byte[], String>>(request.getFamilyPathCount());
-    for(ClientProtos.BulkLoadHFileRequest.FamilyPath el : request.getFamilyPathList()) {
+    for(ClientProtos.FamilyPath el : request.getFamilyPathList()) {
       familyPaths.add(new Pair<byte[], String>(el.getFamily().toByteArray(), el.getPath()));
     }
 
@@ -200,6 +200,8 @@ public class SecureBulkLoadManager {
         bypass = region.getCoprocessorHost().preBulkLoadHFile(familyPaths);
     }
     boolean loaded = false;
+    Map<byte[], List<Path>> map = null;
+
     if (!bypass) {
       // Get the target fs (HBase region server fs) delegation token
       // Since we have checked the permission via 'preBulkLoadHFile', now let's give
@@ -217,9 +219,9 @@ public class SecureBulkLoadManager {
         }
       }
 
-      loaded = ugi.doAs(new PrivilegedAction<Boolean>() {
+      map = ugi.doAs(new PrivilegedAction<Map<byte[], List<Path>>>() {
         @Override
-        public Boolean run() {
+        public Map<byte[], List<Path>> run() {
           FileSystem fs = null;
           try {
             fs = FileSystem.get(conf);
@@ -237,14 +239,14 @@ public class SecureBulkLoadManager {
           } catch (Exception e) {
             LOG.error("Failed to complete bulk load", e);
           }
-          return false;
+          return null;
         }
       });
     }
     if (region.getCoprocessorHost() != null) {
-       loaded = region.getCoprocessorHost().postBulkLoadHFile(familyPaths, loaded);
+       region.getCoprocessorHost().postBulkLoadHFile(familyPaths, loaded);
     }
-    return loaded;
+    return map;
   }
 
   private List<BulkLoadObserver> getBulkLoadObservers(Region region) {
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/mapreduce/TestLoadIncrementalHFiles.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/mapreduce/TestLoadIncrementalHFiles.java
index ebc7c97..d4d4266 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/mapreduce/TestLoadIncrementalHFiles.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/mapreduce/TestLoadIncrementalHFiles.java
@@ -24,9 +24,7 @@ import static org.junit.Assert.assertTrue;
 import static org.junit.Assert.fail;
 
 import java.io.IOException;
-import java.nio.ByteBuffer;
 import java.util.ArrayList;
-import java.util.Deque;
 import java.util.List;
 import java.util.Locale;
 import java.util.Map;
@@ -44,6 +42,7 @@ import org.apache.hadoop.hbase.HTableDescriptor;
 import org.apache.hadoop.hbase.NamespaceDescriptor;
 import org.apache.hadoop.hbase.TableName;
 import org.apache.hadoop.hbase.TableNotFoundException;
+import org.apache.hadoop.hbase.client.FamilyPath;
 import org.apache.hadoop.hbase.client.Table;
 import org.apache.hadoop.hbase.codec.KeyValueCodecWithTags;
 import org.apache.hadoop.hbase.coprocessor.CoprocessorHost;
@@ -352,13 +351,8 @@ public class TestLoadIncrementalHFiles {
     String [] args= {dir.toString(), tableName.toString()};
     if (useMap) {
       fs.delete(last);
-      Map<LoadQueueItem, ByteBuffer> loaded = loader.run(null, map, tableName);
+      Map<FamilyPath, byte[]> loaded = loader.run(null, map, tableName);
       expectedRows -= 1000;
-      for (LoadQueueItem item : loaded.keySet()) {
-        if (item.hfilePath.getName().equals(last.getName())) {
-          fail(last + " should be missing");
-        }
-      }
     } else {
       loader.run(args);
     }
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/mapreduce/TestLoadIncrementalHFilesSplitRecovery.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/mapreduce/TestLoadIncrementalHFilesSplitRecovery.java
index 8337da8..55d51be 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/mapreduce/TestLoadIncrementalHFilesSplitRecovery.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/mapreduce/TestLoadIncrementalHFilesSplitRecovery.java
@@ -51,6 +51,7 @@ import org.apache.hadoop.hbase.client.Admin;
 import org.apache.hadoop.hbase.client.ClusterConnection;
 import org.apache.hadoop.hbase.client.Connection;
 import org.apache.hadoop.hbase.client.ConnectionFactory;
+import org.apache.hadoop.hbase.client.FamilyPath;
 import org.apache.hadoop.hbase.client.RegionLocator;
 import org.apache.hadoop.hbase.client.Result;
 import org.apache.hadoop.hbase.client.ResultScanner;
@@ -68,6 +69,7 @@ import org.apache.hadoop.hbase.testclassification.MapReduceTests;
 import org.apache.hadoop.hbase.util.Bytes;
 import org.apache.hadoop.hbase.util.FSUtils;
 import org.apache.hadoop.hbase.util.Pair;
+import org.apache.hadoop.hbase.util.Triple;
 import org.junit.AfterClass;
 import org.junit.BeforeClass;
 import org.junit.Test;
@@ -283,7 +285,8 @@ public class TestLoadIncrementalHFilesSplitRecovery {
       setupTable(connection, table, 10);
       LoadIncrementalHFiles lih = new LoadIncrementalHFiles(util.getConfiguration()) {
         @Override
-        protected List<LoadQueueItem> tryAtomicRegionLoad(final Connection conn,
+        protected Triple<List<LoadQueueItem>, byte[], List<FamilyPath>> tryAtomicRegionLoad(
+            final Connection conn,
             TableName tableName, final byte[] first, Collection<LoadQueueItem> lqis,
             boolean copyFile) throws IOException {
           int i = attmptedCalls.incrementAndGet();
@@ -363,7 +366,7 @@ public class TestLoadIncrementalHFilesSplitRecovery {
         protected void bulkLoadPhase(final Table htable, final Connection conn,
             ExecutorService pool, Deque<LoadQueueItem> queue,
             final Multimap<ByteBuffer, LoadQueueItem> regionGroups, boolean copyFile,
-            Map<LoadQueueItem, ByteBuffer> item2RegionMap)
+            Map<FamilyPath, byte[]> item2RegionMap)
                 throws IOException {
           int i = attemptedCalls.incrementAndGet();
           if (i == 1) {
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestMobStoreCompaction.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestMobStoreCompaction.java
index 6f71548..fc0b09a 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestMobStoreCompaction.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestMobStoreCompaction.java
@@ -196,8 +196,8 @@ public class TestMobStoreCompaction {
 
     // The following will bulk load the above generated store files and compact, with 600(fileSize)
     // > 300(threshold)
-    boolean result = region.bulkLoadHFiles(hfiles, true, null);
-    assertTrue("Bulkload result:", result);
+    Map<byte[], List<Path>> result = region.bulkLoadHFiles(hfiles, true, null);
+    assertTrue("Bulkload result:", result != null);
     assertEquals("Before compaction: store files", compactionThreshold, countStoreFiles());
     assertEquals("Before compaction: mob file count", 0, countMobFiles());
     assertEquals("Before compaction: rows", compactionThreshold, UTIL.countRows(region));
diff --git a/hbase-client/src/main/java/org/apache/hadoop/hbase/client/FamilyPath.java b/hbase-client/src/main/java/org/apache/hadoop/hbase/client/FamilyPath.java
new file mode 100644
index 0000000..754ae07
--- /dev/null
+++ b/hbase-client/src/main/java/org/apache/hadoop/hbase/client/FamilyPath.java
@@ -0,0 +1,52 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.hbase.client;
+
+import org.apache.hadoop.fs.Path;
+import org.apache.hadoop.hbase.classification.InterfaceAudience;
+import org.apache.hadoop.hbase.classification.InterfaceStability;
+import org.apache.hadoop.hbase.util.Bytes;
+
+/**
+ * Represents an HFile waiting to be loaded.
+ */
+@InterfaceAudience.Public
+@InterfaceStability.Evolving
+public class FamilyPath {
+  final byte[] family;
+  final Path hfilePath;
+
+  public FamilyPath(byte[] family, Path hfilePath) {
+    this.family = family;
+    this.hfilePath = hfilePath;
+  }
+
+  public Path getHFilePath() {
+    return hfilePath;
+  }
+
+  public byte[] getFamily() {
+    return family;
+  }
+
+  @Override
+  public String toString() {
+    return "family:"+ Bytes.toString(family) + " path:" + hfilePath.toString();
+  }
+}
