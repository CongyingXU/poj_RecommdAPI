From dc0776acefe9278ec1256f57ca1c27a8fe8a91ec Mon Sep 17 00:00:00 2001
From: Jan Hentschel <jan.hentschel@ultratendency.com>
Date: Wed, 25 Jan 2017 19:38:26 +0100
Subject: [PATCH] HBASE-17532 Replaced explicit type with diamond operator

Signed-off-by: Michael Stack <stack@apache.org>
---
 .../org/apache/hadoop/hbase/HColumnDescriptor.java |  11 +--
 .../java/org/apache/hadoop/hbase/HRegionInfo.java  |   2 +-
 .../org/apache/hadoop/hbase/HTableDescriptor.java  |  19 ++--
 .../org/apache/hadoop/hbase/MetaTableAccessor.java |  30 +++---
 .../java/org/apache/hadoop/hbase/ServerLoad.java   |   4 +-
 .../org/apache/hadoop/hbase/client/Append.java     |   2 +-
 .../apache/hadoop/hbase/client/AsyncProcess.java   |  11 +--
 .../hbase/client/AsyncRequestFutureImpl.java       |  27 +++---
 .../client/AsyncRpcRetryingCallerFactory.java      |   2 +-
 .../apache/hadoop/hbase/client/BatchErrors.java    |  10 +-
 .../hbase/client/ClientAsyncPrefetchScanner.java   |   4 +-
 .../apache/hadoop/hbase/client/ClientScanner.java  |   6 +-
 .../hadoop/hbase/client/ClusterStatusListener.java |   2 +-
 .../hbase/client/ConnectionImplementation.java     |  11 +--
 .../org/apache/hadoop/hbase/client/Delete.java     |  10 +-
 .../java/org/apache/hadoop/hbase/client/Get.java   |  15 ++-
 .../org/apache/hadoop/hbase/client/HBaseAdmin.java |  32 +++----
 .../apache/hadoop/hbase/client/HRegionLocator.java |   2 +-
 .../org/apache/hadoop/hbase/client/HTable.java     |  26 +++---
 .../hadoop/hbase/client/HTableMultiplexer.java     |  14 +--
 .../org/apache/hadoop/hbase/client/Increment.java  |   5 +-
 .../hadoop/hbase/client/MetricsConnection.java     |   2 +-
 .../apache/hadoop/hbase/client/MultiAction.java    |   2 +-
 .../apache/hadoop/hbase/client/MultiResponse.java  |   2 +-
 .../hadoop/hbase/client/MultiServerCallable.java   |   2 +-
 .../org/apache/hadoop/hbase/client/Mutation.java   |  20 ++--
 .../hbase/client/OperationWithAttributes.java      |   2 +-
 .../client/PreemptiveFastFailInterceptor.java      |   6 +-
 .../java/org/apache/hadoop/hbase/client/Put.java   |   6 +-
 .../org/apache/hadoop/hbase/client/Result.java     |  15 ++-
 .../client/ResultBoundedCompletionService.java     |   2 +-
 .../RetriesExhaustedWithDetailsException.java      |   4 +-
 .../hbase/client/ReversedScannerCallable.java      |   2 +-
 .../hbase/client/RpcRetryingCallerFactory.java     |   4 +-
 .../hadoop/hbase/client/RpcRetryingCallerImpl.java |   3 +-
 .../client/RpcRetryingCallerWithReadReplicas.java  |   2 +-
 .../java/org/apache/hadoop/hbase/client/Scan.java  |  16 ++--
 .../hbase/client/ScannerCallableWithReplicas.java  |   6 +-
 .../hbase/client/ServerStatisticTracker.java       |   3 +-
 .../hbase/client/backoff/ServerStatistics.java     |   3 +-
 .../client/metrics/ServerSideScanMetrics.java      |   2 +-
 .../hbase/client/replication/ReplicationAdmin.java |  10 +-
 .../client/replication/ReplicationSerDeHelper.java |   4 +-
 .../apache/hadoop/hbase/filter/CompareFilter.java  |   2 +-
 .../hadoop/hbase/filter/DependentColumnFilter.java |   2 +-
 .../org/apache/hadoop/hbase/filter/FilterList.java |   2 +-
 .../FirstKeyValueMatchingQualifiersFilter.java     |   2 +-
 .../apache/hadoop/hbase/filter/FuzzyRowFilter.java |  11 +--
 .../hadoop/hbase/filter/MultiRowRangeFilter.java   |   6 +-
 .../hbase/filter/MultipleColumnPrefixFilter.java   |   2 +-
 .../apache/hadoop/hbase/filter/ParseFilter.java    |  14 +--
 .../hadoop/hbase/filter/TimestampsFilter.java      |   6 +-
 .../hadoop/hbase/ipc/NettyRpcDuplexHandler.java    |   2 +-
 .../apache/hadoop/hbase/protobuf/ProtobufUtil.java |   9 +-
 .../apache/hadoop/hbase/quotas/QuotaFilter.java    |   2 +-
 .../apache/hadoop/hbase/quotas/QuotaRetriever.java |   2 +-
 .../hadoop/hbase/quotas/QuotaSettingsFactory.java  |   4 +-
 .../hbase/replication/ReplicationPeerConfig.java   |   4 +-
 .../hbase/replication/ReplicationPeerZKImpl.java   |   2 +-
 .../hbase/replication/ReplicationPeersZKImpl.java  |  10 +-
 .../hbase/replication/ReplicationQueueInfo.java    |   2 +-
 .../hbase/replication/ReplicationQueuesZKImpl.java |   6 +-
 .../hbase/replication/ReplicationTableBase.java    |   8 +-
 .../replication/ReplicationTrackerZKImpl.java      |   7 +-
 .../TableBasedReplicationQueuesClientImpl.java     |   2 +-
 .../TableBasedReplicationQueuesImpl.java           |   2 +-
 .../apache/hadoop/hbase/security/SecurityInfo.java |   2 +-
 .../hbase/security/access/AccessControlClient.java |   2 +-
 .../hbase/security/access/AccessControlUtil.java   |   8 +-
 .../hbase/security/visibility/Authorizations.java  |   2 +-
 .../security/visibility/VisibilityClient.java      |   8 +-
 .../hadoop/hbase/shaded/protobuf/ProtobufUtil.java |  21 ++---
 .../hbase/shaded/protobuf/RequestConverter.java    |   3 +-
 .../hbase/shaded/protobuf/ResponseConverter.java   |   6 +-
 .../java/org/apache/hadoop/hbase/util/PoolMap.java |  18 ++--
 .../org/apache/hadoop/hbase/util/Writables.java    |   2 +-
 .../apache/hadoop/hbase/zookeeper/HQuorumPeer.java |   2 +-
 .../hadoop/hbase/zookeeper/InstancePending.java    |   2 +-
 .../hadoop/hbase/zookeeper/MetaTableLocator.java   |   8 +-
 .../hadoop/hbase/zookeeper/PendingWatcher.java     |   2 +-
 .../hbase/zookeeper/RecoverableZooKeeper.java      |   4 +-
 .../org/apache/hadoop/hbase/zookeeper/ZKUtil.java  |  30 +++---
 .../hadoop/hbase/zookeeper/ZooKeeperWatcher.java   |   7 +-
 .../hbase/TestInterfaceAudienceAnnotations.java    |  11 +--
 .../hadoop/hbase/client/TestAsyncProcess.java      |  42 ++++-----
 .../hadoop/hbase/client/TestClientNoCluster.java   |   8 +-
 .../hadoop/hbase/filter/TestKeyOnlyFilter.java     |   2 +-
 .../hbase/ipc/TestHBaseRpcControllerImpl.java      |   2 +-
 .../apache/hadoop/hbase/util/BuilderStyleTest.java |   2 +-
 .../hbase/zookeeper/TestInstancePending.java       |   4 +-
 .../java/org/apache/hadoop/hbase/CellUtil.java     |   2 +-
 .../java/org/apache/hadoop/hbase/ChoreService.java |  10 +-
 .../apache/hadoop/hbase/CompoundConfiguration.java |   7 +-
 .../java/org/apache/hadoop/hbase/KeyValue.java     |   6 +-
 .../java/org/apache/hadoop/hbase/KeyValueUtil.java |   2 +-
 .../apache/hadoop/hbase/NamespaceDescriptor.java   |   6 +-
 .../java/org/apache/hadoop/hbase/ServerName.java   |   2 +-
 .../java/org/apache/hadoop/hbase/TableName.java    |   2 +-
 .../main/java/org/apache/hadoop/hbase/TagUtil.java |   8 +-
 .../hadoop/hbase/io/BoundedByteBufferPool.java     |   2 +-
 .../hbase/io/ByteBufferListOutputStream.java       |   4 +-
 .../org/apache/hadoop/hbase/io/ByteBufferPool.java |   2 +-
 .../apache/hadoop/hbase/io/crypto/Encryption.java  |   5 +-
 .../io/encoding/BufferedDataBlockEncoder.java      |   2 +-
 .../hadoop/hbase/io/encoding/RowIndexCodecV1.java  |   2 +-
 .../hadoop/hbase/io/encoding/RowIndexSeekerV1.java |   2 +-
 .../apache/hadoop/hbase/io/util/LRUDictionary.java |   2 +-
 .../apache/hadoop/hbase/io/util/StreamUtils.java   |  12 +--
 .../apache/hadoop/hbase/nio/SingleByteBuff.java    |   2 +-
 .../org/apache/hadoop/hbase/security/User.java     |   3 +-
 .../apache/hadoop/hbase/security/UserProvider.java |   2 +-
 .../hadoop/hbase/trace/SpanReceiverHost.java       |   2 +-
 .../apache/hadoop/hbase/types/StructBuilder.java   |   2 +-
 .../org/apache/hadoop/hbase/util/ArrayUtils.java   |   2 +-
 .../java/org/apache/hadoop/hbase/util/AvlUtil.java |   2 +-
 .../hbase/util/BoundedCompletionService.java       |   4 +-
 .../apache/hadoop/hbase/util/ByteRangeUtils.java   |   4 +-
 .../apache/hadoop/hbase/util/CollectionUtils.java  |   3 +-
 .../hadoop/hbase/util/ConcatenatedLists.java       |   2 +-
 .../hadoop/hbase/util/CoprocessorClassLoader.java  |   4 +-
 .../java/org/apache/hadoop/hbase/util/Counter.java |   2 +-
 .../hadoop/hbase/util/DynamicClassLoader.java      |   2 +-
 .../apache/hadoop/hbase/util/IterableUtils.java    |   2 +-
 .../org/apache/hadoop/hbase/util/KeyLocker.java    |   4 +-
 .../java/org/apache/hadoop/hbase/util/Pair.java    |   2 +-
 .../java/org/apache/hadoop/hbase/util/Threads.java |   2 +-
 .../java/org/apache/hadoop/hbase/util/Triple.java  |   2 +-
 .../apache/hadoop/hbase/util/WeakObjectPool.java   |   5 +-
 .../hbase/util/test/RedundantKVGenerator.java      |  16 ++--
 .../java/org/apache/hadoop/hbase/ClassFinder.java  |  10 +-
 .../org/apache/hadoop/hbase/ResourceChecker.java   |   2 +-
 .../hadoop/hbase/ResourceCheckerJUnitListener.java |   6 +-
 .../java/org/apache/hadoop/hbase/TestCellUtil.java |   8 +-
 .../org/apache/hadoop/hbase/TestClassFinder.java   |   2 +-
 .../hadoop/hbase/TestCompoundConfiguration.java    |   9 +-
 .../java/org/apache/hadoop/hbase/TestKeyValue.java |   4 +-
 .../hadoop/hbase/io/TestBoundedByteBufferPool.java |   2 +-
 .../hadoop/hbase/io/TestTagCompressionContext.java |   4 +-
 .../apache/hadoop/hbase/nio/TestMultiByteBuff.java |   2 +-
 .../hadoop/hbase/types/TestFixedLengthWrapper.java |   8 +-
 .../hbase/types/TestStructNullExtension.java       |   2 +-
 .../hadoop/hbase/types/TestTerminatedWrapper.java  |  14 +--
 .../hadoop/hbase/util/ClassLoaderTestHelper.java   |   4 +-
 .../org/apache/hadoop/hbase/util/TestAvlUtil.java  |   6 +-
 .../org/apache/hadoop/hbase/util/TestBase64.java   |   2 +-
 .../hadoop/hbase/util/TestBoundedArrayQueue.java   |   2 +-
 .../hadoop/hbase/util/TestByteBufferUtils.java     |   2 +-
 .../util/TestByteRangeWithKVSerialization.java     |   2 +-
 .../org/apache/hadoop/hbase/util/TestBytes.java    |   4 +-
 .../hadoop/hbase/util/TestConcatenatedLists.java   |  14 +--
 .../apache/hadoop/hbase/util/TestKeyLocker.java    |   2 +-
 .../hadoop/hbase/util/TestLoadTestKVGenerator.java |   2 +-
 .../hadoop/hbase/util/TestWeakObjectPool.java      |   4 +-
 .../client/coprocessor/AggregationClient.java      |  36 ++++---
 .../hbase/coprocessor/AggregateImplementation.java |  14 +--
 .../coprocessor/ColumnAggregationEndpoint.java     |   2 +-
 .../ColumnAggregationEndpointNullResponse.java     |   2 +-
 .../ColumnAggregationEndpointWithErrors.java       |   2 +-
 .../hadoop/hbase/coprocessor/TestClassLoading.java |  10 +-
 .../hbase/coprocessor/TestCoprocessorEndpoint.java |   8 +-
 .../coprocessor/TestCoprocessorTableEndpoint.java  |   2 +-
 .../TestRegionServerCoprocessorEndpoint.java       |   6 +-
 .../coprocessor/TestRowProcessorEndpoint.java      |  19 ++--
 .../regionserver/SecureBulkLoadEndpointClient.java |   9 +-
 ...HRegionServerBulkLoadWithOldSecureEndpoint.java |   3 +-
 .../regionserver/TestServerCustomProtocol.java     |  14 +--
 ...estReplicationSyncUpToolWithBulkLoadedData.java |   2 +-
 .../coprocessor/example/BulkDeleteEndpoint.java    |  10 +-
 .../coprocessor/example/RowCountEndpoint.java      |   4 +-
 .../hadoop/hbase/mapreduce/IndexBuilder.java       |   2 +-
 .../org/apache/hadoop/hbase/thrift/DemoClient.java |  24 ++---
 .../apache/hadoop/hbase/thrift/HttpDoAsClient.java |   6 +-
 .../apache/hadoop/hbase/thrift2/DemoClient.java    |   6 +-
 .../hadoop/hbase/io/hfile/MemcachedBlockCache.java |   2 +-
 .../hbase/CompatibilitySingletonFactory.java       |   2 +-
 .../hbase/TestCompatibilitySingletonFactory.java   |   5 +-
 .../ipc/MetricsHBaseServerSourceFactoryImpl.java   |   5 +-
 .../MetricsStochasticBalancerSourceImpl.java       |   4 +-
 .../org/apache/hadoop/hbase/metrics/Interns.java   |   4 +-
 .../metrics2/util/MetricSampleQuantiles.java       |   4 +-
 .../hadoop/hbase/test/MetricsAssertHelperImpl.java |   6 +-
 .../hadoop/hbase/DistributedHBaseCluster.java      |  12 +--
 .../apache/hadoop/hbase/HBaseClusterManager.java   |   2 +-
 .../hbase/IntegrationTestDDLMasterFailover.java    |  18 ++--
 .../apache/hadoop/hbase/IntegrationTestIngest.java |   4 +-
 .../hadoop/hbase/IntegrationTestIngestWithACL.java |   2 +-
 .../hadoop/hbase/IntegrationTestIngestWithMOB.java |   4 +-
 .../hbase/IntegrationTestIngestWithTags.java       |   2 +-
 .../IntegrationTestIngestWithVisibilityLabels.java |  18 ++--
 .../hadoop/hbase/IntegrationTestLazyCfLoading.java |   3 +-
 .../hbase/IntegrationTestRegionReplicaPerf.java    |   4 +-
 .../IntegrationTestRegionReplicaReplication.java   |   4 +-
 .../apache/hadoop/hbase/RESTApiClusterManager.java |   2 +-
 .../apache/hadoop/hbase/chaos/actions/Action.java  |   6 +-
 .../hbase/chaos/actions/BatchRestartRsAction.java  |   2 +-
 .../chaos/actions/RestartRandomDataNodeAction.java |   2 +-
 .../chaos/actions/RollingBatchRestartRsAction.java |   6 +-
 .../actions/UnbalanceKillAndRebalanceAction.java   |   6 +-
 .../chaos/actions/UnbalanceRegionsAction.java      |   4 +-
 .../chaos/monkies/PolicyBasedChaosMonkey.java      |   2 +-
 .../hbase/chaos/policies/DoActionsOncePolicy.java  |   2 +-
 .../chaos/policies/PeriodicRandomActionPolicy.java |   4 +-
 .../hbase/mapreduce/IntegrationTestBulkLoad.java   |   4 +-
 .../hbase/mapreduce/IntegrationTestImportTsv.java  |   2 +-
 .../hadoop/hbase/mttr/IntegrationTestMTTR.java     |   8 +-
 .../hbase/test/IntegrationTestBigLinkedList.java   |   8 +-
 .../hbase/test/IntegrationTestLoadAndVerify.java   |   2 +-
 .../hbase/test/IntegrationTestReplication.java     |   2 +-
 .../trace/IntegrationTestSendTraceRequests.java    |   2 +-
 .../codec/prefixtree/decode/ArraySearcherPool.java |   3 +-
 .../codec/prefixtree/decode/PrefixTreeCell.java    |   2 +-
 .../codec/prefixtree/encode/EncoderPoolImpl.java   |   3 +-
 .../codec/prefixtree/encode/other/LongEncoder.java |   2 +-
 .../prefixtree/encode/tokenize/Tokenizer.java      |   2 +-
 .../util/byterange/impl/ByteRangeHashSet.java      |   2 +-
 .../util/byterange/impl/ByteRangeTreeSet.java      |   2 +-
 .../row/data/TestRowDataExerciseFInts.java         |   2 +-
 .../row/data/TestRowDataTrivialWithTags.java       |   2 +-
 .../codec/prefixtree/row/data/TestRowDataUrls.java |   2 +-
 .../timestamp/data/TestTimestampDataBasic.java     |   4 +-
 .../timestamp/data/TestTimestampDataNumbers.java   |   4 +-
 .../timestamp/data/TestTimestampDataRepeats.java   |   4 +-
 .../hadoop/hbase/procedure2/ProcedureExecutor.java |  24 ++---
 .../hbase/procedure2/RootProcedureState.java       |   6 +-
 .../hbase/procedure2/StateMachineProcedure.java    |   2 +-
 .../hbase/procedure2/store/ProcedureStoreBase.java |   3 +-
 .../store/wal/ProcedureWALPrettyPrinter.java       |   2 +-
 .../procedure2/store/wal/WALProcedureStore.java    |   8 +-
 .../hbase/procedure2/ProcedureTestingUtility.java  |   8 +-
 .../hbase/procedure2/TestProcedureExecution.java   |  10 +-
 .../hbase/procedure2/TestProcedureReplayOrder.java |   2 +-
 .../TestProcedureSchedulerConcurrency.java         |   3 +-
 .../hbase/procedure2/TestProcedureSuspended.java   |   2 +-
 .../hbase/procedure2/TestYieldProcedures.java      |   2 +-
 .../hbase/procedure2/util/TestDelayedUtil.java     |  10 +-
 .../hadoop/hbase/util/ForeignExceptionUtil.java    |   2 +-
 .../org/apache/hadoop/hbase/rest/RESTServer.java   |   5 +-
 .../org/apache/hadoop/hbase/rest/RowResource.java  |   2 +-
 .../java/org/apache/hadoop/hbase/rest/RowSpec.java |   5 +-
 .../apache/hadoop/hbase/rest/client/Client.java    |   2 +-
 .../hadoop/hbase/rest/client/RemoteHTable.java     |   9 +-
 .../hadoop/hbase/rest/filter/GzipFilter.java       |   2 +-
 .../rest/filter/RestCsrfPreventionFilter.java      |   4 +-
 .../hadoop/hbase/rest/model/CellSetModel.java      |   2 +-
 .../hadoop/hbase/rest/model/ColumnSchemaModel.java |   2 +-
 .../hbase/rest/model/NamespacesInstanceModel.java  |   6 +-
 .../hadoop/hbase/rest/model/NamespacesModel.java   |   4 +-
 .../apache/hadoop/hbase/rest/model/RowModel.java   |   4 +-
 .../hadoop/hbase/rest/model/ScannerModel.java      |  16 ++--
 .../rest/model/StorageClusterStatusModel.java      |   6 +-
 .../hadoop/hbase/rest/model/TableInfoModel.java    |   2 +-
 .../hadoop/hbase/rest/model/TableListModel.java    |   2 +-
 .../hadoop/hbase/rest/model/TableSchemaModel.java  |   4 +-
 .../hadoop/hbase/rest/PerformanceEvaluation.java   |  10 +-
 .../hadoop/hbase/rest/TestGetAndPutResource.java   |   6 +-
 .../hadoop/hbase/rest/TestMultiRowResource.java    |   2 +-
 .../hbase/rest/TestNamespacesInstanceResource.java |  14 +--
 .../hadoop/hbase/rest/TestScannersWithFilters.java |   2 +-
 .../hadoop/hbase/rest/TestSchemaResource.java      |   2 +-
 .../hadoop/hbase/rest/client/TestRemoteTable.java  |  14 +--
 .../rest/model/TestNamespacesInstanceModel.java    |   2 +-
 .../hadoop/hbase/rsgroup/RSGroupAdminClient.java   |   2 +-
 .../hadoop/hbase/rsgroup/RSGroupAdminServer.java   |   6 +-
 .../hbase/rsgroup/RSGroupBasedLoadBalancer.java    |  25 +++--
 .../balancer/TestRSGroupBasedLoadBalancer.java     |  20 ++--
 .../hadoop/hbase/rsgroup/TestRSGroupsBase.java     |   2 +-
 .../hadoop/hbase/HDFSBlocksDistribution.java       |   8 +-
 .../org/apache/hadoop/hbase/HealthChecker.java     |   2 +-
 .../java/org/apache/hadoop/hbase/JMXListener.java  |   2 +-
 .../org/apache/hadoop/hbase/LocalHBaseCluster.java |   9 +-
 .../hbase/SslRMIClientSocketFactorySecure.java     |   2 +-
 .../hbase/SslRMIServerSocketFactorySecure.java     |   2 +-
 .../apache/hadoop/hbase/ZKNamespaceManager.java    |   2 +-
 .../apache/hadoop/hbase/backup/HFileArchiver.java  |   6 +-
 .../backup/example/HFileArchiveTableMonitor.java   |   2 +-
 .../hbase/client/ClientSideRegionScanner.java      |   2 +-
 .../apache/hadoop/hbase/client/HTableWrapper.java  |   2 +-
 .../hadoop/hbase/client/TableSnapshotScanner.java  |   2 +-
 .../hbase/constraint/ConstraintProcessor.java      |   2 +-
 .../hadoop/hbase/constraint/Constraints.java       |   6 +-
 .../ZKSplitLogManagerCoordination.java             |   2 +-
 .../coordination/ZkSplitLogWorkerCoordination.java |   2 +-
 .../hadoop/hbase/coprocessor/CoprocessorHost.java  |  13 ++-
 .../coprocessor/MultiRowMutationEndpoint.java      |   4 +-
 .../hadoop/hbase/coprocessor/ObserverContext.java  |   4 +-
 .../hbase/errorhandling/ForeignException.java      |   3 +-
 .../errorhandling/ForeignExceptionDispatcher.java  |   3 +-
 .../hadoop/hbase/executor/ExecutorService.java     |   5 +-
 .../hbase/favored/FavoredNodeAssignmentHelper.java |  37 ++++----
 .../hbase/favored/FavoredNodeLoadBalancer.java     |  28 +++---
 .../hadoop/hbase/favored/FavoredNodesPlan.java     |   2 +-
 .../apache/hadoop/hbase/http/HttpRequestLog.java   |   2 +-
 .../org/apache/hadoop/hbase/http/HttpServer.java   |  11 +--
 .../hadoop/hbase/http/lib/StaticUserWebFilter.java |   2 +-
 .../java/org/apache/hadoop/hbase/io/FileLink.java  |   2 +-
 .../java/org/apache/hadoop/hbase/io/HFileLink.java |   2 +-
 .../io/asyncfs/FanOutOneBlockAsyncDFSOutput.java   |   2 +-
 .../FanOutOneBlockAsyncDFSOutputHelper.java        |   3 +-
 .../hadoop/hbase/io/hfile/BlockCacheUtil.java      |   5 +-
 .../io/hfile/CacheableDeserializerIdManager.java   |   3 +-
 .../hbase/io/hfile/CompoundBloomFilterWriter.java  |   2 +-
 .../org/apache/hadoop/hbase/io/hfile/HFile.java    |   4 +-
 .../apache/hadoop/hbase/io/hfile/HFileBlock.java   |   3 +-
 .../hadoop/hbase/io/hfile/HFileBlockIndex.java     |  15 ++-
 .../hadoop/hbase/io/hfile/HFilePrettyPrinter.java  |  10 +-
 .../hadoop/hbase/io/hfile/HFileReaderImpl.java     |  10 +-
 .../hadoop/hbase/io/hfile/HFileWriterImpl.java     |   8 +-
 .../hadoop/hbase/io/hfile/PrefetchExecutor.java    |   3 +-
 .../hadoop/hbase/io/hfile/bucket/BucketCache.java  |  17 ++--
 .../hbase/io/hfile/bucket/UniqueIndexMap.java      |   4 +-
 .../hadoop/hbase/io/util/MemorySizeUtil.java       |   4 +-
 .../apache/hadoop/hbase/ipc/FifoRpcScheduler.java  |   2 +-
 .../org/apache/hadoop/hbase/ipc/RpcServer.java     |  18 ++--
 .../apache/hadoop/hbase/ipc/SimpleRpcServer.java   |   7 +-
 .../hadoop/hbase/mapred/GroupingTableMap.java      |   2 +-
 .../apache/hadoop/hbase/mapreduce/CopyTable.java   |   2 +-
 .../DefaultVisibilityExpressionResolver.java       |   2 +-
 .../hbase/mapreduce/GroupingTableMapper.java       |   2 +-
 .../hadoop/hbase/mapreduce/HFileOutputFormat2.java |  24 ++---
 .../apache/hadoop/hbase/mapreduce/HashTable.java   |   6 +-
 .../org/apache/hadoop/hbase/mapreduce/Import.java  |   4 +-
 .../apache/hadoop/hbase/mapreduce/ImportTsv.java   |  10 +-
 .../hbase/mapreduce/KeyValueSortReducer.java       |   2 +-
 .../hbase/mapreduce/LoadIncrementalHFiles.java     |   6 +-
 .../hbase/mapreduce/MultiHFileOutputFormat.java    |   5 +-
 .../hbase/mapreduce/MultiTableInputFormat.java     |   2 +-
 .../hbase/mapreduce/MultiTableInputFormatBase.java |   6 +-
 .../hadoop/hbase/mapreduce/PutSortReducer.java     |   4 +-
 .../hbase/mapreduce/ResultSerialization.java       |   2 +-
 .../hbase/mapreduce/TableInputFormatBase.java      |   9 +-
 .../hadoop/hbase/mapreduce/TableMapReduceUtil.java |   8 +-
 .../hbase/mapreduce/TableSnapshotInputFormat.java  |   2 +-
 .../mapreduce/TableSnapshotInputFormatImpl.java    |   4 +-
 .../hadoop/hbase/mapreduce/TextSortReducer.java    |   4 +-
 .../hadoop/hbase/mapreduce/TsvImporterMapper.java  |   2 +-
 .../hadoop/hbase/mapreduce/WALInputFormat.java     |   4 +-
 .../apache/hadoop/hbase/mapreduce/WALPlayer.java   |   2 +-
 .../hadoop/hbase/master/AssignmentManager.java     |  56 ++++++-----
 .../hbase/master/AssignmentVerificationReport.java |  48 ++++------
 .../org/apache/hadoop/hbase/master/BulkReOpen.java |   2 +-
 .../apache/hadoop/hbase/master/CatalogJanitor.java |  20 ++--
 .../hbase/master/ClusterSchemaServiceImpl.java     |   2 +-
 .../hbase/master/ClusterStatusPublisher.java       |   7 +-
 .../org/apache/hadoop/hbase/master/DeadServer.java |   8 +-
 .../hadoop/hbase/master/GeneralBulkAssigner.java   |   7 +-
 .../org/apache/hadoop/hbase/master/HMaster.java    |  19 ++--
 .../hadoop/hbase/master/MasterMetaBootstrap.java   |   4 +-
 .../hbase/master/MasterMobCompactionThread.java    |   2 +-
 .../hadoop/hbase/master/MasterRpcServices.java     |   6 +-
 .../hadoop/hbase/master/MasterWalManager.java      |   8 +-
 .../apache/hadoop/hbase/master/RackManager.java    |   2 +-
 .../hbase/master/RegionPlacementMaintainer.java    |  35 +++----
 .../apache/hadoop/hbase/master/RegionStates.java   |  84 +++++++----------
 .../apache/hadoop/hbase/master/ServerManager.java  |  33 +++----
 .../master/SnapshotOfRegionAssignmentFromMeta.java |  26 +++---
 .../hadoop/hbase/master/SplitLogManager.java       |  23 +++--
 .../hbase/master/balancer/BaseLoadBalancer.java    |  63 +++++++------
 .../hbase/master/balancer/ClusterLoadState.java    |   2 +-
 .../master/balancer/RegionLocationFinder.java      |  11 +--
 .../hbase/master/balancer/SimpleLoadBalancer.java  |  12 +--
 .../master/balancer/StochasticLoadBalancer.java    |  10 +-
 .../hadoop/hbase/master/cleaner/CleanerChore.java  |   2 +-
 .../master/cleaner/ReplicationMetaCleaner.java     |   2 +-
 .../master/cleaner/ReplicationZKNodeCleaner.java   |   2 +-
 .../hadoop/hbase/master/locking/LockProcedure.java |   2 +-
 .../master/normalizer/SimpleRegionNormalizer.java  |   2 +-
 .../master/procedure/CloneSnapshotProcedure.java   |   9 +-
 .../master/procedure/CreateTableProcedure.java     |   5 +-
 .../master/procedure/DeleteTableProcedure.java     |   4 +-
 .../master/procedure/EnableTableProcedure.java     |   5 +-
 .../master/procedure/MasterDDLOperationHelper.java |   4 +-
 .../procedure/MergeTableRegionsProcedure.java      |   2 +-
 .../master/procedure/ModifyTableProcedure.java     |   2 +-
 .../master/procedure/RestoreSnapshotProcedure.java |  13 +--
 .../master/procedure/ServerCrashProcedure.java     |   8 +-
 .../procedure/SplitTableRegionProcedure.java       |  10 +-
 .../master/procedure/TruncateTableProcedure.java   |   4 +-
 .../snapshot/DisabledTableSnapshotHandler.java     |   2 +-
 .../snapshot/EnabledTableSnapshotHandler.java      |   2 +-
 .../hbase/master/snapshot/SnapshotFileCache.java   |   7 +-
 .../hbase/master/snapshot/SnapshotManager.java     |  13 ++-
 .../hbase/master/snapshot/TakeSnapshotHandler.java |   2 +-
 .../hadoop/hbase/mob/DefaultMobStoreCompactor.java |   2 +-
 .../hadoop/hbase/mob/DefaultMobStoreFlusher.java   |   4 +-
 .../java/org/apache/hadoop/hbase/mob/MobFile.java  |   4 +-
 .../org/apache/hadoop/hbase/mob/MobFileCache.java  |   6 +-
 .../java/org/apache/hadoop/hbase/mob/MobUtils.java |   6 +-
 .../PartitionedMobCompactionRequest.java           |   2 +-
 .../hbase/monitoring/MonitoredRPCHandlerImpl.java  |   2 +-
 .../hadoop/hbase/monitoring/MonitoredTaskImpl.java |   2 +-
 .../hadoop/hbase/monitoring/TaskMonitor.java       |   6 +-
 .../hbase/namespace/NamespaceStateManager.java     |   2 +-
 .../namespace/NamespaceTableAndRegionInfo.java     |   2 +-
 .../procedure/MasterProcedureManagerHost.java      |   3 +-
 .../apache/hadoop/hbase/procedure/Procedure.java   |   6 +-
 .../hbase/procedure/ProcedureCoordinator.java      |   4 +-
 .../hbase/procedure/ProcedureManagerHost.java      |   6 +-
 .../hadoop/hbase/procedure/ProcedureMember.java    |   2 +-
 .../flush/MasterFlushTableProcedureManager.java    |   4 +-
 .../RegionServerFlushTableProcedureManager.java    |   6 +-
 .../hbase/protobuf/ReplicationProtbufUtil.java     |   4 +-
 .../hadoop/hbase/quotas/MasterQuotaManager.java    |   8 +-
 .../org/apache/hadoop/hbase/quotas/QuotaCache.java |  13 +--
 .../org/apache/hadoop/hbase/quotas/QuotaUtil.java  |   4 +-
 .../apache/hadoop/hbase/quotas/UserQuotaState.java |   4 +-
 .../regionserver/AbstractMultiFileWriter.java      |   4 +-
 .../AnnotationReadingPriorityFunction.java         |  12 +--
 .../hbase/regionserver/BaseRowProcessor.java       |   2 +-
 .../apache/hadoop/hbase/regionserver/CellSet.java  |   2 +-
 .../hbase/regionserver/CompactSplitThread.java     |   4 +-
 .../hbase/regionserver/CompactingMemStore.java     |   2 +-
 .../hadoop/hbase/regionserver/CompactionTool.java  |   6 +-
 .../regionserver/CompositeImmutableSegment.java    |   4 +-
 .../regionserver/DateTieredMultiFileWriter.java    |   3 +-
 .../hadoop/hbase/regionserver/DefaultMemStore.java |   4 +-
 .../regionserver/DefaultStoreFileManager.java      |  16 ++--
 .../hbase/regionserver/DefaultStoreFlusher.java    |   2 +-
 .../regionserver/FlushAllLargeStoresPolicy.java    |   2 +-
 .../FlushNonSloppyStoresFirstPolicy.java           |   2 +-
 .../hadoop/hbase/regionserver/HMobStore.java       |   6 +-
 .../apache/hadoop/hbase/regionserver/HRegion.java  | 101 +++++++++-----------
 .../hbase/regionserver/HRegionFileSystem.java      |   4 +-
 .../hadoop/hbase/regionserver/HRegionServer.java   |  26 +++---
 .../apache/hadoop/hbase/regionserver/HStore.java   |  54 ++++++-----
 .../hbase/regionserver/HeapMemoryManager.java      |   2 +-
 .../hbase/regionserver/ImmutableSegment.java       |   2 +-
 .../hadoop/hbase/regionserver/KeyValueHeap.java    |   6 +-
 .../apache/hadoop/hbase/regionserver/Leases.java   |   2 +-
 .../hadoop/hbase/regionserver/LogRoller.java       |   3 +-
 .../hadoop/hbase/regionserver/LruHashMap.java      |   6 +-
 .../MemStoreCompactorSegmentsIterator.java         |   2 +-
 .../hadoop/hbase/regionserver/MemStoreFlusher.java |  10 +-
 .../hadoop/hbase/regionserver/MemStoreLABImpl.java |   2 +-
 .../regionserver/MemStoreSegmentsIterator.java     |   4 +-
 .../regionserver/MultiRowMutationProcessor.java    |   2 +-
 .../MultiVersionConcurrencyControl.java            |   2 +-
 .../hadoop/hbase/regionserver/RSRpcServices.java   |  34 +++----
 .../hbase/regionserver/RegionCoprocessorHost.java  |   4 +-
 .../hbase/regionserver/RegionServerAccounting.java |   2 +-
 .../regionserver/RegionServicesForStores.java      |   2 +-
 .../hbase/regionserver/SecureBulkLoadManager.java  |  12 +--
 .../apache/hadoop/hbase/regionserver/Segment.java  |   4 +-
 .../hadoop/hbase/regionserver/SegmentFactory.java  |   2 +-
 .../hbase/regionserver/ServerNonceManager.java     |   3 +-
 .../hadoop/hbase/regionserver/ShutdownHook.java    |   2 +-
 .../hbase/regionserver/StoreFileScanner.java       |   2 +-
 .../hadoop/hbase/regionserver/StoreFlusher.java    |   2 +-
 .../hadoop/hbase/regionserver/StoreScanner.java    |  14 ++-
 .../regionserver/StorefileRefresherChore.java      |   2 +-
 .../hbase/regionserver/StripeMultiFileWriter.java  |   6 +-
 .../hbase/regionserver/StripeStoreEngine.java      |   2 +-
 .../hbase/regionserver/StripeStoreFileManager.java |  55 +++++------
 .../hbase/regionserver/StripeStoreFlusher.java     |   2 +-
 .../compactions/CompactionRequest.java             |   2 +-
 .../hbase/regionserver/compactions/Compactor.java  |   6 +-
 .../compactions/DateTieredCompactionPolicy.java    |   8 +-
 .../compactions/ExploringCompactionPolicy.java     |  10 +-
 .../compactions/FIFOCompactionPolicy.java          |   2 +-
 .../compactions/SortedCompactionPolicy.java        |   4 +-
 .../compactions/StripeCompactionPolicy.java        |  10 +-
 .../querymatcher/ScanDeleteTracker.java            |   2 +-
 .../snapshot/RegionServerSnapshotManager.java      |   6 +-
 .../PressureAwareThroughputController.java         |   3 +-
 .../hbase/regionserver/wal/AbstractFSWAL.java      |   4 +-
 .../hadoop/hbase/regionserver/wal/AsyncFSWAL.java  |   2 +-
 .../regionserver/wal/AsyncProtobufLogWriter.java   |   2 +-
 .../hadoop/hbase/regionserver/wal/FSHLog.java      |   4 +-
 .../hbase/regionserver/wal/ProtobufLogReader.java  |   2 +-
 .../regionserver/wal/SecureProtobufLogReader.java  |   2 +-
 .../regionserver/wal/SequenceIdAccounting.java     |   4 +-
 .../hadoop/hbase/regionserver/wal/WALEdit.java     |   4 +-
 .../hbase/regionserver/wal/WALEditsReplaySink.java |   4 +-
 .../hbase/replication/BulkLoadCellFilter.java      |   2 +-
 .../hbase/replication/ChainWALEntryFilter.java     |   2 +-
 .../replication/HBaseReplicationEndpoint.java      |   4 +-
 .../regionserver/DumpReplicationQueues.java        |   6 +-
 .../HBaseInterClusterReplicationEndpoint.java      |   8 +-
 .../replication/regionserver/HFileReplicator.java  |   8 +-
 .../replication/regionserver/MetricsSource.java    |   2 +-
 .../RegionReplicaReplicationEndpoint.java          |   8 +-
 .../replication/regionserver/Replication.java      |   2 +-
 .../replication/regionserver/ReplicationLoad.java  |   6 +-
 .../replication/regionserver/ReplicationSink.java  |  23 ++---
 .../regionserver/ReplicationSource.java            |   8 +-
 .../regionserver/ReplicationSourceManager.java     |  26 +++---
 .../hbase/security/access/AccessControlLists.java  |  18 ++--
 .../hbase/security/access/AccessController.java    |  14 ++-
 .../hbase/security/access/TableAuthManager.java    |  13 ++-
 .../hbase/security/access/ZKPermissionWatcher.java |   3 +-
 .../token/AuthenticationTokenSecretManager.java    |   6 +-
 .../hadoop/hbase/security/token/TokenUtil.java     |   2 +-
 .../DefaultVisibilityLabelServiceImpl.java         |  40 ++++----
 .../DefinedSetFilterScanLabelGenerator.java        |   8 +-
 .../visibility/EnforcingScanLabelGenerator.java    |   4 +-
 .../security/visibility/ExpressionParser.java      |   4 +-
 .../visibility/FeedUserAuthScanLabelGenerator.java |   4 +-
 .../security/visibility/VisibilityController.java  |  20 ++--
 .../security/visibility/VisibilityLabelsCache.java |  19 ++--
 .../visibility/VisibilityReplicationEndpoint.java  |   6 +-
 .../visibility/VisibilityScanDeleteTracker.java    |  45 ++++-----
 .../hbase/security/visibility/VisibilityUtils.java |   8 +-
 .../expression/NonLeafExpressionNode.java          |   4 +-
 .../hadoop/hbase/snapshot/ExportSnapshot.java      |  15 ++-
 .../hbase/snapshot/RestoreSnapshotHelper.java      |  34 +++----
 .../apache/hadoop/hbase/snapshot/SnapshotInfo.java |   3 +-
 .../hadoop/hbase/snapshot/SnapshotManifest.java    |  10 +-
 .../hadoop/hbase/snapshot/SnapshotManifestV1.java  |   5 +-
 .../hadoop/hbase/snapshot/SnapshotManifestV2.java  |   5 +-
 .../hbase/snapshot/SnapshotReferenceUtil.java      |   5 +-
 .../java/org/apache/hadoop/hbase/tool/Canary.java  |  30 +++---
 .../hbase/util/BoundedPriorityBlockingQueue.java   |   2 +-
 .../hadoop/hbase/util/CollectionBackedScanner.java |   2 +-
 .../apache/hadoop/hbase/util/ConnectionCache.java  |   5 +-
 .../apache/hadoop/hbase/util/EncryptionTest.java   |   7 +-
 .../org/apache/hadoop/hbase/util/FSHDFSUtils.java  |   2 +-
 .../apache/hadoop/hbase/util/FSRegionScanner.java  |   4 +-
 .../hadoop/hbase/util/FSTableDescriptors.java      |   9 +-
 .../java/org/apache/hadoop/hbase/util/FSUtils.java |  35 ++++---
 .../org/apache/hadoop/hbase/util/HBaseFsck.java    |  77 +++++++--------
 .../java/org/apache/hadoop/hbase/util/IdLock.java  |   3 +-
 .../apache/hadoop/hbase/util/IdReadWriteLock.java  |   3 +-
 .../org/apache/hadoop/hbase/util/JvmVersion.java   |   2 +-
 .../hadoop/hbase/util/ModifyRegionUtils.java       |   8 +-
 .../hadoop/hbase/util/MunkresAssignment.java       |  13 ++-
 .../org/apache/hadoop/hbase/util/RegionMover.java  |  15 ++-
 .../hadoop/hbase/util/RegionSizeCalculator.java    |   2 +-
 .../hadoop/hbase/util/RegionSplitCalculator.java   |   8 +-
 .../apache/hadoop/hbase/util/RegionSplitter.java   |   2 +-
 .../hadoop/hbase/util/ServerCommandLine.java       |   2 +-
 .../hadoop/hbase/util/SortedCopyOnWriteSet.java    |  20 ++--
 .../org/apache/hadoop/hbase/util/SortedList.java   |  18 ++--
 .../hbase/util/hbck/HFileCorruptionChecker.java    |  34 +++----
 .../hadoop/hbase/wal/AbstractFSWALProvider.java    |   2 +-
 .../hadoop/hbase/wal/BoundedGroupingStrategy.java  |   3 +-
 .../hadoop/hbase/wal/DisabledWALProvider.java      |   5 +-
 .../hadoop/hbase/wal/RegionGroupingProvider.java   |   2 +-
 .../org/apache/hadoop/hbase/wal/WALFactory.java    |   4 +-
 .../java/org/apache/hadoop/hbase/wal/WALKey.java   |  10 +-
 .../apache/hadoop/hbase/wal/WALPrettyPrinter.java  |   8 +-
 .../org/apache/hadoop/hbase/wal/WALSplitter.java   |  52 +++++------
 .../hbase/zookeeper/DrainingServerTracker.java     |   2 +-
 .../hbase/zookeeper/MiniZooKeeperCluster.java      |   6 +-
 .../hbase/zookeeper/RegionServerTracker.java       |   5 +-
 .../hadoop/hbase/zookeeper/ZKServerTool.java       |   2 +-
 .../main/resources/hbase-webapps/master/table.jsp  |  10 +-
 .../apache/hadoop/hbase/HBaseTestingUtility.java   |  14 +--
 .../java/org/apache/hadoop/hbase/HTestConst.java   |   2 +-
 .../org/apache/hadoop/hbase/MetaMockingUtil.java   |   2 +-
 .../org/apache/hadoop/hbase/MiniHBaseCluster.java  |   4 +-
 .../hadoop/hbase/MockRegionServerServices.java     |   4 +-
 .../apache/hadoop/hbase/MultithreadedTestUtil.java |   2 +-
 .../apache/hadoop/hbase/PerformanceEvaluation.java |  10 +-
 .../hadoop/hbase/PerformanceEvaluationCommons.java |   2 +-
 .../apache/hadoop/hbase/TestCheckTestClasses.java  |   2 +-
 .../hadoop/hbase/TestGlobalMemStoreSize.java       |   2 +-
 .../hadoop/hbase/TestHDFSBlocksDistribution.java   |   2 +-
 .../hbase/TestMetaTableAccessorNoCluster.java      |   6 +-
 .../hbase/TestPartialResultsFromClientSide.java    |   2 +-
 .../apache/hadoop/hbase/TestRegionRebalancing.java |   2 +-
 .../TestServerSideScanMetricsFromClientSide.java   |   2 +-
 .../hbase/TestStochasticBalancerJmxMetrics.java    |   4 +-
 .../hadoop/hbase/backup/TestHFileArchiving.java    |   6 +-
 .../example/TestZooKeeperTableArchiveClient.java   |  10 +-
 .../org/apache/hadoop/hbase/client/TestAdmin1.java |   6 +-
 .../org/apache/hadoop/hbase/client/TestAdmin2.java |   2 +-
 .../TestAvoidCellReferencesIntoShippedBlocks.java  |   6 +-
 .../hbase/client/TestBlockEvictionFromClient.java  |   4 +-
 .../hbase/client/TestClientOperationInterrupt.java |   2 +-
 .../hadoop/hbase/client/TestConnectionUtils.java   |   4 +-
 .../apache/hadoop/hbase/client/TestFastFail.java   |   2 +-
 .../hadoop/hbase/client/TestFromClientSide.java    |  10 +-
 .../hadoop/hbase/client/TestFromClientSide3.java   |   8 +-
 .../hbase/client/TestHBaseAdminNoCluster.java      |   2 +-
 .../org/apache/hadoop/hbase/client/TestHCM.java    |   4 +-
 .../hadoop/hbase/client/TestHTableMultiplexer.java |   2 +-
 .../hbase/client/TestIllegalTableDescriptor.java   |   2 +-
 .../hbase/client/TestIncrementsFromClientSide.java |   2 +-
 .../hbase/client/TestIntraRowPagination.java       |   6 +-
 .../hadoop/hbase/client/TestMultiParallel.java     |  24 ++---
 .../hbase/client/TestReplicaWithCluster.java       |   4 +-
 .../hadoop/hbase/client/TestReplicasClient.java    |   8 +-
 .../client/TestRestoreSnapshotFromClient.java      |   2 +-
 .../hbase/client/TestScannersFromClientSide.java   |  38 ++++----
 .../hbase/client/TestSnapshotFromClient.java       |   4 +-
 .../hadoop/hbase/client/TestSnapshotMetadata.java  |   6 +-
 .../hadoop/hbase/client/TestTimestampsFilter.java  |   4 +-
 .../client/replication/TestReplicationAdmin.java   |  26 +++---
 .../TestReplicationAdminWithClusters.java          |   3 +-
 .../hadoop/hbase/constraint/TestConstraints.java   |  10 +-
 .../coprocessor/TestCoprocessorInterface.java      |   2 +-
 .../hbase/coprocessor/TestHTableWrapper.java       |   2 +-
 .../coprocessor/TestOpenTableInCoprocessor.java    |   2 +-
 .../coprocessor/TestRegionObserverBypass.java      |   2 +-
 .../coprocessor/TestRegionObserverInterface.java   |   2 +-
 .../hadoop/hbase/coprocessor/TestWALObserver.java  |   9 +-
 .../favored/TestFavoredNodeAssignmentHelper.java   |  45 +++++----
 .../hbase/filter/TestColumnPrefixFilter.java       |  26 +++---
 .../hadoop/hbase/filter/TestColumnRangeFilter.java |  18 ++--
 .../hbase/filter/TestDependentColumnFilter.java    |   6 +-
 .../org/apache/hadoop/hbase/filter/TestFilter.java |  40 ++++----
 .../apache/hadoop/hbase/filter/TestFilterList.java |   8 +-
 .../filter/TestFilterListOrOperatorWithBlkCnt.java |   8 +-
 .../hbase/filter/TestFilterSerialization.java      |  18 ++--
 .../hbase/filter/TestFilterWithScanLimits.java     |   2 +-
 .../hadoop/hbase/filter/TestFilterWrapper.java     |   4 +-
 .../TestFirstKeyValueMatchingQualifiersFilter.java |   2 +-
 .../filter/TestFuzzyRowAndColumnRangeFilter.java   |   4 +-
 .../hbase/filter/TestFuzzyRowFilterEndToEnd.java   |  24 ++---
 .../hbase/filter/TestInvocationRecordFilter.java   |  12 +--
 .../hbase/filter/TestMultiRowRangeFilter.java      |  60 ++++++------
 .../filter/TestMultipleColumnPrefixFilter.java     |  34 ++++---
 .../filter/TestSingleColumnValueExcludeFilter.java |   2 +-
 .../apache/hadoop/hbase/http/TestGlobalFilter.java |   2 +-
 .../apache/hadoop/hbase/http/TestHttpServer.java   |   6 +-
 .../apache/hadoop/hbase/http/TestPathFilter.java   |   2 +-
 .../hadoop/hbase/http/resource/JerseyResource.java |   2 +-
 .../hadoop/hbase/http/ssl/KeyStoreTestUtil.java    |   2 +-
 .../org/apache/hadoop/hbase/io/TestFileLink.java   |   4 +-
 .../hadoop/hbase/io/TestHalfStoreFileReader.java   |   2 +-
 .../io/encoding/TestBufferedDataBlockEncoder.java  |   2 +-
 .../hbase/io/encoding/TestChangingEncoding.java    |   3 +-
 .../hbase/io/encoding/TestDataBlockEncoders.java   |   9 +-
 .../hbase/io/encoding/TestEncodedSeekers.java      |   2 +-
 .../hadoop/hbase/io/encoding/TestPrefixTree.java   |   4 +-
 .../hbase/io/encoding/TestPrefixTreeEncoding.java  |   7 +-
 .../io/encoding/TestSeekBeforeWithReverseScan.java |   4 +-
 .../io/encoding/TestSeekToBlockWithEncoders.java   |  20 ++--
 .../hadoop/hbase/io/hfile/CacheTestUtils.java      |   4 +-
 .../hadoop/hbase/io/hfile/RandomDistribution.java  |   4 +-
 .../hadoop/hbase/io/hfile/TestCacheOnWrite.java    |  13 ++-
 .../hbase/io/hfile/TestFixedFileTrailer.java       |   2 +-
 .../hadoop/hbase/io/hfile/TestHFileBlock.java      |  24 +++--
 .../hadoop/hbase/io/hfile/TestHFileBlockIndex.java |   8 +-
 .../hbase/io/hfile/TestHFileDataBlockEncoder.java  |   3 +-
 .../TestHFileInlineToRootChunkConversion.java      |   2 +-
 .../hadoop/hbase/io/hfile/TestHFileWriterV3.java   |   4 +-
 .../io/hfile/TestLazyDataBlockDecompression.java   |   2 +-
 .../apache/hadoop/hbase/io/hfile/TestReseekTo.java |   4 +-
 .../hbase/io/hfile/TestScannerFromBucketCache.java |   6 +-
 .../hfile/TestScannerSelectionUsingKeyRange.java   |   6 +-
 .../io/hfile/TestScannerSelectionUsingTTL.java     |   4 +-
 .../apache/hadoop/hbase/io/hfile/TestSeekTo.java   |   2 +-
 .../hbase/io/hfile/bucket/TestBucketCache.java     |   4 +-
 .../io/hfile/bucket/TestBucketWriterThread.java    |   2 +-
 .../hadoop/hbase/ipc/TestSimpleRpcScheduler.java   |   4 +-
 .../hbase/mapred/TestTableMapReduceUtil.java       |   2 +-
 .../mapreduce/MultiTableInputFormatTestBase.java   |   2 +-
 .../hadoop/hbase/mapreduce/NMapInputFormat.java    |   5 +-
 .../hbase/mapreduce/TestGroupingTableMapper.java   |   2 +-
 .../hbase/mapreduce/TestHFileOutputFormat2.java    |  14 +--
 .../hbase/mapreduce/TestHRegionPartitioner.java    |   4 +-
 .../hadoop/hbase/mapreduce/TestHashTable.java      |   3 +-
 .../hadoop/hbase/mapreduce/TestImportExport.java   |   2 +-
 .../TestImportTSVWithOperationAttributes.java      |   2 +-
 .../hbase/mapreduce/TestImportTSVWithTTLs.java     |   2 +-
 .../TestImportTSVWithVisibilityLabels.java         |   6 +-
 .../hadoop/hbase/mapreduce/TestImportTsv.java      |   6 +-
 .../hbase/mapreduce/TestImportTsvParser.java       |   2 +-
 .../hbase/mapreduce/TestLoadIncrementalHFiles.java |   4 +-
 .../mapreduce/TestMultiHFileOutputFormat.java      |   4 +-
 .../hadoop/hbase/mapreduce/TestRowCounter.java     |   2 +-
 .../mapreduce/TestSimpleTotalOrderPartitioner.java |   3 +-
 .../hadoop/hbase/mapreduce/TestTableSplit.java     |   4 +-
 .../hbase/mapreduce/TestTimeRangeMapRed.java       |   7 +-
 .../hadoop/hbase/mapreduce/TestWALPlayer.java      |   2 +-
 .../hbase/mapreduce/TestWALRecordReader.java       |   3 +-
 .../hadoop/hbase/master/MockRegionServer.java      |  13 +--
 .../hbase/master/TestAssignmentListener.java       |   6 +-
 .../master/TestAssignmentManagerOnCluster.java     |   8 +-
 .../hadoop/hbase/master/TestCatalogJanitor.java    |  11 +--
 .../hbase/master/TestClusterStatusPublisher.java   |  10 +-
 .../hbase/master/TestDistributedLogSplitting.java  |  14 +--
 .../hadoop/hbase/master/TestMasterNoCluster.java   |   2 +-
 .../TestMasterOperationsForRegionReplicas.java     |   8 +-
 .../hbase/master/TestMasterStatusServlet.java      |   7 +-
 .../hadoop/hbase/master/TestMasterWalManager.java  |   2 +-
 .../hadoop/hbase/master/TestRegionPlacement.java   |  11 +--
 .../hadoop/hbase/master/TestRegionPlacement2.java  |  12 +--
 .../hadoop/hbase/master/TestRollingRestart.java    |   4 +-
 .../hbase/master/balancer/BalancerTestBase.java    |  38 ++++----
 .../master/balancer/TestBaseLoadBalancer.java      |  44 +++++----
 .../master/balancer/TestDefaultLoadBalancer.java   |  10 +-
 .../master/balancer/TestRegionLocationFinder.java  |   4 +-
 .../balancer/TestStochasticLoadBalancer.java       |  13 ++-
 .../hbase/master/cleaner/TestLogsCleaner.java      |   2 +-
 .../cleaner/TestReplicationHFileCleaner.java       |   6 +-
 .../procedure/MasterProcedureTestingUtility.java   |   2 +-
 .../TestMasterProcedureSchedulerConcurrency.java   |   4 +-
 .../master/snapshot/TestSnapshotFileCache.java     |   4 +-
 .../master/snapshot/TestSnapshotHFileCleaner.java  |   2 +-
 .../hbase/mob/compactions/TestMobCompactor.java    |   4 +-
 .../compactions/TestPartitionedMobCompactor.java   |   6 +-
 .../procedure/SimpleMasterProcedureManager.java    |   2 +-
 .../hbase/procedure/SimpleRSProcedureManager.java  |   6 +-
 .../hadoop/hbase/procedure/TestProcedure.java      |   8 +-
 .../hbase/procedure/TestProcedureManager.java      |   2 +-
 .../hadoop/hbase/procedure/TestZKProcedure.java    |  14 ++-
 .../procedure/TestZKProcedureControllers.java      |  14 ++-
 .../hbase/protobuf/TestReplicationProtobuf.java    |   8 +-
 .../AbstractTestDateTieredCompactionPolicy.java    |   2 +-
 .../hbase/regionserver/DataBlockEncodingTool.java  |  13 ++-
 .../regionserver/EncodedSeekPerformanceTest.java   |   4 +-
 .../hbase/regionserver/KeyValueScanFixture.java    |   2 +-
 .../hadoop/hbase/regionserver/MockStoreFile.java   |   2 +-
 .../hbase/regionserver/OOMERegionServer.java       |   2 +-
 .../hadoop/hbase/regionserver/RegionAsTable.java   |   4 +-
 .../hbase/regionserver/TestAtomicOperation.java    |   6 +-
 .../hadoop/hbase/regionserver/TestBlocksRead.java  |   4 +-
 .../hbase/regionserver/TestBlocksScanned.java      |   2 +-
 .../hadoop/hbase/regionserver/TestBulkLoad.java    |  13 ++-
 .../regionserver/TestCacheOnWriteInSchema.java     |   2 +-
 .../hbase/regionserver/TestColumnSeeking.java      |  20 ++--
 .../hbase/regionserver/TestCompactingMemStore.java |   8 +-
 .../hadoop/hbase/regionserver/TestCompaction.java  |  24 +++--
 .../TestCompactionArchiveConcurrentClose.java      |   2 +-
 .../TestCompactionArchiveIOException.java          |   2 +-
 .../hbase/regionserver/TestCompactionPolicy.java   |  10 +-
 .../hbase/regionserver/TestCompactionState.java    |   2 +-
 .../regionserver/TestCompoundBloomFilter.java      |   2 +-
 .../regionserver/TestCorruptedRegionStoreFile.java |   2 +-
 .../regionserver/TestDefaultCompactSelection.java  |   4 +-
 .../hbase/regionserver/TestDefaultMemStore.java    |  18 ++--
 .../regionserver/TestEncryptionKeyRotation.java    |   4 +-
 .../regionserver/TestEncryptionRandomKeying.java   |   2 +-
 .../regionserver/TestEndToEndSplitTransaction.java |   4 +-
 .../hbase/regionserver/TestFSErrorsExposed.java    |   5 +-
 .../regionserver/TestGetClosestAtOrBefore.java     |   4 +-
 .../hadoop/hbase/regionserver/TestHMobStore.java   |  17 ++--
 .../hadoop/hbase/regionserver/TestHRegion.java     | 103 ++++++++++-----------
 .../hbase/regionserver/TestHRegionOnCluster.java   |   2 +-
 .../regionserver/TestHRegionReplayEvents.java      |  11 +--
 .../regionserver/TestHRegionServerBulkLoad.java    |   5 +-
 .../TestHRegionServerBulkLoadWithOldClient.java    |   3 +-
 .../hbase/regionserver/TestJoinedScanners.java     |   2 +-
 .../hadoop/hbase/regionserver/TestKeepDeletes.java |  20 ++--
 .../hbase/regionserver/TestKeyValueHeap.java       |  12 +--
 .../hbase/regionserver/TestMajorCompaction.java    |  11 +--
 .../hadoop/hbase/regionserver/TestMemStoreLAB.java |   2 +-
 .../hadoop/hbase/regionserver/TestMinVersions.java |   2 +-
 .../TestMiniBatchOperationInProgress.java          |   6 +-
 .../hbase/regionserver/TestMultiColumnScanner.java |  16 ++--
 .../hbase/regionserver/TestRegionFavoredNodes.java |   3 +-
 .../hbase/regionserver/TestRegionIncrement.java    |   4 +-
 .../TestRegionMergeTransactionOnCluster.java       |   6 +-
 .../regionserver/TestRegionReplicaFailover.java    |   2 +-
 .../hbase/regionserver/TestRegionReplicas.java     |   2 +-
 .../hbase/regionserver/TestRegionSplitPolicy.java  |   6 +-
 .../hbase/regionserver/TestReversibleScanners.java |   7 +-
 .../regionserver/TestSCVFWithMiniCluster.java      |   2 +-
 .../hbase/regionserver/TestScanWithBloomError.java |  12 +--
 .../hadoop/hbase/regionserver/TestScanner.java     |  16 ++--
 .../regionserver/TestScannerRetriableFailure.java  |   2 +-
 .../hbase/regionserver/TestSeekOptimizations.java  |  18 ++--
 .../hadoop/hbase/regionserver/TestStore.java       |  12 +--
 .../hadoop/hbase/regionserver/TestStoreFile.java   |  10 +-
 .../regionserver/TestStoreFileRefresherChore.java  |   2 +-
 .../hbase/regionserver/TestStoreScanner.java       |  54 +++++------
 .../hbase/regionserver/TestStripeStoreEngine.java  |   4 +-
 .../regionserver/TestStripeStoreFileManager.java   |  12 +--
 .../apache/hadoop/hbase/regionserver/TestTags.java |   6 +-
 .../hadoop/hbase/regionserver/TestWALLockup.java   |   4 +-
 .../hadoop/hbase/regionserver/TestWideScanner.java |   4 +-
 .../compactions/ConstantSizeFileListGenerator.java |   2 +-
 .../regionserver/compactions/EverythingPolicy.java |   4 +-
 .../compactions/GaussianFileListGenerator.java     |   2 +-
 .../compactions/MockStoreFileGenerator.java        |   2 +-
 .../compactions/PerfTestCompactionPolicies.java    |  10 +-
 .../SemiConstantSizeFileListGenerator.java         |   2 +-
 .../compactions/SinusoidalFileListGenerator.java   |   2 +-
 .../compactions/SpikyFileListGenerator.java        |   2 +-
 .../compactions/TestCompactedHFilesDischarger.java |   4 +-
 .../regionserver/compactions/TestCompactor.java    |   8 +-
 .../compactions/TestDateTieredCompactor.java       |   2 +-
 .../compactions/TestStripeCompactionPolicy.java    |  38 ++++----
 .../compactions/TestStripeCompactor.java           |   2 +-
 .../TestCompactionScanQueryMatcher.java            |   3 +-
 .../querymatcher/TestExplicitColumnTracker.java    |  18 ++--
 .../TestScanWildcardColumnTracker.java             |  14 +--
 .../querymatcher/TestUserScanQueryMatcher.java     |  16 ++--
 .../hbase/regionserver/wal/AbstractTestFSWAL.java  |   8 +-
 .../wal/AbstractTestLogRollPeriod.java             |   2 +-
 .../regionserver/wal/AbstractTestWALReplay.java    |  21 ++---
 .../regionserver/wal/FaultyProtobufLogReader.java  |   2 +-
 .../hadoop/hbase/regionserver/wal/TestFSHLog.java  |   2 +-
 .../regionserver/wal/TestKeyValueCompression.java  |   2 +-
 .../hbase/regionserver/wal/TestLogRollAbort.java   |   3 +-
 .../hbase/regionserver/wal/TestLogRolling.java     |   6 +-
 .../regionserver/wal/TestLogRollingNoCluster.java  |   3 +-
 .../regionserver/wal/TestSequenceIdAccounting.java |  10 +-
 .../regionserver/wal/TestWALActionsListener.java   |   5 +-
 .../wal/TestWALCellCodecWithCompression.java       |   4 +-
 .../replication/TestNamespaceReplication.java      |   2 +-
 .../replication/TestPerTableCFReplication.java     |  20 ++--
 .../hbase/replication/TestReplicationBase.java     |   3 +-
 .../hbase/replication/TestReplicationEndpoint.java |   6 +-
 .../replication/TestReplicationSmallTests.java     |   7 +-
 .../replication/TestReplicationStateBasic.java     |  12 +--
 .../replication/TestReplicationSyncUpTool.java     |   2 +-
 .../replication/TestReplicationTrackerZKImpl.java  |   2 +-
 .../TestReplicationWALEntryFilters.java            |  36 +++----
 .../hbase/replication/TestReplicationWithTags.java |   4 +-
 ...stRegionReplicaReplicationEndpointNoMaster.java |   2 +-
 .../regionserver/TestReplicationSink.java          |  32 +++----
 .../regionserver/TestReplicationSourceManager.java |  16 ++--
 .../access/HbaseObjectWritableFor96Migration.java  |   6 +-
 .../security/access/TestAccessControlFilter.java   |   2 +-
 .../security/access/TestAccessController.java      |   4 +-
 .../access/TestCellACLWithMultipleVersions.java    |   2 +-
 .../hadoop/hbase/security/access/TestCellACLs.java |   2 +-
 .../access/TestWithDisabledAuthorization.java      |   2 +-
 .../security/access/TestZKPermissionsWatcher.java  |   4 +-
 .../security/token/TestTokenAuthentication.java    |   7 +-
 .../ExpAsStringVisibilityLabelServiceImpl.java     |  16 ++--
 .../LabelFilteringScanLabelGenerator.java          |   2 +-
 .../security/visibility/TestVisibilityLabels.java  |  14 +--
 ...tVisibilityLabelsOpWithDifferentUsersNoACL.java |   2 +-
 .../TestVisibilityLabelsReplication.java           |   6 +-
 .../visibility/TestVisibilityLabelsWithACL.java    |   4 +-
 ...estVisibilityLabelsWithCustomVisLabService.java |   2 +-
 .../TestVisibilityLabelsWithDeletes.java           |   6 +-
 .../visibility/TestVisibilityLablesWithGroups.java |   4 +-
 .../visibility/TestWithDisabledAuthorization.java  |   4 +-
 .../hbase/snapshot/SnapshotTestingUtils.java       |  10 +-
 .../hadoop/hbase/snapshot/TestExportSnapshot.java  |   6 +-
 .../hbase/snapshot/TestExportSnapshotHelpers.java  |   4 +-
 .../snapshot/TestFlushSnapshotFromClient.java      |   2 +-
 .../hadoop/hbase/util/BaseTestHBaseFsck.java       |   9 +-
 .../hadoop/hbase/util/ConstantDelayQueue.java      |  10 +-
 .../hadoop/hbase/util/HFileArchiveTestingUtil.java |  10 +-
 .../hbase/util/LoadTestDataGeneratorWithTags.java  |   4 +-
 .../org/apache/hadoop/hbase/util/LoadTestTool.java |   4 +-
 .../hadoop/hbase/util/MultiThreadedAction.java     |   2 +-
 .../hadoop/hbase/util/MultiThreadedReader.java     |   2 +-
 .../hbase/util/MultiThreadedReaderWithACL.java     |   4 +-
 .../hadoop/hbase/util/MultiThreadedUpdater.java    |   2 +-
 .../hbase/util/MultiThreadedUpdaterWithACL.java    |   4 +-
 .../hadoop/hbase/util/MultiThreadedWriter.java     |   2 +-
 .../hadoop/hbase/util/MultiThreadedWriterBase.java |   6 +-
 .../hbase/util/ProcessBasedLocalHBaseCluster.java  |  15 ++-
 .../util/TestBoundedPriorityBlockingQueue.java     |   4 +-
 .../hbase/util/TestCoprocessorScanPolicy.java      |   8 +-
 .../apache/hadoop/hbase/util/TestFSVisitor.java    |  12 +--
 .../hadoop/hbase/util/TestHBaseFsckEncryption.java |   2 +-
 .../apache/hadoop/hbase/util/TestHBaseFsckMOB.java |   2 +-
 .../hadoop/hbase/util/TestHBaseFsckOneRS.java      |   6 +-
 .../hadoop/hbase/util/TestHBaseFsckReplicas.java   |   4 +-
 .../hadoop/hbase/util/TestHBaseFsckTwoRS.java      |   2 +-
 .../org/apache/hadoop/hbase/util/TestIdLock.java   |   5 +-
 .../hadoop/hbase/util/TestIdReadWriteLock.java     |   5 +-
 .../hbase/util/TestMiniClusterLoadEncoded.java     |   2 +-
 .../hbase/util/TestMiniClusterLoadSequential.java  |   2 +-
 .../org/apache/hadoop/hbase/util/TestPoolMap.java  |   6 +-
 .../hbase/util/TestRegionSizeCalculator.java       |   2 +-
 .../hbase/util/TestRegionSplitCalculator.java      |  44 +++------
 .../hadoop/hbase/util/TestRegionSplitter.java      |   6 +-
 .../hbase/util/TestSortedCopyOnWriteSet.java       |   5 +-
 .../apache/hadoop/hbase/util/TestSortedList.java   |  11 +--
 .../util/hbck/OfflineMetaRebuildTestCore.java      |   2 +-
 .../apache/hadoop/hbase/wal/IOTestProvider.java    |   2 +-
 .../wal/TestBoundedRegionGroupingStrategy.java     |   2 +-
 .../hadoop/hbase/wal/TestFSHLogProvider.java       |  10 +-
 .../org/apache/hadoop/hbase/wal/TestSecureWAL.java |   3 +-
 .../apache/hadoop/hbase/wal/TestWALFactory.java    |  12 +--
 .../apache/hadoop/hbase/wal/TestWALFiltering.java  |   3 +-
 .../hadoop/hbase/wal/TestWALReaderOnSecureWAL.java |   3 +-
 .../org/apache/hadoop/hbase/wal/TestWALSplit.java  |   8 +-
 .../hadoop/hbase/wal/WALPerformanceEvaluation.java |   7 +-
 .../apache/hadoop/hbase/zookeeper/TestZKMulti.java |  32 +++----
 .../hadoop/hbase/zookeeper/TestZooKeeperACL.java   |   2 +-
 .../org/apache/hadoop/hbase/thrift/CallQueue.java  |   2 +-
 .../hadoop/hbase/thrift/IncrementCoalescer.java    |   4 +-
 .../hbase/thrift/TBoundedThreadPoolServer.java     |   4 +-
 .../hadoop/hbase/thrift/ThriftServerRunner.java    |  31 +++----
 .../hadoop/hbase/thrift/ThriftUtilities.java       |  12 +--
 .../hbase/thrift2/ThriftHBaseServiceHandler.java   |   3 +-
 .../apache/hadoop/hbase/thrift2/ThriftServer.java  |   6 +-
 .../hadoop/hbase/thrift2/ThriftUtilities.java      |  14 +--
 .../apache/hadoop/hbase/thrift/TestCallQueue.java  |   8 +-
 .../hadoop/hbase/thrift/TestThriftHttpServer.java  |   2 +-
 .../hadoop/hbase/thrift/TestThriftServer.java      |  32 +++----
 .../hbase/thrift/TestThriftServerCmdLine.java      |   4 +-
 .../thrift2/TestThriftHBaseServiceHandler.java     | 102 ++++++++++----------
 .../TestThriftHBaseServiceHandlerWithLabels.java   |  34 +++----
 883 files changed, 3173 insertions(+), 3577 deletions(-)

diff --git a/hbase-client/src/main/java/org/apache/hadoop/hbase/HColumnDescriptor.java b/hbase-client/src/main/java/org/apache/hadoop/hbase/HColumnDescriptor.java
index e571aae..545ea61 100644
--- a/hbase-client/src/main/java/org/apache/hadoop/hbase/HColumnDescriptor.java
+++ b/hbase-client/src/main/java/org/apache/hadoop/hbase/HColumnDescriptor.java
@@ -254,10 +254,8 @@ public class HColumnDescriptor implements Comparable<HColumnDescriptor> {
    */
   public static final boolean DEFAULT_PREFETCH_BLOCKS_ON_OPEN = false;
 
-  private final static Map<String, String> DEFAULT_VALUES
-    = new HashMap<String, String>();
-  private final static Set<Bytes> RESERVED_KEYWORDS
-      = new HashSet<Bytes>();
+  private final static Map<String, String> DEFAULT_VALUES = new HashMap<>();
+  private final static Set<Bytes> RESERVED_KEYWORDS = new HashSet<>();
 
   static {
     DEFAULT_VALUES.put(BLOOMFILTER, DEFAULT_BLOOMFILTER);
@@ -293,15 +291,14 @@ public class HColumnDescriptor implements Comparable<HColumnDescriptor> {
   private byte [] name;
 
   // Column metadata
-  private final Map<Bytes, Bytes> values =
-      new HashMap<Bytes, Bytes>();
+  private final Map<Bytes, Bytes> values = new HashMap<>();
 
   /**
    * A map which holds the configuration specific to the column family.
    * The keys of the map have the same names as config keys and override the defaults with
    * cf-specific settings. Example usage may be for compactions, etc.
    */
-  private final Map<String, String> configuration = new HashMap<String, String>();
+  private final Map<String, String> configuration = new HashMap<>();
 
   /*
    * Cache the max versions rather than calculate it every time.
diff --git a/hbase-client/src/main/java/org/apache/hadoop/hbase/HRegionInfo.java b/hbase-client/src/main/java/org/apache/hadoop/hbase/HRegionInfo.java
index da0d941..045f866 100644
--- a/hbase-client/src/main/java/org/apache/hadoop/hbase/HRegionInfo.java
+++ b/hbase-client/src/main/java/org/apache/hadoop/hbase/HRegionInfo.java
@@ -1167,7 +1167,7 @@ public class HRegionInfo implements Comparable<HRegionInfo> {
       throw new IllegalArgumentException("Can't build an object with empty bytes array");
     }
     DataInputBuffer in = new DataInputBuffer();
-    List<HRegionInfo> hris = new ArrayList<HRegionInfo>();
+    List<HRegionInfo> hris = new ArrayList<>();
     try {
       in.reset(bytes, offset, length);
       while (in.available() > 0) {
diff --git a/hbase-client/src/main/java/org/apache/hadoop/hbase/HTableDescriptor.java b/hbase-client/src/main/java/org/apache/hadoop/hbase/HTableDescriptor.java
index 05891df..0a4d4ca 100644
--- a/hbase-client/src/main/java/org/apache/hadoop/hbase/HTableDescriptor.java
+++ b/hbase-client/src/main/java/org/apache/hadoop/hbase/HTableDescriptor.java
@@ -64,15 +64,14 @@ public class HTableDescriptor implements Comparable<HTableDescriptor> {
    * includes values like IS_ROOT, IS_META, DEFERRED_LOG_FLUSH, SPLIT_POLICY,
    * MAX_FILE_SIZE, READONLY, MEMSTORE_FLUSHSIZE etc...
    */
-  private final Map<Bytes, Bytes> values =
-      new HashMap<Bytes, Bytes>();
+  private final Map<Bytes, Bytes> values = new HashMap<>();
 
   /**
    * A map which holds the configuration specific to the table.
    * The keys of the map have the same names as config keys and override the defaults with
    * table-specific settings. Example usage may be for compactions, etc.
    */
-  private final Map<String, String> configuration = new HashMap<String, String>();
+  private final Map<String, String> configuration = new HashMap<>();
 
   public static final String SPLIT_POLICY = "SPLIT_POLICY";
 
@@ -236,10 +235,8 @@ public class HTableDescriptor implements Comparable<HTableDescriptor> {
 
   public static final boolean DEFAULT_REGION_MEMSTORE_REPLICATION = true;
 
-  private final static Map<String, String> DEFAULT_VALUES
-    = new HashMap<String, String>();
-  private final static Set<Bytes> RESERVED_KEYWORDS
-      = new HashSet<Bytes>();
+  private final static Map<String, String> DEFAULT_VALUES = new HashMap<>();
+  private final static Set<Bytes> RESERVED_KEYWORDS = new HashSet<>();
 
   static {
     DEFAULT_VALUES.put(MAX_FILESIZE,
@@ -278,7 +275,7 @@ public class HTableDescriptor implements Comparable<HTableDescriptor> {
    * Maps column family name to the respective HColumnDescriptors
    */
   private final Map<byte [], HColumnDescriptor> families =
-    new TreeMap<byte [], HColumnDescriptor>(Bytes.BYTES_RAWCOMPARATOR);
+    new TreeMap<>(Bytes.BYTES_RAWCOMPARATOR);
 
   /**
    * <em> INTERNAL </em> Private constructor used internally creating table descriptors for
@@ -933,8 +930,8 @@ public class HTableDescriptor implements Comparable<HTableDescriptor> {
     StringBuilder s = new StringBuilder();
 
     // step 1: set partitioning and pruning
-    Set<Bytes> reservedKeys = new TreeSet<Bytes>();
-    Set<Bytes> userKeys = new TreeSet<Bytes>();
+    Set<Bytes> reservedKeys = new TreeSet<>();
+    Set<Bytes> userKeys = new TreeSet<>();
     for (Map.Entry<Bytes, Bytes> entry : values.entrySet()) {
       if (entry.getKey() == null || entry.getKey().get() == null) continue;
       String key = Bytes.toString(entry.getKey().get());
@@ -1412,7 +1409,7 @@ public class HTableDescriptor implements Comparable<HTableDescriptor> {
    * @return The list of co-processors classNames
    */
   public List<String> getCoprocessors() {
-    List<String> result = new ArrayList<String>(this.values.entrySet().size());
+    List<String> result = new ArrayList<>(this.values.entrySet().size());
     Matcher keyMatcher;
     for (Map.Entry<Bytes, Bytes> e : this.values.entrySet()) {
       keyMatcher = HConstants.CP_HTD_ATTR_KEY_PATTERN.matcher(Bytes.toString(e.getKey().get()));
diff --git a/hbase-client/src/main/java/org/apache/hadoop/hbase/MetaTableAccessor.java b/hbase-client/src/main/java/org/apache/hadoop/hbase/MetaTableAccessor.java
index 61d4c66..ee8d5fd 100644
--- a/hbase-client/src/main/java/org/apache/hadoop/hbase/MetaTableAccessor.java
+++ b/hbase-client/src/main/java/org/apache/hadoop/hbase/MetaTableAccessor.java
@@ -170,8 +170,7 @@ public class MetaTableAccessor {
   @Deprecated
   public static NavigableMap<HRegionInfo, ServerName> allTableRegions(
       Connection connection, final TableName tableName) throws IOException {
-    final NavigableMap<HRegionInfo, ServerName> regions =
-      new TreeMap<HRegionInfo, ServerName>();
+    final NavigableMap<HRegionInfo, ServerName> regions = new TreeMap<>();
     Visitor visitor = new TableVisitorBase(tableName) {
       @Override
       public boolean visitInternal(Result result) throws IOException {
@@ -311,7 +310,7 @@ public class MetaTableAccessor {
     HRegionLocation location = getRegionLocation(connection, regionName);
     return location == null
       ? null
-      : new Pair<HRegionInfo, ServerName>(location.getRegionInfo(), location.getServerName());
+      : new Pair<>(location.getRegionInfo(), location.getServerName());
   }
 
   /**
@@ -402,7 +401,7 @@ public class MetaTableAccessor {
     if (mergeA == null && mergeB == null) {
       return null;
     }
-    return new Pair<HRegionInfo, HRegionInfo>(mergeA, mergeB);
+    return new Pair<>(mergeA, mergeB);
  }
 
   /**
@@ -477,7 +476,7 @@ public class MetaTableAccessor {
   @Nullable
   static List<HRegionInfo> getListOfHRegionInfos(final List<Pair<HRegionInfo, ServerName>> pairs) {
     if (pairs == null || pairs.isEmpty()) return null;
-    List<HRegionInfo> result = new ArrayList<HRegionInfo>(pairs.size());
+    List<HRegionInfo> result = new ArrayList<>(pairs.size());
     for (Pair<HRegionInfo, ServerName> pair: pairs) {
       result.add(pair.getFirst());
     }
@@ -635,8 +634,7 @@ public class MetaTableAccessor {
           }
           for (HRegionLocation loc : current.getRegionLocations()) {
             if (loc != null) {
-              this.results.add(new Pair<HRegionInfo, ServerName>(
-                loc.getRegionInfo(), loc.getServerName()));
+              this.results.add(new Pair<>(loc.getRegionInfo(), loc.getServerName()));
             }
           }
         }
@@ -658,7 +656,7 @@ public class MetaTableAccessor {
   public static NavigableMap<HRegionInfo, Result>
   getServerUserRegions(Connection connection, final ServerName serverName)
     throws IOException {
-    final NavigableMap<HRegionInfo, Result> hris = new TreeMap<HRegionInfo, Result>();
+    final NavigableMap<HRegionInfo, Result> hris = new TreeMap<>();
     // Fill the above hris map with entries from hbase:meta that have the passed
     // servername.
     CollectingVisitor<Result> v = new CollectingVisitor<Result>() {
@@ -981,7 +979,7 @@ public class MetaTableAccessor {
     HRegionInfo regionInfo = getHRegionInfo(r, getRegionInfoColumn());
     if (regionInfo == null) return null;
 
-    List<HRegionLocation> locations = new ArrayList<HRegionLocation>(1);
+    List<HRegionLocation> locations = new ArrayList<>(1);
     NavigableMap<byte[],NavigableMap<byte[],byte[]>> familyMap = r.getNoVersionMap();
 
     locations.add(getRegionLocation(r, regionInfo, 0));
@@ -1069,7 +1067,7 @@ public class MetaTableAccessor {
     HRegionInfo splitA = getHRegionInfo(data, HConstants.SPLITA_QUALIFIER);
     HRegionInfo splitB = getHRegionInfo(data, HConstants.SPLITB_QUALIFIER);
 
-    return new PairOfSameType<HRegionInfo>(splitA, splitB);
+    return new PairOfSameType<>(splitA, splitB);
   }
 
   /**
@@ -1083,7 +1081,7 @@ public class MetaTableAccessor {
     HRegionInfo mergeA = getHRegionInfo(data, HConstants.MERGEA_QUALIFIER);
     HRegionInfo mergeB = getHRegionInfo(data, HConstants.MERGEB_QUALIFIER);
 
-    return new PairOfSameType<HRegionInfo>(mergeA, mergeB);
+    return new PairOfSameType<>(mergeA, mergeB);
   }
 
   /**
@@ -1183,7 +1181,7 @@ public class MetaTableAccessor {
    * A {@link Visitor} that collects content out of passed {@link Result}.
    */
   static abstract class CollectingVisitor<T> implements Visitor {
-    final List<T> results = new ArrayList<T>();
+    final List<T> results = new ArrayList<>();
     @Override
     public boolean visit(Result r) throws IOException {
       if (r ==  null || r.isEmpty()) return true;
@@ -1426,7 +1424,7 @@ public class MetaTableAccessor {
    */
   static void deleteFromMetaTable(final Connection connection, final Delete d)
     throws IOException {
-    List<Delete> dels = new ArrayList<Delete>(1);
+    List<Delete> dels = new ArrayList<>(1);
     dels.add(d);
     deleteFromMetaTable(connection, dels);
   }
@@ -1594,7 +1592,7 @@ public class MetaTableAccessor {
   public static void addRegionsToMeta(Connection connection,
       List<HRegionInfo> regionInfos, int regionReplication, long ts)
           throws IOException {
-    List<Put> puts = new ArrayList<Put>();
+    List<Put> puts = new ArrayList<>();
     for (HRegionInfo regionInfo : regionInfos) {
       if (RegionReplicaUtil.isDefaultReplica(regionInfo)) {
         Put put = makePutFromRegionInfo(regionInfo, ts);
@@ -1970,7 +1968,7 @@ public class MetaTableAccessor {
    */
   public static void deleteRegions(Connection connection,
                                    List<HRegionInfo> regionsInfo, long ts) throws IOException {
-    List<Delete> deletes = new ArrayList<Delete>(regionsInfo.size());
+    List<Delete> deletes = new ArrayList<>(regionsInfo.size());
     for (HRegionInfo hri: regionsInfo) {
       Delete e = new Delete(hri.getRegionName());
       e.addFamily(getCatalogFamily(), ts);
@@ -1991,7 +1989,7 @@ public class MetaTableAccessor {
                                    final List<HRegionInfo> regionsToRemove,
                                    final List<HRegionInfo> regionsToAdd)
     throws IOException {
-    List<Mutation> mutation = new ArrayList<Mutation>();
+    List<Mutation> mutation = new ArrayList<>();
     if (regionsToRemove != null) {
       for (HRegionInfo hri: regionsToRemove) {
         mutation.add(makeDeleteFromRegionInfo(hri));
diff --git a/hbase-client/src/main/java/org/apache/hadoop/hbase/ServerLoad.java b/hbase-client/src/main/java/org/apache/hadoop/hbase/ServerLoad.java
index d16c90f..e884e51 100644
--- a/hbase-client/src/main/java/org/apache/hadoop/hbase/ServerLoad.java
+++ b/hbase-client/src/main/java/org/apache/hadoop/hbase/ServerLoad.java
@@ -230,7 +230,7 @@ public class ServerLoad {
    */
   public Map<byte[], RegionLoad> getRegionsLoad() {
     Map<byte[], RegionLoad> regionLoads =
-      new TreeMap<byte[], RegionLoad>(Bytes.BYTES_COMPARATOR);
+      new TreeMap<>(Bytes.BYTES_COMPARATOR);
     for (ClusterStatusProtos.RegionLoad rl : serverLoad.getRegionLoadsList()) {
       RegionLoad regionLoad = new RegionLoad(rl);
       regionLoads.put(regionLoad.getName(), regionLoad);
@@ -261,7 +261,7 @@ public class ServerLoad {
   public String[] getRsCoprocessors() {
     // Need a set to remove duplicates, but since generated Coprocessor class
     // is not Comparable, make it a Set<String> instead of Set<Coprocessor>
-    TreeSet<String> coprocessSet = new TreeSet<String>();
+    TreeSet<String> coprocessSet = new TreeSet<>();
     for (Coprocessor coprocessor : obtainServerLoadPB().getCoprocessorsList()) {
       coprocessSet.add(coprocessor.getName());
     }
diff --git a/hbase-client/src/main/java/org/apache/hadoop/hbase/client/Append.java b/hbase-client/src/main/java/org/apache/hadoop/hbase/client/Append.java
index fd2df93..15497ce 100644
--- a/hbase-client/src/main/java/org/apache/hadoop/hbase/client/Append.java
+++ b/hbase-client/src/main/java/org/apache/hadoop/hbase/client/Append.java
@@ -123,7 +123,7 @@ public class Append extends Mutation {
     byte [] family = CellUtil.cloneFamily(cell);
     List<Cell> list = this.familyMap.get(family);
     if (list == null) {
-      list  = new ArrayList<Cell>(1);
+      list  = new ArrayList<>(1);
     }
     // find where the new entry should be placed in the List
     list.add(cell);
diff --git a/hbase-client/src/main/java/org/apache/hadoop/hbase/client/AsyncProcess.java b/hbase-client/src/main/java/org/apache/hadoop/hbase/client/AsyncProcess.java
index 269d316..a65d327 100644
--- a/hbase-client/src/main/java/org/apache/hadoop/hbase/client/AsyncProcess.java
+++ b/hbase-client/src/main/java/org/apache/hadoop/hbase/client/AsyncProcess.java
@@ -251,9 +251,8 @@ class AsyncProcess {
     boolean atLeastOne) throws InterruptedIOException {
     TableName tableName = task.getTableName();
     RowAccess<? extends Row> rows = task.getRowAccess();
-    Map<ServerName, MultiAction> actionsByServer =
-        new HashMap<ServerName, MultiAction>();
-    List<Action> retainedActions = new ArrayList<Action>(rows.size());
+    Map<ServerName, MultiAction> actionsByServer = new HashMap<>();
+    List<Action> retainedActions = new ArrayList<>(rows.size());
 
     NonceGenerator ng = this.connection.getNonceGenerator();
     long nonceGroup = ng.getNonceGroup(); // Currently, nonce group is per entire client.
@@ -287,8 +286,8 @@ class AsyncProcess {
           }
           loc = locs.getDefaultRegionLocation();
         } catch (IOException ex) {
-          locationErrors = new ArrayList<Exception>(1);
-          locationErrorRows = new ArrayList<Integer>(1);
+          locationErrors = new ArrayList<>(1);
+          locationErrorRows = new ArrayList<>(1);
           LOG.error("Failed to get region location ", ex);
           // This action failed before creating ars. Retain it, but do not add to submit list.
           // We will then add it to ars in an already-failed state.
@@ -368,7 +367,7 @@ class AsyncProcess {
    */
   private <CResult> AsyncRequestFuture submitAll(AsyncProcessTask task) {
     RowAccess<? extends Row> rows = task.getRowAccess();
-    List<Action> actions = new ArrayList<Action>(rows.size());
+    List<Action> actions = new ArrayList<>(rows.size());
 
     // The position will be used by the processBatch to match the object array returned.
     int posInList = -1;
diff --git a/hbase-client/src/main/java/org/apache/hadoop/hbase/client/AsyncRequestFutureImpl.java b/hbase-client/src/main/java/org/apache/hadoop/hbase/client/AsyncRequestFutureImpl.java
index c3caff8..41431bb 100644
--- a/hbase-client/src/main/java/org/apache/hadoop/hbase/client/AsyncRequestFutureImpl.java
+++ b/hbase-client/src/main/java/org/apache/hadoop/hbase/client/AsyncRequestFutureImpl.java
@@ -103,9 +103,8 @@ class AsyncRequestFutureImpl<CResult> implements AsyncRequestFuture {
         }
       }
       if (done) return; // Done within primary timeout
-      Map<ServerName, MultiAction> actionsByServer =
-          new HashMap<ServerName, MultiAction>();
-      List<Action> unknownLocActions = new ArrayList<Action>();
+      Map<ServerName, MultiAction> actionsByServer = new HashMap<>();
+      List<Action> unknownLocActions = new ArrayList<>();
       if (replicaGetIndices == null) {
         for (int i = 0; i < results.length; ++i) {
           addReplicaActions(i, actionsByServer, unknownLocActions);
@@ -119,7 +118,7 @@ class AsyncRequestFutureImpl<CResult> implements AsyncRequestFuture {
         sendMultiAction(actionsByServer, 1, null, unknownLocActions.isEmpty());
       }
       if (!unknownLocActions.isEmpty()) {
-        actionsByServer = new HashMap<ServerName, MultiAction>();
+        actionsByServer = new HashMap<>();
         for (Action action : unknownLocActions) {
           addReplicaActionsAgain(action, actionsByServer);
         }
@@ -374,7 +373,7 @@ class AsyncRequestFutureImpl<CResult> implements AsyncRequestFuture {
           hasAnyReplicaGets = true;
           if (hasAnyNonReplicaReqs) { // Mixed case
             if (replicaGetIndices == null) {
-              replicaGetIndices = new ArrayList<Integer>(actions.size() - 1);
+              replicaGetIndices = new ArrayList<>(actions.size() - 1);
             }
             replicaGetIndices.add(posInList);
           }
@@ -384,7 +383,7 @@ class AsyncRequestFutureImpl<CResult> implements AsyncRequestFuture {
           if (posInList > 0) {
             // Add all the previous requests to the index lists. We know they are all
             // replica-gets because this is the first non-multi-replica request in the list.
-            replicaGetIndices = new ArrayList<Integer>(actions.size() - 1);
+            replicaGetIndices = new ArrayList<>(actions.size() - 1);
             for (int i = 0; i < posInList; ++i) {
               replicaGetIndices.add(i);
             }
@@ -445,8 +444,7 @@ class AsyncRequestFutureImpl<CResult> implements AsyncRequestFuture {
    * @param numAttempt - the current numAttempt (first attempt is 1)
    */
   void groupAndSendMultiAction(List<Action> currentActions, int numAttempt) {
-    Map<ServerName, MultiAction> actionsByServer =
-        new HashMap<ServerName, MultiAction>();
+    Map<ServerName, MultiAction> actionsByServer = new HashMap<>();
 
     boolean isReplica = false;
     List<Action> unknownReplicaActions = null;
@@ -463,7 +461,7 @@ class AsyncRequestFutureImpl<CResult> implements AsyncRequestFuture {
       if (loc == null || loc.getServerName() == null) {
         if (isReplica) {
           if (unknownReplicaActions == null) {
-            unknownReplicaActions = new ArrayList<Action>(1);
+            unknownReplicaActions = new ArrayList<>(1);
           }
           unknownReplicaActions.add(action);
         } else {
@@ -485,7 +483,7 @@ class AsyncRequestFutureImpl<CResult> implements AsyncRequestFuture {
     }
 
     if (hasUnknown) {
-      actionsByServer = new HashMap<ServerName, MultiAction>();
+      actionsByServer = new HashMap<>();
       for (Action action : unknownReplicaActions) {
         HRegionLocation loc = getReplicaLocationOrFail(action);
         if (loc == null) continue;
@@ -616,8 +614,7 @@ class AsyncRequestFutureImpl<CResult> implements AsyncRequestFuture {
     }
 
     // group the actions by the amount of delay
-    Map<Long, DelayingRunner> actions = new HashMap<Long, DelayingRunner>(multiAction
-        .size());
+    Map<Long, DelayingRunner> actions = new HashMap<>(multiAction.size());
 
     // split up the actions
     for (Map.Entry<byte[], List<Action>> e : multiAction.actions.entrySet()) {
@@ -630,7 +627,7 @@ class AsyncRequestFutureImpl<CResult> implements AsyncRequestFuture {
       }
     }
 
-    List<Runnable> toReturn = new ArrayList<Runnable>(actions.size());
+    List<Runnable> toReturn = new ArrayList<>(actions.size());
     for (DelayingRunner runner : actions.values()) {
       asyncProcess.incTaskCounters(runner.getActions().getRegions(), server);
       String traceText = "AsyncProcess.sendMultiAction";
@@ -736,7 +733,7 @@ class AsyncRequestFutureImpl<CResult> implements AsyncRequestFuture {
       asyncProcess.connection.clearCaches(server);
     }
     int failed = 0, stopped = 0;
-    List<Action> toReplay = new ArrayList<Action>();
+    List<Action> toReplay = new ArrayList<>();
     for (Map.Entry<byte[], List<Action>> e : rsActions.actions.entrySet()) {
       byte[] regionName = e.getKey();
       byte[] row = e.getValue().iterator().next().getAction().getRow();
@@ -850,7 +847,7 @@ class AsyncRequestFutureImpl<CResult> implements AsyncRequestFuture {
     //  - DoNotRetryIOException: we continue to retry for other actions
     //  - RegionMovedException: we update the cache with the new region location
 
-    List<Action> toReplay = new ArrayList<Action>();
+    List<Action> toReplay = new ArrayList<>();
     Throwable throwable = null;
     int failureCount = 0;
     boolean canRetry = true;
diff --git a/hbase-client/src/main/java/org/apache/hadoop/hbase/client/AsyncRpcRetryingCallerFactory.java b/hbase-client/src/main/java/org/apache/hadoop/hbase/client/AsyncRpcRetryingCallerFactory.java
index 9bc651d..08f52fc 100644
--- a/hbase-client/src/main/java/org/apache/hadoop/hbase/client/AsyncRpcRetryingCallerFactory.java
+++ b/hbase-client/src/main/java/org/apache/hadoop/hbase/client/AsyncRpcRetryingCallerFactory.java
@@ -295,7 +295,7 @@ class AsyncRpcRetryingCallerFactory {
     }
 
     public <T> AsyncBatchRpcRetryingCaller<T> build() {
-      return new AsyncBatchRpcRetryingCaller<T>(retryTimer, conn, tableName, actions, pauseNs,
+      return new AsyncBatchRpcRetryingCaller<>(retryTimer, conn, tableName, actions, pauseNs,
           maxAttempts, operationTimeoutNs, rpcTimeoutNs, startLogErrorsCnt);
     }
 
diff --git a/hbase-client/src/main/java/org/apache/hadoop/hbase/client/BatchErrors.java b/hbase-client/src/main/java/org/apache/hadoop/hbase/client/BatchErrors.java
index b13c127..95b3484 100644
--- a/hbase-client/src/main/java/org/apache/hadoop/hbase/client/BatchErrors.java
+++ b/hbase-client/src/main/java/org/apache/hadoop/hbase/client/BatchErrors.java
@@ -28,9 +28,9 @@ import java.util.List;
 
 class BatchErrors {
   private static final Log LOG = LogFactory.getLog(BatchErrors.class);
-  final List<Throwable> throwables = new ArrayList<Throwable>();
-  final List<Row> actions = new ArrayList<Row>();
-  final List<String> addresses = new ArrayList<String>();
+  final List<Throwable> throwables = new ArrayList<>();
+  final List<Row> actions = new ArrayList<>();
+  final List<String> addresses = new ArrayList<>();
 
   public synchronized void add(Throwable ex, Row row, ServerName serverName) {
     if (row == null){
@@ -51,8 +51,8 @@ class BatchErrors {
       LOG.error("Exception occurred! Exception details: " + throwables + ";\nActions: "
               + actions);
     }
-    return new RetriesExhaustedWithDetailsException(new ArrayList<Throwable>(throwables),
-            new ArrayList<Row>(actions), new ArrayList<String>(addresses));
+    return new RetriesExhaustedWithDetailsException(new ArrayList<>(throwables),
+            new ArrayList<>(actions), new ArrayList<>(addresses));
   }
 
   public synchronized void clear() {
diff --git a/hbase-client/src/main/java/org/apache/hadoop/hbase/client/ClientAsyncPrefetchScanner.java b/hbase-client/src/main/java/org/apache/hadoop/hbase/client/ClientAsyncPrefetchScanner.java
index f632bcb..b1fc2da 100644
--- a/hbase-client/src/main/java/org/apache/hadoop/hbase/client/ClientAsyncPrefetchScanner.java
+++ b/hbase-client/src/main/java/org/apache/hadoop/hbase/client/ClientAsyncPrefetchScanner.java
@@ -76,9 +76,9 @@ public class ClientAsyncPrefetchScanner extends ClientSimpleScanner {
   protected void initCache() {
     // concurrent cache
     cacheCapacity = calcCacheCapacity();
-    cache = new LinkedBlockingQueue<Result>();
+    cache = new LinkedBlockingQueue<>();
     cacheSizeInBytes = new AtomicLong(0);
-    exceptionsQueue = new ConcurrentLinkedQueue<Exception>();
+    exceptionsQueue = new ConcurrentLinkedQueue<>();
     prefetchRunnable = new PrefetchRunnable();
     prefetchRunning = new AtomicBoolean(false);
     closingThreadId = new AtomicLong(NO_THREAD);
diff --git a/hbase-client/src/main/java/org/apache/hadoop/hbase/client/ClientScanner.java b/hbase-client/src/main/java/org/apache/hadoop/hbase/client/ClientScanner.java
index 47270a7..9bd6397 100644
--- a/hbase-client/src/main/java/org/apache/hadoop/hbase/client/ClientScanner.java
+++ b/hbase-client/src/main/java/org/apache/hadoop/hbase/client/ClientScanner.java
@@ -75,7 +75,7 @@ public abstract class ClientScanner extends AbstractClientScanner {
    * contain results if this scanner does not have enough partial results to form the complete
    * result.
    */
-  protected final LinkedList<Result> partialResults = new LinkedList<Result>();
+  protected final LinkedList<Result> partialResults = new LinkedList<>();
   /**
    * The row for which we are accumulating partial Results (i.e. the row of the Results stored
    * inside partialResults). Changes to partialResultsRow and partialResults are kept in sync via
@@ -312,7 +312,7 @@ public abstract class ClientScanner extends AbstractClientScanner {
   }
 
   protected void initSyncCache() {
-    cache = new LinkedList<Result>();
+    cache = new LinkedList<>();
   }
 
   protected Result nextWithSyncCache() throws IOException {
@@ -586,7 +586,7 @@ public abstract class ClientScanner extends AbstractClientScanner {
   protected List<Result> getResultsToAddToCache(Result[] resultsFromServer,
       boolean heartbeatMessage) throws IOException {
     int resultSize = resultsFromServer != null ? resultsFromServer.length : 0;
-    List<Result> resultsToAddToCache = new ArrayList<Result>(resultSize);
+    List<Result> resultsToAddToCache = new ArrayList<>(resultSize);
 
     final boolean isBatchSet = scan != null && scan.getBatch() > 0;
     final boolean allowPartials = scan != null && scan.getAllowPartialResults();
diff --git a/hbase-client/src/main/java/org/apache/hadoop/hbase/client/ClusterStatusListener.java b/hbase-client/src/main/java/org/apache/hadoop/hbase/client/ClusterStatusListener.java
index f3c0241..240587b 100644
--- a/hbase-client/src/main/java/org/apache/hadoop/hbase/client/ClusterStatusListener.java
+++ b/hbase-client/src/main/java/org/apache/hadoop/hbase/client/ClusterStatusListener.java
@@ -64,7 +64,7 @@ import org.apache.hadoop.hbase.util.Threads;
 @InterfaceAudience.Private
 class ClusterStatusListener implements Closeable {
   private static final Log LOG = LogFactory.getLog(ClusterStatusListener.class);
-  private final List<ServerName> deadServers = new ArrayList<ServerName>();
+  private final List<ServerName> deadServers = new ArrayList<>();
   protected final DeadServerHandler deadServerHandler;
   private final Listener listener;
 
diff --git a/hbase-client/src/main/java/org/apache/hadoop/hbase/client/ConnectionImplementation.java b/hbase-client/src/main/java/org/apache/hadoop/hbase/client/ConnectionImplementation.java
index 0fb9758..adf1496 100644
--- a/hbase-client/src/main/java/org/apache/hadoop/hbase/client/ConnectionImplementation.java
+++ b/hbase-client/src/main/java/org/apache/hadoop/hbase/client/ConnectionImplementation.java
@@ -415,7 +415,7 @@ class ConnectionImplementation implements ClusterConnection, Closeable {
     BlockingQueue<Runnable> workQueue = passedWorkQueue;
     if (workQueue == null) {
       workQueue =
-        new LinkedBlockingQueue<Runnable>(maxThreads *
+        new LinkedBlockingQueue<>(maxThreads *
             conf.getInt(HConstants.HBASE_CLIENT_MAX_TOTAL_TASKS,
                 HConstants.DEFAULT_HBASE_CLIENT_MAX_TOTAL_TASKS));
       coreThreads = maxThreads;
@@ -443,7 +443,7 @@ class ConnectionImplementation implements ClusterConnection, Closeable {
           this.metaLookupPool = getThreadPool(
              threads,
              threads,
-             "-metaLookup-shared-", new LinkedBlockingQueue<Runnable>());
+             "-metaLookup-shared-", new LinkedBlockingQueue<>());
         }
       }
     }
@@ -661,7 +661,7 @@ class ConnectionImplementation implements ClusterConnection, Closeable {
       final boolean useCache, final boolean offlined) throws IOException {
     List<HRegionInfo> regions = MetaTableAccessor
         .getTableRegions(this, tableName, !offlined);
-    final List<HRegionLocation> locations = new ArrayList<HRegionLocation>();
+    final List<HRegionLocation> locations = new ArrayList<>();
     for (HRegionInfo regionInfo : regions) {
       RegionLocations list = locateRegion(tableName, regionInfo.getStartKey(), useCache, true);
       if (list != null) {
@@ -967,7 +967,7 @@ class ConnectionImplementation implements ClusterConnection, Closeable {
   }
 
   // Map keyed by service name + regionserver to service stub implementation
-  private final ConcurrentMap<String, Object> stubs = new ConcurrentHashMap<String, Object>();
+  private final ConcurrentMap<String, Object> stubs = new ConcurrentHashMap<>();
 
   /**
    * State of the MasterService connection/setup.
@@ -1012,8 +1012,7 @@ class ConnectionImplementation implements ClusterConnection, Closeable {
    */
   static class ServerErrorTracker {
     // We need a concurrent map here, as we could have multiple threads updating it in parallel.
-    private final ConcurrentMap<ServerName, ServerErrors> errorsByServer =
-        new ConcurrentHashMap<ServerName, ServerErrors>();
+    private final ConcurrentMap<ServerName, ServerErrors> errorsByServer = new ConcurrentHashMap<>();
     private final long canRetryUntil;
     private final int maxTries;// max number to try
     private final long startTrackingTime;
diff --git a/hbase-client/src/main/java/org/apache/hadoop/hbase/client/Delete.java b/hbase-client/src/main/java/org/apache/hadoop/hbase/client/Delete.java
index 9c6c1a5..0eb1d2b 100644
--- a/hbase-client/src/main/java/org/apache/hadoop/hbase/client/Delete.java
+++ b/hbase-client/src/main/java/org/apache/hadoop/hbase/client/Delete.java
@@ -172,7 +172,7 @@ public class Delete extends Mutation implements Comparable<Row> {
     byte [] family = CellUtil.cloneFamily(kv);
     List<Cell> list = familyMap.get(family);
     if (list == null) {
-      list = new ArrayList<Cell>(1);
+      list = new ArrayList<>(1);
     }
     list.add(kv);
     familyMap.put(family, list);
@@ -209,7 +209,7 @@ public class Delete extends Mutation implements Comparable<Row> {
     }
     List<Cell> list = familyMap.get(family);
     if(list == null) {
-      list = new ArrayList<Cell>(1);
+      list = new ArrayList<>(1);
     } else if(!list.isEmpty()) {
       list.clear();
     }
@@ -229,7 +229,7 @@ public class Delete extends Mutation implements Comparable<Row> {
   public Delete addFamilyVersion(final byte [] family, final long timestamp) {
     List<Cell> list = familyMap.get(family);
     if(list == null) {
-      list = new ArrayList<Cell>(1);
+      list = new ArrayList<>(1);
     }
     list.add(new KeyValue(row, family, null, timestamp,
           KeyValue.Type.DeleteFamilyVersion));
@@ -262,7 +262,7 @@ public class Delete extends Mutation implements Comparable<Row> {
     }
     List<Cell> list = familyMap.get(family);
     if (list == null) {
-      list = new ArrayList<Cell>(1);
+      list = new ArrayList<>(1);
     }
     list.add(new KeyValue(this.row, family, qualifier, timestamp,
         KeyValue.Type.DeleteColumn));
@@ -297,7 +297,7 @@ public class Delete extends Mutation implements Comparable<Row> {
     }
     List<Cell> list = familyMap.get(family);
     if(list == null) {
-      list = new ArrayList<Cell>(1);
+      list = new ArrayList<>(1);
     }
     KeyValue kv = new KeyValue(this.row, family, qualifier, timestamp, KeyValue.Type.Delete);
     list.add(kv);
diff --git a/hbase-client/src/main/java/org/apache/hadoop/hbase/client/Get.java b/hbase-client/src/main/java/org/apache/hadoop/hbase/client/Get.java
index 947b54a..a581ed5 100644
--- a/hbase-client/src/main/java/org/apache/hadoop/hbase/client/Get.java
+++ b/hbase-client/src/main/java/org/apache/hadoop/hbase/client/Get.java
@@ -76,8 +76,7 @@ public class Get extends Query
   private int storeOffset = 0;
   private boolean checkExistenceOnly = false;
   private boolean closestRowBefore = false;
-  private Map<byte [], NavigableSet<byte []>> familyMap =
-    new TreeMap<byte [], NavigableSet<byte []>>(Bytes.BYTES_COMPARATOR);
+  private Map<byte [], NavigableSet<byte []>> familyMap = new TreeMap<>(Bytes.BYTES_COMPARATOR);
 
   /**
    * Create a Get operation for the specified row.
@@ -184,7 +183,7 @@ public class Get extends Query
   public Get addColumn(byte [] family, byte [] qualifier) {
     NavigableSet<byte []> set = familyMap.get(family);
     if(set == null) {
-      set = new TreeSet<byte []>(Bytes.BYTES_COMPARATOR);
+      set = new TreeSet<>(Bytes.BYTES_COMPARATOR);
     }
     if (qualifier == null) {
       qualifier = HConstants.EMPTY_BYTE_ARRAY;
@@ -399,8 +398,8 @@ public class Get extends Query
    */
   @Override
   public Map<String, Object> getFingerprint() {
-    Map<String, Object> map = new HashMap<String, Object>();
-    List<String> families = new ArrayList<String>(this.familyMap.entrySet().size());
+    Map<String, Object> map = new HashMap<>();
+    List<String> families = new ArrayList<>(this.familyMap.entrySet().size());
     map.put("families", families);
     for (Map.Entry<byte [], NavigableSet<byte[]>> entry :
       this.familyMap.entrySet()) {
@@ -422,13 +421,13 @@ public class Get extends Query
     Map<String, Object> map = getFingerprint();
     // replace the fingerprint's simple list of families with a
     // map from column families to lists of qualifiers and kv details
-    Map<String, List<String>> columns = new HashMap<String, List<String>>();
+    Map<String, List<String>> columns = new HashMap<>();
     map.put("families", columns);
     // add scalar information first
     map.put("row", Bytes.toStringBinary(this.row));
     map.put("maxVersions", this.maxVersions);
     map.put("cacheBlocks", this.cacheBlocks);
-    List<Long> timeRange = new ArrayList<Long>(2);
+    List<Long> timeRange = new ArrayList<>(2);
     timeRange.add(this.tr.getMin());
     timeRange.add(this.tr.getMax());
     map.put("timeRange", timeRange);
@@ -436,7 +435,7 @@ public class Get extends Query
     // iterate through affected families and add details
     for (Map.Entry<byte [], NavigableSet<byte[]>> entry :
       this.familyMap.entrySet()) {
-      List<String> familyList = new ArrayList<String>();
+      List<String> familyList = new ArrayList<>();
       columns.put(Bytes.toStringBinary(entry.getKey()), familyList);
       if(entry.getValue() == null) {
         colCount++;
diff --git a/hbase-client/src/main/java/org/apache/hadoop/hbase/client/HBaseAdmin.java b/hbase-client/src/main/java/org/apache/hadoop/hbase/client/HBaseAdmin.java
index c68d3bb..5265616 100644
--- a/hbase-client/src/main/java/org/apache/hadoop/hbase/client/HBaseAdmin.java
+++ b/hbase-client/src/main/java/org/apache/hadoop/hbase/client/HBaseAdmin.java
@@ -618,7 +618,7 @@ public class HBaseAdmin implements Admin {
    */
   @Override
   public HTableDescriptor[] deleteTables(Pattern pattern) throws IOException {
-    List<HTableDescriptor> failed = new LinkedList<HTableDescriptor>();
+    List<HTableDescriptor> failed = new LinkedList<>();
     for (HTableDescriptor table : listTables(pattern)) {
       try {
         deleteTable(table.getTableName());
@@ -743,7 +743,7 @@ public class HBaseAdmin implements Admin {
 
   @Override
   public HTableDescriptor[] enableTables(Pattern pattern) throws IOException {
-    List<HTableDescriptor> failed = new LinkedList<HTableDescriptor>();
+    List<HTableDescriptor> failed = new LinkedList<>();
     for (HTableDescriptor table : listTables(pattern)) {
       if (isTableDisabled(table.getTableName())) {
         try {
@@ -807,7 +807,7 @@ public class HBaseAdmin implements Admin {
 
   @Override
   public HTableDescriptor[] disableTables(Pattern pattern) throws IOException {
-    List<HTableDescriptor> failed = new LinkedList<HTableDescriptor>();
+    List<HTableDescriptor> failed = new LinkedList<>();
     for (HTableDescriptor table : listTables(pattern)) {
       if (isTableEnabled(table.getTableName())) {
         try {
@@ -1098,8 +1098,7 @@ public class HBaseAdmin implements Admin {
       LOG.info("Table is disabled: " + tableName.getNameAsString());
       return;
     }
-    execProcedure("flush-table-proc", tableName.getNameAsString(),
-      new HashMap<String, String>());
+    execProcedure("flush-table-proc", tableName.getNameAsString(), new HashMap<>());
   }
 
   @Override
@@ -1796,8 +1795,7 @@ public class HBaseAdmin implements Admin {
     Pair<HRegionInfo, ServerName> pair =
       MetaTableAccessor.getRegion(connection, regionName);
     if (pair == null) {
-      final AtomicReference<Pair<HRegionInfo, ServerName>> result =
-        new AtomicReference<Pair<HRegionInfo, ServerName>>(null);
+      final AtomicReference<Pair<HRegionInfo, ServerName>> result = new AtomicReference<>(null);
       final String encodedName = Bytes.toString(regionName);
       MetaTableAccessor.Visitor visitor = new MetaTableAccessor.Visitor() {
         @Override
@@ -1820,7 +1818,7 @@ public class HBaseAdmin implements Admin {
             }
           }
           if (!matched) return true;
-          result.set(new Pair<HRegionInfo, ServerName>(info, sn));
+          result.set(new Pair<>(info, sn));
           return false; // found the region, stop
         }
       };
@@ -1954,7 +1952,7 @@ public class HBaseAdmin implements Admin {
     AdminService.BlockingInterface admin = this.connection.getAdmin(sn);
     HBaseRpcController controller = rpcControllerFactory.newController();
     List<RegionLoad> regionLoads = ProtobufUtil.getRegionLoad(controller, admin, tableName);
-    Map<byte[], RegionLoad> resultMap = new TreeMap<byte[], RegionLoad>(Bytes.BYTES_COMPARATOR);
+    Map<byte[], RegionLoad> resultMap = new TreeMap<>(Bytes.BYTES_COMPARATOR);
     for (RegionLoad regionLoad : regionLoads) {
       resultMap.put(regionLoad.getName(), regionLoad);
     }
@@ -2279,7 +2277,7 @@ public class HBaseAdmin implements Admin {
    */
   private HTableDescriptor getTableDescriptorByTableName(TableName tableName)
       throws IOException {
-    List<TableName> tableNames = new ArrayList<TableName>(1);
+    List<TableName> tableNames = new ArrayList<>(1);
     tableNames.add(tableName);
 
     HTableDescriptor[] htdl = getTableDescriptorsByTableName(tableNames);
@@ -2295,7 +2293,7 @@ public class HBaseAdmin implements Admin {
   @Override
   public HTableDescriptor[] getTableDescriptors(List<String> names)
   throws IOException {
-    List<TableName> tableNames = new ArrayList<TableName>(names.size());
+    List<TableName> tableNames = new ArrayList<>(names.size());
     for(String name : names) {
       tableNames.add(TableName.valueOf(name));
     }
@@ -2829,7 +2827,7 @@ public class HBaseAdmin implements Admin {
             .getCompletedSnapshots(getRpcController(),
                 GetCompletedSnapshotsRequest.newBuilder().build())
             .getSnapshotsList();
-        List<SnapshotDescription> result = new ArrayList<SnapshotDescription>(snapshotsList.size());
+        List<SnapshotDescription> result = new ArrayList<>(snapshotsList.size());
         for (HBaseProtos.SnapshotDescription snapshot : snapshotsList) {
           result.add(ProtobufUtil.createSnapshotDesc(snapshot));
         }
@@ -2845,7 +2843,7 @@ public class HBaseAdmin implements Admin {
 
   @Override
   public List<SnapshotDescription> listSnapshots(Pattern pattern) throws IOException {
-    List<SnapshotDescription> matched = new LinkedList<SnapshotDescription>();
+    List<SnapshotDescription> matched = new LinkedList<>();
     List<SnapshotDescription> snapshots = listSnapshots();
     for (SnapshotDescription snapshot : snapshots) {
       if (pattern.matcher(snapshot.getName()).matches()) {
@@ -2866,7 +2864,7 @@ public class HBaseAdmin implements Admin {
       Pattern snapshotNamePattern) throws IOException {
     TableName[] tableNames = listTableNames(tableNamePattern);
 
-    List<SnapshotDescription> tableSnapshots = new LinkedList<SnapshotDescription>();
+    List<SnapshotDescription> tableSnapshots = new LinkedList<>();
     List<SnapshotDescription> snapshots = listSnapshots(snapshotNamePattern);
 
     List<TableName> listOfTableNames = Arrays.asList(tableNames);
@@ -3985,7 +3983,7 @@ public class HBaseAdmin implements Admin {
 
   @Override
   public void drainRegionServers(List<ServerName> servers) throws IOException {
-    final List<HBaseProtos.ServerName> pbServers = new ArrayList<HBaseProtos.ServerName>(servers.size());
+    final List<HBaseProtos.ServerName> pbServers = new ArrayList<>(servers.size());
     for (ServerName server : servers) {
       // Parse to ServerName to do simple validation.
       ServerName.parseServerName(server.toString());
@@ -4010,7 +4008,7 @@ public class HBaseAdmin implements Admin {
       @Override
       public List<ServerName> rpcCall() throws ServiceException {
         ListDrainingRegionServersRequest req = ListDrainingRegionServersRequest.newBuilder().build();
-        List<ServerName> servers = new ArrayList<ServerName>();
+        List<ServerName> servers = new ArrayList<>();
         for (HBaseProtos.ServerName server : master.listDrainingRegionServers(null, req)
             .getServerNameList()) {
           servers.add(ProtobufUtil.toServerName(server));
@@ -4022,7 +4020,7 @@ public class HBaseAdmin implements Admin {
 
   @Override
   public void removeDrainFromRegionServers(List<ServerName> servers) throws IOException {
-    final List<HBaseProtos.ServerName> pbServers = new ArrayList<HBaseProtos.ServerName>(servers.size());
+    final List<HBaseProtos.ServerName> pbServers = new ArrayList<>(servers.size());
     for (ServerName server : servers) {
       pbServers.add(ProtobufUtil.toServerName(server));
     }
diff --git a/hbase-client/src/main/java/org/apache/hadoop/hbase/client/HRegionLocator.java b/hbase-client/src/main/java/org/apache/hadoop/hbase/client/HRegionLocator.java
index 4d2311d..f2c5746 100644
--- a/hbase-client/src/main/java/org/apache/hadoop/hbase/client/HRegionLocator.java
+++ b/hbase-client/src/main/java/org/apache/hadoop/hbase/client/HRegionLocator.java
@@ -142,7 +142,7 @@ public class HRegionLocator implements RegionLocator {
 
   @VisibleForTesting
   List<RegionLocations> listRegionLocations() throws IOException {
-    final List<RegionLocations> regions = new ArrayList<RegionLocations>();
+    final List<RegionLocations> regions = new ArrayList<>();
     MetaTableAccessor.Visitor visitor = new MetaTableAccessor.TableVisitorBase(tableName) {
       @Override
       public boolean visitInternal(Result result) throws IOException {
diff --git a/hbase-client/src/main/java/org/apache/hadoop/hbase/client/HTable.java b/hbase-client/src/main/java/org/apache/hadoop/hbase/client/HTable.java
index 0c383fc..3bdbed5 100644
--- a/hbase-client/src/main/java/org/apache/hadoop/hbase/client/HTable.java
+++ b/hbase-client/src/main/java/org/apache/hadoop/hbase/client/HTable.java
@@ -143,7 +143,7 @@ public class HTable implements Table {
     // we only create as many Runnables as there are region servers. It means
     // it also scales when new region servers are added.
     ThreadPoolExecutor pool = new ThreadPoolExecutor(corePoolSize, maxThreads, keepAliveTime,
-      TimeUnit.SECONDS, new SynchronousQueue<Runnable>(), Threads.newDaemonThreadFactory("htable"));
+      TimeUnit.SECONDS, new SynchronousQueue<>(), Threads.newDaemonThreadFactory("htable"));
     pool.allowCoreThreadTimeOut(true);
     return pool;
   }
@@ -309,8 +309,8 @@ public class HTable implements Table {
         "Invalid range: " + Bytes.toStringBinary(startKey) +
         " > " + Bytes.toStringBinary(endKey));
     }
-    List<byte[]> keysInRange = new ArrayList<byte[]>();
-    List<HRegionLocation> regionsInRange = new ArrayList<HRegionLocation>();
+    List<byte[]> keysInRange = new ArrayList<>();
+    List<HRegionLocation> regionsInRange = new ArrayList<>();
     byte[] currentKey = startKey;
     do {
       HRegionLocation regionLocation = getRegionLocator().getRegionLocation(currentKey, reload);
@@ -320,8 +320,7 @@ public class HTable implements Table {
     } while (!Bytes.equals(currentKey, HConstants.EMPTY_END_ROW)
         && (endKeyIsEndOfTable || Bytes.compareTo(currentKey, endKey) < 0
             || (includeEndKey && Bytes.compareTo(currentKey, endKey) == 0)));
-    return new Pair<List<byte[]>, List<HRegionLocation>>(keysInRange,
-        regionsInRange);
+    return new Pair<>(keysInRange, regionsInRange);
   }
 
   /**
@@ -915,7 +914,7 @@ public class HTable implements Table {
     if (gets.isEmpty()) return new boolean[]{};
     if (gets.size() == 1) return new boolean[]{exists(gets.get(0))};
 
-    ArrayList<Get> exists = new ArrayList<Get>(gets.size());
+    ArrayList<Get> exists = new ArrayList<>(gets.size());
     for (Get g: gets){
       Get ge = new Get(g);
       ge.setCheckExistenceOnly(true);
@@ -1099,8 +1098,7 @@ public class HTable implements Table {
       final Batch.Callback<R> callback) throws ServiceException, Throwable {
     // get regions covered by the row range
     List<byte[]> keys = getStartKeysInRange(startKey, endKey);
-    Map<byte[],Future<R>> futures =
-        new TreeMap<byte[],Future<R>>(Bytes.BYTES_COMPARATOR);
+    Map<byte[],Future<R>> futures = new TreeMap<>(Bytes.BYTES_COMPARATOR);
     for (final byte[] r : keys) {
       final RegionCoprocessorRpcChannel channel =
           new RegionCoprocessorRpcChannel(connection, tableName, r);
@@ -1245,10 +1243,8 @@ public class HTable implements Table {
       return;
     }
 
-    List<RegionCoprocessorServiceExec> execs =
-        new ArrayList<RegionCoprocessorServiceExec>(keys.size());
-    final Map<byte[], RegionCoprocessorServiceExec> execsByRow =
-        new TreeMap<byte[], RegionCoprocessorServiceExec>(Bytes.BYTES_COMPARATOR);
+    List<RegionCoprocessorServiceExec> execs = new ArrayList<>(keys.size());
+    final Map<byte[], RegionCoprocessorServiceExec> execsByRow = new TreeMap<>(Bytes.BYTES_COMPARATOR);
     for (int i = 0; i < keys.size(); i++) {
       final byte[] rowKey = keys.get(i);
       final byte[] region = regions.get(i).getRegionInfo().getRegionName();
@@ -1260,9 +1256,9 @@ public class HTable implements Table {
 
     // tracking for any possible deserialization errors on success callback
     // TODO: it would be better to be able to reuse AsyncProcess.BatchErrors here
-    final List<Throwable> callbackErrorExceptions = new ArrayList<Throwable>();
-    final List<Row> callbackErrorActions = new ArrayList<Row>();
-    final List<String> callbackErrorServers = new ArrayList<String>();
+    final List<Throwable> callbackErrorExceptions = new ArrayList<>();
+    final List<Row> callbackErrorActions = new ArrayList<>();
+    final List<String> callbackErrorServers = new ArrayList<>();
     Object[] results = new Object[execs.size()];
 
     AsyncProcess asyncProcess =
diff --git a/hbase-client/src/main/java/org/apache/hadoop/hbase/client/HTableMultiplexer.java b/hbase-client/src/main/java/org/apache/hadoop/hbase/client/HTableMultiplexer.java
index 27393ba..f3a58ad 100644
--- a/hbase-client/src/main/java/org/apache/hadoop/hbase/client/HTableMultiplexer.java
+++ b/hbase-client/src/main/java/org/apache/hadoop/hbase/client/HTableMultiplexer.java
@@ -169,7 +169,7 @@ public class HTableMultiplexer {
 
         // Create the failed puts list if necessary
         if (failedPuts == null) {
-          failedPuts = new ArrayList<Put>();
+          failedPuts = new ArrayList<>();
         }
         // Add the put to the failed puts list
         failedPuts.add(put);
@@ -288,10 +288,10 @@ public class HTableMultiplexer {
       this.totalFailedPutCounter = 0;
       this.maxLatency = 0;
       this.overallAverageLatency = 0;
-      this.serverToBufferedCounterMap = new HashMap<String, Long>();
-      this.serverToFailedCounterMap = new HashMap<String, Long>();
-      this.serverToAverageLatencyMap = new HashMap<String, Long>();
-      this.serverToMaxLatencyMap = new HashMap<String, Long>();
+      this.serverToBufferedCounterMap = new HashMap<>();
+      this.serverToFailedCounterMap = new HashMap<>();
+      this.serverToAverageLatencyMap = new HashMap<>();
+      this.serverToMaxLatencyMap = new HashMap<>();
       this.initialize(serverToFlushWorkerMap);
     }
 
@@ -412,7 +412,7 @@ public class HTableMultiplexer {
     }
 
     public synchronized SimpleEntry<Long, Integer> getComponents() {
-      return new SimpleEntry<Long, Integer>(sum, count);
+      return new SimpleEntry<>(sum, count);
     }
 
     public synchronized void reset() {
@@ -614,7 +614,7 @@ public class HTableMultiplexer {
               failedCount--;
             } else {
               if (failed == null) {
-                failed = new ArrayList<PutStatus>();
+                failed = new ArrayList<>();
               }
               failed.add(processingList.get(i));
             }
diff --git a/hbase-client/src/main/java/org/apache/hadoop/hbase/client/Increment.java b/hbase-client/src/main/java/org/apache/hadoop/hbase/client/Increment.java
index 9538361..eb1cbc5 100644
--- a/hbase-client/src/main/java/org/apache/hadoop/hbase/client/Increment.java
+++ b/hbase-client/src/main/java/org/apache/hadoop/hbase/client/Increment.java
@@ -204,10 +204,9 @@ public class Increment extends Mutation implements Comparable<Row> {
    */
   public Map<byte[], NavigableMap<byte [], Long>> getFamilyMapOfLongs() {
     NavigableMap<byte[], List<Cell>> map = super.getFamilyCellMap();
-    Map<byte [], NavigableMap<byte[], Long>> results =
-      new TreeMap<byte[], NavigableMap<byte [], Long>>(Bytes.BYTES_COMPARATOR);
+    Map<byte [], NavigableMap<byte[], Long>> results = new TreeMap<>(Bytes.BYTES_COMPARATOR);
     for (Map.Entry<byte [], List<Cell>> entry: map.entrySet()) {
-      NavigableMap<byte [], Long> longs = new TreeMap<byte [], Long>(Bytes.BYTES_COMPARATOR);
+      NavigableMap<byte [], Long> longs = new TreeMap<>(Bytes.BYTES_COMPARATOR);
       for (Cell cell: entry.getValue()) {
         longs.put(CellUtil.cloneQualifier(cell),
             Bytes.toLong(cell.getValueArray(), cell.getValueOffset(), cell.getValueLength()));
diff --git a/hbase-client/src/main/java/org/apache/hadoop/hbase/client/MetricsConnection.java b/hbase-client/src/main/java/org/apache/hadoop/hbase/client/MetricsConnection.java
index 64b1661..ea64900 100644
--- a/hbase-client/src/main/java/org/apache/hadoop/hbase/client/MetricsConnection.java
+++ b/hbase-client/src/main/java/org/apache/hadoop/hbase/client/MetricsConnection.java
@@ -193,7 +193,7 @@ public class MetricsConnection implements StatisticTrackable {
 
   @VisibleForTesting
   protected ConcurrentHashMap<ServerName, ConcurrentMap<byte[], RegionStats>> serverStats
-          = new ConcurrentHashMap<ServerName, ConcurrentMap<byte[], RegionStats>>();
+          = new ConcurrentHashMap<>();
 
   public void updateServerStats(ServerName serverName, byte[] regionName,
                                 Object r) {
diff --git a/hbase-client/src/main/java/org/apache/hadoop/hbase/client/MultiAction.java b/hbase-client/src/main/java/org/apache/hadoop/hbase/client/MultiAction.java
index dc4ec62..a4aa71d 100644
--- a/hbase-client/src/main/java/org/apache/hadoop/hbase/client/MultiAction.java
+++ b/hbase-client/src/main/java/org/apache/hadoop/hbase/client/MultiAction.java
@@ -82,7 +82,7 @@ public final class MultiAction {
   public void add(byte[] regionName, List<Action> actionList){
     List<Action> rsActions = actions.get(regionName);
     if (rsActions == null) {
-      rsActions = new ArrayList<Action>(actionList.size());
+      rsActions = new ArrayList<>(actionList.size());
       actions.put(regionName, rsActions);
     }
     rsActions.addAll(actionList);
diff --git a/hbase-client/src/main/java/org/apache/hadoop/hbase/client/MultiResponse.java b/hbase-client/src/main/java/org/apache/hadoop/hbase/client/MultiResponse.java
index 937e1b5..7d6744f 100644
--- a/hbase-client/src/main/java/org/apache/hadoop/hbase/client/MultiResponse.java
+++ b/hbase-client/src/main/java/org/apache/hadoop/hbase/client/MultiResponse.java
@@ -41,7 +41,7 @@ public class MultiResponse extends AbstractResponse {
    * It's a part of the protobuf definition.
    */
   private Map<byte[], Throwable> exceptions =
-      new TreeMap<byte[], Throwable>(Bytes.BYTES_COMPARATOR);
+      new TreeMap<>(Bytes.BYTES_COMPARATOR);
 
   public MultiResponse() {
     super();
diff --git a/hbase-client/src/main/java/org/apache/hadoop/hbase/client/MultiServerCallable.java b/hbase-client/src/main/java/org/apache/hadoop/hbase/client/MultiServerCallable.java
index c4adf34..38a1950 100644
--- a/hbase-client/src/main/java/org/apache/hadoop/hbase/client/MultiServerCallable.java
+++ b/hbase-client/src/main/java/org/apache/hadoop/hbase/client/MultiServerCallable.java
@@ -107,7 +107,7 @@ class MultiServerCallable extends CancellableRegionServerCallable<MultiResponse>
           HBaseProtos.RegionSpecifier.RegionSpecifierType.REGION_NAME, regionName));
       if (this.cellBlock) {
         // Pre-size. Presume at least a KV per Action.  There are likely more.
-        if (cells == null) cells = new ArrayList<CellScannable>(countOfActions);
+        if (cells == null) cells = new ArrayList<>(countOfActions);
         // Send data in cellblocks. The call to buildNoDataMultiRequest will skip RowMutations.
         // They have already been handled above. Guess at count of cells
         regionActionBuilder = RequestConverter.buildNoDataRegionAction(regionName, actions, cells,
diff --git a/hbase-client/src/main/java/org/apache/hadoop/hbase/client/Mutation.java b/hbase-client/src/main/java/org/apache/hadoop/hbase/client/Mutation.java
index 53631d9..fb55fdd 100644
--- a/hbase-client/src/main/java/org/apache/hadoop/hbase/client/Mutation.java
+++ b/hbase-client/src/main/java/org/apache/hadoop/hbase/client/Mutation.java
@@ -92,8 +92,7 @@ public abstract class Mutation extends OperationWithAttributes implements Row, C
   protected Durability durability = Durability.USE_DEFAULT;
 
   // A Map sorted by column family.
-  protected NavigableMap<byte [], List<Cell>> familyMap =
-    new TreeMap<byte [], List<Cell>>(Bytes.BYTES_COMPARATOR);
+  protected NavigableMap<byte [], List<Cell>> familyMap = new TreeMap<>(Bytes.BYTES_COMPARATOR);
 
   @Override
   public CellScanner cellScanner() {
@@ -110,7 +109,7 @@ public abstract class Mutation extends OperationWithAttributes implements Row, C
   List<Cell> getCellList(byte[] family) {
     List<Cell> list = this.familyMap.get(family);
     if (list == null) {
-      list = new ArrayList<Cell>();
+      list = new ArrayList<>();
     }
     return list;
   }
@@ -158,8 +157,8 @@ public abstract class Mutation extends OperationWithAttributes implements Row, C
    */
   @Override
   public Map<String, Object> getFingerprint() {
-    Map<String, Object> map = new HashMap<String, Object>();
-    List<String> families = new ArrayList<String>(this.familyMap.entrySet().size());
+    Map<String, Object> map = new HashMap<>();
+    List<String> families = new ArrayList<>(this.familyMap.entrySet().size());
     // ideally, we would also include table information, but that information
     // is not stored in each Operation instance.
     map.put("families", families);
@@ -182,15 +181,14 @@ public abstract class Mutation extends OperationWithAttributes implements Row, C
     Map<String, Object> map = getFingerprint();
     // replace the fingerprint's simple list of families with a
     // map from column families to lists of qualifiers and kv details
-    Map<String, List<Map<String, Object>>> columns =
-      new HashMap<String, List<Map<String, Object>>>();
+    Map<String, List<Map<String, Object>>> columns = new HashMap<>();
     map.put("families", columns);
     map.put("row", Bytes.toStringBinary(this.row));
     int colCount = 0;
     // iterate through all column families affected
     for (Map.Entry<byte [], List<Cell>> entry : this.familyMap.entrySet()) {
       // map from this family to details for each cell affected within the family
-      List<Map<String, Object>> qualifierDetails = new ArrayList<Map<String, Object>>();
+      List<Map<String, Object>> qualifierDetails = new ArrayList<>();
       columns.put(Bytes.toStringBinary(entry.getKey()), qualifierDetails);
       colCount += entry.getValue().size();
       if (maxCols <= 0) {
@@ -220,14 +218,14 @@ public abstract class Mutation extends OperationWithAttributes implements Row, C
   }
 
   private static Map<String, Object> cellToStringMap(Cell c) {
-    Map<String, Object> stringMap = new HashMap<String, Object>();
+    Map<String, Object> stringMap = new HashMap<>();
     stringMap.put("qualifier", Bytes.toStringBinary(c.getQualifierArray(), c.getQualifierOffset(),
                 c.getQualifierLength()));
     stringMap.put("timestamp", c.getTimestamp());
     stringMap.put("vlen", c.getValueLength());
     List<Tag> tags = CellUtil.getTags(c);
     if (tags != null) {
-      List<String> tagsString = new ArrayList<String>(tags.size());
+      List<String> tagsString = new ArrayList<>(tags.size());
       for (Tag t : tags) {
         tagsString.add((t.getType()) + ":" + Bytes.toStringBinary(TagUtil.cloneValue(t)));
       }
@@ -317,7 +315,7 @@ public abstract class Mutation extends OperationWithAttributes implements Row, C
    * @return the set of clusterIds that have consumed the mutation
    */
   public List<UUID> getClusterIds() {
-    List<UUID> clusterIds = new ArrayList<UUID>();
+    List<UUID> clusterIds = new ArrayList<>();
     byte[] bytes = getAttribute(CONSUMED_CLUSTER_IDS);
     if(bytes != null) {
       ByteArrayDataInput in = ByteStreams.newDataInput(bytes);
diff --git a/hbase-client/src/main/java/org/apache/hadoop/hbase/client/OperationWithAttributes.java b/hbase-client/src/main/java/org/apache/hadoop/hbase/client/OperationWithAttributes.java
index 9fdd577..cc863b9 100644
--- a/hbase-client/src/main/java/org/apache/hadoop/hbase/client/OperationWithAttributes.java
+++ b/hbase-client/src/main/java/org/apache/hadoop/hbase/client/OperationWithAttributes.java
@@ -44,7 +44,7 @@ public abstract class OperationWithAttributes extends Operation implements Attri
     }
 
     if (attributes == null) {
-      attributes = new HashMap<String, byte[]>();
+      attributes = new HashMap<>();
     }
 
     if (value == null) {
diff --git a/hbase-client/src/main/java/org/apache/hadoop/hbase/client/PreemptiveFastFailInterceptor.java b/hbase-client/src/main/java/org/apache/hadoop/hbase/client/PreemptiveFastFailInterceptor.java
index 448e5b1..a29a662 100644
--- a/hbase-client/src/main/java/org/apache/hadoop/hbase/client/PreemptiveFastFailInterceptor.java
+++ b/hbase-client/src/main/java/org/apache/hadoop/hbase/client/PreemptiveFastFailInterceptor.java
@@ -75,8 +75,7 @@ class PreemptiveFastFailInterceptor extends RetryingCallerInterceptor {
 
   // Keeps track of failures when we cannot talk to a server. Helps in
   // fast failing clients if the server is down for a long time.
-  protected final ConcurrentMap<ServerName, FailureInfo> repeatedFailuresMap =
-      new ConcurrentHashMap<ServerName, FailureInfo>();
+  protected final ConcurrentMap<ServerName, FailureInfo> repeatedFailuresMap = new ConcurrentHashMap<>();
 
   // We populate repeatedFailuresMap every time there is a failure. So, to
   // keep it from growing unbounded, we garbage collect the failure information
@@ -90,8 +89,7 @@ class PreemptiveFastFailInterceptor extends RetryingCallerInterceptor {
   // fast fail mode for any reason.
   private long fastFailClearingTimeMilliSec;
 
-  private final ThreadLocal<MutableBoolean> threadRetryingInFastFailMode =
-      new ThreadLocal<MutableBoolean>();
+  private final ThreadLocal<MutableBoolean> threadRetryingInFastFailMode = new ThreadLocal<>();
 
   public PreemptiveFastFailInterceptor(Configuration conf) {
     this.fastFailThresholdMilliSec = conf.getLong(
diff --git a/hbase-client/src/main/java/org/apache/hadoop/hbase/client/Put.java b/hbase-client/src/main/java/org/apache/hadoop/hbase/client/Put.java
index a6ebd03..701dceb 100644
--- a/hbase-client/src/main/java/org/apache/hadoop/hbase/client/Put.java
+++ b/hbase-client/src/main/java/org/apache/hadoop/hbase/client/Put.java
@@ -161,9 +161,9 @@ public class Put extends Mutation implements HeapSize, Comparable<Row> {
    */
   public Put(Put putToCopy) {
     this(putToCopy.getRow(), putToCopy.ts);
-    this.familyMap = new TreeMap<byte [], List<Cell>>(Bytes.BYTES_COMPARATOR);
+    this.familyMap = new TreeMap<>(Bytes.BYTES_COMPARATOR);
     for(Map.Entry<byte [], List<Cell>> entry: putToCopy.getFamilyCellMap().entrySet()) {
-      this.familyMap.put(entry.getKey(), new ArrayList<Cell>(entry.getValue()));
+      this.familyMap.put(entry.getKey(), new ArrayList<>(entry.getValue()));
     }
     this.durability = putToCopy.durability;
     for (Map.Entry<String, byte[]> entry : putToCopy.getAttributesMap().entrySet()) {
@@ -464,7 +464,7 @@ public class Put extends Mutation implements HeapSize, Comparable<Row> {
    * returns an empty list if one doesn't exist for the given family.
    */
   public List<Cell> get(byte[] family, byte[] qualifier) {
-    List<Cell> filteredList = new ArrayList<Cell>();
+    List<Cell> filteredList = new ArrayList<>();
     for (Cell cell: getCellList(family)) {
       if (CellUtil.matchingQualifier(cell, qualifier)) {
         filteredList.add(cell);
diff --git a/hbase-client/src/main/java/org/apache/hadoop/hbase/client/Result.java b/hbase-client/src/main/java/org/apache/hadoop/hbase/client/Result.java
index 232e3d3..d946e0c 100644
--- a/hbase-client/src/main/java/org/apache/hadoop/hbase/client/Result.java
+++ b/hbase-client/src/main/java/org/apache/hadoop/hbase/client/Result.java
@@ -94,7 +94,7 @@ public class Result implements CellScannable, CellScanner {
   private transient NavigableMap<byte[], NavigableMap<byte[], NavigableMap<Long, byte[]>>>
       familyMap = null;
 
-  private static ThreadLocal<byte[]> localBuffer = new ThreadLocal<byte[]>();
+  private static ThreadLocal<byte[]> localBuffer = new ThreadLocal<>();
   private static final int PAD_WIDTH = 128;
   public static final Result EMPTY_RESULT = new Result(true);
 
@@ -246,7 +246,7 @@ public class Result implements CellScannable, CellScanner {
    * did not exist in the result set
    */
   public List<Cell> getColumnCells(byte [] family, byte [] qualifier) {
-    List<Cell> result = new ArrayList<Cell>();
+    List<Cell> result = new ArrayList<>();
 
     Cell [] kvs = rawCells();
 
@@ -661,12 +661,10 @@ public class Result implements CellScannable, CellScanner {
     if(isEmpty()) {
       return null;
     }
-    NavigableMap<byte[], NavigableMap<byte[], byte[]>> returnMap =
-      new TreeMap<byte[], NavigableMap<byte[], byte[]>>(Bytes.BYTES_COMPARATOR);
+    NavigableMap<byte[], NavigableMap<byte[], byte[]>> returnMap = new TreeMap<>(Bytes.BYTES_COMPARATOR);
     for(Map.Entry<byte[], NavigableMap<byte[], NavigableMap<Long, byte[]>>>
       familyEntry : familyMap.entrySet()) {
-      NavigableMap<byte[], byte[]> qualifierMap =
-        new TreeMap<byte[], byte[]>(Bytes.BYTES_COMPARATOR);
+      NavigableMap<byte[], byte[]> qualifierMap = new TreeMap<>(Bytes.BYTES_COMPARATOR);
       for(Map.Entry<byte[], NavigableMap<Long, byte[]>> qualifierEntry :
         familyEntry.getValue().entrySet()) {
         byte [] value =
@@ -692,8 +690,7 @@ public class Result implements CellScannable, CellScanner {
     if(isEmpty()) {
       return null;
     }
-    NavigableMap<byte[], byte[]> returnMap =
-      new TreeMap<byte[], byte[]>(Bytes.BYTES_COMPARATOR);
+    NavigableMap<byte[], byte[]> returnMap = new TreeMap<>(Bytes.BYTES_COMPARATOR);
     NavigableMap<byte[], NavigableMap<Long, byte[]>> qualifierMap =
       familyMap.get(family);
     if(qualifierMap == null) {
@@ -796,7 +793,7 @@ public class Result implements CellScannable, CellScanner {
    */
   public static Result createCompleteResult(List<Result> partialResults)
       throws IOException {
-    List<Cell> cells = new ArrayList<Cell>();
+    List<Cell> cells = new ArrayList<>();
     boolean stale = false;
     byte[] prevRow = null;
     byte[] currentRow = null;
diff --git a/hbase-client/src/main/java/org/apache/hadoop/hbase/client/ResultBoundedCompletionService.java b/hbase-client/src/main/java/org/apache/hadoop/hbase/client/ResultBoundedCompletionService.java
index 2848c9d..50c3d2c 100644
--- a/hbase-client/src/main/java/org/apache/hadoop/hbase/client/ResultBoundedCompletionService.java
+++ b/hbase-client/src/main/java/org/apache/hadoop/hbase/client/ResultBoundedCompletionService.java
@@ -167,7 +167,7 @@ public class ResultBoundedCompletionService<V> {
 
 
   public void submit(RetryingCallable<V> task, int callTimeout, int id) {
-    QueueingFuture<V> newFuture = new QueueingFuture<V>(task, callTimeout, id);
+    QueueingFuture<V> newFuture = new QueueingFuture<>(task, callTimeout, id);
     executor.execute(Trace.wrap(newFuture));
     tasks[id] = newFuture;
   }
diff --git a/hbase-client/src/main/java/org/apache/hadoop/hbase/client/RetriesExhaustedWithDetailsException.java b/hbase-client/src/main/java/org/apache/hadoop/hbase/client/RetriesExhaustedWithDetailsException.java
index f24e614..8b09222 100644
--- a/hbase-client/src/main/java/org/apache/hadoop/hbase/client/RetriesExhaustedWithDetailsException.java
+++ b/hbase-client/src/main/java/org/apache/hadoop/hbase/client/RetriesExhaustedWithDetailsException.java
@@ -110,7 +110,7 @@ extends RetriesExhaustedException {
     String s = getDesc(classifyExs(exceptions));
     StringBuilder addrs = new StringBuilder(s);
     addrs.append("servers with issues: ");
-    Set<String> uniqAddr = new HashSet<String>();
+    Set<String> uniqAddr = new HashSet<>();
     uniqAddr.addAll(hostnamePort);
 
     for(String addr : uniqAddr) {
@@ -143,7 +143,7 @@ extends RetriesExhaustedException {
 
 
   public static Map<String, Integer> classifyExs(List<Throwable> ths) {
-    Map<String, Integer> cls = new HashMap<String, Integer>();
+    Map<String, Integer> cls = new HashMap<>();
     for (Throwable t : ths) {
       if (t == null) continue;
       String name = "";
diff --git a/hbase-client/src/main/java/org/apache/hadoop/hbase/client/ReversedScannerCallable.java b/hbase-client/src/main/java/org/apache/hadoop/hbase/client/ReversedScannerCallable.java
index 6e5235b..1d46ab4 100644
--- a/hbase-client/src/main/java/org/apache/hadoop/hbase/client/ReversedScannerCallable.java
+++ b/hbase-client/src/main/java/org/apache/hadoop/hbase/client/ReversedScannerCallable.java
@@ -139,7 +139,7 @@ public class ReversedScannerCallable extends ScannerCallable {
           + Bytes.toStringBinary(startKey) + " > "
           + Bytes.toStringBinary(endKey));
     }
-    List<HRegionLocation> regionList = new ArrayList<HRegionLocation>();
+    List<HRegionLocation> regionList = new ArrayList<>();
     byte[] currentKey = startKey;
     do {
       RegionLocations rl = RpcRetryingCallerWithReadReplicas.getRegionLocations(reload, id,
diff --git a/hbase-client/src/main/java/org/apache/hadoop/hbase/client/RpcRetryingCallerFactory.java b/hbase-client/src/main/java/org/apache/hadoop/hbase/client/RpcRetryingCallerFactory.java
index cc8c23a..41a514a 100644
--- a/hbase-client/src/main/java/org/apache/hadoop/hbase/client/RpcRetryingCallerFactory.java
+++ b/hbase-client/src/main/java/org/apache/hadoop/hbase/client/RpcRetryingCallerFactory.java
@@ -84,7 +84,7 @@ public class RpcRetryingCallerFactory {
   public <T> RpcRetryingCaller<T> newCaller(int rpcTimeout) {
     // We store the values in the factory instance. This way, constructing new objects
     //  is cheap as it does not require parsing a complex structure.
-    RpcRetryingCaller<T> caller = new RpcRetryingCallerImpl<T>(pause, pauseForCQTBE, retries,
+    RpcRetryingCaller<T> caller = new RpcRetryingCallerImpl<>(pause, pauseForCQTBE, retries,
         interceptor, startLogErrorsCnt, rpcTimeout);
     return caller;
   }
@@ -95,7 +95,7 @@ public class RpcRetryingCallerFactory {
   public <T> RpcRetryingCaller<T> newCaller() {
     // We store the values in the factory instance. This way, constructing new objects
     //  is cheap as it does not require parsing a complex structure.
-    RpcRetryingCaller<T> caller = new RpcRetryingCallerImpl<T>(pause, pauseForCQTBE, retries,
+    RpcRetryingCaller<T> caller = new RpcRetryingCallerImpl<>(pause, pauseForCQTBE, retries,
         interceptor, startLogErrorsCnt, rpcTimeout);
     return caller;
   }
diff --git a/hbase-client/src/main/java/org/apache/hadoop/hbase/client/RpcRetryingCallerImpl.java b/hbase-client/src/main/java/org/apache/hadoop/hbase/client/RpcRetryingCallerImpl.java
index 6450adf..3f65e6e 100644
--- a/hbase-client/src/main/java/org/apache/hadoop/hbase/client/RpcRetryingCallerImpl.java
+++ b/hbase-client/src/main/java/org/apache/hadoop/hbase/client/RpcRetryingCallerImpl.java
@@ -94,8 +94,7 @@ public class RpcRetryingCallerImpl<T> implements RpcRetryingCaller<T> {
   @Override
   public T callWithRetries(RetryingCallable<T> callable, int callTimeout)
   throws IOException, RuntimeException {
-    List<RetriesExhaustedException.ThrowableWithExtraContext> exceptions =
-      new ArrayList<RetriesExhaustedException.ThrowableWithExtraContext>();
+    List<RetriesExhaustedException.ThrowableWithExtraContext> exceptions = new ArrayList<>();
     tracker.start();
     context.clear();
     for (int tries = 0;; tries++) {
diff --git a/hbase-client/src/main/java/org/apache/hadoop/hbase/client/RpcRetryingCallerWithReadReplicas.java b/hbase-client/src/main/java/org/apache/hadoop/hbase/client/RpcRetryingCallerWithReadReplicas.java
index 316fad1..0050269 100644
--- a/hbase-client/src/main/java/org/apache/hadoop/hbase/client/RpcRetryingCallerWithReadReplicas.java
+++ b/hbase-client/src/main/java/org/apache/hadoop/hbase/client/RpcRetryingCallerWithReadReplicas.java
@@ -173,7 +173,7 @@ public class RpcRetryingCallerWithReadReplicas {
     RegionLocations rl = getRegionLocations(true, (isTargetReplicaSpecified ? get.getReplicaId()
         : RegionReplicaUtil.DEFAULT_REPLICA_ID), cConnection, tableName, get.getRow());
    final ResultBoundedCompletionService<Result> cs =
-        new ResultBoundedCompletionService<Result>(this.rpcRetryingCallerFactory, pool, rl.size());
+        new ResultBoundedCompletionService<>(this.rpcRetryingCallerFactory, pool, rl.size());
     int startIndex = 0;
     int endIndex = rl.size();
 
diff --git a/hbase-client/src/main/java/org/apache/hadoop/hbase/client/Scan.java b/hbase-client/src/main/java/org/apache/hadoop/hbase/client/Scan.java
index c4b7044..28000ce 100644
--- a/hbase-client/src/main/java/org/apache/hadoop/hbase/client/Scan.java
+++ b/hbase-client/src/main/java/org/apache/hadoop/hbase/client/Scan.java
@@ -143,8 +143,7 @@ public class Scan extends Query {
   private long maxResultSize = -1;
   private boolean cacheBlocks = true;
   private boolean reversed = false;
-  private Map<byte[], NavigableSet<byte[]>> familyMap =
-      new TreeMap<byte[], NavigableSet<byte[]>>(Bytes.BYTES_COMPARATOR);
+  private Map<byte[], NavigableSet<byte[]>> familyMap = new TreeMap<>(Bytes.BYTES_COMPARATOR);
   private Boolean asyncPrefetch = null;
 
   /**
@@ -339,7 +338,7 @@ public class Scan extends Query {
   public Scan addColumn(byte [] family, byte [] qualifier) {
     NavigableSet<byte []> set = familyMap.get(family);
     if(set == null) {
-      set = new TreeSet<byte []>(Bytes.BYTES_COMPARATOR);
+      set = new TreeSet<>(Bytes.BYTES_COMPARATOR);
     }
     if (qualifier == null) {
       qualifier = HConstants.EMPTY_BYTE_ARRAY;
@@ -887,8 +886,8 @@ public class Scan extends Query {
    */
   @Override
   public Map<String, Object> getFingerprint() {
-    Map<String, Object> map = new HashMap<String, Object>();
-    List<String> families = new ArrayList<String>();
+    Map<String, Object> map = new HashMap<>();
+    List<String> families = new ArrayList<>();
     if(this.familyMap.isEmpty()) {
       map.put("families", "ALL");
       return map;
@@ -914,8 +913,7 @@ public class Scan extends Query {
     // start with the fingerpring map and build on top of it
     Map<String, Object> map = getFingerprint();
     // map from families to column list replaces fingerprint's list of families
-    Map<String, List<String>> familyColumns =
-      new HashMap<String, List<String>>();
+    Map<String, List<String>> familyColumns = new HashMap<>();
     map.put("families", familyColumns);
     // add scalar information first
     map.put("startRow", Bytes.toStringBinary(this.startRow));
@@ -926,7 +924,7 @@ public class Scan extends Query {
     map.put("maxResultSize", this.maxResultSize);
     map.put("cacheBlocks", this.cacheBlocks);
     map.put("loadColumnFamiliesOnDemand", this.loadColumnFamiliesOnDemand);
-    List<Long> timeRange = new ArrayList<Long>(2);
+    List<Long> timeRange = new ArrayList<>(2);
     timeRange.add(this.tr.getMin());
     timeRange.add(this.tr.getMax());
     map.put("timeRange", timeRange);
@@ -934,7 +932,7 @@ public class Scan extends Query {
     // iterate through affected families and list out up to maxCols columns
     for (Map.Entry<byte [], NavigableSet<byte[]>> entry :
       this.familyMap.entrySet()) {
-      List<String> columns = new ArrayList<String>();
+      List<String> columns = new ArrayList<>();
       familyColumns.put(Bytes.toStringBinary(entry.getKey()), columns);
       if(entry.getValue() == null) {
         colCount++;
diff --git a/hbase-client/src/main/java/org/apache/hadoop/hbase/client/ScannerCallableWithReplicas.java b/hbase-client/src/main/java/org/apache/hadoop/hbase/client/ScannerCallableWithReplicas.java
index 1000753..7f8cec5 100644
--- a/hbase-client/src/main/java/org/apache/hadoop/hbase/client/ScannerCallableWithReplicas.java
+++ b/hbase-client/src/main/java/org/apache/hadoop/hbase/client/ScannerCallableWithReplicas.java
@@ -69,7 +69,7 @@ class ScannerCallableWithReplicas implements RetryingCallable<Result[]> {
   private final TableName tableName;
   private Configuration conf;
   private int scannerTimeout;
-  private Set<ScannerCallable> outstandingCallables = new HashSet<ScannerCallable>();
+  private Set<ScannerCallable> outstandingCallables = new HashSet<>();
   private boolean someRPCcancelled = false; //required for testing purposes only
 
   public ScannerCallableWithReplicas(TableName tableName, ClusterConnection cConnection,
@@ -149,7 +149,7 @@ class ScannerCallableWithReplicas implements RetryingCallable<Result[]> {
     // allocate a boundedcompletion pool of some multiple of number of replicas.
     // We want to accomodate some RPCs for redundant replica scans (but are still in progress)
     ResultBoundedCompletionService<Pair<Result[], ScannerCallable>> cs =
-        new ResultBoundedCompletionService<Pair<Result[], ScannerCallable>>(
+        new ResultBoundedCompletionService<>(
             RpcRetryingCallerFactory.instantiate(ScannerCallableWithReplicas.this.conf), pool,
             rl.size() * 5);
 
@@ -359,7 +359,7 @@ class ScannerCallableWithReplicas implements RetryingCallable<Result[]> {
         return null;
       }
       Result[] res = this.caller.callWithoutRetries(this.callable, callTimeout);
-      return new Pair<Result[], ScannerCallable>(res, this.callable);
+      return new Pair<>(res, this.callable);
     }
 
     @Override
diff --git a/hbase-client/src/main/java/org/apache/hadoop/hbase/client/ServerStatisticTracker.java b/hbase-client/src/main/java/org/apache/hadoop/hbase/client/ServerStatisticTracker.java
index f66e7fc..f78ca41 100644
--- a/hbase-client/src/main/java/org/apache/hadoop/hbase/client/ServerStatisticTracker.java
+++ b/hbase-client/src/main/java/org/apache/hadoop/hbase/client/ServerStatisticTracker.java
@@ -35,8 +35,7 @@ import org.apache.hadoop.hbase.client.backoff.ServerStatistics;
 @InterfaceAudience.Private
 public class ServerStatisticTracker implements StatisticTrackable {
 
-  private final ConcurrentHashMap<ServerName, ServerStatistics> stats =
-      new ConcurrentHashMap<ServerName, ServerStatistics>();
+  private final ConcurrentHashMap<ServerName, ServerStatistics> stats = new ConcurrentHashMap<>();
 
   @Override
   public void updateRegionStats(ServerName server, byte[] region, RegionLoadStats currentStats) {
diff --git a/hbase-client/src/main/java/org/apache/hadoop/hbase/client/backoff/ServerStatistics.java b/hbase-client/src/main/java/org/apache/hadoop/hbase/client/backoff/ServerStatistics.java
index e33e2bc..a953e8c 100644
--- a/hbase-client/src/main/java/org/apache/hadoop/hbase/client/backoff/ServerStatistics.java
+++ b/hbase-client/src/main/java/org/apache/hadoop/hbase/client/backoff/ServerStatistics.java
@@ -30,8 +30,7 @@ import java.util.TreeMap;
 @InterfaceAudience.Private
 public class ServerStatistics {
 
-  private Map<byte[], RegionStatistics>
-      stats = new TreeMap<byte[], RegionStatistics>(Bytes.BYTES_COMPARATOR);
+  private Map<byte[], RegionStatistics> stats = new TreeMap<>(Bytes.BYTES_COMPARATOR);
 
   /**
    * Good enough attempt. Last writer wins. It doesn't really matter which one gets to update,
diff --git a/hbase-client/src/main/java/org/apache/hadoop/hbase/client/metrics/ServerSideScanMetrics.java b/hbase-client/src/main/java/org/apache/hadoop/hbase/client/metrics/ServerSideScanMetrics.java
index 46b67d4..59e4d60 100644
--- a/hbase-client/src/main/java/org/apache/hadoop/hbase/client/metrics/ServerSideScanMetrics.java
+++ b/hbase-client/src/main/java/org/apache/hadoop/hbase/client/metrics/ServerSideScanMetrics.java
@@ -35,7 +35,7 @@ public class ServerSideScanMetrics {
   /**
    * Hash to hold the String -&gt; Atomic Long mappings for each metric
    */
-  private final Map<String, AtomicLong> counters = new HashMap<String, AtomicLong>();
+  private final Map<String, AtomicLong> counters = new HashMap<>();
 
   /**
    * Create a new counter with the specified name
diff --git a/hbase-client/src/main/java/org/apache/hadoop/hbase/client/replication/ReplicationAdmin.java b/hbase-client/src/main/java/org/apache/hadoop/hbase/client/replication/ReplicationAdmin.java
index 4e74d87..c7f040e 100644
--- a/hbase-client/src/main/java/org/apache/hadoop/hbase/client/replication/ReplicationAdmin.java
+++ b/hbase-client/src/main/java/org/apache/hadoop/hbase/client/replication/ReplicationAdmin.java
@@ -273,7 +273,7 @@ public class ReplicationAdmin implements Closeable {
   @Deprecated
   public Map<String, ReplicationPeerConfig> listPeerConfigs() throws IOException {
     List<ReplicationPeerDescription> peers = this.admin.listReplicationPeers();
-    Map<String, ReplicationPeerConfig> result = new TreeMap<String, ReplicationPeerConfig>();
+    Map<String, ReplicationPeerConfig> result = new TreeMap<>();
     for (ReplicationPeerDescription peer : peers) {
       result.put(peer.getPeerId(), peer.getPeerConfig());
     }
@@ -343,7 +343,7 @@ public class ReplicationAdmin implements Closeable {
         if (cfs == null || appendCfs == null || appendCfs.isEmpty()) {
           preTableCfs.put(table, null);
         } else {
-          Set<String> cfSet = new HashSet<String>(cfs);
+          Set<String> cfSet = new HashSet<>(cfs);
           cfSet.addAll(appendCfs);
           preTableCfs.put(table, Lists.newArrayList(cfSet));
         }
@@ -400,7 +400,7 @@ public class ReplicationAdmin implements Closeable {
         if (cfs == null && (removeCfs == null || removeCfs.isEmpty())) {
           preTableCfs.remove(table);
         } else if (cfs != null && (removeCfs != null && !removeCfs.isEmpty())) {
-          Set<String> cfSet = new HashSet<String>(cfs);
+          Set<String> cfSet = new HashSet<>(cfs);
           cfSet.removeAll(removeCfs);
           if (cfSet.isEmpty()) {
             preTableCfs.remove(table);
@@ -484,7 +484,7 @@ public class ReplicationAdmin implements Closeable {
         tableCFs.getColumnFamilyMap()
             .forEach(
               (cf, scope) -> {
-                HashMap<String, String> replicationEntry = new HashMap<String, String>();
+                HashMap<String, String> replicationEntry = new HashMap<>();
                 replicationEntry.put(TNAME, table);
                 replicationEntry.put(CFNAME, cf);
                 replicationEntry.put(REPLICATIONTYPE,
@@ -531,7 +531,7 @@ public class ReplicationAdmin implements Closeable {
     if (peers == null || peers.size() <= 0) {
       return null;
     }
-    List<ReplicationPeer> listOfPeers = new ArrayList<ReplicationPeer>(peers.size());
+    List<ReplicationPeer> listOfPeers = new ArrayList<>(peers.size());
     for (Entry<String, ReplicationPeerConfig> peerEntry : peers.entrySet()) {
       String peerId = peerEntry.getKey();
       try {
diff --git a/hbase-client/src/main/java/org/apache/hadoop/hbase/client/replication/ReplicationSerDeHelper.java b/hbase-client/src/main/java/org/apache/hadoop/hbase/client/replication/ReplicationSerDeHelper.java
index 2965219..2d5539c 100644
--- a/hbase-client/src/main/java/org/apache/hadoop/hbase/client/replication/ReplicationSerDeHelper.java
+++ b/hbase-client/src/main/java/org/apache/hadoop/hbase/client/replication/ReplicationSerDeHelper.java
@@ -215,7 +215,7 @@ public final class ReplicationSerDeHelper {
     if (tableCFs == null || tableCFs.length == 0) {
       return null;
     }
-    Map<TableName, List<String>> tableCFsMap = new HashMap<TableName, List<String>>();
+    Map<TableName, List<String>> tableCFsMap = new HashMap<>();
     for (int i = 0, n = tableCFs.length; i < n; i++) {
       ReplicationProtos.TableCF tableCF = tableCFs[i];
       List<String> families = new ArrayList<>();
@@ -283,7 +283,7 @@ public final class ReplicationSerDeHelper {
     }
     List<ByteString> namespacesList = peer.getNamespacesList();
     if (namespacesList != null && namespacesList.size() != 0) {
-      Set<String> namespaces = new HashSet<String>();
+      Set<String> namespaces = new HashSet<>();
       for (ByteString namespace : namespacesList) {
         namespaces.add(namespace.toStringUtf8());
       }
diff --git a/hbase-client/src/main/java/org/apache/hadoop/hbase/filter/CompareFilter.java b/hbase-client/src/main/java/org/apache/hadoop/hbase/filter/CompareFilter.java
index e74797d..bbc31ec 100644
--- a/hbase-client/src/main/java/org/apache/hadoop/hbase/filter/CompareFilter.java
+++ b/hbase-client/src/main/java/org/apache/hadoop/hbase/filter/CompareFilter.java
@@ -177,7 +177,7 @@ public abstract class CompareFilter extends FilterBase {
                                             " can only be used with EQUAL and NOT_EQUAL");
       }
     }
-    ArrayList<Object> arguments = new ArrayList<Object>(2);
+    ArrayList<Object> arguments = new ArrayList<>(2);
     arguments.add(compareOp);
     arguments.add(comparator);
     return arguments;
diff --git a/hbase-client/src/main/java/org/apache/hadoop/hbase/filter/DependentColumnFilter.java b/hbase-client/src/main/java/org/apache/hadoop/hbase/filter/DependentColumnFilter.java
index 287a090..d82eaec 100644
--- a/hbase-client/src/main/java/org/apache/hadoop/hbase/filter/DependentColumnFilter.java
+++ b/hbase-client/src/main/java/org/apache/hadoop/hbase/filter/DependentColumnFilter.java
@@ -54,7 +54,7 @@ public class DependentColumnFilter extends CompareFilter {
   protected byte[] columnQualifier;
   protected boolean dropDependentColumn;
 
-  protected Set<Long> stampSet = new HashSet<Long>();
+  protected Set<Long> stampSet = new HashSet<>();
   
   /**
    * Build a dependent column filter with value checking
diff --git a/hbase-client/src/main/java/org/apache/hadoop/hbase/filter/FilterList.java b/hbase-client/src/main/java/org/apache/hadoop/hbase/filter/FilterList.java
index c10d18c..04eba0c 100644
--- a/hbase-client/src/main/java/org/apache/hadoop/hbase/filter/FilterList.java
+++ b/hbase-client/src/main/java/org/apache/hadoop/hbase/filter/FilterList.java
@@ -453,7 +453,7 @@ final public class FilterList extends FilterBase {
       throw new DeserializationException(e);
     }
 
-    List<Filter> rowFilters = new ArrayList<Filter>(proto.getFiltersCount());
+    List<Filter> rowFilters = new ArrayList<>(proto.getFiltersCount());
     try {
       List<FilterProtos.Filter> filtersList = proto.getFiltersList();
       int listSize = filtersList.size();
diff --git a/hbase-client/src/main/java/org/apache/hadoop/hbase/filter/FirstKeyValueMatchingQualifiersFilter.java b/hbase-client/src/main/java/org/apache/hadoop/hbase/filter/FirstKeyValueMatchingQualifiersFilter.java
index 82d6c57..6b202ad 100644
--- a/hbase-client/src/main/java/org/apache/hadoop/hbase/filter/FirstKeyValueMatchingQualifiersFilter.java
+++ b/hbase-client/src/main/java/org/apache/hadoop/hbase/filter/FirstKeyValueMatchingQualifiersFilter.java
@@ -108,7 +108,7 @@ public class FirstKeyValueMatchingQualifiersFilter extends FirstKeyOnlyFilter {
       throw new DeserializationException(e);
     }
 
-    TreeSet<byte []> qualifiers = new TreeSet<byte []>(Bytes.BYTES_COMPARATOR);
+    TreeSet<byte []> qualifiers = new TreeSet<>(Bytes.BYTES_COMPARATOR);
     for (ByteString qualifier : proto.getQualifiersList()) {
       qualifiers.add(qualifier.toByteArray());
     }
diff --git a/hbase-client/src/main/java/org/apache/hadoop/hbase/filter/FuzzyRowFilter.java b/hbase-client/src/main/java/org/apache/hadoop/hbase/filter/FuzzyRowFilter.java
index 5fc12b9..65c2a61 100644
--- a/hbase-client/src/main/java/org/apache/hadoop/hbase/filter/FuzzyRowFilter.java
+++ b/hbase-client/src/main/java/org/apache/hadoop/hbase/filter/FuzzyRowFilter.java
@@ -83,7 +83,7 @@ public class FuzzyRowFilter extends FilterBase {
       p = fuzzyKeysData.get(i);
       if (p.getFirst().length != p.getSecond().length) {
         Pair<String, String> readable =
-            new Pair<String, String>(Bytes.toStringBinary(p.getFirst()), Bytes.toStringBinary(p
+            new Pair<>(Bytes.toStringBinary(p.getFirst()), Bytes.toStringBinary(p
                 .getSecond()));
         throw new IllegalArgumentException("Fuzzy pair lengths do not match: " + readable);
       }
@@ -191,8 +191,7 @@ public class FuzzyRowFilter extends FilterBase {
     private boolean initialized = false;
 
     RowTracker() {
-      nextRows =
-          new PriorityQueue<Pair<byte[], Pair<byte[], byte[]>>>(fuzzyKeysData.size(),
+      nextRows = new PriorityQueue<>(fuzzyKeysData.size(),
               new Comparator<Pair<byte[], Pair<byte[], byte[]>>>() {
                 @Override
                 public int compare(Pair<byte[], Pair<byte[], byte[]>> o1,
@@ -239,7 +238,7 @@ public class FuzzyRowFilter extends FilterBase {
           getNextForFuzzyRule(isReversed(), currentCell.getRowArray(), currentCell.getRowOffset(),
             currentCell.getRowLength(), fuzzyData.getFirst(), fuzzyData.getSecond());
       if (nextRowKeyCandidate != null) {
-        nextRows.add(new Pair<byte[], Pair<byte[], byte[]>>(nextRowKeyCandidate, fuzzyData));
+        nextRows.add(new Pair<>(nextRowKeyCandidate, fuzzyData));
       }
     }
 
@@ -278,12 +277,12 @@ public class FuzzyRowFilter extends FilterBase {
       throw new DeserializationException(e);
     }
     int count = proto.getFuzzyKeysDataCount();
-    ArrayList<Pair<byte[], byte[]>> fuzzyKeysData = new ArrayList<Pair<byte[], byte[]>>(count);
+    ArrayList<Pair<byte[], byte[]>> fuzzyKeysData = new ArrayList<>(count);
     for (int i = 0; i < count; ++i) {
       BytesBytesPair current = proto.getFuzzyKeysData(i);
       byte[] keyBytes = current.getFirst().toByteArray();
       byte[] keyMeta = current.getSecond().toByteArray();
-      fuzzyKeysData.add(new Pair<byte[], byte[]>(keyBytes, keyMeta));
+      fuzzyKeysData.add(new Pair<>(keyBytes, keyMeta));
     }
     return new FuzzyRowFilter(fuzzyKeysData);
   }
diff --git a/hbase-client/src/main/java/org/apache/hadoop/hbase/filter/MultiRowRangeFilter.java b/hbase-client/src/main/java/org/apache/hadoop/hbase/filter/MultiRowRangeFilter.java
index 2cc754a..77fbaf4 100644
--- a/hbase-client/src/main/java/org/apache/hadoop/hbase/filter/MultiRowRangeFilter.java
+++ b/hbase-client/src/main/java/org/apache/hadoop/hbase/filter/MultiRowRangeFilter.java
@@ -174,7 +174,7 @@ public class MultiRowRangeFilter extends FilterBase {
     }
     int length = proto.getRowRangeListCount();
     List<FilterProtos.RowRange> rangeProtos = proto.getRowRangeListList();
-    List<RowRange> rangeList = new ArrayList<RowRange>(length);
+    List<RowRange> rangeList = new ArrayList<>(length);
     for (FilterProtos.RowRange rangeProto : rangeProtos) {
       RowRange range = new RowRange(rangeProto.hasStartRow() ? rangeProto.getStartRow()
           .toByteArray() : null, rangeProto.getStartRowInclusive(), rangeProto.hasStopRow() ?
@@ -252,8 +252,8 @@ public class MultiRowRangeFilter extends FilterBase {
     if (ranges.isEmpty()) {
       throw new IllegalArgumentException("No ranges found.");
     }
-    List<RowRange> invalidRanges = new ArrayList<RowRange>();
-    List<RowRange> newRanges = new ArrayList<RowRange>(ranges.size());
+    List<RowRange> invalidRanges = new ArrayList<>();
+    List<RowRange> newRanges = new ArrayList<>(ranges.size());
     Collections.sort(ranges);
     if(ranges.get(0).isValid()) {
       if (ranges.size() == 1) {
diff --git a/hbase-client/src/main/java/org/apache/hadoop/hbase/filter/MultipleColumnPrefixFilter.java b/hbase-client/src/main/java/org/apache/hadoop/hbase/filter/MultipleColumnPrefixFilter.java
index bc26812..12d9ac7 100644
--- a/hbase-client/src/main/java/org/apache/hadoop/hbase/filter/MultipleColumnPrefixFilter.java
+++ b/hbase-client/src/main/java/org/apache/hadoop/hbase/filter/MultipleColumnPrefixFilter.java
@@ -164,7 +164,7 @@ public class MultipleColumnPrefixFilter extends FilterBase {
   }
 
   public TreeSet<byte []> createTreeSet() {
-    return new TreeSet<byte []>(new Comparator<Object>() {
+    return new TreeSet<>(new Comparator<Object>() {
         @Override
           public int compare (Object o1, Object o2) {
           if (o1 == null || o2 == null)
diff --git a/hbase-client/src/main/java/org/apache/hadoop/hbase/filter/ParseFilter.java b/hbase-client/src/main/java/org/apache/hadoop/hbase/filter/ParseFilter.java
index f59ddb5..0823785 100644
--- a/hbase-client/src/main/java/org/apache/hadoop/hbase/filter/ParseFilter.java
+++ b/hbase-client/src/main/java/org/apache/hadoop/hbase/filter/ParseFilter.java
@@ -56,7 +56,7 @@ public class ParseFilter {
 
   static {
     // Registers all the filter supported by the Filter Language
-    filterHashMap = new HashMap<String, String>();
+    filterHashMap = new HashMap<>();
     filterHashMap.put("KeyOnlyFilter", ParseConstants.FILTER_PACKAGE + "." +
                       "KeyOnlyFilter");
     filterHashMap.put("FirstKeyOnlyFilter", ParseConstants.FILTER_PACKAGE + "." +
@@ -95,7 +95,7 @@ public class ParseFilter {
                       "DependentColumnFilter");
 
     // Creates the operatorPrecedenceHashMap
-    operatorPrecedenceHashMap = new HashMap<ByteBuffer, Integer>();
+    operatorPrecedenceHashMap = new HashMap<>();
     operatorPrecedenceHashMap.put(ParseConstants.SKIP_BUFFER, 1);
     operatorPrecedenceHashMap.put(ParseConstants.WHILE_BUFFER, 1);
     operatorPrecedenceHashMap.put(ParseConstants.AND_BUFFER, 2);
@@ -122,9 +122,9 @@ public class ParseFilter {
   public Filter parseFilterString (byte [] filterStringAsByteArray)
     throws CharacterCodingException {
     // stack for the operators and parenthesis
-    Stack <ByteBuffer> operatorStack = new Stack<ByteBuffer>();
+    Stack <ByteBuffer> operatorStack = new Stack<>();
     // stack for the filter objects
-    Stack <Filter> filterStack = new Stack<Filter>();
+    Stack <Filter> filterStack = new Stack<>();
 
     Filter filter = null;
     for (int i=0; i<filterStringAsByteArray.length; i++) {
@@ -309,7 +309,7 @@ public class ParseFilter {
 
     int argumentStartIndex = 0;
     int argumentEndIndex = 0;
-    ArrayList<byte []> filterArguments = new ArrayList<byte []>();
+    ArrayList<byte []> filterArguments = new ArrayList<>();
 
     for (int i = argumentListStartIndex + 1; i<filterStringAsByteArray.length; i++) {
 
@@ -393,7 +393,7 @@ public class ParseFilter {
     if (argumentOnTopOfStack.equals(ParseConstants.OR_BUFFER)) {
       // The top of the stack is an OR
       try {
-        ArrayList<Filter> listOfFilters = new ArrayList<Filter>();
+        ArrayList<Filter> listOfFilters = new ArrayList<>();
         while (!operatorStack.empty() && operatorStack.peek().equals(ParseConstants.OR_BUFFER)) {
           Filter filter = filterStack.pop();
           listOfFilters.add(0, filter);
@@ -410,7 +410,7 @@ public class ParseFilter {
     } else if (argumentOnTopOfStack.equals(ParseConstants.AND_BUFFER)) {
       // The top of the stack is an AND
       try {
-        ArrayList<Filter> listOfFilters = new ArrayList<Filter>();
+        ArrayList<Filter> listOfFilters = new ArrayList<>();
         while (!operatorStack.empty() && operatorStack.peek().equals(ParseConstants.AND_BUFFER)) {
           Filter filter = filterStack.pop();
           listOfFilters.add(0, filter);
diff --git a/hbase-client/src/main/java/org/apache/hadoop/hbase/filter/TimestampsFilter.java b/hbase-client/src/main/java/org/apache/hadoop/hbase/filter/TimestampsFilter.java
index 921b7b4..8c58f91 100644
--- a/hbase-client/src/main/java/org/apache/hadoop/hbase/filter/TimestampsFilter.java
+++ b/hbase-client/src/main/java/org/apache/hadoop/hbase/filter/TimestampsFilter.java
@@ -77,7 +77,7 @@ public class TimestampsFilter extends FilterBase {
       Preconditions.checkArgument(timestamp >= 0, "must be positive %s", timestamp);
     }
     this.canHint = canHint;
-    this.timestamps = new TreeSet<Long>(timestamps);
+    this.timestamps = new TreeSet<>(timestamps);
     init();
   }
 
@@ -85,7 +85,7 @@ public class TimestampsFilter extends FilterBase {
    * @return the list of timestamps
    */
   public List<Long> getTimestamps() {
-    List<Long> list = new ArrayList<Long>(timestamps.size());
+    List<Long> list = new ArrayList<>(timestamps.size());
     list.addAll(timestamps);
     return list;
   }
@@ -157,7 +157,7 @@ public class TimestampsFilter extends FilterBase {
   }
 
   public static Filter createFilterFromArguments(ArrayList<byte []> filterArguments) {
-    ArrayList<Long> timestamps = new ArrayList<Long>(filterArguments.size());
+    ArrayList<Long> timestamps = new ArrayList<>(filterArguments.size());
     for (int i = 0; i<filterArguments.size(); i++) {
       long timestamp = ParseFilter.convertByteArrayToLong(filterArguments.get(i));
       timestamps.add(timestamp);
diff --git a/hbase-client/src/main/java/org/apache/hadoop/hbase/ipc/NettyRpcDuplexHandler.java b/hbase-client/src/main/java/org/apache/hadoop/hbase/ipc/NettyRpcDuplexHandler.java
index a302d48..e69b42d 100644
--- a/hbase-client/src/main/java/org/apache/hadoop/hbase/ipc/NettyRpcDuplexHandler.java
+++ b/hbase-client/src/main/java/org/apache/hadoop/hbase/ipc/NettyRpcDuplexHandler.java
@@ -62,7 +62,7 @@ class NettyRpcDuplexHandler extends ChannelDuplexHandler {
 
   private final CompressionCodec compressor;
 
-  private final Map<Integer, Call> id2Call = new HashMap<Integer, Call>();
+  private final Map<Integer, Call> id2Call = new HashMap<>();
 
   public NettyRpcDuplexHandler(NettyRpcConnection conn, CellBlockBuilder cellBlockBuilder,
       Codec codec, CompressionCodec compressor) {
diff --git a/hbase-client/src/main/java/org/apache/hadoop/hbase/protobuf/ProtobufUtil.java b/hbase-client/src/main/java/org/apache/hadoop/hbase/protobuf/ProtobufUtil.java
index 52ee8a5..89d45e2 100644
--- a/hbase-client/src/main/java/org/apache/hadoop/hbase/protobuf/ProtobufUtil.java
+++ b/hbase-client/src/main/java/org/apache/hadoop/hbase/protobuf/ProtobufUtil.java
@@ -118,8 +118,7 @@ public final class ProtobufUtil {
   /**
    * Primitive type to class mapping.
    */
-  private final static Map<String, Class<?>>
-    PRIMITIVES = new HashMap<String, Class<?>>();
+  private final static Map<String, Class<?>> PRIMITIVES = new HashMap<>();
 
   /**
    * Many results are simple: no cell, exists true or false. To save on object creations,
@@ -1384,7 +1383,7 @@ public final class ProtobufUtil {
       return proto.getStale() ? EMPTY_RESULT_STALE : EMPTY_RESULT;
     }
 
-    List<Cell> cells = new ArrayList<Cell>(values.size());
+    List<Cell> cells = new ArrayList<>(values.size());
     for (CellProtos.Cell c : values) {
       cells.add(toCell(c));
     }
@@ -1418,7 +1417,7 @@ public final class ProtobufUtil {
     List<Cell> cells = null;
     if (proto.hasAssociatedCellCount()) {
       int count = proto.getAssociatedCellCount();
-      cells = new ArrayList<Cell>(count + values.size());
+      cells = new ArrayList<>(count + values.size());
       for (int i = 0; i < count; i++) {
         if (!scanner.advance()) throw new IOException("Failed get " + i + " of " + count);
         cells.add(scanner.current());
@@ -1426,7 +1425,7 @@ public final class ProtobufUtil {
     }
 
     if (!values.isEmpty()){
-      if (cells == null) cells = new ArrayList<Cell>(values.size());
+      if (cells == null) cells = new ArrayList<>(values.size());
       for (CellProtos.Cell c: values) {
         cells.add(toCell(c));
       }
diff --git a/hbase-client/src/main/java/org/apache/hadoop/hbase/quotas/QuotaFilter.java b/hbase-client/src/main/java/org/apache/hadoop/hbase/quotas/QuotaFilter.java
index c3db6ee..309dd9c 100644
--- a/hbase-client/src/main/java/org/apache/hadoop/hbase/quotas/QuotaFilter.java
+++ b/hbase-client/src/main/java/org/apache/hadoop/hbase/quotas/QuotaFilter.java
@@ -30,7 +30,7 @@ import org.apache.hadoop.hbase.util.Strings;
 @InterfaceAudience.Public
 @InterfaceStability.Evolving
 public class QuotaFilter {
-  private Set<QuotaType> types = new HashSet<QuotaType>();
+  private Set<QuotaType> types = new HashSet<>();
   private boolean hasFilters = false;
   private String namespaceRegex;
   private String tableRegex;
diff --git a/hbase-client/src/main/java/org/apache/hadoop/hbase/quotas/QuotaRetriever.java b/hbase-client/src/main/java/org/apache/hadoop/hbase/quotas/QuotaRetriever.java
index 37e4a92..fecd2d1 100644
--- a/hbase-client/src/main/java/org/apache/hadoop/hbase/quotas/QuotaRetriever.java
+++ b/hbase-client/src/main/java/org/apache/hadoop/hbase/quotas/QuotaRetriever.java
@@ -47,7 +47,7 @@ import org.apache.hadoop.util.StringUtils;
 public class QuotaRetriever implements Closeable, Iterable<QuotaSettings> {
   private static final Log LOG = LogFactory.getLog(QuotaRetriever.class);
 
-  private final Queue<QuotaSettings> cache = new LinkedList<QuotaSettings>();
+  private final Queue<QuotaSettings> cache = new LinkedList<>();
   private ResultScanner scanner;
   /**
    * Connection to use.
diff --git a/hbase-client/src/main/java/org/apache/hadoop/hbase/quotas/QuotaSettingsFactory.java b/hbase-client/src/main/java/org/apache/hadoop/hbase/quotas/QuotaSettingsFactory.java
index a7c49b3..1a8b934 100644
--- a/hbase-client/src/main/java/org/apache/hadoop/hbase/quotas/QuotaSettingsFactory.java
+++ b/hbase-client/src/main/java/org/apache/hadoop/hbase/quotas/QuotaSettingsFactory.java
@@ -84,7 +84,7 @@ public class QuotaSettingsFactory {
 
   private static List<QuotaSettings> fromQuotas(final String userName, final TableName tableName,
       final String namespace, final Quotas quotas) {
-    List<QuotaSettings> settings = new ArrayList<QuotaSettings>();
+    List<QuotaSettings> settings = new ArrayList<>();
     if (quotas.hasThrottle()) {
       settings.addAll(fromThrottle(userName, tableName, namespace, quotas.getThrottle()));
     }
@@ -96,7 +96,7 @@ public class QuotaSettingsFactory {
 
   private static List<QuotaSettings> fromThrottle(final String userName, final TableName tableName,
       final String namespace, final QuotaProtos.Throttle throttle) {
-    List<QuotaSettings> settings = new ArrayList<QuotaSettings>();
+    List<QuotaSettings> settings = new ArrayList<>();
     if (throttle.hasReqNum()) {
       settings.add(ThrottleSettings.fromTimedQuota(userName, tableName, namespace,
           ThrottleType.REQUEST_NUMBER, throttle.getReqNum()));
diff --git a/hbase-client/src/main/java/org/apache/hadoop/hbase/replication/ReplicationPeerConfig.java b/hbase-client/src/main/java/org/apache/hadoop/hbase/replication/ReplicationPeerConfig.java
index 790f021..f7cc2dd 100644
--- a/hbase-client/src/main/java/org/apache/hadoop/hbase/replication/ReplicationPeerConfig.java
+++ b/hbase-client/src/main/java/org/apache/hadoop/hbase/replication/ReplicationPeerConfig.java
@@ -46,8 +46,8 @@ public class ReplicationPeerConfig {
   private long bandwidth = 0;
 
   public ReplicationPeerConfig() {
-    this.peerData = new TreeMap<byte[], byte[]>(Bytes.BYTES_COMPARATOR);
-    this.configuration = new HashMap<String, String>(0);
+    this.peerData = new TreeMap<>(Bytes.BYTES_COMPARATOR);
+    this.configuration = new HashMap<>(0);
   }
 
   /**
diff --git a/hbase-client/src/main/java/org/apache/hadoop/hbase/replication/ReplicationPeerZKImpl.java b/hbase-client/src/main/java/org/apache/hadoop/hbase/replication/ReplicationPeerZKImpl.java
index 8b13f75..3973be9 100644
--- a/hbase-client/src/main/java/org/apache/hadoop/hbase/replication/ReplicationPeerZKImpl.java
+++ b/hbase-client/src/main/java/org/apache/hadoop/hbase/replication/ReplicationPeerZKImpl.java
@@ -49,7 +49,7 @@ public class ReplicationPeerZKImpl extends ReplicationStateZKBase
   private ReplicationPeerConfig peerConfig;
   private final String id;
   private volatile PeerState peerState;
-  private volatile Map<TableName, List<String>> tableCFs = new HashMap<TableName, List<String>>();
+  private volatile Map<TableName, List<String>> tableCFs = new HashMap<>();
   private final Configuration conf;
   private PeerStateTracker peerStateTracker;
   private PeerConfigTracker peerConfigTracker;
diff --git a/hbase-client/src/main/java/org/apache/hadoop/hbase/replication/ReplicationPeersZKImpl.java b/hbase-client/src/main/java/org/apache/hadoop/hbase/replication/ReplicationPeersZKImpl.java
index cf5be83..02fe2f1 100644
--- a/hbase-client/src/main/java/org/apache/hadoop/hbase/replication/ReplicationPeersZKImpl.java
+++ b/hbase-client/src/main/java/org/apache/hadoop/hbase/replication/ReplicationPeersZKImpl.java
@@ -87,7 +87,7 @@ public class ReplicationPeersZKImpl extends ReplicationStateZKBase implements Re
       final ReplicationQueuesClient queuesClient, Abortable abortable) {
     super(zk, conf, abortable);
     this.abortable = abortable;
-    this.peerClusters = new ConcurrentHashMap<String, ReplicationPeerZKImpl>();
+    this.peerClusters = new ConcurrentHashMap<>();
     this.queuesClient = queuesClient;
   }
 
@@ -128,7 +128,7 @@ public class ReplicationPeersZKImpl extends ReplicationStateZKBase implements Re
 
       ZKUtil.createWithParents(this.zookeeper, this.peersZNode);
 
-      List<ZKUtilOp> listOfOps = new ArrayList<ZKUtil.ZKUtilOp>(2);
+      List<ZKUtilOp> listOfOps = new ArrayList<>(2);
       ZKUtilOp op1 = ZKUtilOp.createAndFailSilent(getPeerNode(id),
         ReplicationSerDeHelper.toByteArray(peerConfig));
       // b/w PeerWatcher and ReplicationZookeeper#add method to create the
@@ -246,7 +246,7 @@ public class ReplicationPeersZKImpl extends ReplicationStateZKBase implements Re
 
   @Override
   public Map<String, ReplicationPeerConfig> getAllPeerConfigs() {
-    Map<String, ReplicationPeerConfig> peers = new TreeMap<String, ReplicationPeerConfig>();
+    Map<String, ReplicationPeerConfig> peers = new TreeMap<>();
     List<String> ids = null;
     try {
       ids = ZKUtil.listChildrenNoWatch(this.zookeeper, this.peersZNode);
@@ -331,10 +331,10 @@ public class ReplicationPeersZKImpl extends ReplicationStateZKBase implements Re
       CompoundConfiguration compound = new CompoundConfiguration();
       compound.add(otherConf);
       compound.addStringMap(peerConfig.getConfiguration());
-      return new Pair<ReplicationPeerConfig, Configuration>(peerConfig, compound);
+      return new Pair<>(peerConfig, compound);
     }
 
-    return new Pair<ReplicationPeerConfig, Configuration>(peerConfig, otherConf);
+    return new Pair<>(peerConfig, otherConf);
   }
 
   @Override
diff --git a/hbase-client/src/main/java/org/apache/hadoop/hbase/replication/ReplicationQueueInfo.java b/hbase-client/src/main/java/org/apache/hadoop/hbase/replication/ReplicationQueueInfo.java
index 64eedfb..1403f6d 100644
--- a/hbase-client/src/main/java/org/apache/hadoop/hbase/replication/ReplicationQueueInfo.java
+++ b/hbase-client/src/main/java/org/apache/hadoop/hbase/replication/ReplicationQueueInfo.java
@@ -42,7 +42,7 @@ public class ReplicationQueueInfo {
   private final String peerClusterZnode;
   private boolean queueRecovered;
   // List of all the dead region servers that had this queue (if recovered)
-  private List<String> deadRegionServers = new ArrayList<String>();
+  private List<String> deadRegionServers = new ArrayList<>();
 
   /**
    * The passed znode will be either the id of the peer cluster or
diff --git a/hbase-client/src/main/java/org/apache/hadoop/hbase/replication/ReplicationQueuesZKImpl.java b/hbase-client/src/main/java/org/apache/hadoop/hbase/replication/ReplicationQueuesZKImpl.java
index 484084e..4733706 100644
--- a/hbase-client/src/main/java/org/apache/hadoop/hbase/replication/ReplicationQueuesZKImpl.java
+++ b/hbase-client/src/main/java/org/apache/hadoop/hbase/replication/ReplicationQueuesZKImpl.java
@@ -248,7 +248,7 @@ public class ReplicationQueuesZKImpl extends ReplicationStateZKBase implements R
       this.abortable.abort("Failed to get a list of queues for region server: "
           + this.myQueuesZnode, e);
     }
-    return listOfQueues == null ? new ArrayList<String>() : listOfQueues;
+    return listOfQueues == null ? new ArrayList<>() : listOfQueues;
   }
 
   /**
@@ -329,7 +329,7 @@ public class ReplicationQueuesZKImpl extends ReplicationStateZKBase implements R
     }
 
     int size = pairs.size();
-    List<ZKUtilOp> listOfOps = new ArrayList<ZKUtil.ZKUtilOp>(size);
+    List<ZKUtilOp> listOfOps = new ArrayList<>(size);
 
     for (int i = 0; i < size; i++) {
       listOfOps.add(ZKUtilOp.createAndFailSilent(
@@ -356,7 +356,7 @@ public class ReplicationQueuesZKImpl extends ReplicationStateZKBase implements R
     }
 
     int size = files.size();
-    List<ZKUtilOp> listOfOps = new ArrayList<ZKUtil.ZKUtilOp>(size);
+    List<ZKUtilOp> listOfOps = new ArrayList<>(size);
 
     for (int i = 0; i < size; i++) {
       listOfOps.add(ZKUtilOp.deleteNodeFailSilent(ZKUtil.joinZNode(peerZnode, files.get(i))));
diff --git a/hbase-client/src/main/java/org/apache/hadoop/hbase/replication/ReplicationTableBase.java b/hbase-client/src/main/java/org/apache/hadoop/hbase/replication/ReplicationTableBase.java
index 61bb041..4606e22 100644
--- a/hbase-client/src/main/java/org/apache/hadoop/hbase/replication/ReplicationTableBase.java
+++ b/hbase-client/src/main/java/org/apache/hadoop/hbase/replication/ReplicationTableBase.java
@@ -142,7 +142,7 @@ abstract class ReplicationTableBase {
    */
   private Executor setUpExecutor() {
     ThreadPoolExecutor tempExecutor = new ThreadPoolExecutor(NUM_INITIALIZE_WORKERS,
-        NUM_INITIALIZE_WORKERS, 100, TimeUnit.MILLISECONDS, new LinkedBlockingQueue<Runnable>());
+        NUM_INITIALIZE_WORKERS, 100, TimeUnit.MILLISECONDS, new LinkedBlockingQueue<>());
     ThreadFactoryBuilder tfb = new ThreadFactoryBuilder();
     tfb.setNameFormat("ReplicationTableExecutor-%d");
     tfb.setDaemon(true);
@@ -223,7 +223,7 @@ abstract class ReplicationTableBase {
    */
   protected List<String> getListOfReplicators() {
     // scan all of the queues and return a list of all unique OWNER values
-    Set<String> peerServers = new HashSet<String>();
+    Set<String> peerServers = new HashSet<>();
     ResultScanner allQueuesInCluster = null;
     try (Table replicationTable = getOrBlockOnReplicationTable()){
       Scan scan = new Scan();
@@ -240,11 +240,11 @@ abstract class ReplicationTableBase {
         allQueuesInCluster.close();
       }
     }
-    return new ArrayList<String>(peerServers);
+    return new ArrayList<>(peerServers);
   }
 
   protected List<String> getAllQueues(String serverName) {
-    List<String> allQueues = new ArrayList<String>();
+    List<String> allQueues = new ArrayList<>();
     ResultScanner queueScanner = null;
     try {
       queueScanner = getQueuesBelongingToServer(serverName);
diff --git a/hbase-client/src/main/java/org/apache/hadoop/hbase/replication/ReplicationTrackerZKImpl.java b/hbase-client/src/main/java/org/apache/hadoop/hbase/replication/ReplicationTrackerZKImpl.java
index 9d182dc..9865d83 100644
--- a/hbase-client/src/main/java/org/apache/hadoop/hbase/replication/ReplicationTrackerZKImpl.java
+++ b/hbase-client/src/main/java/org/apache/hadoop/hbase/replication/ReplicationTrackerZKImpl.java
@@ -45,10 +45,9 @@ public class ReplicationTrackerZKImpl extends ReplicationStateZKBase implements
   // All about stopping
   private final Stoppable stopper;
   // listeners to be notified
-  private final List<ReplicationListener> listeners =
-      new CopyOnWriteArrayList<ReplicationListener>();
+  private final List<ReplicationListener> listeners = new CopyOnWriteArrayList<>();
   // List of all the other region servers in this cluster
-  private final ArrayList<String> otherRegionServers = new ArrayList<String>();
+  private final ArrayList<String> otherRegionServers = new ArrayList<>();
   private final ReplicationPeers replicationPeers;
 
   public ReplicationTrackerZKImpl(ZooKeeperWatcher zookeeper,
@@ -80,7 +79,7 @@ public class ReplicationTrackerZKImpl extends ReplicationStateZKBase implements
 
     List<String> list = null;
     synchronized (otherRegionServers) {
-      list = new ArrayList<String>(otherRegionServers);
+      list = new ArrayList<>(otherRegionServers);
     }
     return list;
   }
diff --git a/hbase-client/src/main/java/org/apache/hadoop/hbase/replication/TableBasedReplicationQueuesClientImpl.java b/hbase-client/src/main/java/org/apache/hadoop/hbase/replication/TableBasedReplicationQueuesClientImpl.java
index dcbed7a..3507547 100644
--- a/hbase-client/src/main/java/org/apache/hadoop/hbase/replication/TableBasedReplicationQueuesClientImpl.java
+++ b/hbase-client/src/main/java/org/apache/hadoop/hbase/replication/TableBasedReplicationQueuesClientImpl.java
@@ -72,7 +72,7 @@ public class TableBasedReplicationQueuesClientImpl extends ReplicationTableBase
 
   @Override
   public Set<String> getAllWALs() {
-    Set<String> allWals = new HashSet<String>();
+    Set<String> allWals = new HashSet<>();
     ResultScanner allQueues = null;
     try (Table replicationTable = getOrBlockOnReplicationTable()) {
       allQueues = replicationTable.getScanner(new Scan());
diff --git a/hbase-client/src/main/java/org/apache/hadoop/hbase/replication/TableBasedReplicationQueuesImpl.java b/hbase-client/src/main/java/org/apache/hadoop/hbase/replication/TableBasedReplicationQueuesImpl.java
index 1023e0d..bf55e8c 100644
--- a/hbase-client/src/main/java/org/apache/hadoop/hbase/replication/TableBasedReplicationQueuesImpl.java
+++ b/hbase-client/src/main/java/org/apache/hadoop/hbase/replication/TableBasedReplicationQueuesImpl.java
@@ -201,7 +201,7 @@ public class TableBasedReplicationQueuesImpl extends ReplicationTableBase
   public List<String> getLogsInQueue(String queueId) {
     String errMsg = "Failed getting logs in queue queueId=" + queueId;
     byte[] rowKey = queueIdToRowKey(queueId);
-    List<String> logs = new ArrayList<String>();
+    List<String> logs = new ArrayList<>();
     try {
       Get getQueue = new Get(rowKey);
       Result queue = getResultIfOwner(getQueue);
diff --git a/hbase-client/src/main/java/org/apache/hadoop/hbase/security/SecurityInfo.java b/hbase-client/src/main/java/org/apache/hadoop/hbase/security/SecurityInfo.java
index e48f81d..0e8a68d 100644
--- a/hbase-client/src/main/java/org/apache/hadoop/hbase/security/SecurityInfo.java
+++ b/hbase-client/src/main/java/org/apache/hadoop/hbase/security/SecurityInfo.java
@@ -33,7 +33,7 @@ import org.apache.hadoop.hbase.shaded.protobuf.generated.RegionServerStatusProto
 @InterfaceAudience.Private
 public class SecurityInfo {
   /** Maps RPC service names to authentication information */
-  private static ConcurrentMap<String,SecurityInfo> infos = new ConcurrentHashMap<String,SecurityInfo>();
+  private static ConcurrentMap<String,SecurityInfo> infos = new ConcurrentHashMap<>();
   // populate info for known services
   static {
     infos.put(AdminProtos.AdminService.getDescriptor().getName(),
diff --git a/hbase-client/src/main/java/org/apache/hadoop/hbase/security/access/AccessControlClient.java b/hbase-client/src/main/java/org/apache/hadoop/hbase/security/access/AccessControlClient.java
index eeac9c7..1c4a868 100644
--- a/hbase-client/src/main/java/org/apache/hadoop/hbase/security/access/AccessControlClient.java
+++ b/hbase-client/src/main/java/org/apache/hadoop/hbase/security/access/AccessControlClient.java
@@ -266,7 +266,7 @@ public class AccessControlClient {
     HBaseRpcController controller
       = ((ClusterConnection) connection).getRpcControllerFactory().newController();
       */
-    List<UserPermission> permList = new ArrayList<UserPermission>();
+    List<UserPermission> permList = new ArrayList<>();
     try (Table table = connection.getTable(ACL_TABLE_NAME)) {
       try (Admin admin = connection.getAdmin()) {
         CoprocessorRpcChannel service = table.coprocessorService(HConstants.EMPTY_START_ROW);
diff --git a/hbase-client/src/main/java/org/apache/hadoop/hbase/security/access/AccessControlUtil.java b/hbase-client/src/main/java/org/apache/hadoop/hbase/security/access/AccessControlUtil.java
index 1d26366..1873ea3 100644
--- a/hbase-client/src/main/java/org/apache/hadoop/hbase/security/access/AccessControlUtil.java
+++ b/hbase-client/src/main/java/org/apache/hadoop/hbase/security/access/AccessControlUtil.java
@@ -367,7 +367,7 @@ public class AccessControlUtil {
    */
   public static List<Permission.Action> toPermissionActions(
       List<AccessControlProtos.Permission.Action> protoActions) {
-    List<Permission.Action> actions = new ArrayList<Permission.Action>(protoActions.size());
+    List<Permission.Action> actions = new ArrayList<>(protoActions.size());
     for (AccessControlProtos.Permission.Action a : protoActions) {
       actions.add(toPermissionAction(a));
     }
@@ -644,7 +644,7 @@ public class AccessControlUtil {
     AccessControlProtos.GetUserPermissionsRequest request = builder.build();
     AccessControlProtos.GetUserPermissionsResponse response =
         protocol.getUserPermissions(controller, request);
-    List<UserPermission> perms = new ArrayList<UserPermission>(response.getUserPermissionCount());
+    List<UserPermission> perms = new ArrayList<>(response.getUserPermissionCount());
     for (AccessControlProtos.UserPermission perm: response.getUserPermissionList()) {
       perms.add(toUserPermission(perm));
     }
@@ -672,7 +672,7 @@ public class AccessControlUtil {
     AccessControlProtos.GetUserPermissionsRequest request = builder.build();
     AccessControlProtos.GetUserPermissionsResponse response =
         protocol.getUserPermissions(controller, request);
-    List<UserPermission> perms = new ArrayList<UserPermission>(response.getUserPermissionCount());
+    List<UserPermission> perms = new ArrayList<>(response.getUserPermissionCount());
     for (AccessControlProtos.UserPermission perm: response.getUserPermissionList()) {
       perms.add(toUserPermission(perm));
     }
@@ -700,7 +700,7 @@ public class AccessControlUtil {
     AccessControlProtos.GetUserPermissionsRequest request = builder.build();
     AccessControlProtos.GetUserPermissionsResponse response =
         protocol.getUserPermissions(controller, request);
-    List<UserPermission> perms = new ArrayList<UserPermission>(response.getUserPermissionCount());
+    List<UserPermission> perms = new ArrayList<>(response.getUserPermissionCount());
     for (AccessControlProtos.UserPermission perm: response.getUserPermissionList()) {
       perms.add(toUserPermission(perm));
     }
diff --git a/hbase-client/src/main/java/org/apache/hadoop/hbase/security/visibility/Authorizations.java b/hbase-client/src/main/java/org/apache/hadoop/hbase/security/visibility/Authorizations.java
index 4b3ed54..5fdeee9 100644
--- a/hbase-client/src/main/java/org/apache/hadoop/hbase/security/visibility/Authorizations.java
+++ b/hbase-client/src/main/java/org/apache/hadoop/hbase/security/visibility/Authorizations.java
@@ -34,7 +34,7 @@ public class Authorizations {
 
   private List<String> labels;
   public Authorizations(String... labels) {
-    this.labels = new ArrayList<String>(labels.length);
+    this.labels = new ArrayList<>(labels.length);
     Collections.addAll(this.labels, labels);
   }
 
diff --git a/hbase-client/src/main/java/org/apache/hadoop/hbase/security/visibility/VisibilityClient.java b/hbase-client/src/main/java/org/apache/hadoop/hbase/security/visibility/VisibilityClient.java
index cd153f1..d87bf14 100644
--- a/hbase-client/src/main/java/org/apache/hadoop/hbase/security/visibility/VisibilityClient.java
+++ b/hbase-client/src/main/java/org/apache/hadoop/hbase/security/visibility/VisibilityClient.java
@@ -130,7 +130,7 @@ public class VisibilityClient {
           new Batch.Call<VisibilityLabelsService, VisibilityLabelsResponse>() {
             ServerRpcController controller = new ServerRpcController();
             CoprocessorRpcUtils.BlockingRpcCallback<VisibilityLabelsResponse> rpcCallback =
-                new CoprocessorRpcUtils.BlockingRpcCallback<VisibilityLabelsResponse>();
+                new CoprocessorRpcUtils.BlockingRpcCallback<>();
 
             public VisibilityLabelsResponse call(VisibilityLabelsService service)
                 throws IOException {
@@ -215,7 +215,7 @@ public class VisibilityClient {
             new Batch.Call<VisibilityLabelsService, GetAuthsResponse>() {
           ServerRpcController controller = new ServerRpcController();
           CoprocessorRpcUtils.BlockingRpcCallback<GetAuthsResponse> rpcCallback =
-              new CoprocessorRpcUtils.BlockingRpcCallback<GetAuthsResponse>();
+              new CoprocessorRpcUtils.BlockingRpcCallback<>();
 
           public GetAuthsResponse call(VisibilityLabelsService service) throws IOException {
             GetAuthsRequest.Builder getAuthReqBuilder = GetAuthsRequest.newBuilder();
@@ -268,7 +268,7 @@ public class VisibilityClient {
           new Batch.Call<VisibilityLabelsService, ListLabelsResponse>() {
             ServerRpcController controller = new ServerRpcController();
             CoprocessorRpcUtils.BlockingRpcCallback<ListLabelsResponse> rpcCallback =
-                new CoprocessorRpcUtils.BlockingRpcCallback<ListLabelsResponse>();
+                new CoprocessorRpcUtils.BlockingRpcCallback<>();
 
             public ListLabelsResponse call(VisibilityLabelsService service) throws IOException {
               ListLabelsRequest.Builder listAuthLabelsReqBuilder = ListLabelsRequest.newBuilder();
@@ -340,7 +340,7 @@ public class VisibilityClient {
             new Batch.Call<VisibilityLabelsService, VisibilityLabelsResponse>() {
           ServerRpcController controller = new ServerRpcController();
           CoprocessorRpcUtils.BlockingRpcCallback<VisibilityLabelsResponse> rpcCallback =
-              new CoprocessorRpcUtils.BlockingRpcCallback<VisibilityLabelsResponse>();
+              new CoprocessorRpcUtils.BlockingRpcCallback<>();
 
           public VisibilityLabelsResponse call(VisibilityLabelsService service) throws IOException {
             SetAuthsRequest.Builder setAuthReqBuilder = SetAuthsRequest.newBuilder();
diff --git a/hbase-client/src/main/java/org/apache/hadoop/hbase/shaded/protobuf/ProtobufUtil.java b/hbase-client/src/main/java/org/apache/hadoop/hbase/shaded/protobuf/ProtobufUtil.java
index 271a0de..72bd7ea 100644
--- a/hbase-client/src/main/java/org/apache/hadoop/hbase/shaded/protobuf/ProtobufUtil.java
+++ b/hbase-client/src/main/java/org/apache/hadoop/hbase/shaded/protobuf/ProtobufUtil.java
@@ -198,8 +198,7 @@ public final class ProtobufUtil {
   /**
    * Primitive type to class mapping.
    */
-  private final static Map<String, Class<?>>
-    PRIMITIVES = new HashMap<String, Class<?>>();
+  private final static Map<String, Class<?>> PRIMITIVES = new HashMap<>();
 
   /**
    * Many results are simple: no cell, exists true or false. To save on object creations,
@@ -1498,7 +1497,7 @@ public final class ProtobufUtil {
       return proto.getStale() ? EMPTY_RESULT_STALE : EMPTY_RESULT;
     }
 
-    List<Cell> cells = new ArrayList<Cell>(values.size());
+    List<Cell> cells = new ArrayList<>(values.size());
     for (CellProtos.Cell c : values) {
       cells.add(toCell(c));
     }
@@ -1532,7 +1531,7 @@ public final class ProtobufUtil {
     List<Cell> cells = null;
     if (proto.hasAssociatedCellCount()) {
       int count = proto.getAssociatedCellCount();
-      cells = new ArrayList<Cell>(count + values.size());
+      cells = new ArrayList<>(count + values.size());
       for (int i = 0; i < count; i++) {
         if (!scanner.advance()) throw new IOException("Failed get " + i + " of " + count);
         cells.add(scanner.current());
@@ -1540,7 +1539,7 @@ public final class ProtobufUtil {
     }
 
     if (!values.isEmpty()){
-      if (cells == null) cells = new ArrayList<Cell>(values.size());
+      if (cells == null) cells = new ArrayList<>(values.size());
       for (CellProtos.Cell c: values) {
         cells.add(toCell(c));
       }
@@ -1910,7 +1909,7 @@ public final class ProtobufUtil {
    */
   static List<HRegionInfo> getRegionInfos(final GetOnlineRegionResponse proto) {
     if (proto == null) return null;
-    List<HRegionInfo> regionInfos = new ArrayList<HRegionInfo>(proto.getRegionInfoList().size());
+    List<HRegionInfo> regionInfos = new ArrayList<>(proto.getRegionInfoList().size());
     for (RegionInfo regionInfo: proto.getRegionInfoList()) {
       regionInfos.add(HRegionInfo.convert(regionInfo));
     }
@@ -2726,7 +2725,7 @@ public final class ProtobufUtil {
 
   public static List<ReplicationLoadSource> toReplicationLoadSourceList(
       List<ClusterStatusProtos.ReplicationLoadSource> clsList) {
-    ArrayList<ReplicationLoadSource> rlsList = new ArrayList<ReplicationLoadSource>(clsList.size());
+    ArrayList<ReplicationLoadSource> rlsList = new ArrayList<>(clsList.size());
     for (ClusterStatusProtos.ReplicationLoadSource cls : clsList) {
       rlsList.add(toReplicationLoadSource(cls));
     }
@@ -2983,26 +2982,26 @@ public final class ProtobufUtil {
   public static ClusterStatus convert(ClusterStatusProtos.ClusterStatus proto) {
 
     Map<ServerName, ServerLoad> servers = null;
-    servers = new HashMap<ServerName, ServerLoad>(proto.getLiveServersList().size());
+    servers = new HashMap<>(proto.getLiveServersList().size());
     for (LiveServerInfo lsi : proto.getLiveServersList()) {
       servers.put(ProtobufUtil.toServerName(
           lsi.getServer()), new ServerLoad(lsi.getServerLoad()));
     }
 
     Collection<ServerName> deadServers = null;
-    deadServers = new ArrayList<ServerName>(proto.getDeadServersList().size());
+    deadServers = new ArrayList<>(proto.getDeadServersList().size());
     for (HBaseProtos.ServerName sn : proto.getDeadServersList()) {
       deadServers.add(ProtobufUtil.toServerName(sn));
     }
 
     Collection<ServerName> backupMasters = null;
-    backupMasters = new ArrayList<ServerName>(proto.getBackupMastersList().size());
+    backupMasters = new ArrayList<>(proto.getBackupMastersList().size());
     for (HBaseProtos.ServerName sn : proto.getBackupMastersList()) {
       backupMasters.add(ProtobufUtil.toServerName(sn));
     }
 
     Set<RegionState> rit = null;
-    rit = new HashSet<RegionState>(proto.getRegionsInTransitionList().size());
+    rit = new HashSet<>(proto.getRegionsInTransitionList().size());
     for (RegionInTransition region : proto.getRegionsInTransitionList()) {
       RegionState value = RegionState.convert(region.getRegionState());
       rit.add(value);
diff --git a/hbase-client/src/main/java/org/apache/hadoop/hbase/shaded/protobuf/RequestConverter.java b/hbase-client/src/main/java/org/apache/hadoop/hbase/shaded/protobuf/RequestConverter.java
index d3ef7b8..998b3c0 100644
--- a/hbase-client/src/main/java/org/apache/hadoop/hbase/shaded/protobuf/RequestConverter.java
+++ b/hbase-client/src/main/java/org/apache/hadoop/hbase/shaded/protobuf/RequestConverter.java
@@ -572,8 +572,7 @@ public final class RequestConverter {
             .setService(userToken.getService().toString()).build();
     }
 
-    List<ClientProtos.BulkLoadHFileRequest.FamilyPath> protoFamilyPaths =
-        new ArrayList<ClientProtos.BulkLoadHFileRequest.FamilyPath>(familyPaths.size());
+    List<ClientProtos.BulkLoadHFileRequest.FamilyPath> protoFamilyPaths = new ArrayList<>(familyPaths.size());
     if (!familyPaths.isEmpty()) {
       ClientProtos.BulkLoadHFileRequest.FamilyPath.Builder pathBuilder
         = ClientProtos.BulkLoadHFileRequest.FamilyPath.newBuilder();
diff --git a/hbase-client/src/main/java/org/apache/hadoop/hbase/shaded/protobuf/ResponseConverter.java b/hbase-client/src/main/java/org/apache/hadoop/hbase/shaded/protobuf/ResponseConverter.java
index cbcad80..ecadbbc 100644
--- a/hbase-client/src/main/java/org/apache/hadoop/hbase/shaded/protobuf/ResponseConverter.java
+++ b/hbase-client/src/main/java/org/apache/hadoop/hbase/shaded/protobuf/ResponseConverter.java
@@ -233,7 +233,7 @@ public final class ResponseConverter {
   public static List<RegionOpeningState> getRegionOpeningStateList(
       final OpenRegionResponse proto) {
     if (proto == null) return null;
-    List<RegionOpeningState> regionOpeningStates = new ArrayList<RegionOpeningState>(proto.getOpeningStateCount());
+    List<RegionOpeningState> regionOpeningStates = new ArrayList<>(proto.getOpeningStateCount());
     for (int i = 0; i < proto.getOpeningStateCount(); i++) {
       regionOpeningStates.add(RegionOpeningState.valueOf(
           proto.getOpeningState(i).name()));
@@ -394,7 +394,7 @@ public final class ResponseConverter {
         boolean isPartial =
             response.getPartialFlagPerResultCount() > i ?
                 response.getPartialFlagPerResult(i) : false;
-        List<Cell> cells = new ArrayList<Cell>(noOfCells);
+        List<Cell> cells = new ArrayList<>(noOfCells);
         for (int j = 0; j < noOfCells; j++) {
           try {
             if (cellScanner.advance() == false) {
@@ -426,7 +426,7 @@ public final class ResponseConverter {
   }
 
   public static Map<String, Long> getScanMetrics(ScanResponse response) {
-    Map<String, Long> metricMap = new HashMap<String, Long>();
+    Map<String, Long> metricMap = new HashMap<>();
     if (response == null || !response.hasScanMetrics() || response.getScanMetrics() == null) {
       return metricMap;
     }
diff --git a/hbase-client/src/main/java/org/apache/hadoop/hbase/util/PoolMap.java b/hbase-client/src/main/java/org/apache/hadoop/hbase/util/PoolMap.java
index b683fcc..2131db3 100644
--- a/hbase-client/src/main/java/org/apache/hadoop/hbase/util/PoolMap.java
+++ b/hbase-client/src/main/java/org/apache/hadoop/hbase/util/PoolMap.java
@@ -57,7 +57,7 @@ public class PoolMap<K, V> implements Map<K, V> {
 
   private int poolMaxSize;
 
-  private Map<K, Pool<V>> pools = new ConcurrentHashMap<K, Pool<V>>();
+  private Map<K, Pool<V>> pools = new ConcurrentHashMap<>();
 
   public PoolMap(PoolType poolType) {
     this.poolType = poolType;
@@ -107,7 +107,7 @@ public class PoolMap<K, V> implements Map<K, V> {
 
   @Override
   public Collection<V> values() {
-    Collection<V> values = new ArrayList<V>();
+    Collection<V> values = new ArrayList<>();
     for (Pool<V> pool : pools.values()) {
       Collection<V> poolValues = pool.values();
       if (poolValues != null) {
@@ -118,7 +118,7 @@ public class PoolMap<K, V> implements Map<K, V> {
   }
 
   public Collection<V> values(K key) {
-    Collection<V> values = new ArrayList<V>();
+    Collection<V> values = new ArrayList<>();
     Pool<V> pool = pools.get(key);
     if (pool != null) {
       Collection<V> poolValues = pool.values();
@@ -185,7 +185,7 @@ public class PoolMap<K, V> implements Map<K, V> {
 
   @Override
   public Set<Map.Entry<K, V>> entrySet() {
-    Set<Map.Entry<K, V>> entries = new HashSet<Entry<K, V>>();
+    Set<Map.Entry<K, V>> entries = new HashSet<>();
     for (Map.Entry<K, Pool<V>> poolEntry : pools.entrySet()) {
       final K poolKey = poolEntry.getKey();
       final Pool<V> pool = poolEntry.getValue();
@@ -271,11 +271,11 @@ public class PoolMap<K, V> implements Map<K, V> {
   protected Pool<V> createPool() {
     switch (poolType) {
     case Reusable:
-      return new ReusablePool<V>(poolMaxSize);
+      return new ReusablePool<>(poolMaxSize);
     case RoundRobin:
-      return new RoundRobinPool<V>(poolMaxSize);
+      return new RoundRobinPool<>(poolMaxSize);
     case ThreadLocal:
-      return new ThreadLocalPool<V>();
+      return new ThreadLocalPool<>();
     }
     return null;
   }
@@ -389,7 +389,7 @@ public class PoolMap<K, V> implements Map<K, V> {
    *          the type of the resource
    */
   static class ThreadLocalPool<R> extends ThreadLocal<R> implements Pool<R> {
-    private static final Map<ThreadLocalPool<?>, AtomicInteger> poolSizes = new HashMap<ThreadLocalPool<?>, AtomicInteger>();
+    private static final Map<ThreadLocalPool<?>, AtomicInteger> poolSizes = new HashMap<>();
 
     public ThreadLocalPool() {
     }
@@ -441,7 +441,7 @@ public class PoolMap<K, V> implements Map<K, V> {
 
     @Override
     public Collection<R> values() {
-      List<R> values = new ArrayList<R>();
+      List<R> values = new ArrayList<>();
       values.add(get());
       return values;
     }
diff --git a/hbase-client/src/main/java/org/apache/hadoop/hbase/util/Writables.java b/hbase-client/src/main/java/org/apache/hadoop/hbase/util/Writables.java
index 940d523..abe3079 100644
--- a/hbase-client/src/main/java/org/apache/hadoop/hbase/util/Writables.java
+++ b/hbase-client/src/main/java/org/apache/hadoop/hbase/util/Writables.java
@@ -68,7 +68,7 @@ public class Writables {
    * @throws IOException e
    */
   public static byte [] getBytes(final Writable... ws) throws IOException {
-    List<byte []> bytes = new ArrayList<byte []>(ws.length);
+    List<byte []> bytes = new ArrayList<>(ws.length);
     int size = 0;
     for (Writable w: ws) {
       byte [] b = getBytes(w);
diff --git a/hbase-client/src/main/java/org/apache/hadoop/hbase/zookeeper/HQuorumPeer.java b/hbase-client/src/main/java/org/apache/hadoop/hbase/zookeeper/HQuorumPeer.java
index 6127997..0090b6f 100644
--- a/hbase-client/src/main/java/org/apache/hadoop/hbase/zookeeper/HQuorumPeer.java
+++ b/hbase-client/src/main/java/org/apache/hadoop/hbase/zookeeper/HQuorumPeer.java
@@ -106,7 +106,7 @@ public class HQuorumPeer {
         conf.get("hbase.zookeeper.dns.interface","default"),
         conf.get("hbase.zookeeper.dns.nameserver","default")));
 
-    List<String> ips = new ArrayList<String>();
+    List<String> ips = new ArrayList<>();
 
     // Add what could be the best (configured) match
     ips.add(myAddress.contains(".") ?
diff --git a/hbase-client/src/main/java/org/apache/hadoop/hbase/zookeeper/InstancePending.java b/hbase-client/src/main/java/org/apache/hadoop/hbase/zookeeper/InstancePending.java
index 7458ac7..e63bfc5 100644
--- a/hbase-client/src/main/java/org/apache/hadoop/hbase/zookeeper/InstancePending.java
+++ b/hbase-client/src/main/java/org/apache/hadoop/hbase/zookeeper/InstancePending.java
@@ -74,7 +74,7 @@ class InstancePending<T> {
    */
   void prepare(T instance) {
     assert instance != null;
-    instanceHolder = new InstanceHolder<T>(instance);
+    instanceHolder = new InstanceHolder<>(instance);
     pendingLatch.countDown();
   }
 }
diff --git a/hbase-client/src/main/java/org/apache/hadoop/hbase/zookeeper/MetaTableLocator.java b/hbase-client/src/main/java/org/apache/hadoop/hbase/zookeeper/MetaTableLocator.java
index e8431a2..afab54a 100644
--- a/hbase-client/src/main/java/org/apache/hadoop/hbase/zookeeper/MetaTableLocator.java
+++ b/hbase-client/src/main/java/org/apache/hadoop/hbase/zookeeper/MetaTableLocator.java
@@ -106,8 +106,8 @@ public class MetaTableLocator {
   public List<Pair<HRegionInfo, ServerName>> getMetaRegionsAndLocations(ZooKeeperWatcher zkw,
       int replicaId) {
     ServerName serverName = getMetaRegionLocation(zkw, replicaId);
-    List<Pair<HRegionInfo, ServerName>> list = new ArrayList<Pair<HRegionInfo, ServerName>>(1);
-    list.add(new Pair<HRegionInfo, ServerName>(RegionReplicaUtil.getRegionInfoForReplica(
+    List<Pair<HRegionInfo, ServerName>> list = new ArrayList<>(1);
+    list.add(new Pair<>(RegionReplicaUtil.getRegionInfoForReplica(
         HRegionInfo.FIRST_META_REGIONINFO, replicaId), serverName));
     return list;
   }
@@ -135,7 +135,7 @@ public class MetaTableLocator {
   private List<HRegionInfo> getListOfHRegionInfos(
       final List<Pair<HRegionInfo, ServerName>> pairs) {
     if (pairs == null || pairs.isEmpty()) return null;
-    List<HRegionInfo> result = new ArrayList<HRegionInfo>(pairs.size());
+    List<HRegionInfo> result = new ArrayList<>(pairs.size());
     for (Pair<HRegionInfo, ServerName> pair: pairs) {
       result.add(pair.getFirst());
     }
@@ -550,7 +550,7 @@ public class MetaTableLocator {
           throws InterruptedException {
     int numReplicasConfigured = 1;
 
-    List<ServerName> servers = new ArrayList<ServerName>();
+    List<ServerName> servers = new ArrayList<>();
     // Make the blocking call first so that we do the wait to know
     // the znodes are all in place or timeout.
     ServerName server = blockUntilAvailable(zkw, timeout);
diff --git a/hbase-client/src/main/java/org/apache/hadoop/hbase/zookeeper/PendingWatcher.java b/hbase-client/src/main/java/org/apache/hadoop/hbase/zookeeper/PendingWatcher.java
index 11d0e5d..da7d176 100644
--- a/hbase-client/src/main/java/org/apache/hadoop/hbase/zookeeper/PendingWatcher.java
+++ b/hbase-client/src/main/java/org/apache/hadoop/hbase/zookeeper/PendingWatcher.java
@@ -33,7 +33,7 @@ import org.apache.zookeeper.Watcher;
  * and then call the method {@code PendingWatcher.prepare}.
  */
 class PendingWatcher implements Watcher {
-  private final InstancePending<Watcher> pending = new InstancePending<Watcher>();
+  private final InstancePending<Watcher> pending = new InstancePending<>();
 
   @Override
   public void process(WatchedEvent event) {
diff --git a/hbase-client/src/main/java/org/apache/hadoop/hbase/zookeeper/RecoverableZooKeeper.java b/hbase-client/src/main/java/org/apache/hadoop/hbase/zookeeper/RecoverableZooKeeper.java
index 14532cf..43a5ad9 100644
--- a/hbase-client/src/main/java/org/apache/hadoop/hbase/zookeeper/RecoverableZooKeeper.java
+++ b/hbase-client/src/main/java/org/apache/hadoop/hbase/zookeeper/RecoverableZooKeeper.java
@@ -637,7 +637,7 @@ public class RecoverableZooKeeper {
   throws UnsupportedOperationException {
     if(ops == null) return null;
 
-    List<Op> preparedOps = new LinkedList<Op>();
+    List<Op> preparedOps = new LinkedList<>();
     for (Op op : ops) {
       if (op.getType() == ZooDefs.OpCode.create) {
         CreateRequest create = (CreateRequest)op.toRequestRecord();
@@ -777,7 +777,7 @@ public class RecoverableZooKeeper {
    */
   private static List<String> filterByPrefix(List<String> nodes,
       String... prefixes) {
-    List<String> lockChildren = new ArrayList<String>();
+    List<String> lockChildren = new ArrayList<>();
     for (String child : nodes){
       for (String prefix : prefixes){
         if (child.startsWith(prefix)){
diff --git a/hbase-client/src/main/java/org/apache/hadoop/hbase/zookeeper/ZKUtil.java b/hbase-client/src/main/java/org/apache/hadoop/hbase/zookeeper/ZKUtil.java
index 3e00e04..c678a7c 100644
--- a/hbase-client/src/main/java/org/apache/hadoop/hbase/zookeeper/ZKUtil.java
+++ b/hbase-client/src/main/java/org/apache/hadoop/hbase/zookeeper/ZKUtil.java
@@ -235,8 +235,7 @@ public class ZKUtil {
     private static final String CLIENT_KEYTAB_KERBEROS_CONFIG_NAME =
       "zookeeper-client-keytab-kerberos";
 
-    private static final Map<String, String> BASIC_JAAS_OPTIONS =
-      new HashMap<String,String>();
+    private static final Map<String, String> BASIC_JAAS_OPTIONS = new HashMap<>();
     static {
       String jaasEnvVar = System.getenv("HBASE_JAAS_DEBUG");
       if (jaasEnvVar != null && "true".equalsIgnoreCase(jaasEnvVar)) {
@@ -244,8 +243,7 @@ public class ZKUtil {
       }
     }
 
-    private static final Map<String,String> KEYTAB_KERBEROS_OPTIONS =
-      new HashMap<String,String>();
+    private static final Map<String,String> KEYTAB_KERBEROS_OPTIONS = new HashMap<>();
     static {
       KEYTAB_KERBEROS_OPTIONS.put("doNotPrompt", "true");
       KEYTAB_KERBEROS_OPTIONS.put("storeKey", "true");
@@ -746,7 +744,7 @@ public class ZKUtil {
     List<String> nodes =
       ZKUtil.listChildrenAndWatchForNewChildren(zkw, baseNode);
     if (nodes != null) {
-      List<NodeAndData> newNodes = new ArrayList<NodeAndData>();
+      List<NodeAndData> newNodes = new ArrayList<>();
       for (String node : nodes) {
         String nodePath = ZKUtil.joinZNode(baseNode, node);
         byte[] data = ZKUtil.getDataAndWatch(zkw, nodePath);
@@ -905,11 +903,11 @@ public class ZKUtil {
       return Ids.OPEN_ACL_UNSAFE;
     }
     if (isSecureZooKeeper) {
-      ArrayList<ACL> acls = new ArrayList<ACL>();
+      ArrayList<ACL> acls = new ArrayList<>();
       // add permission to hbase supper user
       String[] superUsers = zkw.getConfiguration().getStrings(Superusers.SUPERUSER_CONF_KEY);
       if (superUsers != null) {
-        List<String> groups = new ArrayList<String>();
+        List<String> groups = new ArrayList<>();
         for (String user : superUsers) {
           if (AuthUtil.isGroupPrincipal(user)) {
             // TODO: Set node ACL for groups when ZK supports this feature
@@ -1327,7 +1325,7 @@ public class ZKUtil {
       LOG.warn("Given path is not valid!");
       return;
     }
-    List<ZKUtilOp> ops = new ArrayList<ZKUtil.ZKUtilOp>();
+    List<ZKUtilOp> ops = new ArrayList<>();
     for (String eachRoot : pathRoots) {
       List<String> children = listChildrenBFSNoWatch(zkw, eachRoot);
       // Delete the leaves first and eventually get rid of the root
@@ -1377,7 +1375,7 @@ public class ZKUtil {
       LOG.warn("Given path is not valid!");
       return;
     }
-    List<ZKUtilOp> ops = new ArrayList<ZKUtil.ZKUtilOp>();
+    List<ZKUtilOp> ops = new ArrayList<>();
     for (String eachRoot : pathRoots) {
       // ZooKeeper Watches are one time triggers; When children of parent nodes are deleted
       // recursively, must set another watch, get notified of delete node
@@ -1415,8 +1413,8 @@ public class ZKUtil {
    */
   private static List<String> listChildrenBFSNoWatch(ZooKeeperWatcher zkw,
       final String znode) throws KeeperException {
-    Deque<String> queue = new LinkedList<String>();
-    List<String> tree = new ArrayList<String>();
+    Deque<String> queue = new LinkedList<>();
+    List<String> tree = new ArrayList<>();
     queue.add(znode);
     while (true) {
       String node = queue.pollFirst();
@@ -1451,8 +1449,8 @@ public class ZKUtil {
    */
   private static List<String> listChildrenBFSAndWatchThem(ZooKeeperWatcher zkw, final String znode)
       throws KeeperException {
-    Deque<String> queue = new LinkedList<String>();
-    List<String> tree = new ArrayList<String>();
+    Deque<String> queue = new LinkedList<>();
+    List<String> tree = new ArrayList<>();
     queue.add(znode);
     while (true) {
       String node = queue.pollFirst();
@@ -1648,7 +1646,7 @@ public class ZKUtil {
     }
     if (ops == null) return;
 
-    List<Op> zkOps = new LinkedList<Op>();
+    List<Op> zkOps = new LinkedList<>();
     for (ZKUtilOp op : ops) {
       zkOps.add(toZooKeeperOp(zkw, op));
     }
@@ -1816,7 +1814,7 @@ public class ZKUtil {
 
   private static void appendRSZnodes(ZooKeeperWatcher zkw, String znode, StringBuilder sb)
       throws KeeperException {
-    List<String> stack = new LinkedList<String>();
+    List<String> stack = new LinkedList<>();
     stack.add(znode);
     do {
       String znodeToProcess = stack.remove(stack.size() - 1);
@@ -1927,7 +1925,7 @@ public class ZKUtil {
         socket.getInputStream()));
       out.println("stat");
       out.flush();
-      ArrayList<String> res = new ArrayList<String>();
+      ArrayList<String> res = new ArrayList<>();
       while (true) {
         String line = in.readLine();
         if (line != null) {
diff --git a/hbase-client/src/main/java/org/apache/hadoop/hbase/zookeeper/ZooKeeperWatcher.java b/hbase-client/src/main/java/org/apache/hadoop/hbase/zookeeper/ZooKeeperWatcher.java
index c8462fb..f18b8ba 100644
--- a/hbase-client/src/main/java/org/apache/hadoop/hbase/zookeeper/ZooKeeperWatcher.java
+++ b/hbase-client/src/main/java/org/apache/hadoop/hbase/zookeeper/ZooKeeperWatcher.java
@@ -83,8 +83,7 @@ public class ZooKeeperWatcher implements Watcher, Abortable, Closeable {
   public final ZNodePaths znodePaths;
 
   // listeners to be notified
-  private final List<ZooKeeperListener> listeners =
-    new CopyOnWriteArrayList<ZooKeeperListener>();
+  private final List<ZooKeeperListener> listeners = new CopyOnWriteArrayList<>();
 
   // Used by ZKUtil:waitForZKConnectionIfAuthenticating to wait for SASL
   // negotiation to complete
@@ -374,7 +373,7 @@ public class ZooKeeperWatcher implements Watcher, Abortable, Closeable {
    */
   public List<String> getMetaReplicaNodes() throws KeeperException {
     List<String> childrenOfBaseNode = ZKUtil.listChildrenNoWatch(this, znodePaths.baseZNode);
-    List<String> metaReplicaNodes = new ArrayList<String>(2);
+    List<String> metaReplicaNodes = new ArrayList<>(2);
     if (childrenOfBaseNode != null) {
       String pattern = conf.get("zookeeper.znode.metaserver","meta-region-server");
       for (String child : childrenOfBaseNode) {
@@ -416,7 +415,7 @@ public class ZooKeeperWatcher implements Watcher, Abortable, Closeable {
    * Get a copy of current registered listeners
    */
   public List<ZooKeeperListener> getListeners() {
-    return new ArrayList<ZooKeeperListener>(listeners);
+    return new ArrayList<>(listeners);
   }
 
   /**
diff --git a/hbase-client/src/test/java/org/apache/hadoop/hbase/TestInterfaceAudienceAnnotations.java b/hbase-client/src/test/java/org/apache/hadoop/hbase/TestInterfaceAudienceAnnotations.java
index d0b6317..9acbb43 100644
--- a/hbase-client/src/test/java/org/apache/hadoop/hbase/TestInterfaceAudienceAnnotations.java
+++ b/hbase-client/src/test/java/org/apache/hadoop/hbase/TestInterfaceAudienceAnnotations.java
@@ -342,7 +342,7 @@ public class TestInterfaceAudienceAnnotations {
   @Test
   public void testProtosInReturnTypes() throws ClassNotFoundException, IOException, LinkageError {
     Set<Class<?>> classes = findPublicClasses();
-    List<Pair<Class<?>, Method>> protosReturnType = new ArrayList<Pair<Class<?>, Method>>();
+    List<Pair<Class<?>, Method>> protosReturnType = new ArrayList<>();
     for (Class<?> clazz : classes) {
       findProtoInReturnType(clazz, protosReturnType);
     }
@@ -374,8 +374,7 @@ public class TestInterfaceAudienceAnnotations {
   @Test
   public void testProtosInParamTypes() throws ClassNotFoundException, IOException, LinkageError {
     Set<Class<?>> classes = findPublicClasses();
-    List<Triple<Class<?>, Method, Class<?>>> protosParamType =
-        new ArrayList<Triple<Class<?>, Method, Class<?>>>();
+    List<Triple<Class<?>, Method, Class<?>>> protosParamType = new ArrayList<>();
     for (Class<?> clazz : classes) {
       findProtoInParamType(clazz, protosParamType);
     }
@@ -395,7 +394,7 @@ public class TestInterfaceAudienceAnnotations {
   @Test
   public void testProtosInConstructors() throws ClassNotFoundException, IOException, LinkageError {
     Set<Class<?>> classes = findPublicClasses();
-    List<Class<?>> classList = new ArrayList<Class<?>>();
+    List<Class<?>> classList = new ArrayList<>();
     for (Class<?> clazz : classes) {
       Constructor<?>[] constructors = clazz.getConstructors();
       for (Constructor<?> cons : constructors) {
@@ -424,7 +423,7 @@ public class TestInterfaceAudienceAnnotations {
 
   private void findProtoInReturnType(Class<?> clazz,
       List<Pair<Class<?>, Method>> protosReturnType) {
-    Pair<Class<?>, Method> returnTypePair = new Pair<Class<?>, Method>();
+    Pair<Class<?>, Method> returnTypePair = new Pair<>();
     Method[] methods = clazz.getMethods();
     returnTypePair.setFirst(clazz);
     for (Method method : methods) {
@@ -443,7 +442,7 @@ public class TestInterfaceAudienceAnnotations {
 
   private void findProtoInParamType(Class<?> clazz,
       List<Triple<Class<?>, Method, Class<?>>> protosParamType) {
-    Triple<Class<?>, Method, Class<?>> paramType = new Triple<Class<?>, Method, Class<?>>();
+    Triple<Class<?>, Method, Class<?>> paramType = new Triple<>();
     Method[] methods = clazz.getMethods();
     paramType.setFirst(clazz);
     for (Method method : methods) {
diff --git a/hbase-client/src/test/java/org/apache/hadoop/hbase/client/TestAsyncProcess.java b/hbase-client/src/test/java/org/apache/hadoop/hbase/client/TestAsyncProcess.java
index 75199a6..f2f0467 100644
--- a/hbase-client/src/test/java/org/apache/hadoop/hbase/client/TestAsyncProcess.java
+++ b/hbase-client/src/test/java/org/apache/hadoop/hbase/client/TestAsyncProcess.java
@@ -147,7 +147,7 @@ public class TestAsyncProcess {
   static class MyAsyncProcess extends AsyncProcess {
     final AtomicInteger nbMultiResponse = new AtomicInteger();
     final AtomicInteger nbActions = new AtomicInteger();
-    public List<AsyncRequestFuture> allReqs = new ArrayList<AsyncRequestFuture>();
+    public List<AsyncRequestFuture> allReqs = new ArrayList<>();
     public AtomicInteger callsCt = new AtomicInteger();
 
     private long previousTimeout = -1;
@@ -162,7 +162,7 @@ public class TestAsyncProcess {
           return DUMMY_TABLE;
         }
       };
-      AsyncRequestFutureImpl<Res> r = new MyAsyncRequestFutureImpl<Res>(
+      AsyncRequestFutureImpl<Res> r = new MyAsyncRequestFutureImpl<>(
           wrap, actions, nonceGroup, this);
       allReqs.add(r);
       return r;
@@ -326,9 +326,9 @@ public class TestAsyncProcess {
     }
   }
   class MyAsyncProcessWithReplicas extends MyAsyncProcess {
-    private Set<byte[]> failures = new TreeSet<byte[]>(new Bytes.ByteArrayComparator());
+    private Set<byte[]> failures = new TreeSet<>(new Bytes.ByteArrayComparator());
     private long primarySleepMs = 0, replicaSleepMs = 0;
-    private Map<ServerName, Long> customPrimarySleepMs = new HashMap<ServerName, Long>();
+    private Map<ServerName, Long> customPrimarySleepMs = new HashMap<>();
     private final AtomicLong replicaCalls = new AtomicLong(0);
 
     public void addFailures(HRegionInfo... hris) {
@@ -683,7 +683,7 @@ public class TestAsyncProcess {
     ClusterConnection hc = createHConnection();
     MyAsyncProcess ap = new MyAsyncProcess(hc, CONF);
 
-    List<Put> puts = new ArrayList<Put>(1);
+    List<Put> puts = new ArrayList<>(1);
     puts.add(createPut(1, true));
 
     ap.submit(null, DUMMY_TABLE, puts, false, null, false);
@@ -702,7 +702,7 @@ public class TestAsyncProcess {
     };
     MyAsyncProcess ap = new MyAsyncProcess(hc, CONF);
 
-    List<Put> puts = new ArrayList<Put>(1);
+    List<Put> puts = new ArrayList<>(1);
     puts.add(createPut(1, true));
 
     final AsyncRequestFuture ars = ap.submit(null, DUMMY_TABLE, puts, false, cb, false);
@@ -719,7 +719,7 @@ public class TestAsyncProcess {
       SimpleRequestController.class.getName());
     MyAsyncProcess ap = new MyAsyncProcess(conn, CONF);
     SimpleRequestController controller = (SimpleRequestController) ap.requestController;
-    List<Put> puts = new ArrayList<Put>(1);
+    List<Put> puts = new ArrayList<>(1);
     puts.add(createPut(1, true));
 
     for (int i = 0; i != controller.maxConcurrentTasksPerRegion; ++i) {
@@ -748,7 +748,7 @@ public class TestAsyncProcess {
     SimpleRequestController controller = (SimpleRequestController) ap.requestController;
     controller.taskCounterPerServer.put(sn2, new AtomicInteger(controller.maxConcurrentTasksPerServer));
 
-    List<Put> puts = new ArrayList<Put>(4);
+    List<Put> puts = new ArrayList<>(4);
     puts.add(createPut(1, true));
     puts.add(createPut(3, true)); // <== this one won't be taken, the rs is busy
     puts.add(createPut(1, true)); // <== this one will make it, the region is already in
@@ -770,7 +770,7 @@ public class TestAsyncProcess {
   public void testFail() throws Exception {
     MyAsyncProcess ap = new MyAsyncProcess(createHConnection(), CONF, false);
 
-    List<Put> puts = new ArrayList<Put>(1);
+    List<Put> puts = new ArrayList<>(1);
     Put p = createPut(1, false);
     puts.add(p);
 
@@ -818,7 +818,7 @@ public class TestAsyncProcess {
       }
     };
 
-    List<Put> puts = new ArrayList<Put>(1);
+    List<Put> puts = new ArrayList<>(1);
     Put p = createPut(1, true);
     puts.add(p);
 
@@ -844,7 +844,7 @@ public class TestAsyncProcess {
   public void testFailAndSuccess() throws Exception {
     MyAsyncProcess ap = new MyAsyncProcess(createHConnection(), CONF, false);
 
-    List<Put> puts = new ArrayList<Put>(3);
+    List<Put> puts = new ArrayList<>(3);
     puts.add(createPut(1, false));
     puts.add(createPut(1, true));
     puts.add(createPut(1, true));
@@ -871,7 +871,7 @@ public class TestAsyncProcess {
   public void testFlush() throws Exception {
     MyAsyncProcess ap = new MyAsyncProcess(createHConnection(), CONF, false);
 
-    List<Put> puts = new ArrayList<Put>(3);
+    List<Put> puts = new ArrayList<>(3);
     puts.add(createPut(1, false));
     puts.add(createPut(1, true));
     puts.add(createPut(1, true));
@@ -956,7 +956,7 @@ public class TestAsyncProcess {
       }
     };
 
-    List<Put> puts = new ArrayList<Put>(1);
+    List<Put> puts = new ArrayList<>(1);
     puts.add(createPut(1, true));
 
     t.start();
@@ -981,7 +981,7 @@ public class TestAsyncProcess {
     t2.start();
 
     long start = System.currentTimeMillis();
-    ap.submit(null, DUMMY_TABLE, new ArrayList<Row>(), false, null, false);
+    ap.submit(null, DUMMY_TABLE, new ArrayList<>(), false, null, false);
     long end = System.currentTimeMillis();
 
     //Adds 100 to secure us against approximate timing.
@@ -1008,7 +1008,7 @@ public class TestAsyncProcess {
     setMockLocation(hc, DUMMY_BYTES_1, hrls1);
     setMockLocation(hc, DUMMY_BYTES_2, hrls2);
     setMockLocation(hc, DUMMY_BYTES_3, hrls3);
-    List<HRegionLocation> locations = new ArrayList<HRegionLocation>();
+    List<HRegionLocation> locations = new ArrayList<>();
     for (HRegionLocation loc : hrls1.getRegionLocations()) {
       locations.add(loc);
     }
@@ -1172,7 +1172,7 @@ public class TestAsyncProcess {
     HTable ht = new HTable(conn, mutator);
     ht.multiAp = new MyAsyncProcess(conn, CONF, false);
 
-    List<Put> puts = new ArrayList<Put>(7);
+    List<Put> puts = new ArrayList<>(7);
     puts.add(createPut(1, true));
     puts.add(createPut(1, true));
     puts.add(createPut(1, true));
@@ -1309,8 +1309,8 @@ public class TestAsyncProcess {
   @Test
   public void testThreadCreation() throws Exception {
     final int NB_REGS = 100;
-    List<HRegionLocation> hrls = new ArrayList<HRegionLocation>(NB_REGS);
-    List<Get> gets = new ArrayList<Get>(NB_REGS);
+    List<HRegionLocation> hrls = new ArrayList<>(NB_REGS);
+    List<Get> gets = new ArrayList<>(NB_REGS);
     for (int i = 0; i < NB_REGS; i++) {
       HRegionInfo hri = new HRegionInfo(
           DUMMY_TABLE, Bytes.toBytes(i * 10L), Bytes.toBytes(i * 10L + 9L), false, i);
@@ -1518,7 +1518,7 @@ public class TestAsyncProcess {
   }
 
   private static List<Get> makeTimelineGets(byte[]... rows) {
-    List<Get> result = new ArrayList<Get>(rows.length);
+    List<Get> result = new ArrayList<>(rows.length);
     for (byte[] row : rows) {
       Get get = new Get(row);
       get.setConsistency(Consistency.TIMELINE);
@@ -1609,10 +1609,10 @@ public class TestAsyncProcess {
     ClusterConnection hc = createHConnection();
     MyThreadPoolExecutor myPool =
         new MyThreadPoolExecutor(1, 20, 60, TimeUnit.SECONDS,
-            new LinkedBlockingQueue<Runnable>(200));
+            new LinkedBlockingQueue<>(200));
     AsyncProcess ap = new AsyncProcessForThrowableCheck(hc, CONF);
 
-    List<Put> puts = new ArrayList<Put>(1);
+    List<Put> puts = new ArrayList<>(1);
     puts.add(createPut(1, true));
     AsyncProcessTask task = AsyncProcessTask.newBuilder()
             .setPool(myPool)
diff --git a/hbase-client/src/test/java/org/apache/hadoop/hbase/client/TestClientNoCluster.java b/hbase-client/src/test/java/org/apache/hadoop/hbase/client/TestClientNoCluster.java
index a4be9a2..d20c7c8 100644
--- a/hbase-client/src/test/java/org/apache/hadoop/hbase/client/TestClientNoCluster.java
+++ b/hbase-client/src/test/java/org/apache/hadoop/hbase/client/TestClientNoCluster.java
@@ -368,8 +368,7 @@ public class TestClientNoCluster extends Configured implements Tool {
     throws IOException {
       super(conf, pool, user);
       int serverCount = conf.getInt("hbase.test.servers", 10);
-      this.serversByClient =
-        new HashMap<ServerName, ClientService.BlockingInterface>(serverCount);
+      this.serversByClient = new HashMap<>(serverCount);
       this.meta = makeMeta(Bytes.toBytes(
         conf.get("hbase.test.tablename", Bytes.toString(BIG_USER_TABLE))),
         conf.getInt("hbase.test.regions", 100),
@@ -694,14 +693,13 @@ public class TestClientNoCluster extends Configured implements Tool {
       final int regionCount, final long namespaceSpan, final int serverCount) {
     // I need a comparator for meta rows so we sort properly.
     SortedMap<byte [], Pair<HRegionInfo, ServerName>> meta =
-      new ConcurrentSkipListMap<byte[], Pair<HRegionInfo,ServerName>>(new MetaRowsComparator());
+      new ConcurrentSkipListMap<>(new MetaRowsComparator());
     HRegionInfo [] hris = makeHRegionInfos(tableName, regionCount, namespaceSpan);
     ServerName [] serverNames = makeServerNames(serverCount);
     int per = regionCount / serverCount;
     int count = 0;
     for (HRegionInfo hri: hris) {
-      Pair<HRegionInfo, ServerName> p =
-        new Pair<HRegionInfo, ServerName>(hri, serverNames[count++ / per]);
+      Pair<HRegionInfo, ServerName> p = new Pair<>(hri, serverNames[count++ / per]);
       meta.put(hri.getRegionName(), p);
     }
     return meta;
diff --git a/hbase-client/src/test/java/org/apache/hadoop/hbase/filter/TestKeyOnlyFilter.java b/hbase-client/src/test/java/org/apache/hadoop/hbase/filter/TestKeyOnlyFilter.java
index e93319a..f22e5d4 100644
--- a/hbase-client/src/test/java/org/apache/hadoop/hbase/filter/TestKeyOnlyFilter.java
+++ b/hbase-client/src/test/java/org/apache/hadoop/hbase/filter/TestKeyOnlyFilter.java
@@ -48,7 +48,7 @@ public class TestKeyOnlyFilter {
 
   @Parameters
   public static Collection<Object[]> parameters() {
-    List<Object[]> paramList = new ArrayList<Object[]>(2);
+    List<Object[]> paramList = new ArrayList<>(2);
     {
       paramList.add(new Object[] { false });
       paramList.add(new Object[] { true });
diff --git a/hbase-client/src/test/java/org/apache/hadoop/hbase/ipc/TestHBaseRpcControllerImpl.java b/hbase-client/src/test/java/org/apache/hadoop/hbase/ipc/TestHBaseRpcControllerImpl.java
index 0659f30..0ec78ad 100644
--- a/hbase-client/src/test/java/org/apache/hadoop/hbase/ipc/TestHBaseRpcControllerImpl.java
+++ b/hbase-client/src/test/java/org/apache/hadoop/hbase/ipc/TestHBaseRpcControllerImpl.java
@@ -40,7 +40,7 @@ public class TestHBaseRpcControllerImpl {
   @Test
   public void testListOfCellScannerables() throws IOException {
     final int count = 10;
-    List<CellScannable> cells = new ArrayList<CellScannable>(count);
+    List<CellScannable> cells = new ArrayList<>(count);
 
     for (int i = 0; i < count; i++) {
       cells.add(createCell(i));
diff --git a/hbase-client/src/test/java/org/apache/hadoop/hbase/util/BuilderStyleTest.java b/hbase-client/src/test/java/org/apache/hadoop/hbase/util/BuilderStyleTest.java
index d2d0a53..771cf52 100644
--- a/hbase-client/src/test/java/org/apache/hadoop/hbase/util/BuilderStyleTest.java
+++ b/hbase-client/src/test/java/org/apache/hadoop/hbase/util/BuilderStyleTest.java
@@ -80,7 +80,7 @@ public class BuilderStyleTest {
           }
           Set<Method> sigMethods = methodsBySignature.get(sig);
           if (sigMethods == null) {
-            sigMethods = new HashSet<Method>();
+            sigMethods = new HashSet<>();
             methodsBySignature.put(sig, sigMethods);
           }
           sigMethods.add(method);
diff --git a/hbase-client/src/test/java/org/apache/hadoop/hbase/zookeeper/TestInstancePending.java b/hbase-client/src/test/java/org/apache/hadoop/hbase/zookeeper/TestInstancePending.java
index 667fed8..e67c9fd 100644
--- a/hbase-client/src/test/java/org/apache/hadoop/hbase/zookeeper/TestInstancePending.java
+++ b/hbase-client/src/test/java/org/apache/hadoop/hbase/zookeeper/TestInstancePending.java
@@ -29,8 +29,8 @@ import org.junit.experimental.categories.Category;
 public class TestInstancePending {
   @Test(timeout = 1000)
   public void test() throws Exception {
-    final InstancePending<String> pending = new InstancePending<String>();
-    final AtomicReference<String> getResultRef = new AtomicReference<String>();
+    final InstancePending<String> pending = new InstancePending<>();
+    final AtomicReference<String> getResultRef = new AtomicReference<>();
 
     new Thread() {
       @Override
diff --git a/hbase-common/src/main/java/org/apache/hadoop/hbase/CellUtil.java b/hbase-common/src/main/java/org/apache/hadoop/hbase/CellUtil.java
index 28c1d88..5930928 100644
--- a/hbase-common/src/main/java/org/apache/hadoop/hbase/CellUtil.java
+++ b/hbase-common/src/main/java/org/apache/hadoop/hbase/CellUtil.java
@@ -1597,7 +1597,7 @@ public final class CellUtil {
    * @return Tags in the given Cell as a List
    */
   public static List<Tag> getTags(Cell cell) {
-    List<Tag> tags = new ArrayList<Tag>();
+    List<Tag> tags = new ArrayList<>();
     Iterator<Tag> tagsItr = tagsIterator(cell);
     while (tagsItr.hasNext()) {
       tags.add(tagsItr.next());
diff --git a/hbase-common/src/main/java/org/apache/hadoop/hbase/ChoreService.java b/hbase-common/src/main/java/org/apache/hadoop/hbase/ChoreService.java
index 99dc163..d4ec48e 100644
--- a/hbase-common/src/main/java/org/apache/hadoop/hbase/ChoreService.java
+++ b/hbase-common/src/main/java/org/apache/hadoop/hbase/ChoreService.java
@@ -134,8 +134,8 @@ public class ChoreService implements ChoreServicer {
     }
 
     scheduler.setRemoveOnCancelPolicy(true);
-    scheduledChores = new HashMap<ScheduledChore, ScheduledFuture<?>>();
-    choresMissingStartTime = new HashMap<ScheduledChore, Boolean>();
+    scheduledChores = new HashMap<>();
+    choresMissingStartTime = new HashMap<>();
   }
 
   /**
@@ -348,7 +348,7 @@ public class ChoreService implements ChoreServicer {
   }
 
   private void cancelAllChores(final boolean mayInterruptIfRunning) {
-    ArrayList<ScheduledChore> choresToCancel = new ArrayList<ScheduledChore>(scheduledChores.keySet().size());
+    ArrayList<ScheduledChore> choresToCancel = new ArrayList<>(scheduledChores.keySet().size());
     // Build list of chores to cancel so we can iterate through a set that won't change
     // as chores are cancelled. If we tried to cancel each chore while iterating through
     // keySet the results would be undefined because the keySet would be changing
@@ -365,7 +365,7 @@ public class ChoreService implements ChoreServicer {
    * Prints a summary of important details about the chore. Used for debugging purposes
    */
   private void printChoreDetails(final String header, ScheduledChore chore) {
-    LinkedHashMap<String, String> output = new LinkedHashMap<String, String>();
+    LinkedHashMap<String, String> output = new LinkedHashMap<>();
     output.put(header, "");
     output.put("Chore name: ", chore.getName());
     output.put("Chore period: ", Integer.toString(chore.getPeriod()));
@@ -380,7 +380,7 @@ public class ChoreService implements ChoreServicer {
    * Prints a summary of important details about the service. Used for debugging purposes
    */
   private void printChoreServiceDetails(final String header) {
-    LinkedHashMap<String, String> output = new LinkedHashMap<String, String>();
+    LinkedHashMap<String, String> output = new LinkedHashMap<>();
     output.put(header, "");
     output.put("ChoreService corePoolSize: ", Integer.toString(getCorePoolSize()));
     output.put("ChoreService scheduledChores: ", Integer.toString(getNumberOfScheduledChores()));
diff --git a/hbase-common/src/main/java/org/apache/hadoop/hbase/CompoundConfiguration.java b/hbase-common/src/main/java/org/apache/hadoop/hbase/CompoundConfiguration.java
index 0eda1e5..a7fcba6 100644
--- a/hbase-common/src/main/java/org/apache/hadoop/hbase/CompoundConfiguration.java
+++ b/hbase-common/src/main/java/org/apache/hadoop/hbase/CompoundConfiguration.java
@@ -72,8 +72,7 @@ public class CompoundConfiguration extends Configuration {
     int size();
   }
 
-  private final List<ImmutableConfigMap> configs
-    = new ArrayList<ImmutableConfigMap>();
+  private final List<ImmutableConfigMap> configs = new ArrayList<>();
 
   static class ImmutableConfWrapper implements  ImmutableConfigMap {
    private final Configuration c;
@@ -167,7 +166,7 @@ public class CompoundConfiguration extends Configuration {
 
       @Override
       public Iterator<Map.Entry<String,String>> iterator() {
-        Map<String, String> ret = new HashMap<String, String>();
+        Map<String, String> ret = new HashMap<>();
         for (Map.Entry<Bytes, Bytes> entry : map.entrySet()) {
           String key = Bytes.toString(entry.getKey().get());
           String val = entry.getValue() == null ? null : Bytes.toString(entry.getValue().get());
@@ -366,7 +365,7 @@ public class CompoundConfiguration extends Configuration {
 
   @Override
   public Iterator<Map.Entry<String, String>> iterator() {
-    Map<String, String> ret = new HashMap<String, String>();
+    Map<String, String> ret = new HashMap<>();
 
     // add in reverse order so that oldest get overridden.
     if (!configs.isEmpty()) {
diff --git a/hbase-common/src/main/java/org/apache/hadoop/hbase/KeyValue.java b/hbase-common/src/main/java/org/apache/hadoop/hbase/KeyValue.java
index 0434820..96fc30b 100644
--- a/hbase-common/src/main/java/org/apache/hadoop/hbase/KeyValue.java
+++ b/hbase-common/src/main/java/org/apache/hadoop/hbase/KeyValue.java
@@ -81,7 +81,7 @@ import com.google.common.annotations.VisibleForTesting;
  */
 @InterfaceAudience.Private
 public class KeyValue implements ExtendedCell {
-  private static final ArrayList<Tag> EMPTY_ARRAY_LIST = new ArrayList<Tag>();
+  private static final ArrayList<Tag> EMPTY_ARRAY_LIST = new ArrayList<>();
 
   private static final Log LOG = LogFactory.getLog(KeyValue.class);
 
@@ -1174,7 +1174,7 @@ public class KeyValue implements ExtendedCell {
    * @return the Map&lt;String,?&gt; containing data from this key
    */
   public Map<String, Object> toStringMap() {
-    Map<String, Object> stringMap = new HashMap<String, Object>();
+    Map<String, Object> stringMap = new HashMap<>();
     stringMap.put("row", Bytes.toStringBinary(getRowArray(), getRowOffset(), getRowLength()));
     stringMap.put("family",
       Bytes.toStringBinary(getFamilyArray(), getFamilyOffset(), getFamilyLength()));
@@ -1184,7 +1184,7 @@ public class KeyValue implements ExtendedCell {
     stringMap.put("vlen", getValueLength());
     List<Tag> tags = getTags();
     if (tags != null) {
-      List<String> tagsString = new ArrayList<String>(tags.size());
+      List<String> tagsString = new ArrayList<>(tags.size());
       for (Tag t : tags) {
         tagsString.add(t.toString());
       }
diff --git a/hbase-common/src/main/java/org/apache/hadoop/hbase/KeyValueUtil.java b/hbase-common/src/main/java/org/apache/hadoop/hbase/KeyValueUtil.java
index ca990cf..807749a 100644
--- a/hbase-common/src/main/java/org/apache/hadoop/hbase/KeyValueUtil.java
+++ b/hbase-common/src/main/java/org/apache/hadoop/hbase/KeyValueUtil.java
@@ -496,7 +496,7 @@ public class KeyValueUtil {
         return KeyValueUtil.ensureKeyValue(arg0);
       }
     });
-    return new ArrayList<KeyValue>(lazyList);
+    return new ArrayList<>(lazyList);
   }
   /**
    * Write out a KeyValue in the manner in which we used to when KeyValue was a
diff --git a/hbase-common/src/main/java/org/apache/hadoop/hbase/NamespaceDescriptor.java b/hbase-common/src/main/java/org/apache/hadoop/hbase/NamespaceDescriptor.java
index e1ceace..23876ab 100644
--- a/hbase-common/src/main/java/org/apache/hadoop/hbase/NamespaceDescriptor.java
+++ b/hbase-common/src/main/java/org/apache/hadoop/hbase/NamespaceDescriptor.java
@@ -57,14 +57,14 @@ public class NamespaceDescriptor {
 
   public final static Set<String> RESERVED_NAMESPACES;
   static {
-    Set<String> set = new HashSet<String>();
+    Set<String> set = new HashSet<>();
     set.add(NamespaceDescriptor.DEFAULT_NAMESPACE_NAME_STR);
     set.add(NamespaceDescriptor.SYSTEM_NAMESPACE_NAME_STR);
     RESERVED_NAMESPACES = Collections.unmodifiableSet(set);
   }
   public final static Set<byte[]> RESERVED_NAMESPACES_BYTES;
   static {
-    Set<byte[]> set = new TreeSet<byte[]>(Bytes.BYTES_RAWCOMPARATOR);
+    Set<byte[]> set = new TreeSet<>(Bytes.BYTES_RAWCOMPARATOR);
     for(String name: RESERVED_NAMESPACES) {
       set.add(Bytes.toBytes(name));
     }
@@ -165,7 +165,7 @@ public class NamespaceDescriptor {
   @InterfaceStability.Evolving
   public static class Builder {
     private String bName;
-    private Map<String, String> bConfiguration = new TreeMap<String, String>();
+    private Map<String, String> bConfiguration = new TreeMap<>();
 
     private Builder(NamespaceDescriptor ns) {
       this.bName = ns.name;
diff --git a/hbase-common/src/main/java/org/apache/hadoop/hbase/ServerName.java b/hbase-common/src/main/java/org/apache/hadoop/hbase/ServerName.java
index 499ffd9..0c0a7ff 100644
--- a/hbase-common/src/main/java/org/apache/hadoop/hbase/ServerName.java
+++ b/hbase-common/src/main/java/org/apache/hadoop/hbase/ServerName.java
@@ -99,7 +99,7 @@ public class ServerName implements Comparable<ServerName>, Serializable {
    * @see #getVersionedBytes()
    */
   private byte [] bytes;
-  public static final List<ServerName> EMPTY_SERVER_LIST = new ArrayList<ServerName>(0);
+  public static final List<ServerName> EMPTY_SERVER_LIST = new ArrayList<>(0);
 
   protected ServerName(final String hostname, final int port, final long startcode) {
     this(Address.fromParts(hostname, port), startcode);
diff --git a/hbase-common/src/main/java/org/apache/hadoop/hbase/TableName.java b/hbase-common/src/main/java/org/apache/hadoop/hbase/TableName.java
index 63066b3..9b9755b 100644
--- a/hbase-common/src/main/java/org/apache/hadoop/hbase/TableName.java
+++ b/hbase-common/src/main/java/org/apache/hadoop/hbase/TableName.java
@@ -57,7 +57,7 @@ import org.apache.hadoop.hbase.KeyValue.KVComparator;
 public final class TableName implements Comparable<TableName> {
 
   /** See {@link #createTableNameIfNecessary(ByteBuffer, ByteBuffer)} */
-  private static final Set<TableName> tableCache = new CopyOnWriteArraySet<TableName>();
+  private static final Set<TableName> tableCache = new CopyOnWriteArraySet<>();
 
   /** Namespace delimiter */
   //this should always be only 1 byte long
diff --git a/hbase-common/src/main/java/org/apache/hadoop/hbase/TagUtil.java b/hbase-common/src/main/java/org/apache/hadoop/hbase/TagUtil.java
index f9668dd..936d8c2 100644
--- a/hbase-common/src/main/java/org/apache/hadoop/hbase/TagUtil.java
+++ b/hbase-common/src/main/java/org/apache/hadoop/hbase/TagUtil.java
@@ -72,7 +72,7 @@ public final class TagUtil {
    * @return List of tags
    */
   public static List<Tag> asList(byte[] b, int offset, int length) {
-    List<Tag> tags = new ArrayList<Tag>();
+    List<Tag> tags = new ArrayList<>();
     int pos = offset;
     while (pos < offset + length) {
       int tagLen = Bytes.readAsInt(b, pos, TAG_LENGTH_SIZE);
@@ -91,7 +91,7 @@ public final class TagUtil {
    * @return List of tags
    */
   public static List<Tag> asList(ByteBuffer b, int offset, int length) {
-    List<Tag> tags = new ArrayList<Tag>();
+    List<Tag> tags = new ArrayList<>();
     int pos = offset;
     while (pos < offset + length) {
       int tagLen = ByteBufferUtils.readAsInt(b, pos, TAG_LENGTH_SIZE);
@@ -239,7 +239,7 @@ public final class TagUtil {
     }
     List<Tag> tags = tagsOrNull;
     if (tags == null) {
-      tags = new ArrayList<Tag>();
+      tags = new ArrayList<>();
     }
     while (itr.hasNext()) {
       tags.add(itr.next());
@@ -276,7 +276,7 @@ public final class TagUtil {
     // in the array so set its size to '1' (I saw this being done in earlier version of
     // tag-handling).
     if (tags == null) {
-      tags = new ArrayList<Tag>(1);
+      tags = new ArrayList<>(1);
     }
     tags.add(new ArrayBackedTag(TagType.TTL_TAG_TYPE, Bytes.toBytes(ttl)));
     return tags;
diff --git a/hbase-common/src/main/java/org/apache/hadoop/hbase/io/BoundedByteBufferPool.java b/hbase-common/src/main/java/org/apache/hadoop/hbase/io/BoundedByteBufferPool.java
index 939d12d..079a277 100644
--- a/hbase-common/src/main/java/org/apache/hadoop/hbase/io/BoundedByteBufferPool.java
+++ b/hbase-common/src/main/java/org/apache/hadoop/hbase/io/BoundedByteBufferPool.java
@@ -51,7 +51,7 @@ import com.google.common.annotations.VisibleForTesting;
 public class BoundedByteBufferPool {
   private static final Log LOG = LogFactory.getLog(BoundedByteBufferPool.class);
 
-  private final Queue<ByteBuffer> buffers = new ConcurrentLinkedQueue<ByteBuffer>();
+  private final Queue<ByteBuffer> buffers = new ConcurrentLinkedQueue<>();
 
   @VisibleForTesting
   int getQueueSize() {
diff --git a/hbase-common/src/main/java/org/apache/hadoop/hbase/io/ByteBufferListOutputStream.java b/hbase-common/src/main/java/org/apache/hadoop/hbase/io/ByteBufferListOutputStream.java
index c334a5a..9c6796e 100644
--- a/hbase-common/src/main/java/org/apache/hadoop/hbase/io/ByteBufferListOutputStream.java
+++ b/hbase-common/src/main/java/org/apache/hadoop/hbase/io/ByteBufferListOutputStream.java
@@ -44,8 +44,8 @@ public class ByteBufferListOutputStream extends ByteBufferOutputStream {
   // it is not available will make a new one our own and keep writing to that. We keep track of all
   // the BBs that we got from pool, separately so that on closeAndPutbackBuffers, we can make sure
   // to return back all of them to pool
-  protected List<ByteBuffer> allBufs = new ArrayList<ByteBuffer>();
-  protected List<ByteBuffer> bufsFromPool = new ArrayList<ByteBuffer>();
+  protected List<ByteBuffer> allBufs = new ArrayList<>();
+  protected List<ByteBuffer> bufsFromPool = new ArrayList<>();
 
   private boolean lastBufFlipped = false;// Indicate whether the curBuf/lastBuf is flipped already
 
diff --git a/hbase-common/src/main/java/org/apache/hadoop/hbase/io/ByteBufferPool.java b/hbase-common/src/main/java/org/apache/hadoop/hbase/io/ByteBufferPool.java
index 115671d..07ba3db 100644
--- a/hbase-common/src/main/java/org/apache/hadoop/hbase/io/ByteBufferPool.java
+++ b/hbase-common/src/main/java/org/apache/hadoop/hbase/io/ByteBufferPool.java
@@ -54,7 +54,7 @@ public class ByteBufferPool {
   public static final int DEFAULT_BUFFER_SIZE = 64 * 1024;// 64 KB. Making it same as the chunk size
                                                           // what we will write/read to/from the
                                                           // socket channel.
-  private final Queue<ByteBuffer> buffers = new ConcurrentLinkedQueue<ByteBuffer>();
+  private final Queue<ByteBuffer> buffers = new ConcurrentLinkedQueue<>();
 
   private final int bufferSize;
   private final int maxPoolSize;
diff --git a/hbase-common/src/main/java/org/apache/hadoop/hbase/io/crypto/Encryption.java b/hbase-common/src/main/java/org/apache/hadoop/hbase/io/crypto/Encryption.java
index ad89ca0..b6c2e97 100644
--- a/hbase-common/src/main/java/org/apache/hadoop/hbase/io/crypto/Encryption.java
+++ b/hbase-common/src/main/java/org/apache/hadoop/hbase/io/crypto/Encryption.java
@@ -533,15 +533,14 @@ public final class Encryption {
     }
   }
 
-  static final Map<Pair<String,String>,KeyProvider> keyProviderCache =
-      new ConcurrentHashMap<Pair<String,String>,KeyProvider>();
+  static final Map<Pair<String,String>,KeyProvider> keyProviderCache = new ConcurrentHashMap<>();
 
   public static KeyProvider getKeyProvider(Configuration conf) {
     String providerClassName = conf.get(HConstants.CRYPTO_KEYPROVIDER_CONF_KEY,
       KeyStoreKeyProvider.class.getName());
     String providerParameters = conf.get(HConstants.CRYPTO_KEYPROVIDER_PARAMETERS_KEY, "");
     try {
-      Pair<String,String> providerCacheKey = new Pair<String,String>(providerClassName,
+      Pair<String,String> providerCacheKey = new Pair<>(providerClassName,
         providerParameters);
       KeyProvider provider = keyProviderCache.get(providerCacheKey);
       if (provider != null) {
diff --git a/hbase-common/src/main/java/org/apache/hadoop/hbase/io/encoding/BufferedDataBlockEncoder.java b/hbase-common/src/main/java/org/apache/hadoop/hbase/io/encoding/BufferedDataBlockEncoder.java
index cef51d8..22d7e3e 100644
--- a/hbase-common/src/main/java/org/apache/hadoop/hbase/io/encoding/BufferedDataBlockEncoder.java
+++ b/hbase-common/src/main/java/org/apache/hadoop/hbase/io/encoding/BufferedDataBlockEncoder.java
@@ -738,7 +738,7 @@ abstract class BufferedDataBlockEncoder extends AbstractDataBlockEncoder {
     protected  KeyValue.KeyOnlyKeyValue keyOnlyKV = new KeyValue.KeyOnlyKeyValue();
     // A temp pair object which will be reused by ByteBuff#asSubByteBuffer calls. This avoids too
     // many object creations.
-    protected final ObjectIntPair<ByteBuffer> tmpPair = new ObjectIntPair<ByteBuffer>();
+    protected final ObjectIntPair<ByteBuffer> tmpPair = new ObjectIntPair<>();
     protected STATE current, previous;
 
     public BufferedEncodedSeeker(CellComparator comparator,
diff --git a/hbase-common/src/main/java/org/apache/hadoop/hbase/io/encoding/RowIndexCodecV1.java b/hbase-common/src/main/java/org/apache/hadoop/hbase/io/encoding/RowIndexCodecV1.java
index d81bb4a..0f8ea01 100644
--- a/hbase-common/src/main/java/org/apache/hadoop/hbase/io/encoding/RowIndexCodecV1.java
+++ b/hbase-common/src/main/java/org/apache/hadoop/hbase/io/encoding/RowIndexCodecV1.java
@@ -110,7 +110,7 @@ public class RowIndexCodecV1 extends AbstractDataBlockEncoder {
       RowIndexSeekerV1 seeker = new RowIndexSeekerV1(CellComparator.COMPARATOR,
           decodingCtx);
       seeker.setCurrentBuffer(new SingleByteBuff(sourceAsBuffer));
-      List<Cell> kvs = new ArrayList<Cell>();
+      List<Cell> kvs = new ArrayList<>();
       kvs.add(seeker.getCell());
       while (seeker.next()) {
         kvs.add(seeker.getCell());
diff --git a/hbase-common/src/main/java/org/apache/hadoop/hbase/io/encoding/RowIndexSeekerV1.java b/hbase-common/src/main/java/org/apache/hadoop/hbase/io/encoding/RowIndexSeekerV1.java
index 4e14acb..6ac5645 100644
--- a/hbase-common/src/main/java/org/apache/hadoop/hbase/io/encoding/RowIndexSeekerV1.java
+++ b/hbase-common/src/main/java/org/apache/hadoop/hbase/io/encoding/RowIndexSeekerV1.java
@@ -40,7 +40,7 @@ public class RowIndexSeekerV1 extends AbstractEncodedSeeker {
 
   // A temp pair object which will be reused by ByteBuff#asSubByteBuffer calls. This avoids too
   // many object creations.
-  protected final ObjectIntPair<ByteBuffer> tmpPair = new ObjectIntPair<ByteBuffer>();
+  protected final ObjectIntPair<ByteBuffer> tmpPair = new ObjectIntPair<>();
 
   private ByteBuff currentBuffer;
   private SeekerState current = new SeekerState(); // always valid
diff --git a/hbase-common/src/main/java/org/apache/hadoop/hbase/io/util/LRUDictionary.java b/hbase-common/src/main/java/org/apache/hadoop/hbase/io/util/LRUDictionary.java
index 99780ba..2456961 100644
--- a/hbase-common/src/main/java/org/apache/hadoop/hbase/io/util/LRUDictionary.java
+++ b/hbase-common/src/main/java/org/apache/hadoop/hbase/io/util/LRUDictionary.java
@@ -86,7 +86,7 @@ public class LRUDictionary implements Dictionary {
     private Node head;
     private Node tail;
 
-    private HashMap<Node, Short> nodeToIndex = new HashMap<Node, Short>();
+    private HashMap<Node, Short> nodeToIndex = new HashMap<>();
     private Node[] indexToNode;
     private int initSize = 0;
 
diff --git a/hbase-common/src/main/java/org/apache/hadoop/hbase/io/util/StreamUtils.java b/hbase-common/src/main/java/org/apache/hadoop/hbase/io/util/StreamUtils.java
index 0e1c3ae..9325284 100644
--- a/hbase-common/src/main/java/org/apache/hadoop/hbase/io/util/StreamUtils.java
+++ b/hbase-common/src/main/java/org/apache/hadoop/hbase/io/util/StreamUtils.java
@@ -135,7 +135,7 @@ public class StreamUtils {
     int newOffset = offset;
     byte tmp = input[newOffset++];
     if (tmp >= 0) {
-      return new Pair<Integer, Integer>((int) tmp, newOffset - offset);
+      return new Pair<>((int) tmp, newOffset - offset);
     }
     int result = tmp & 0x7f;
     tmp = input[newOffset++];
@@ -160,7 +160,7 @@ public class StreamUtils {
             for (int i = 0; i < 5; i++) {
               tmp = input[newOffset++];
               if (tmp >= 0) {
-                return new Pair<Integer, Integer>(result, newOffset - offset);
+                return new Pair<>(result, newOffset - offset);
               }
             }
             throw new IOException("Malformed varint");
@@ -168,7 +168,7 @@ public class StreamUtils {
         }
       }
     }
-    return new Pair<Integer, Integer>(result, newOffset - offset);
+    return new Pair<>(result, newOffset - offset);
   }
 
   public static Pair<Integer, Integer> readRawVarint32(ByteBuffer input, int offset)
@@ -176,7 +176,7 @@ public class StreamUtils {
     int newOffset = offset;
     byte tmp = input.get(newOffset++);
     if (tmp >= 0) {
-      return new Pair<Integer, Integer>((int) tmp, newOffset - offset);
+      return new Pair<>((int) tmp, newOffset - offset);
     }
     int result = tmp & 0x7f;
     tmp = input.get(newOffset++);
@@ -201,7 +201,7 @@ public class StreamUtils {
             for (int i = 0; i < 5; i++) {
               tmp = input.get(newOffset++);
               if (tmp >= 0) {
-                return new Pair<Integer, Integer>(result, newOffset - offset);
+                return new Pair<>(result, newOffset - offset);
               }
             }
             throw new IOException("Malformed varint");
@@ -209,7 +209,7 @@ public class StreamUtils {
         }
       }
     }
-    return new Pair<Integer, Integer>(result, newOffset - offset);
+    return new Pair<>(result, newOffset - offset);
   }
 
   public static short toShort(byte hi, byte lo) {
diff --git a/hbase-common/src/main/java/org/apache/hadoop/hbase/nio/SingleByteBuff.java b/hbase-common/src/main/java/org/apache/hadoop/hbase/nio/SingleByteBuff.java
index 0e45410..9f6b7b5 100644
--- a/hbase-common/src/main/java/org/apache/hadoop/hbase/nio/SingleByteBuff.java
+++ b/hbase-common/src/main/java/org/apache/hadoop/hbase/nio/SingleByteBuff.java
@@ -202,7 +202,7 @@ public class SingleByteBuff extends ByteBuff {
     } else {
       // TODO we can do some optimization here? Call to asSubByteBuffer might
       // create a copy.
-      ObjectIntPair<ByteBuffer> pair = new ObjectIntPair<ByteBuffer>();
+      ObjectIntPair<ByteBuffer> pair = new ObjectIntPair<>();
       src.asSubByteBuffer(srcOffset, length, pair);
       if (pair.getFirst() != null) {
         ByteBufferUtils.copyFromBufferToBuffer(pair.getFirst(), this.buf, pair.getSecond(), offset,
diff --git a/hbase-common/src/main/java/org/apache/hadoop/hbase/security/User.java b/hbase-common/src/main/java/org/apache/hadoop/hbase/security/User.java
index c480dad..be2a0d3 100644
--- a/hbase-common/src/main/java/org/apache/hadoop/hbase/security/User.java
+++ b/hbase-common/src/main/java/org/apache/hadoop/hbase/security/User.java
@@ -354,8 +354,7 @@ public abstract class User {
   }
 
   static class TestingGroups extends Groups {
-    private final Map<String, List<String>> userToGroupsMapping =
-        new HashMap<String,List<String>>();
+    private final Map<String, List<String>> userToGroupsMapping = new HashMap<>();
     private Groups underlyingImplementation;
 
     TestingGroups(Groups underlyingImplementation) {
diff --git a/hbase-common/src/main/java/org/apache/hadoop/hbase/security/UserProvider.java b/hbase-common/src/main/java/org/apache/hadoop/hbase/security/UserProvider.java
index 43b1c89..955abfc 100644
--- a/hbase-common/src/main/java/org/apache/hadoop/hbase/security/UserProvider.java
+++ b/hbase-common/src/main/java/org/apache/hadoop/hbase/security/UserProvider.java
@@ -90,7 +90,7 @@ public class UserProvider extends BaseConfigurable {
 
           private String[] getGroupStrings(String ugi) {
             try {
-              Set<String> result = new LinkedHashSet<String>(groups.getGroups(ugi));
+              Set<String> result = new LinkedHashSet<>(groups.getGroups(ugi));
               return result.toArray(new String[result.size()]);
             } catch (Exception e) {
               return new String[0];
diff --git a/hbase-common/src/main/java/org/apache/hadoop/hbase/trace/SpanReceiverHost.java b/hbase-common/src/main/java/org/apache/hadoop/hbase/trace/SpanReceiverHost.java
index f632ae0..1b6a67d 100644
--- a/hbase-common/src/main/java/org/apache/hadoop/hbase/trace/SpanReceiverHost.java
+++ b/hbase-common/src/main/java/org/apache/hadoop/hbase/trace/SpanReceiverHost.java
@@ -63,7 +63,7 @@ public class SpanReceiverHost {
   }
 
   SpanReceiverHost(Configuration conf) {
-    receivers = new HashSet<SpanReceiver>();
+    receivers = new HashSet<>();
     this.conf = conf;
   }
 
diff --git a/hbase-common/src/main/java/org/apache/hadoop/hbase/types/StructBuilder.java b/hbase-common/src/main/java/org/apache/hadoop/hbase/types/StructBuilder.java
index c9c3b64..d73a17d 100644
--- a/hbase-common/src/main/java/org/apache/hadoop/hbase/types/StructBuilder.java
+++ b/hbase-common/src/main/java/org/apache/hadoop/hbase/types/StructBuilder.java
@@ -30,7 +30,7 @@ import org.apache.hadoop.hbase.classification.InterfaceStability;
 @InterfaceStability.Evolving
 public class StructBuilder {
 
-  protected final List<DataType<?>> fields = new ArrayList<DataType<?>>();
+  protected final List<DataType<?>> fields = new ArrayList<>();
 
   /**
    * Create an empty {@code StructBuilder}.
diff --git a/hbase-common/src/main/java/org/apache/hadoop/hbase/util/ArrayUtils.java b/hbase-common/src/main/java/org/apache/hadoop/hbase/util/ArrayUtils.java
index 4e3374e..51b87f7 100644
--- a/hbase-common/src/main/java/org/apache/hadoop/hbase/util/ArrayUtils.java
+++ b/hbase-common/src/main/java/org/apache/hadoop/hbase/util/ArrayUtils.java
@@ -102,7 +102,7 @@ public class ArrayUtils {
 
   public static ArrayList<Long> toList(long[] array){
     int length = length(array);
-    ArrayList<Long> list = new ArrayList<Long>(length);
+    ArrayList<Long> list = new ArrayList<>(length);
     for(int i=0; i < length; ++i){
       list.add(array[i]);
     }
diff --git a/hbase-common/src/main/java/org/apache/hadoop/hbase/util/AvlUtil.java b/hbase-common/src/main/java/org/apache/hadoop/hbase/util/AvlUtil.java
index 260a8b2..58c50a8 100644
--- a/hbase-common/src/main/java/org/apache/hadoop/hbase/util/AvlUtil.java
+++ b/hbase-common/src/main/java/org/apache/hadoop/hbase/util/AvlUtil.java
@@ -261,7 +261,7 @@ public final class AvlUtil {
         final AvlNodeVisitor<TNode> visitor) {
       if (root == null) return;
 
-      final AvlTreeIterator<TNode> iterator = new AvlTreeIterator<TNode>(root);
+      final AvlTreeIterator<TNode> iterator = new AvlTreeIterator<>(root);
       boolean visitNext = true;
       while (visitNext && iterator.hasNext()) {
         visitNext = visitor.visitNode(iterator.next());
diff --git a/hbase-common/src/main/java/org/apache/hadoop/hbase/util/BoundedCompletionService.java b/hbase-common/src/main/java/org/apache/hadoop/hbase/util/BoundedCompletionService.java
index c3fa547..ba38097 100644
--- a/hbase-common/src/main/java/org/apache/hadoop/hbase/util/BoundedCompletionService.java
+++ b/hbase-common/src/main/java/org/apache/hadoop/hbase/util/BoundedCompletionService.java
@@ -58,8 +58,8 @@ public class BoundedCompletionService<V> {
 
   public BoundedCompletionService(Executor executor, int maxTasks) {
     this.executor = executor;
-    this.tasks = new ArrayList<Future<V>>(maxTasks);
-    this.completed = new ArrayBlockingQueue<Future<V>>(maxTasks);
+    this.tasks = new ArrayList<>(maxTasks);
+    this.completed = new ArrayBlockingQueue<>(maxTasks);
   }
 
 
diff --git a/hbase-common/src/main/java/org/apache/hadoop/hbase/util/ByteRangeUtils.java b/hbase-common/src/main/java/org/apache/hadoop/hbase/util/ByteRangeUtils.java
index 7de1b13..9248b41 100644
--- a/hbase-common/src/main/java/org/apache/hadoop/hbase/util/ByteRangeUtils.java
+++ b/hbase-common/src/main/java/org/apache/hadoop/hbase/util/ByteRangeUtils.java
@@ -49,7 +49,7 @@ public class ByteRangeUtils {
 
   public static ArrayList<byte[]> copyToNewArrays(Collection<ByteRange> ranges) {
     if (ranges == null) {
-      return new ArrayList<byte[]>(0);
+      return new ArrayList<>(0);
     }
     ArrayList<byte[]> arrays = Lists.newArrayListWithCapacity(ranges.size());
     for (ByteRange range : ranges) {
@@ -60,7 +60,7 @@ public class ByteRangeUtils {
 
   public static ArrayList<ByteRange> fromArrays(Collection<byte[]> arrays) {
     if (arrays == null) {
-      return new ArrayList<ByteRange>(0);
+      return new ArrayList<>(0);
     }
     ArrayList<ByteRange> ranges = Lists.newArrayListWithCapacity(arrays.size());
     for (byte[] array : arrays) {
diff --git a/hbase-common/src/main/java/org/apache/hadoop/hbase/util/CollectionUtils.java b/hbase-common/src/main/java/org/apache/hadoop/hbase/util/CollectionUtils.java
index 8cc71a3..1470d5c 100644
--- a/hbase-common/src/main/java/org/apache/hadoop/hbase/util/CollectionUtils.java
+++ b/hbase-common/src/main/java/org/apache/hadoop/hbase/util/CollectionUtils.java
@@ -34,8 +34,7 @@ import org.apache.hadoop.hbase.classification.InterfaceAudience;
 @InterfaceAudience.Private
 public class CollectionUtils {
 
-  private static final List<Object> EMPTY_LIST = Collections.unmodifiableList(
-    new ArrayList<Object>(0));
+  private static final List<Object> EMPTY_LIST = Collections.unmodifiableList(new ArrayList<>(0));
 
   
   @SuppressWarnings("unchecked")
diff --git a/hbase-common/src/main/java/org/apache/hadoop/hbase/util/ConcatenatedLists.java b/hbase-common/src/main/java/org/apache/hadoop/hbase/util/ConcatenatedLists.java
index 0f00132..ba54f9d 100644
--- a/hbase-common/src/main/java/org/apache/hadoop/hbase/util/ConcatenatedLists.java
+++ b/hbase-common/src/main/java/org/apache/hadoop/hbase/util/ConcatenatedLists.java
@@ -34,7 +34,7 @@ import org.apache.hadoop.hbase.classification.InterfaceAudience;
  */
 @InterfaceAudience.Private
 public class ConcatenatedLists<T> implements Collection<T> {
-  protected final ArrayList<List<T>> components = new ArrayList<List<T>>();
+  protected final ArrayList<List<T>> components = new ArrayList<>();
   protected int size = 0;
 
   public void addAllSublists(List<? extends List<T>> items) {
diff --git a/hbase-common/src/main/java/org/apache/hadoop/hbase/util/CoprocessorClassLoader.java b/hbase-common/src/main/java/org/apache/hadoop/hbase/util/CoprocessorClassLoader.java
index c3635cb..bc51440 100644
--- a/hbase-common/src/main/java/org/apache/hadoop/hbase/util/CoprocessorClassLoader.java
+++ b/hbase-common/src/main/java/org/apache/hadoop/hbase/util/CoprocessorClassLoader.java
@@ -130,13 +130,13 @@ public class CoprocessorClassLoader extends ClassLoaderBase {
   /**
    * A locker used to synchronize class loader initialization per coprocessor jar file
    */
-  private static final KeyLocker<String> locker = new KeyLocker<String>();
+  private static final KeyLocker<String> locker = new KeyLocker<>();
 
   /**
    * A set used to synchronized parent path clean up.  Generally, there
    * should be only one parent path, but using a set so that we can support more.
    */
-  static final HashSet<String> parentDirLockSet = new HashSet<String>();
+  static final HashSet<String> parentDirLockSet = new HashSet<>();
 
   /**
    * Creates a JarClassLoader that loads classes from the given paths.
diff --git a/hbase-common/src/main/java/org/apache/hadoop/hbase/util/Counter.java b/hbase-common/src/main/java/org/apache/hadoop/hbase/util/Counter.java
index 0d3a5c6..36ca7ad 100644
--- a/hbase-common/src/main/java/org/apache/hadoop/hbase/util/Counter.java
+++ b/hbase-common/src/main/java/org/apache/hadoop/hbase/util/Counter.java
@@ -94,7 +94,7 @@ public class Counter {
   }
 
   private Counter(Cell initCell) {
-    containerRef = new AtomicReference<Container>(new Container(initCell));
+    containerRef = new AtomicReference<>(new Container(initCell));
   }
 
   private static int hash() {
diff --git a/hbase-common/src/main/java/org/apache/hadoop/hbase/util/DynamicClassLoader.java b/hbase-common/src/main/java/org/apache/hadoop/hbase/util/DynamicClassLoader.java
index 595cc5b..1a73069 100644
--- a/hbase-common/src/main/java/org/apache/hadoop/hbase/util/DynamicClassLoader.java
+++ b/hbase-common/src/main/java/org/apache/hadoop/hbase/util/DynamicClassLoader.java
@@ -102,7 +102,7 @@ public class DynamicClassLoader extends ClassLoaderBase {
   // FindBugs: Making synchronized to avoid IS2_INCONSISTENT_SYNC complaints about
   // remoteDirFs and jarModifiedTime being part synchronized protected.
   private synchronized void initTempDir(final Configuration conf) {
-    jarModifiedTime = new HashMap<String, Long>();
+    jarModifiedTime = new HashMap<>();
     String localDirPath = conf.get(
       LOCAL_DIR_KEY, DEFAULT_LOCAL_DIR) + DYNAMIC_JARS_DIR;
     localDir = new File(localDirPath);
diff --git a/hbase-common/src/main/java/org/apache/hadoop/hbase/util/IterableUtils.java b/hbase-common/src/main/java/org/apache/hadoop/hbase/util/IterableUtils.java
index 41e837d..862da43 100644
--- a/hbase-common/src/main/java/org/apache/hadoop/hbase/util/IterableUtils.java
+++ b/hbase-common/src/main/java/org/apache/hadoop/hbase/util/IterableUtils.java
@@ -31,7 +31,7 @@ import org.apache.hadoop.hbase.classification.InterfaceAudience;
 public class IterableUtils {
 
   private static final List<Object> EMPTY_LIST = Collections
-      .unmodifiableList(new ArrayList<Object>(0));
+      .unmodifiableList(new ArrayList<>(0));
 
   @SuppressWarnings("unchecked")
   public static <T> Iterable<T> nullSafe(Iterable<T> in) {
diff --git a/hbase-common/src/main/java/org/apache/hadoop/hbase/util/KeyLocker.java b/hbase-common/src/main/java/org/apache/hadoop/hbase/util/KeyLocker.java
index 05bd66d..6acf584 100644
--- a/hbase-common/src/main/java/org/apache/hadoop/hbase/util/KeyLocker.java
+++ b/hbase-common/src/main/java/org/apache/hadoop/hbase/util/KeyLocker.java
@@ -49,7 +49,7 @@ public class KeyLocker<K> {
   private static final int NB_CONCURRENT_LOCKS = 1000;
 
   private final WeakObjectPool<K, ReentrantLock> lockPool =
-      new WeakObjectPool<K, ReentrantLock>(
+      new WeakObjectPool<>(
           new WeakObjectPool.ObjectFactory<K, ReentrantLock>() {
             @Override
             public ReentrantLock createObject(K key) {
@@ -85,7 +85,7 @@ public class KeyLocker<K> {
     Arrays.sort(keyArray);
 
     lockPool.purge();
-    Map<K, Lock> locks = new LinkedHashMap<K, Lock>(keyArray.length);
+    Map<K, Lock> locks = new LinkedHashMap<>(keyArray.length);
     for (Object o : keyArray) {
       @SuppressWarnings("unchecked")
       K key = (K)o;
diff --git a/hbase-common/src/main/java/org/apache/hadoop/hbase/util/Pair.java b/hbase-common/src/main/java/org/apache/hadoop/hbase/util/Pair.java
index 159924f..719d1ee 100644
--- a/hbase-common/src/main/java/org/apache/hadoop/hbase/util/Pair.java
+++ b/hbase-common/src/main/java/org/apache/hadoop/hbase/util/Pair.java
@@ -64,7 +64,7 @@ public class Pair<T1, T2> implements Serializable
    * @return a new pair containing the passed arguments
    */
   public static <T1,T2> Pair<T1,T2> newPair(T1 a, T2 b) {
-    return new Pair<T1,T2>(a, b);
+    return new Pair<>(a, b);
   }
   
   /**
diff --git a/hbase-common/src/main/java/org/apache/hadoop/hbase/util/Threads.java b/hbase-common/src/main/java/org/apache/hadoop/hbase/util/Threads.java
index 21b376c..279ce95 100644
--- a/hbase-common/src/main/java/org/apache/hadoop/hbase/util/Threads.java
+++ b/hbase-common/src/main/java/org/apache/hadoop/hbase/util/Threads.java
@@ -191,7 +191,7 @@ public class Threads {
       ThreadFactory threadFactory) {
     ThreadPoolExecutor boundedCachedThreadPool =
       new ThreadPoolExecutor(maxCachedThread, maxCachedThread, timeout,
-        unit, new LinkedBlockingQueue<Runnable>(), threadFactory);
+        unit, new LinkedBlockingQueue<>(), threadFactory);
     // allow the core pool threads timeout and terminate
     boundedCachedThreadPool.allowCoreThreadTimeOut(true);
     return boundedCachedThreadPool;
diff --git a/hbase-common/src/main/java/org/apache/hadoop/hbase/util/Triple.java b/hbase-common/src/main/java/org/apache/hadoop/hbase/util/Triple.java
index 1de6bee..9ee0ab5 100644
--- a/hbase-common/src/main/java/org/apache/hadoop/hbase/util/Triple.java
+++ b/hbase-common/src/main/java/org/apache/hadoop/hbase/util/Triple.java
@@ -41,7 +41,7 @@ public class Triple<A, B, C> {
 
   // ctor cannot infer types w/o warning but a method can.
   public static <A, B, C> Triple<A, B, C> create(A first, B second, C third) {
-    return new Triple<A, B, C>(first, second, third);
+    return new Triple<>(first, second, third);
   }
 
   public int hashCode() {
diff --git a/hbase-common/src/main/java/org/apache/hadoop/hbase/util/WeakObjectPool.java b/hbase-common/src/main/java/org/apache/hadoop/hbase/util/WeakObjectPool.java
index 7757c6c..478864b 100644
--- a/hbase-common/src/main/java/org/apache/hadoop/hbase/util/WeakObjectPool.java
+++ b/hbase-common/src/main/java/org/apache/hadoop/hbase/util/WeakObjectPool.java
@@ -50,7 +50,7 @@ public class WeakObjectPool<K, V> {
     V createObject(K key);
   }
 
-  private final ReferenceQueue<V> staleRefQueue = new ReferenceQueue<V>();
+  private final ReferenceQueue<V> staleRefQueue = new ReferenceQueue<>();
 
   private class ObjectReference extends WeakReference<V> {
     final K key;
@@ -126,8 +126,7 @@ public class WeakObjectPool<K, V> {
     }
     this.objectFactory = objectFactory;
 
-    this.referenceCache = new ConcurrentHashMap<K, ObjectReference>(
-        initialCapacity, 0.75f, concurrencyLevel);
+    this.referenceCache = new ConcurrentHashMap<>(initialCapacity, 0.75f, concurrencyLevel);
     // 0.75f is the default load factor threshold of ConcurrentHashMap.
   }
 
diff --git a/hbase-common/src/main/java/org/apache/hadoop/hbase/util/test/RedundantKVGenerator.java b/hbase-common/src/main/java/org/apache/hadoop/hbase/util/test/RedundantKVGenerator.java
index c73705a..87d56a9 100644
--- a/hbase-common/src/main/java/org/apache/hadoop/hbase/util/test/RedundantKVGenerator.java
+++ b/hbase-common/src/main/java/org/apache/hadoop/hbase/util/test/RedundantKVGenerator.java
@@ -171,7 +171,7 @@ public class RedundantKVGenerator {
 
   private List<byte[]> generateRows() {
     // generate prefixes
-    List<byte[]> prefixes = new ArrayList<byte[]>();
+    List<byte[]> prefixes = new ArrayList<>();
     prefixes.add(new byte[0]);
     for (int i = 1; i < numberOfRowPrefixes; ++i) {
       int prefixLength = averagePrefixLength;
@@ -184,7 +184,7 @@ public class RedundantKVGenerator {
     }
 
     // generate rest of the row
-    List<byte[]> rows = new ArrayList<byte[]>();
+    List<byte[]> rows = new ArrayList<>();
     for (int i = 0; i < numberOfRows; ++i) {
       int suffixLength = averageSuffixLength;
       suffixLength += randomizer.nextInt(2 * suffixLengthVariance + 1) -
@@ -213,10 +213,10 @@ public class RedundantKVGenerator {
    * @return sorted list of key values
    */
   public List<KeyValue> generateTestKeyValues(int howMany, boolean useTags) {
-    List<KeyValue> result = new ArrayList<KeyValue>();
+    List<KeyValue> result = new ArrayList<>();
 
     List<byte[]> rows = generateRows();
-    Map<Integer, List<byte[]>> rowsToQualifier = new HashMap<Integer, List<byte[]>>();
+    Map<Integer, List<byte[]>> rowsToQualifier = new HashMap<>();
 
     if(family==null){
       family = new byte[columnFamilyLength];
@@ -249,7 +249,7 @@ public class RedundantKVGenerator {
 
         // add it to map
         if (!rowsToQualifier.containsKey(rowId)) {
-          rowsToQualifier.put(rowId, new ArrayList<byte[]>());
+          rowsToQualifier.put(rowId, new ArrayList<>());
         }
         rowsToQualifier.get(rowId).add(qualifier);
       } else if (qualifierChance > chanceForSameQualifier) {
@@ -299,9 +299,9 @@ public class RedundantKVGenerator {
    * @return sorted list of key values
    */
   public List<Cell> generateTestExtendedOffheapKeyValues(int howMany, boolean useTags) {
-    List<Cell> result = new ArrayList<Cell>();
+    List<Cell> result = new ArrayList<>();
     List<byte[]> rows = generateRows();
-    Map<Integer, List<byte[]>> rowsToQualifier = new HashMap<Integer, List<byte[]>>();
+    Map<Integer, List<byte[]>> rowsToQualifier = new HashMap<>();
 
     if (family == null) {
       family = new byte[columnFamilyLength];
@@ -334,7 +334,7 @@ public class RedundantKVGenerator {
 
         // add it to map
         if (!rowsToQualifier.containsKey(rowId)) {
-          rowsToQualifier.put(rowId, new ArrayList<byte[]>());
+          rowsToQualifier.put(rowId, new ArrayList<>());
         }
         rowsToQualifier.get(rowId).add(qualifier);
       } else if (qualifierChance > chanceForSameQualifier) {
diff --git a/hbase-common/src/test/java/org/apache/hadoop/hbase/ClassFinder.java b/hbase-common/src/test/java/org/apache/hadoop/hbase/ClassFinder.java
index 01d387c..0aa30ee 100644
--- a/hbase-common/src/test/java/org/apache/hadoop/hbase/ClassFinder.java
+++ b/hbase-common/src/test/java/org/apache/hadoop/hbase/ClassFinder.java
@@ -148,8 +148,8 @@ public class ClassFinder {
     final Pattern jarResourceRe = Pattern.compile("^file:(.+\\.jar)!/" + path + "$");
 
     Enumeration<URL> resources = ClassLoader.getSystemClassLoader().getResources(path);
-    List<File> dirs = new ArrayList<File>();
-    List<String> jars = new ArrayList<String>();
+    List<File> dirs = new ArrayList<>();
+    List<String> jars = new ArrayList<>();
 
     while (resources.hasMoreElements()) {
       URL resource = resources.nextElement();
@@ -168,7 +168,7 @@ public class ClassFinder {
       }
     }
 
-    Set<Class<?>> classes = new HashSet<Class<?>>();
+    Set<Class<?>> classes = new HashSet<>();
     for (File directory : dirs) {
       classes.addAll(findClassesFromFiles(directory, packageName, proceedOnExceptions));
     }
@@ -189,7 +189,7 @@ public class ClassFinder {
       throw ioEx;
     }
 
-    Set<Class<?>> classes = new HashSet<Class<?>>();
+    Set<Class<?>> classes = new HashSet<>();
     JarEntry entry = null;
     try {
       while (true) {
@@ -236,7 +236,7 @@ public class ClassFinder {
 
   private Set<Class<?>> findClassesFromFiles(File baseDirectory, String packageName,
       boolean proceedOnExceptions) throws ClassNotFoundException, LinkageError {
-    Set<Class<?>> classes = new HashSet<Class<?>>();
+    Set<Class<?>> classes = new HashSet<>();
     if (!baseDirectory.exists()) {
       LOG.warn(baseDirectory.getAbsolutePath() + " does not exist");
       return classes;
diff --git a/hbase-common/src/test/java/org/apache/hadoop/hbase/ResourceChecker.java b/hbase-common/src/test/java/org/apache/hadoop/hbase/ResourceChecker.java
index 310a2fb..c0b9836 100644
--- a/hbase-common/src/test/java/org/apache/hadoop/hbase/ResourceChecker.java
+++ b/hbase-common/src/test/java/org/apache/hadoop/hbase/ResourceChecker.java
@@ -94,7 +94,7 @@ public class ResourceChecker {
     public List<String> getStringsToLog() { return null; }
   }
 
-  private List<ResourceAnalyzer> ras = new ArrayList<ResourceAnalyzer>();
+  private List<ResourceAnalyzer> ras = new ArrayList<>();
   private int[] initialValues;
   private int[] endingValues;
 
diff --git a/hbase-common/src/test/java/org/apache/hadoop/hbase/ResourceCheckerJUnitListener.java b/hbase-common/src/test/java/org/apache/hadoop/hbase/ResourceCheckerJUnitListener.java
index 6264a5e..751b9e3 100644
--- a/hbase-common/src/test/java/org/apache/hadoop/hbase/ResourceCheckerJUnitListener.java
+++ b/hbase-common/src/test/java/org/apache/hadoop/hbase/ResourceCheckerJUnitListener.java
@@ -41,10 +41,10 @@ import org.junit.runner.notification.RunListener;
  * When surefire forkMode=once/always/perthread, this code is executed on the forked process.
  */
 public class ResourceCheckerJUnitListener extends RunListener {
-  private Map<String, ResourceChecker> rcs = new ConcurrentHashMap<String, ResourceChecker>();
+  private Map<String, ResourceChecker> rcs = new ConcurrentHashMap<>();
 
   static class ThreadResourceAnalyzer extends ResourceChecker.ResourceAnalyzer {
-    private static Set<String> initialThreadNames = new HashSet<String>();
+    private static Set<String> initialThreadNames = new HashSet<>();
     private static List<String> stringsToLog = null;
 
     @Override
@@ -57,7 +57,7 @@ public class ResourceCheckerJUnitListener extends RunListener {
         }
       } else if (phase == Phase.END) {
         if (stackTraces.size() > initialThreadNames.size()) {
-          stringsToLog = new ArrayList<String>();
+          stringsToLog = new ArrayList<>();
           for (Thread t : stackTraces.keySet()) {
             if (!initialThreadNames.contains(t.getName())) {
               stringsToLog.add("\nPotentially hanging thread: " + t.getName() + "\n");
diff --git a/hbase-common/src/test/java/org/apache/hadoop/hbase/TestCellUtil.java b/hbase-common/src/test/java/org/apache/hadoop/hbase/TestCellUtil.java
index 41a011d..441d1b5 100644
--- a/hbase-common/src/test/java/org/apache/hadoop/hbase/TestCellUtil.java
+++ b/hbase-common/src/test/java/org/apache/hadoop/hbase/TestCellUtil.java
@@ -215,13 +215,13 @@ public class TestCellUtil {
     consume(doCreateCellArray(1), 1);
     consume(doCreateCellArray(0), 0);
     consume(doCreateCellArray(3), 3);
-    List<CellScannable> cells = new ArrayList<CellScannable>(hundredK);
+    List<CellScannable> cells = new ArrayList<>(hundredK);
     for (int i = 0; i < hundredK; i++) {
       cells.add(new TestCellScannable(1));
     }
     consume(CellUtil.createCellScanner(cells), hundredK * 1);
-    NavigableMap<byte [], List<Cell>> m = new TreeMap<byte [], List<Cell>>(Bytes.BYTES_COMPARATOR);
-    List<Cell> cellArray = new ArrayList<Cell>(hundredK);
+    NavigableMap<byte [], List<Cell>> m = new TreeMap<>(Bytes.BYTES_COMPARATOR);
+    List<Cell> cellArray = new ArrayList<>(hundredK);
     for (int i = 0; i < hundredK; i++) cellArray.add(new TestCell(i));
     m.put(new byte [] {'f'}, cellArray);
     consume(CellUtil.createCellScanner(m), hundredK * 1);
@@ -237,7 +237,7 @@ public class TestCellUtil {
 
   private CellScanner doCreateCellScanner(final int listsCount, final int itemsPerList)
   throws IOException {
-    List<CellScannable> cells = new ArrayList<CellScannable>(listsCount);
+    List<CellScannable> cells = new ArrayList<>(listsCount);
     for (int i = 0; i < listsCount; i++) {
       CellScannable cs = new CellScannable() {
         @Override
diff --git a/hbase-common/src/test/java/org/apache/hadoop/hbase/TestClassFinder.java b/hbase-common/src/test/java/org/apache/hadoop/hbase/TestClassFinder.java
index 5154810..244c267 100644
--- a/hbase-common/src/test/java/org/apache/hadoop/hbase/TestClassFinder.java
+++ b/hbase-common/src/test/java/org/apache/hadoop/hbase/TestClassFinder.java
@@ -381,7 +381,7 @@ public class TestClassFinder {
     // Directory entries for all packages have to be added explicitly for
     // resources to be findable via ClassLoader. Directory entries must end
     // with "/"; the initial one is expected to, also.
-    Set<String> pathsInJar = new HashSet<String>();
+    Set<String> pathsInJar = new HashSet<>();
     for (FileAndPath fileAndPath : filesInJar) {
       String pathToAdd = fileAndPath.path;
       while (pathsInJar.add(pathToAdd)) {
diff --git a/hbase-common/src/test/java/org/apache/hadoop/hbase/TestCompoundConfiguration.java b/hbase-common/src/test/java/org/apache/hadoop/hbase/TestCompoundConfiguration.java
index 57409b6..0a0a1d2 100644
--- a/hbase-common/src/test/java/org/apache/hadoop/hbase/TestCompoundConfiguration.java
+++ b/hbase-common/src/test/java/org/apache/hadoop/hbase/TestCompoundConfiguration.java
@@ -121,8 +121,7 @@ public class TestCompoundConfiguration extends TestCase {
 
   @Test
   public void testWithIbwMap() {
-    Map<Bytes, Bytes> map =
-      new HashMap<Bytes, Bytes>();
+    Map<Bytes, Bytes> map = new HashMap<>();
     map.put(strToIb("B"), strToIb("2b"));
     map.put(strToIb("C"), strToIb("33"));
     map.put(strToIb("D"), strToIb("4"));
@@ -162,7 +161,7 @@ public class TestCompoundConfiguration extends TestCase {
 
   @Test
   public void testWithStringMap() {
-    Map<String, String> map = new HashMap<String, String>();
+    Map<String, String> map = new HashMap<>();
     map.put("B", "2b");
     map.put("C", "33");
     map.put("D", "4");
@@ -199,10 +198,10 @@ public class TestCompoundConfiguration extends TestCase {
 
   @Test
   public void testLaterConfigsOverrideEarlier() {
-    Map<String, String> map1 = new HashMap<String, String>();
+    Map<String, String> map1 = new HashMap<>();
     map1.put("A", "2");
     map1.put("D", "5");
-    Map<String, String> map2 = new HashMap<String, String>();
+    Map<String, String> map2 = new HashMap<>();
     String newValueForA = "3", newValueForB = "4";
     map2.put("A", newValueForA);
     map2.put("B", newValueForB);
diff --git a/hbase-common/src/test/java/org/apache/hadoop/hbase/TestKeyValue.java b/hbase-common/src/test/java/org/apache/hadoop/hbase/TestKeyValue.java
index 4e0090d..562c008 100644
--- a/hbase-common/src/test/java/org/apache/hadoop/hbase/TestKeyValue.java
+++ b/hbase-common/src/test/java/org/apache/hadoop/hbase/TestKeyValue.java
@@ -222,7 +222,7 @@ public class TestKeyValue extends TestCase {
   }
 
   public void testBinaryKeys() throws Exception {
-    Set<KeyValue> set = new TreeSet<KeyValue>(CellComparator.COMPARATOR);
+    Set<KeyValue> set = new TreeSet<>(CellComparator.COMPARATOR);
     final byte [] fam = Bytes.toBytes("col");
     final byte [] qf = Bytes.toBytes("umn");
     final byte [] nb = new byte[0];
@@ -248,7 +248,7 @@ public class TestKeyValue extends TestCase {
     }
     assertTrue(assertion);
     // Make set with good comparator
-    set = new TreeSet<KeyValue>(CellComparator.META_COMPARATOR);
+    set = new TreeSet<>(CellComparator.META_COMPARATOR);
     Collections.addAll(set, keys);
     count = 0;
     for (KeyValue k: set) {
diff --git a/hbase-common/src/test/java/org/apache/hadoop/hbase/io/TestBoundedByteBufferPool.java b/hbase-common/src/test/java/org/apache/hadoop/hbase/io/TestBoundedByteBufferPool.java
index 8775d7f..eca7712 100644
--- a/hbase-common/src/test/java/org/apache/hadoop/hbase/io/TestBoundedByteBufferPool.java
+++ b/hbase-common/src/test/java/org/apache/hadoop/hbase/io/TestBoundedByteBufferPool.java
@@ -93,7 +93,7 @@ public class TestBoundedByteBufferPool {
 
   @Test
   public void testBufferSizeGrowWithMultiThread() throws Exception {
-    final ConcurrentLinkedDeque<ByteBuffer> bufferQueue = new ConcurrentLinkedDeque<ByteBuffer>();
+    final ConcurrentLinkedDeque<ByteBuffer> bufferQueue = new ConcurrentLinkedDeque<>();
     int takeBufferThreadsCount = 30;
     int putBufferThreadsCount = 1;
     Thread takeBufferThreads[] = new Thread[takeBufferThreadsCount];
diff --git a/hbase-common/src/test/java/org/apache/hadoop/hbase/io/TestTagCompressionContext.java b/hbase-common/src/test/java/org/apache/hadoop/hbase/io/TestTagCompressionContext.java
index 5e609ad..dddd9e7 100644
--- a/hbase-common/src/test/java/org/apache/hadoop/hbase/io/TestTagCompressionContext.java
+++ b/hbase-common/src/test/java/org/apache/hadoop/hbase/io/TestTagCompressionContext.java
@@ -150,7 +150,7 @@ public class TestTagCompressionContext {
   }
 
   private KeyValue createKVWithTags(int noOfTags) {
-    List<Tag> tags = new ArrayList<Tag>();
+    List<Tag> tags = new ArrayList<>();
     for (int i = 0; i < noOfTags; i++) {
       tags.add(new ArrayBackedTag((byte) i, "tagValue" + i));
     }
@@ -159,7 +159,7 @@ public class TestTagCompressionContext {
   }
 
   private Cell createOffheapKVWithTags(int noOfTags) {
-    List<Tag> tags = new ArrayList<Tag>();
+    List<Tag> tags = new ArrayList<>();
     for (int i = 0; i < noOfTags; i++) {
       tags.add(new ArrayBackedTag((byte) i, "tagValue" + i));
     }
diff --git a/hbase-common/src/test/java/org/apache/hadoop/hbase/nio/TestMultiByteBuff.java b/hbase-common/src/test/java/org/apache/hadoop/hbase/nio/TestMultiByteBuff.java
index af4c464..48922d9 100644
--- a/hbase-common/src/test/java/org/apache/hadoop/hbase/nio/TestMultiByteBuff.java
+++ b/hbase-common/src/test/java/org/apache/hadoop/hbase/nio/TestMultiByteBuff.java
@@ -243,7 +243,7 @@ public class TestMultiByteBuff {
     assertFalse(bb2 == sub);
     assertEquals(l2, ByteBufferUtils.toLong(sub, sub.position()));
     multi.rewind();
-    ObjectIntPair<ByteBuffer> p = new ObjectIntPair<ByteBuffer>();
+    ObjectIntPair<ByteBuffer> p = new ObjectIntPair<>();
     multi.asSubByteBuffer(8, Bytes.SIZEOF_LONG, p);
     assertFalse(bb1 == p.getFirst());
     assertFalse(bb2 == p.getFirst());
diff --git a/hbase-common/src/test/java/org/apache/hadoop/hbase/types/TestFixedLengthWrapper.java b/hbase-common/src/test/java/org/apache/hadoop/hbase/types/TestFixedLengthWrapper.java
index b259429..c2c5a6d 100644
--- a/hbase-common/src/test/java/org/apache/hadoop/hbase/types/TestFixedLengthWrapper.java
+++ b/hbase-common/src/test/java/org/apache/hadoop/hbase/types/TestFixedLengthWrapper.java
@@ -51,7 +51,7 @@ public class TestFixedLengthWrapper {
       for (Order ord : new Order[] { Order.ASCENDING, Order.DESCENDING }) {
         for (byte[] val : VALUES) {
           buff.setPosition(0);
-          DataType<byte[]> type = new FixedLengthWrapper<byte[]>(new RawBytes(ord), limit);
+          DataType<byte[]> type = new FixedLengthWrapper<>(new RawBytes(ord), limit);
           assertEquals(limit, type.encode(buff, val));
           buff.setPosition(0);
           byte[] actual = type.decode(buff);
@@ -67,21 +67,21 @@ public class TestFixedLengthWrapper {
   @Test(expected = IllegalArgumentException.class)
   public void testInsufficientRemainingRead() {
     PositionedByteRange buff = new SimplePositionedMutableByteRange(0);
-    DataType<byte[]> type = new FixedLengthWrapper<byte[]>(new RawBytes(), 3);
+    DataType<byte[]> type = new FixedLengthWrapper<>(new RawBytes(), 3);
     type.decode(buff);
   }
 
   @Test(expected = IllegalArgumentException.class)
   public void testInsufficientRemainingWrite() {
     PositionedByteRange buff = new SimplePositionedMutableByteRange(0);
-    DataType<byte[]> type = new FixedLengthWrapper<byte[]>(new RawBytes(), 3);
+    DataType<byte[]> type = new FixedLengthWrapper<>(new RawBytes(), 3);
     type.encode(buff, Bytes.toBytes(""));
   }
 
   @Test(expected = IllegalArgumentException.class)
   public void testOverflowPassthrough() {
     PositionedByteRange buff = new SimplePositionedMutableByteRange(3);
-    DataType<byte[]> type = new FixedLengthWrapper<byte[]>(new RawBytes(), 0);
+    DataType<byte[]> type = new FixedLengthWrapper<>(new RawBytes(), 0);
     type.encode(buff, Bytes.toBytes("foo"));
   }
 }
diff --git a/hbase-common/src/test/java/org/apache/hadoop/hbase/types/TestStructNullExtension.java b/hbase-common/src/test/java/org/apache/hadoop/hbase/types/TestStructNullExtension.java
index e87438d..2b2efe6 100644
--- a/hbase-common/src/test/java/org/apache/hadoop/hbase/types/TestStructNullExtension.java
+++ b/hbase-common/src/test/java/org/apache/hadoop/hbase/types/TestStructNullExtension.java
@@ -58,7 +58,7 @@ public class TestStructNullExtension {
     Struct shorter = builder.toStruct();
     Struct longer = builder
         // intentionally include a wrapped instance to test wrapper behavior.
-        .add(new TerminatedWrapper<String>(OrderedString.ASCENDING, "/"))
+        .add(new TerminatedWrapper<>(OrderedString.ASCENDING, "/"))
         .add(OrderedNumeric.ASCENDING)
         .toStruct();
 
diff --git a/hbase-common/src/test/java/org/apache/hadoop/hbase/types/TestTerminatedWrapper.java b/hbase-common/src/test/java/org/apache/hadoop/hbase/types/TestTerminatedWrapper.java
index e36a141..310067b 100644
--- a/hbase-common/src/test/java/org/apache/hadoop/hbase/types/TestTerminatedWrapper.java
+++ b/hbase-common/src/test/java/org/apache/hadoop/hbase/types/TestTerminatedWrapper.java
@@ -47,7 +47,7 @@ public class TestTerminatedWrapper {
 
   @Test(expected = IllegalArgumentException.class)
   public void testEmptyDelimiter() {
-    new TerminatedWrapper<byte[]>(new RawBytes(), "");
+    new TerminatedWrapper<>(new RawBytes(), "");
   }
 
   @Test(expected = IllegalArgumentException.class)
@@ -58,7 +58,7 @@ public class TestTerminatedWrapper {
 
   @Test(expected = IllegalArgumentException.class)
   public void testEncodedValueContainsTerm() {
-    DataType<byte[]> type = new TerminatedWrapper<byte[]>(new RawBytes(), "foo");
+    DataType<byte[]> type = new TerminatedWrapper<>(new RawBytes(), "foo");
     PositionedByteRange buff = new SimplePositionedMutableByteRange(16);
     type.encode(buff, Bytes.toBytes("hello foobar!"));
   }
@@ -72,7 +72,7 @@ public class TestTerminatedWrapper {
       for (byte[] term : TERMINATORS) {
         for (String val : VALUES_STRINGS) {
           buff.setPosition(0);
-          DataType<String> type = new TerminatedWrapper<String>(t, term);
+          DataType<String> type = new TerminatedWrapper<>(t, term);
           assertEquals(val.length() + 2 + term.length, type.encode(buff, val));
           buff.setPosition(0);
           assertEquals(val, type.decode(buff));
@@ -89,7 +89,7 @@ public class TestTerminatedWrapper {
       for (byte[] term : TERMINATORS) {
         for (byte[] val : VALUES_BYTES) {
           buff.setPosition(0);
-          DataType<byte[]> type = new TerminatedWrapper<byte[]>(new RawBytes(ord), term);
+          DataType<byte[]> type = new TerminatedWrapper<>(new RawBytes(ord), term);
           assertEquals(val.length + term.length, type.encode(buff, val));
           buff.setPosition(0);
           assertArrayEquals(val, type.decode(buff));
@@ -108,7 +108,7 @@ public class TestTerminatedWrapper {
       for (byte[] term : TERMINATORS) {
         for (String val : VALUES_STRINGS) {
           buff.setPosition(0);
-          DataType<String> type = new TerminatedWrapper<String>(t, term);
+          DataType<String> type = new TerminatedWrapper<>(t, term);
           int expected = val.length() + 2 + term.length;
           assertEquals(expected, type.encode(buff, val));
           buff.setPosition(0);
@@ -126,7 +126,7 @@ public class TestTerminatedWrapper {
       for (byte[] term : TERMINATORS) {
         for (byte[] val : VALUES_BYTES) {
           buff.setPosition(0);
-          DataType<byte[]> type = new TerminatedWrapper<byte[]>(new RawBytes(ord), term);
+          DataType<byte[]> type = new TerminatedWrapper<>(new RawBytes(ord), term);
           int expected = type.encode(buff, val);
           buff.setPosition(0);
           assertEquals(expected, type.skip(buff));
@@ -139,7 +139,7 @@ public class TestTerminatedWrapper {
   @Test(expected = IllegalArgumentException.class)
   public void testInvalidSkip() {
     PositionedByteRange buff = new SimplePositionedMutableByteRange(Bytes.toBytes("foo"));
-    DataType<byte[]> type = new TerminatedWrapper<byte[]>(new RawBytes(), new byte[] { 0x00 });
+    DataType<byte[]> type = new TerminatedWrapper<>(new RawBytes(), new byte[] { 0x00 });
     type.skip(buff);
   }
 }
diff --git a/hbase-common/src/test/java/org/apache/hadoop/hbase/util/ClassLoaderTestHelper.java b/hbase-common/src/test/java/org/apache/hadoop/hbase/util/ClassLoaderTestHelper.java
index 30e33d9..ba6cea0 100644
--- a/hbase-common/src/test/java/org/apache/hadoop/hbase/util/ClassLoaderTestHelper.java
+++ b/hbase-common/src/test/java/org/apache/hadoop/hbase/util/ClassLoaderTestHelper.java
@@ -133,13 +133,13 @@ public class ClassLoaderTestHelper {
 
     // compile it by JavaCompiler
     JavaCompiler compiler = ToolProvider.getSystemJavaCompiler();
-    ArrayList<String> srcFileNames = new ArrayList<String>(1);
+    ArrayList<String> srcFileNames = new ArrayList<>(1);
     srcFileNames.add(sourceCodeFile.toString());
     StandardJavaFileManager fm = compiler.getStandardFileManager(null, null,
       null);
     Iterable<? extends JavaFileObject> cu =
       fm.getJavaFileObjects(sourceCodeFile);
-    List<String> options = new ArrayList<String>(2);
+    List<String> options = new ArrayList<>(2);
     options.add("-classpath");
     // only add hbase classes to classpath. This is a little bit tricky: assume
     // the classpath is {hbaseSrc}/target/classes.
diff --git a/hbase-common/src/test/java/org/apache/hadoop/hbase/util/TestAvlUtil.java b/hbase-common/src/test/java/org/apache/hadoop/hbase/util/TestAvlUtil.java
index 3c7b680..554e108 100644
--- a/hbase-common/src/test/java/org/apache/hadoop/hbase/util/TestAvlUtil.java
+++ b/hbase-common/src/test/java/org/apache/hadoop/hbase/util/TestAvlUtil.java
@@ -48,7 +48,7 @@ public class TestAvlUtil {
     final int MAX_KEY = 99999999;
     final int NELEM = 10000;
 
-    final TreeMap<Integer, Object> treeMap = new TreeMap<Integer, Object>();
+    final TreeMap<Integer, Object> treeMap = new TreeMap<>();
     TestAvlNode root = null;
 
     final Random rand = new Random();
@@ -117,7 +117,7 @@ public class TestAvlUtil {
       root = AvlTree.insert(root, new TestAvlNode(i));
     }
 
-    AvlTreeIterator<TestAvlNode> iter = new AvlTreeIterator<TestAvlNode>(root);
+    AvlTreeIterator<TestAvlNode> iter = new AvlTreeIterator<>(root);
     assertTrue(iter.hasNext());
     long prevKey = 0;
     while (iter.hasNext()) {
@@ -139,7 +139,7 @@ public class TestAvlUtil {
     }
 
     for (int i = MIN_KEY - 1; i < MAX_KEY + 1; ++i) {
-      AvlTreeIterator<TestAvlNode> iter = new AvlTreeIterator<TestAvlNode>(root, i, KEY_COMPARATOR);
+      AvlTreeIterator<TestAvlNode> iter = new AvlTreeIterator<>(root, i, KEY_COMPARATOR);
       if (i < MAX_KEY) {
         assertTrue(iter.hasNext());
       } else {
diff --git a/hbase-common/src/test/java/org/apache/hadoop/hbase/util/TestBase64.java b/hbase-common/src/test/java/org/apache/hadoop/hbase/util/TestBase64.java
index 09ef707..7c74bca 100644
--- a/hbase-common/src/test/java/org/apache/hadoop/hbase/util/TestBase64.java
+++ b/hbase-common/src/test/java/org/apache/hadoop/hbase/util/TestBase64.java
@@ -55,7 +55,7 @@ public class TestBase64 extends TestCase {
    * @throws UnsupportedEncodingException
    */
   public void testBase64() throws UnsupportedEncodingException {
-    TreeMap<String, String> sorted = new TreeMap<String, String>();
+    TreeMap<String, String> sorted = new TreeMap<>();
 
     for (int i = 0; i < uris.length; i++) {
       byte[] bytes = uris[i].getBytes("UTF-8");
diff --git a/hbase-common/src/test/java/org/apache/hadoop/hbase/util/TestBoundedArrayQueue.java b/hbase-common/src/test/java/org/apache/hadoop/hbase/util/TestBoundedArrayQueue.java
index 2cc3751..6d9c496 100644
--- a/hbase-common/src/test/java/org/apache/hadoop/hbase/util/TestBoundedArrayQueue.java
+++ b/hbase-common/src/test/java/org/apache/hadoop/hbase/util/TestBoundedArrayQueue.java
@@ -31,7 +31,7 @@ import org.junit.experimental.categories.Category;
 public class TestBoundedArrayQueue {
 
   private int qMaxElements = 5;
-  private BoundedArrayQueue<Integer> queue = new BoundedArrayQueue<Integer>(qMaxElements);
+  private BoundedArrayQueue<Integer> queue = new BoundedArrayQueue<>(qMaxElements);
 
   @Test
   public void testBoundedArrayQueueOperations() throws Exception {
diff --git a/hbase-common/src/test/java/org/apache/hadoop/hbase/util/TestByteBufferUtils.java b/hbase-common/src/test/java/org/apache/hadoop/hbase/util/TestByteBufferUtils.java
index e94293c..b78574a 100644
--- a/hbase-common/src/test/java/org/apache/hadoop/hbase/util/TestByteBufferUtils.java
+++ b/hbase-common/src/test/java/org/apache/hadoop/hbase/util/TestByteBufferUtils.java
@@ -77,7 +77,7 @@ public class TestByteBufferUtils {
   }
 
   static {
-    SortedSet<Long> a = new TreeSet<Long>();
+    SortedSet<Long> a = new TreeSet<>();
     for (int i = 0; i <= 63; ++i) {
       long v = (-1L) << i;
       assertTrue(v < 0);
diff --git a/hbase-common/src/test/java/org/apache/hadoop/hbase/util/TestByteRangeWithKVSerialization.java b/hbase-common/src/test/java/org/apache/hadoop/hbase/util/TestByteRangeWithKVSerialization.java
index 717e24c..8ae2a29 100644
--- a/hbase-common/src/test/java/org/apache/hadoop/hbase/util/TestByteRangeWithKVSerialization.java
+++ b/hbase-common/src/test/java/org/apache/hadoop/hbase/util/TestByteRangeWithKVSerialization.java
@@ -64,7 +64,7 @@ public class TestByteRangeWithKVSerialization {
     final byte[] QUALIFIER = Bytes.toBytes("q1");
     final byte[] VALUE = Bytes.toBytes("v");
     int kvCount = 1000000;
-    List<KeyValue> kvs = new ArrayList<KeyValue>(kvCount);
+    List<KeyValue> kvs = new ArrayList<>(kvCount);
     int totalSize = 0;
     Tag[] tags = new Tag[] { new ArrayBackedTag((byte) 1, "tag1") };
     for (int i = 0; i < kvCount; i++) {
diff --git a/hbase-common/src/test/java/org/apache/hadoop/hbase/util/TestBytes.java b/hbase-common/src/test/java/org/apache/hadoop/hbase/util/TestBytes.java
index e145642..38b01b8 100644
--- a/hbase-common/src/test/java/org/apache/hadoop/hbase/util/TestBytes.java
+++ b/hbase-common/src/test/java/org/apache/hadoop/hbase/util/TestBytes.java
@@ -498,7 +498,7 @@ public class TestBytes extends TestCase {
   }
   
   public void testToFromHex() {
-    List<String> testStrings = new ArrayList<String>(8);
+    List<String> testStrings = new ArrayList<>(8);
     testStrings.addAll(Arrays.asList(new String[] {
         "",
         "00",
@@ -517,7 +517,7 @@ public class TestBytes extends TestCase {
       Assert.assertTrue(testString.equalsIgnoreCase(result));
     }
     
-    List<byte[]> testByteData = new ArrayList<byte[]>(5);
+    List<byte[]> testByteData = new ArrayList<>(5);
     testByteData.addAll(Arrays.asList(new byte[][] {
       new byte[0],
       new byte[1],
diff --git a/hbase-common/src/test/java/org/apache/hadoop/hbase/util/TestConcatenatedLists.java b/hbase-common/src/test/java/org/apache/hadoop/hbase/util/TestConcatenatedLists.java
index fd4baf5..cfd288d 100644
--- a/hbase-common/src/test/java/org/apache/hadoop/hbase/util/TestConcatenatedLists.java
+++ b/hbase-common/src/test/java/org/apache/hadoop/hbase/util/TestConcatenatedLists.java
@@ -39,7 +39,7 @@ public class TestConcatenatedLists {
   @Test
   public void testUnsupportedOps() {
     // If adding support, add tests.
-    ConcatenatedLists<Long> c = new ConcatenatedLists<Long>();
+    ConcatenatedLists<Long> c = new ConcatenatedLists<>();
     c.addSublist(Arrays.asList(0L, 1L));
     try {
       c.add(2L);
@@ -82,19 +82,19 @@ public class TestConcatenatedLists {
 
   @Test
   public void testEmpty() {
-    verify(new ConcatenatedLists<Long>(), -1);
+    verify(new ConcatenatedLists<>(), -1);
   }
 
   @Test
   public void testOneOne() {
-    ConcatenatedLists<Long> c = new ConcatenatedLists<Long>();
+    ConcatenatedLists<Long> c = new ConcatenatedLists<>();
     c.addSublist(Arrays.asList(0L));
     verify(c, 0);
   }
 
   @Test
   public void testOneMany() {
-    ConcatenatedLists<Long> c = new ConcatenatedLists<Long>();
+    ConcatenatedLists<Long> c = new ConcatenatedLists<>();
     c.addSublist(Arrays.asList(0L, 1L, 2L));
     verify(c, 2);
   }
@@ -102,7 +102,7 @@ public class TestConcatenatedLists {
   @Test
   @SuppressWarnings("unchecked")
   public void testManyOne() {
-    ConcatenatedLists<Long> c = new ConcatenatedLists<Long>();
+    ConcatenatedLists<Long> c = new ConcatenatedLists<>();
     c.addSublist(Arrays.asList(0L));
     c.addAllSublists(Arrays.asList(Arrays.asList(1L), Arrays.asList(2L)));
     verify(c, 2);
@@ -111,7 +111,7 @@ public class TestConcatenatedLists {
   @Test
   @SuppressWarnings("unchecked")
   public void testManyMany() {
-    ConcatenatedLists<Long> c = new ConcatenatedLists<Long>();
+    ConcatenatedLists<Long> c = new ConcatenatedLists<>();
     c.addAllSublists(Arrays.asList(Arrays.asList(0L, 1L)));
     c.addSublist(Arrays.asList(2L, 3L, 4L));
     c.addAllSublists(Arrays.asList(Arrays.asList(5L), Arrays.asList(6L, 7L)));
@@ -123,7 +123,7 @@ public class TestConcatenatedLists {
     assertEquals(last + 1, c.size());
     assertTrue(c.containsAll(c));
     Long[] array = c.toArray(new Long[c.size()]);
-    List<Long> all = new ArrayList<Long>();
+    List<Long> all = new ArrayList<>();
     Iterator<Long> iter = c.iterator();
     for (Long i = 0L; i <= last; ++i) {
       assertTrue(iter.hasNext());
diff --git a/hbase-common/src/test/java/org/apache/hadoop/hbase/util/TestKeyLocker.java b/hbase-common/src/test/java/org/apache/hadoop/hbase/util/TestKeyLocker.java
index 40b918c..edf2f78 100644
--- a/hbase-common/src/test/java/org/apache/hadoop/hbase/util/TestKeyLocker.java
+++ b/hbase-common/src/test/java/org/apache/hadoop/hbase/util/TestKeyLocker.java
@@ -30,7 +30,7 @@ import org.junit.experimental.categories.Category;
 public class TestKeyLocker {
   @Test
   public void testLocker(){
-    KeyLocker<String> locker = new KeyLocker<String>();
+    KeyLocker<String> locker = new KeyLocker<>();
     ReentrantLock lock1 = locker.acquireLock("l1");
     Assert.assertTrue(lock1.isHeldByCurrentThread());
 
diff --git a/hbase-common/src/test/java/org/apache/hadoop/hbase/util/TestLoadTestKVGenerator.java b/hbase-common/src/test/java/org/apache/hadoop/hbase/util/TestLoadTestKVGenerator.java
index 120f2b6..cf74a3e 100644
--- a/hbase-common/src/test/java/org/apache/hadoop/hbase/util/TestLoadTestKVGenerator.java
+++ b/hbase-common/src/test/java/org/apache/hadoop/hbase/util/TestLoadTestKVGenerator.java
@@ -64,7 +64,7 @@ public class TestLoadTestKVGenerator {
 
   @Test
   public void testCorrectAndUniqueKeys() {
-    Set<String> keys = new HashSet<String>();
+    Set<String> keys = new HashSet<>();
     for (int i = 0; i < 1000; ++i) {
       String k = LoadTestKVGenerator.md5PrefixedKey(i);
       assertFalse(keys.contains(k));
diff --git a/hbase-common/src/test/java/org/apache/hadoop/hbase/util/TestWeakObjectPool.java b/hbase-common/src/test/java/org/apache/hadoop/hbase/util/TestWeakObjectPool.java
index bf1b4eb..d9fefa2 100644
--- a/hbase-common/src/test/java/org/apache/hadoop/hbase/util/TestWeakObjectPool.java
+++ b/hbase-common/src/test/java/org/apache/hadoop/hbase/util/TestWeakObjectPool.java
@@ -35,7 +35,7 @@ public class TestWeakObjectPool {
 
   @Before
   public void setUp() {
-    pool = new WeakObjectPool<String, Object>(
+    pool = new WeakObjectPool<>(
         new WeakObjectPool.ObjectFactory<String, Object>() {
           @Override
           public Object createObject(String key) {
@@ -94,7 +94,7 @@ public class TestWeakObjectPool {
     final int THREAD_COUNT = 100;
 
     final AtomicBoolean assertionFailed = new AtomicBoolean();
-    final AtomicReference<Object> expectedObjRef = new AtomicReference<Object>();
+    final AtomicReference<Object> expectedObjRef = new AtomicReference<>();
     final CountDownLatch prepareLatch = new CountDownLatch(THREAD_COUNT);
     final CountDownLatch startLatch = new CountDownLatch(1);
     final CountDownLatch endLatch = new CountDownLatch(THREAD_COUNT);
diff --git a/hbase-endpoint/src/main/java/org/apache/hadoop/hbase/client/coprocessor/AggregationClient.java b/hbase-endpoint/src/main/java/org/apache/hadoop/hbase/client/coprocessor/AggregationClient.java
index 304722e..810778b 100644
--- a/hbase-endpoint/src/main/java/org/apache/hadoop/hbase/client/coprocessor/AggregationClient.java
+++ b/hbase-endpoint/src/main/java/org/apache/hadoop/hbase/client/coprocessor/AggregationClient.java
@@ -208,7 +208,7 @@ public class AggregationClient implements Closeable {
           public R call(AggregateService instance) throws IOException {
             RpcController controller = new AggregationClientRpcController();
             CoprocessorRpcUtils.BlockingRpcCallback<AggregateResponse> rpcCallback =
-                new CoprocessorRpcUtils.BlockingRpcCallback<AggregateResponse>();
+                new CoprocessorRpcUtils.BlockingRpcCallback<>();
             instance.getMax(controller, requestArg, rpcCallback);
             AggregateResponse response = rpcCallback.get();
             if (controller.failed()) {
@@ -280,7 +280,7 @@ public class AggregationClient implements Closeable {
           public R call(AggregateService instance) throws IOException {
             RpcController controller = new AggregationClientRpcController();
             CoprocessorRpcUtils.BlockingRpcCallback<AggregateResponse> rpcCallback =
-                new CoprocessorRpcUtils.BlockingRpcCallback<AggregateResponse>();
+                new CoprocessorRpcUtils.BlockingRpcCallback<>();
             instance.getMin(controller, requestArg, rpcCallback);
             AggregateResponse response = rpcCallback.get();
             if (controller.failed()) {
@@ -355,7 +355,7 @@ public class AggregationClient implements Closeable {
           public Long call(AggregateService instance) throws IOException {
             RpcController controller = new AggregationClientRpcController();
             CoprocessorRpcUtils.BlockingRpcCallback<AggregateResponse> rpcCallback =
-                new CoprocessorRpcUtils.BlockingRpcCallback<AggregateResponse>();
+                new CoprocessorRpcUtils.BlockingRpcCallback<>();
             instance.getRowNum(controller, requestArg, rpcCallback);
             AggregateResponse response = rpcCallback.get();
             if (controller.failed()) {
@@ -421,7 +421,7 @@ public class AggregationClient implements Closeable {
             RpcController controller = new AggregationClientRpcController();
             // Not sure what is going on here why I have to do these casts. TODO.
             CoprocessorRpcUtils.BlockingRpcCallback<AggregateResponse> rpcCallback =
-                new CoprocessorRpcUtils.BlockingRpcCallback<AggregateResponse>();
+                new CoprocessorRpcUtils.BlockingRpcCallback<>();
             instance.getSum(controller, requestArg, rpcCallback);
             AggregateResponse response = rpcCallback.get();
             if (controller.failed()) {
@@ -472,7 +472,7 @@ public class AggregationClient implements Closeable {
       Long rowCount = 0l;
 
       public synchronized Pair<S, Long> getAvgArgs() {
-        return new Pair<S, Long>(sum, rowCount);
+        return new Pair<>(sum, rowCount);
       }
 
       @Override
@@ -488,13 +488,13 @@ public class AggregationClient implements Closeable {
           public Pair<S, Long> call(AggregateService instance) throws IOException {
             RpcController controller = new AggregationClientRpcController();
             CoprocessorRpcUtils.BlockingRpcCallback<AggregateResponse> rpcCallback =
-                new CoprocessorRpcUtils.BlockingRpcCallback<AggregateResponse>();
+                new CoprocessorRpcUtils.BlockingRpcCallback<>();
             instance.getAvg(controller, requestArg, rpcCallback);
             AggregateResponse response = rpcCallback.get();
             if (controller.failed()) {
               throw new IOException(controller.errorText());
             }
-            Pair<S, Long> pair = new Pair<S, Long>(null, 0L);
+            Pair<S, Long> pair = new Pair<>(null, 0L);
             if (response.getFirstPartCount() == 0) {
               return pair;
             }
@@ -569,10 +569,10 @@ public class AggregationClient implements Closeable {
       S sumVal = null, sumSqVal = null;
 
       public synchronized Pair<List<S>, Long> getStdParams() {
-        List<S> l = new ArrayList<S>(2);
+        List<S> l = new ArrayList<>(2);
         l.add(sumVal);
         l.add(sumSqVal);
-        Pair<List<S>, Long> p = new Pair<List<S>, Long>(l, rowCountVal);
+        Pair<List<S>, Long> p = new Pair<>(l, rowCountVal);
         return p;
       }
 
@@ -592,17 +592,17 @@ public class AggregationClient implements Closeable {
           public Pair<List<S>, Long> call(AggregateService instance) throws IOException {
             RpcController controller = new AggregationClientRpcController();
             CoprocessorRpcUtils.BlockingRpcCallback<AggregateResponse> rpcCallback =
-                new CoprocessorRpcUtils.BlockingRpcCallback<AggregateResponse>();
+                new CoprocessorRpcUtils.BlockingRpcCallback<>();
             instance.getStd(controller, requestArg, rpcCallback);
             AggregateResponse response = rpcCallback.get();
             if (controller.failed()) {
               throw new IOException(controller.errorText());
             }
-            Pair<List<S>, Long> pair = new Pair<List<S>, Long>(new ArrayList<S>(), 0L);
+            Pair<List<S>, Long> pair = new Pair<>(new ArrayList<>(), 0L);
             if (response.getFirstPartCount() == 0) {
               return pair;
             }
-            List<S> list = new ArrayList<S>();
+            List<S> list = new ArrayList<>();
             for (int i = 0; i < response.getFirstPartCount(); i++) {
               ByteString b = response.getFirstPart(i);
               T t = getParsedGenericInstance(ci.getClass(), 4, b);
@@ -680,17 +680,15 @@ public class AggregationClient implements Closeable {
   getMedianArgs(final Table table,
       final ColumnInterpreter<R, S, P, Q, T> ci, final Scan scan) throws Throwable {
     final AggregateRequest requestArg = validateArgAndGetPB(scan, ci, false);
-    final NavigableMap<byte[], List<S>> map =
-      new TreeMap<byte[], List<S>>(Bytes.BYTES_COMPARATOR);
+    final NavigableMap<byte[], List<S>> map = new TreeMap<>(Bytes.BYTES_COMPARATOR);
     class StdCallback implements Batch.Callback<List<S>> {
       S sumVal = null, sumWeights = null;
 
       public synchronized Pair<NavigableMap<byte[], List<S>>, List<S>> getMedianParams() {
-        List<S> l = new ArrayList<S>(2);
+        List<S> l = new ArrayList<>(2);
         l.add(sumVal);
         l.add(sumWeights);
-        Pair<NavigableMap<byte[], List<S>>, List<S>> p =
-          new Pair<NavigableMap<byte[], List<S>>, List<S>>(map, l);
+        Pair<NavigableMap<byte[], List<S>>, List<S>> p = new Pair<>(map, l);
         return p;
       }
 
@@ -708,14 +706,14 @@ public class AggregationClient implements Closeable {
           public List<S> call(AggregateService instance) throws IOException {
             RpcController controller = new AggregationClientRpcController();
             CoprocessorRpcUtils.BlockingRpcCallback<AggregateResponse> rpcCallback =
-                new CoprocessorRpcUtils.BlockingRpcCallback<AggregateResponse>();
+                new CoprocessorRpcUtils.BlockingRpcCallback<>();
             instance.getMedian(controller, requestArg, rpcCallback);
             AggregateResponse response = rpcCallback.get();
             if (controller.failed()) {
               throw new IOException(controller.errorText());
             }
 
-            List<S> list = new ArrayList<S>();
+            List<S> list = new ArrayList<>();
             for (int i = 0; i < response.getFirstPartCount(); i++) {
               ByteString b = response.getFirstPart(i);
               T t = getParsedGenericInstance(ci.getClass(), 4, b);
diff --git a/hbase-endpoint/src/main/java/org/apache/hadoop/hbase/coprocessor/AggregateImplementation.java b/hbase-endpoint/src/main/java/org/apache/hadoop/hbase/coprocessor/AggregateImplementation.java
index bccb76a..3fbbd52 100644
--- a/hbase-endpoint/src/main/java/org/apache/hadoop/hbase/coprocessor/AggregateImplementation.java
+++ b/hbase-endpoint/src/main/java/org/apache/hadoop/hbase/coprocessor/AggregateImplementation.java
@@ -83,7 +83,7 @@ extends AggregateService implements CoprocessorService, Coprocessor {
       T temp;
       Scan scan = ProtobufUtil.toScan(request.getScan());
       scanner = env.getRegion().getScanner(scan);
-      List<Cell> results = new ArrayList<Cell>();
+      List<Cell> results = new ArrayList<>();
       byte[] colFamily = scan.getFamilies()[0];
       NavigableSet<byte[]> qualifiers = scan.getFamilyMap().get(colFamily);
       byte[] qualifier = null;
@@ -138,7 +138,7 @@ extends AggregateService implements CoprocessorService, Coprocessor {
       T temp;
       Scan scan = ProtobufUtil.toScan(request.getScan());
       scanner = env.getRegion().getScanner(scan);
-      List<Cell> results = new ArrayList<Cell>();
+      List<Cell> results = new ArrayList<>();
       byte[] colFamily = scan.getFamilies()[0];
       NavigableSet<byte[]> qualifiers = scan.getFamilyMap().get(colFamily);
       byte[] qualifier = null;
@@ -198,7 +198,7 @@ extends AggregateService implements CoprocessorService, Coprocessor {
       if (qualifiers != null && !qualifiers.isEmpty()) {
         qualifier = qualifiers.pollFirst();
       }
-      List<Cell> results = new ArrayList<Cell>();
+      List<Cell> results = new ArrayList<>();
       boolean hasMoreRows = false;
       do {
         hasMoreRows = scanner.next(results);
@@ -237,7 +237,7 @@ extends AggregateService implements CoprocessorService, Coprocessor {
       RpcCallback<AggregateResponse> done) {
     AggregateResponse response = null;
     long counter = 0l;
-    List<Cell> results = new ArrayList<Cell>();
+    List<Cell> results = new ArrayList<>();
     InternalScanner scanner = null;
     try {
       Scan scan = ProtobufUtil.toScan(request.getScan());
@@ -308,7 +308,7 @@ extends AggregateService implements CoprocessorService, Coprocessor {
       if (qualifiers != null && !qualifiers.isEmpty()) {
         qualifier = qualifiers.pollFirst();
       }
-      List<Cell> results = new ArrayList<Cell>();
+      List<Cell> results = new ArrayList<>();
       boolean hasMoreRows = false;
     
       do {
@@ -368,7 +368,7 @@ extends AggregateService implements CoprocessorService, Coprocessor {
       if (qualifiers != null && !qualifiers.isEmpty()) {
         qualifier = qualifiers.pollFirst();
       }
-      List<Cell> results = new ArrayList<Cell>();
+      List<Cell> results = new ArrayList<>();
 
       boolean hasMoreRows = false;
     
@@ -434,7 +434,7 @@ extends AggregateService implements CoprocessorService, Coprocessor {
         // if weighted median is requested, get qualifier for the weight column
         weightQualifier = qualifiers.pollLast();
       }
-      List<Cell> results = new ArrayList<Cell>();
+      List<Cell> results = new ArrayList<>();
 
       boolean hasMoreRows = false;
     
diff --git a/hbase-endpoint/src/test/java/org/apache/hadoop/hbase/coprocessor/ColumnAggregationEndpoint.java b/hbase-endpoint/src/test/java/org/apache/hadoop/hbase/coprocessor/ColumnAggregationEndpoint.java
index a9d10e8..b52e5f9 100644
--- a/hbase-endpoint/src/test/java/org/apache/hadoop/hbase/coprocessor/ColumnAggregationEndpoint.java
+++ b/hbase-endpoint/src/test/java/org/apache/hadoop/hbase/coprocessor/ColumnAggregationEndpoint.java
@@ -84,7 +84,7 @@ implements Coprocessor, CoprocessorService {
     InternalScanner scanner = null;
     try {
       scanner = this.env.getRegion().getScanner(scan);
-      List<Cell> curVals = new ArrayList<Cell>();
+      List<Cell> curVals = new ArrayList<>();
       boolean hasMore = false;
       do {
         curVals.clear();
diff --git a/hbase-endpoint/src/test/java/org/apache/hadoop/hbase/coprocessor/ColumnAggregationEndpointNullResponse.java b/hbase-endpoint/src/test/java/org/apache/hadoop/hbase/coprocessor/ColumnAggregationEndpointNullResponse.java
index 22dac6d..54e3358 100644
--- a/hbase-endpoint/src/test/java/org/apache/hadoop/hbase/coprocessor/ColumnAggregationEndpointNullResponse.java
+++ b/hbase-endpoint/src/test/java/org/apache/hadoop/hbase/coprocessor/ColumnAggregationEndpointNullResponse.java
@@ -94,7 +94,7 @@ implements Coprocessor, CoprocessorService  {
         return;
       }
       scanner = region.getScanner(scan);
-      List<Cell> curVals = new ArrayList<Cell>();
+      List<Cell> curVals = new ArrayList<>();
       boolean hasMore = false;
       do {
         curVals.clear();
diff --git a/hbase-endpoint/src/test/java/org/apache/hadoop/hbase/coprocessor/ColumnAggregationEndpointWithErrors.java b/hbase-endpoint/src/test/java/org/apache/hadoop/hbase/coprocessor/ColumnAggregationEndpointWithErrors.java
index c75fb31..6e8c571 100644
--- a/hbase-endpoint/src/test/java/org/apache/hadoop/hbase/coprocessor/ColumnAggregationEndpointWithErrors.java
+++ b/hbase-endpoint/src/test/java/org/apache/hadoop/hbase/coprocessor/ColumnAggregationEndpointWithErrors.java
@@ -94,7 +94,7 @@ implements Coprocessor, CoprocessorService  {
         throw new DoNotRetryIOException("An expected exception");
       }
       scanner = region.getScanner(scan);
-      List<Cell> curVals = new ArrayList<Cell>();
+      List<Cell> curVals = new ArrayList<>();
       boolean hasMore = false;
       do {
         curVals.clear();
diff --git a/hbase-endpoint/src/test/java/org/apache/hadoop/hbase/coprocessor/TestClassLoading.java b/hbase-endpoint/src/test/java/org/apache/hadoop/hbase/coprocessor/TestClassLoading.java
index 3ed8a56..56fdca6 100644
--- a/hbase-endpoint/src/test/java/org/apache/hadoop/hbase/coprocessor/TestClassLoading.java
+++ b/hbase-endpoint/src/test/java/org/apache/hadoop/hbase/coprocessor/TestClassLoading.java
@@ -168,8 +168,7 @@ public class TestClassLoading {
     // verify that the coprocessors were loaded
     boolean foundTableRegion=false;
     boolean found1 = true, found2 = true, found2_k1 = true, found2_k2 = true, found2_k3 = true;
-    Map<Region, Set<ClassLoader>> regionsActiveClassLoaders =
-        new HashMap<Region, Set<ClassLoader>>();
+    Map<Region, Set<ClassLoader>> regionsActiveClassLoaders = new HashMap<>();
     MiniHBaseCluster hbase = TEST_UTIL.getHBaseCluster();
     for (Region region:
         hbase.getRegionServer(0).getOnlineRegionsLocalContext()) {
@@ -209,7 +208,7 @@ public class TestClassLoading {
       " of external jar files",
       2, CoprocessorClassLoader.getAllCached().size());
     //check if region active classloaders are shared across all RS regions
-    Set<ClassLoader> externalClassLoaders = new HashSet<ClassLoader>(
+    Set<ClassLoader> externalClassLoaders = new HashSet<>(
       CoprocessorClassLoader.getAllCached());
     for (Map.Entry<Region, Set<ClassLoader>> regionCP : regionsActiveClassLoaders.entrySet()) {
       assertTrue("Some CP classloaders for region " + regionCP.getKey() + " are not cached."
@@ -312,7 +311,7 @@ public class TestClassLoading {
     // add 2 coprocessor by using new htd.addCoprocessor() api
     htd.addCoprocessor(cpName5, new Path(getLocalPath(jarFile5)),
         Coprocessor.PRIORITY_USER, null);
-    Map<String, String> kvs = new HashMap<String, String>();
+    Map<String, String> kvs = new HashMap<>();
     kvs.put("k1", "v1");
     kvs.put("k2", "v2");
     kvs.put("k3", "v3");
@@ -466,8 +465,7 @@ public class TestClassLoading {
    * @return subset of all servers.
    */
   Map<ServerName, ServerLoad> serversForTable(String tableName) {
-    Map<ServerName, ServerLoad> serverLoadHashMap =
-        new HashMap<ServerName, ServerLoad>();
+    Map<ServerName, ServerLoad> serverLoadHashMap = new HashMap<>();
     for(Map.Entry<ServerName,ServerLoad> server:
         TEST_UTIL.getMiniHBaseCluster().getMaster().getServerManager().
             getOnlineServers().entrySet()) {
diff --git a/hbase-endpoint/src/test/java/org/apache/hadoop/hbase/coprocessor/TestCoprocessorEndpoint.java b/hbase-endpoint/src/test/java/org/apache/hadoop/hbase/coprocessor/TestCoprocessorEndpoint.java
index 547b7e9..adfd8d5 100644
--- a/hbase-endpoint/src/test/java/org/apache/hadoop/hbase/coprocessor/TestCoprocessorEndpoint.java
+++ b/hbase-endpoint/src/test/java/org/apache/hadoop/hbase/coprocessor/TestCoprocessorEndpoint.java
@@ -124,7 +124,7 @@ public class TestCoprocessorEndpoint {
         public Long call(ColumnAggregationProtos.ColumnAggregationService instance)
         throws IOException {
           CoprocessorRpcUtils.BlockingRpcCallback<ColumnAggregationProtos.SumResponse> rpcCallback =
-              new CoprocessorRpcUtils.BlockingRpcCallback<ColumnAggregationProtos.SumResponse>();
+              new CoprocessorRpcUtils.BlockingRpcCallback<>();
           ColumnAggregationProtos.SumRequest.Builder builder =
             ColumnAggregationProtos.SumRequest.newBuilder();
           builder.setFamily(ByteStringer.wrap(family));
@@ -193,7 +193,7 @@ public class TestCoprocessorEndpoint {
                 throws IOException {
               LOG.debug("Default response is " + TestProtos.EchoRequestProto.getDefaultInstance());
               CoprocessorRpcUtils.BlockingRpcCallback<TestProtos.EchoResponseProto> callback =
-                  new CoprocessorRpcUtils.BlockingRpcCallback<TestProtos.EchoResponseProto>();
+                  new CoprocessorRpcUtils.BlockingRpcCallback<>();
               instance.echo(controller, request, callback);
               TestProtos.EchoResponseProto response = callback.get();
               LOG.debug("Batch.Call returning result " + response);
@@ -226,7 +226,7 @@ public class TestCoprocessorEndpoint {
                 throws IOException {
               LOG.debug("Default response is " + TestProtos.EchoRequestProto.getDefaultInstance());
               CoprocessorRpcUtils.BlockingRpcCallback<TestProtos.EchoResponseProto> callback =
-                  new CoprocessorRpcUtils.BlockingRpcCallback<TestProtos.EchoResponseProto>();
+                  new CoprocessorRpcUtils.BlockingRpcCallback<>();
               instance.echo(controller, request, callback);
               TestProtos.EchoResponseProto response = callback.get();
               LOG.debug("Batch.Call returning result " + response);
@@ -271,7 +271,7 @@ public class TestCoprocessorEndpoint {
             public String call(TestRpcServiceProtos.TestProtobufRpcProto instance)
                 throws IOException {
               CoprocessorRpcUtils.BlockingRpcCallback<TestProtos.EchoResponseProto> callback =
-                  new CoprocessorRpcUtils.BlockingRpcCallback<TestProtos.EchoResponseProto>();
+                  new CoprocessorRpcUtils.BlockingRpcCallback<>();
               instance.echo(controller, request, callback);
               TestProtos.EchoResponseProto response = callback.get();
               LOG.debug("Batch.Call got result " + response);
diff --git a/hbase-endpoint/src/test/java/org/apache/hadoop/hbase/coprocessor/TestCoprocessorTableEndpoint.java b/hbase-endpoint/src/test/java/org/apache/hadoop/hbase/coprocessor/TestCoprocessorTableEndpoint.java
index 0af655a..0783131 100644
--- a/hbase-endpoint/src/test/java/org/apache/hadoop/hbase/coprocessor/TestCoprocessorTableEndpoint.java
+++ b/hbase-endpoint/src/test/java/org/apache/hadoop/hbase/coprocessor/TestCoprocessorTableEndpoint.java
@@ -117,7 +117,7 @@ public class TestCoprocessorTableEndpoint {
       public Long call(ColumnAggregationProtos.ColumnAggregationService instance)
       throws IOException {
         CoprocessorRpcUtils.BlockingRpcCallback<ColumnAggregationProtos.SumResponse> rpcCallback =
-            new CoprocessorRpcUtils.BlockingRpcCallback<ColumnAggregationProtos.SumResponse>();
+            new CoprocessorRpcUtils.BlockingRpcCallback<>();
         ColumnAggregationProtos.SumRequest.Builder builder =
           ColumnAggregationProtos.SumRequest.newBuilder();
         builder.setFamily(ByteString.copyFrom(family));
diff --git a/hbase-endpoint/src/test/java/org/apache/hadoop/hbase/coprocessor/TestRegionServerCoprocessorEndpoint.java b/hbase-endpoint/src/test/java/org/apache/hadoop/hbase/coprocessor/TestRegionServerCoprocessorEndpoint.java
index 69742a6..9dc4822 100644
--- a/hbase-endpoint/src/test/java/org/apache/hadoop/hbase/coprocessor/TestRegionServerCoprocessorEndpoint.java
+++ b/hbase-endpoint/src/test/java/org/apache/hadoop/hbase/coprocessor/TestRegionServerCoprocessorEndpoint.java
@@ -73,8 +73,7 @@ public class TestRegionServerCoprocessorEndpoint {
     final ServerName serverName = TEST_UTIL.getHBaseCluster().getRegionServer(0).getServerName();
     final ServerRpcController controller = new ServerRpcController();
     final CoprocessorRpcUtils.BlockingRpcCallback<DummyRegionServerEndpointProtos.DummyResponse>
-        rpcCallback =
-      new CoprocessorRpcUtils.BlockingRpcCallback<DummyRegionServerEndpointProtos.DummyResponse>();
+        rpcCallback = new CoprocessorRpcUtils.BlockingRpcCallback<>();
     DummyRegionServerEndpointProtos.DummyService service =
         ProtobufUtil.newServiceStub(DummyRegionServerEndpointProtos.DummyService.class,
           TEST_UTIL.getAdmin().coprocessorService(serverName));
@@ -91,8 +90,7 @@ public class TestRegionServerCoprocessorEndpoint {
     final ServerName serverName = TEST_UTIL.getHBaseCluster().getRegionServer(0).getServerName();
     final ServerRpcController controller = new ServerRpcController();
     final CoprocessorRpcUtils.BlockingRpcCallback<DummyRegionServerEndpointProtos.DummyResponse>
-        rpcCallback =
-      new CoprocessorRpcUtils.BlockingRpcCallback<DummyRegionServerEndpointProtos.DummyResponse>();
+        rpcCallback = new CoprocessorRpcUtils.BlockingRpcCallback<>();
     DummyRegionServerEndpointProtos.DummyService service =
         ProtobufUtil.newServiceStub(DummyRegionServerEndpointProtos.DummyService.class,
             TEST_UTIL.getAdmin().coprocessorService(serverName));
diff --git a/hbase-endpoint/src/test/java/org/apache/hadoop/hbase/coprocessor/TestRowProcessorEndpoint.java b/hbase-endpoint/src/test/java/org/apache/hadoop/hbase/coprocessor/TestRowProcessorEndpoint.java
index 07d2042..ed53027 100644
--- a/hbase-endpoint/src/test/java/org/apache/hadoop/hbase/coprocessor/TestRowProcessorEndpoint.java
+++ b/hbase-endpoint/src/test/java/org/apache/hadoop/hbase/coprocessor/TestRowProcessorEndpoint.java
@@ -166,10 +166,9 @@ public class TestRowProcessorEndpoint {
     ProcessResponse protoResult = service.process(null, request);
     FriendsOfFriendsProcessorResponse response =
         FriendsOfFriendsProcessorResponse.parseFrom(protoResult.getRowProcessorResult());
-    Set<String> result = new HashSet<String>();
+    Set<String> result = new HashSet<>();
     result.addAll(response.getResultList());
-    Set<String> expected =
-      new HashSet<String>(Arrays.asList(new String[]{"d", "e", "f", "g"}));
+    Set<String> expected = new HashSet<>(Arrays.asList(new String[]{"d", "e", "f", "g"}));
     Get get = new Get(ROW);
     LOG.debug("row keyvalues:" + stringifyKvs(table.get(get).listCells()));
     assertEquals(expected, result);
@@ -349,7 +348,7 @@ public class TestRowProcessorEndpoint {
       public void process(long now, HRegion region,
           List<Mutation> mutations, WALEdit walEdit) throws IOException {
         // Scan current counter
-        List<Cell> kvs = new ArrayList<Cell>();
+        List<Cell> kvs = new ArrayList<>();
         Scan scan = new Scan(row, row);
         scan.addColumn(FAM, COUNTER);
         doScan(region, scan, kvs);
@@ -398,7 +397,7 @@ public class TestRowProcessorEndpoint {
         BaseRowProcessor<FriendsOfFriendsProcessorRequest, FriendsOfFriendsProcessorResponse> {
       byte[] row = null;
       byte[] person = null;
-      final Set<String> result = new HashSet<String>();
+      final Set<String> result = new HashSet<>();
 
       /**
        * Empty constructor for Writable
@@ -432,7 +431,7 @@ public class TestRowProcessorEndpoint {
       @Override
       public void process(long now, HRegion region,
           List<Mutation> mutations, WALEdit walEdit) throws IOException {
-        List<Cell> kvs = new ArrayList<Cell>();
+        List<Cell> kvs = new ArrayList<>();
         { // First scan to get friends of the person
           Scan scan = new Scan(row, row);
           scan.addColumn(FAM, person);
@@ -497,7 +496,7 @@ public class TestRowProcessorEndpoint {
 
       @Override
       public Collection<byte[]> getRowsToLock() {
-        List<byte[]> rows = new ArrayList<byte[]>(2);
+        List<byte[]> rows = new ArrayList<>(2);
         rows.add(row1);
         rows.add(row2);
         return rows;
@@ -522,8 +521,8 @@ public class TestRowProcessorEndpoint {
         now = myTimer.getAndIncrement();
 
         // Scan both rows
-        List<Cell> kvs1 = new ArrayList<Cell>();
-        List<Cell> kvs2 = new ArrayList<Cell>();
+        List<Cell> kvs1 = new ArrayList<>();
+        List<Cell> kvs2 = new ArrayList<>();
         doScan(region, new Scan(row1, row1), kvs1);
         doScan(region, new Scan(row2, row2), kvs2);
 
@@ -538,7 +537,7 @@ public class TestRowProcessorEndpoint {
         swapped = !swapped;
 
         // Add and delete keyvalues
-        List<List<Cell>> kvs = new ArrayList<List<Cell>>(2);
+        List<List<Cell>> kvs = new ArrayList<>(2);
         kvs.add(kvs1);
         kvs.add(kvs2);
         byte[][] rows = new byte[][]{row1, row2};
diff --git a/hbase-endpoint/src/test/java/org/apache/hadoop/hbase/regionserver/SecureBulkLoadEndpointClient.java b/hbase-endpoint/src/test/java/org/apache/hadoop/hbase/regionserver/SecureBulkLoadEndpointClient.java
index 06e45eb..323999d 100644
--- a/hbase-endpoint/src/test/java/org/apache/hadoop/hbase/regionserver/SecureBulkLoadEndpointClient.java
+++ b/hbase-endpoint/src/test/java/org/apache/hadoop/hbase/regionserver/SecureBulkLoadEndpointClient.java
@@ -64,7 +64,7 @@ public class SecureBulkLoadEndpointClient {
       ServerRpcController controller = new ServerRpcController();
 
       CoprocessorRpcUtils.BlockingRpcCallback<PrepareBulkLoadResponse> rpcCallback =
-          new CoprocessorRpcUtils.BlockingRpcCallback<PrepareBulkLoadResponse>();
+          new CoprocessorRpcUtils.BlockingRpcCallback<>();
 
       PrepareBulkLoadRequest request =
           PrepareBulkLoadRequest.newBuilder()
@@ -92,7 +92,7 @@ public class SecureBulkLoadEndpointClient {
       ServerRpcController controller = new ServerRpcController();
 
       CoprocessorRpcUtils.BlockingRpcCallback<CleanupBulkLoadResponse> rpcCallback =
-          new CoprocessorRpcUtils.BlockingRpcCallback<CleanupBulkLoadResponse>();
+          new CoprocessorRpcUtils.BlockingRpcCallback<>();
 
       CleanupBulkLoadRequest request =
           CleanupBulkLoadRequest.newBuilder()
@@ -133,7 +133,7 @@ public class SecureBulkLoadEndpointClient {
       }
 
       List<ClientProtos.BulkLoadHFileRequest.FamilyPath> protoFamilyPaths =
-          new ArrayList<ClientProtos.BulkLoadHFileRequest.FamilyPath>(familyPaths.size());
+          new ArrayList<>(familyPaths.size());
       for(Pair<byte[], String> el: familyPaths) {
         protoFamilyPaths.add(ClientProtos.BulkLoadHFileRequest.FamilyPath.newBuilder()
           .setFamily(ByteStringer.wrap(el.getFirst()))
@@ -148,8 +148,7 @@ public class SecureBulkLoadEndpointClient {
 
       ServerRpcController controller = new ServerRpcController();
       CoprocessorRpcUtils.BlockingRpcCallback<SecureBulkLoadProtos.SecureBulkLoadHFilesResponse>
-            rpcCallback =
-          new CoprocessorRpcUtils.BlockingRpcCallback<SecureBulkLoadProtos.SecureBulkLoadHFilesResponse>();
+            rpcCallback = new CoprocessorRpcUtils.BlockingRpcCallback<>();
       instance.secureBulkLoadHFiles(controller,
         request,
         rpcCallback);
diff --git a/hbase-endpoint/src/test/java/org/apache/hadoop/hbase/regionserver/TestHRegionServerBulkLoadWithOldSecureEndpoint.java b/hbase-endpoint/src/test/java/org/apache/hadoop/hbase/regionserver/TestHRegionServerBulkLoadWithOldSecureEndpoint.java
index c0d2719..10a4d19 100644
--- a/hbase-endpoint/src/test/java/org/apache/hadoop/hbase/regionserver/TestHRegionServerBulkLoadWithOldSecureEndpoint.java
+++ b/hbase-endpoint/src/test/java/org/apache/hadoop/hbase/regionserver/TestHRegionServerBulkLoadWithOldSecureEndpoint.java
@@ -92,8 +92,7 @@ public class TestHRegionServerBulkLoadWithOldSecureEndpoint extends TestHRegionS
       // create HFiles for different column families
       FileSystem fs = UTIL.getTestFileSystem();
       byte[] val = Bytes.toBytes(String.format("%010d", iteration));
-      final List<Pair<byte[], String>> famPaths = new ArrayList<Pair<byte[], String>>(
-          NUM_CFS);
+      final List<Pair<byte[], String>> famPaths = new ArrayList<>(NUM_CFS);
       for (int i = 0; i < NUM_CFS; i++) {
         Path hfile = new Path(dir, family(i));
         byte[] fam = Bytes.toBytes(family(i));
diff --git a/hbase-endpoint/src/test/java/org/apache/hadoop/hbase/regionserver/TestServerCustomProtocol.java b/hbase-endpoint/src/test/java/org/apache/hadoop/hbase/regionserver/TestServerCustomProtocol.java
index 9bff701..83c7dbf 100644
--- a/hbase-endpoint/src/test/java/org/apache/hadoop/hbase/regionserver/TestServerCustomProtocol.java
+++ b/hbase-endpoint/src/test/java/org/apache/hadoop/hbase/regionserver/TestServerCustomProtocol.java
@@ -198,7 +198,7 @@ public class TestServerCustomProtocol {
         @Override
         public Integer call(PingProtos.PingService instance) throws IOException {
           CoprocessorRpcUtils.BlockingRpcCallback<PingProtos.CountResponse> rpcCallback =
-            new CoprocessorRpcUtils.BlockingRpcCallback<PingProtos.CountResponse>();
+            new CoprocessorRpcUtils.BlockingRpcCallback<>();
           instance.count(null, PingProtos.CountRequest.newBuilder().build(), rpcCallback);
           return rpcCallback.get().getCount();
         }
@@ -215,7 +215,7 @@ public class TestServerCustomProtocol {
         @Override
         public Integer call(PingProtos.PingService instance) throws IOException {
           CoprocessorRpcUtils.BlockingRpcCallback<PingProtos.IncrementCountResponse> rpcCallback =
-            new CoprocessorRpcUtils.BlockingRpcCallback<PingProtos.IncrementCountResponse>();
+            new CoprocessorRpcUtils.BlockingRpcCallback<>();
           instance.increment(null,
               PingProtos.IncrementCountRequest.newBuilder().setDiff(diff).build(),
             rpcCallback);
@@ -253,7 +253,7 @@ public class TestServerCustomProtocol {
           @Override
           public String call(PingProtos.PingService instance) throws IOException {
             CoprocessorRpcUtils.BlockingRpcCallback<PingProtos.HelloResponse> rpcCallback =
-              new CoprocessorRpcUtils.BlockingRpcCallback<PingProtos.HelloResponse>();
+              new CoprocessorRpcUtils.BlockingRpcCallback<>();
             PingProtos.HelloRequest.Builder builder = PingProtos.HelloRequest.newBuilder();
             if (send != null) builder.setName(send);
             instance.hello(null, builder.build(), rpcCallback);
@@ -272,7 +272,7 @@ public class TestServerCustomProtocol {
           @Override
           public String call(PingProtos.PingService instance) throws IOException {
             CoprocessorRpcUtils.BlockingRpcCallback<PingProtos.HelloResponse> rpcCallback =
-              new CoprocessorRpcUtils.BlockingRpcCallback<PingProtos.HelloResponse>();
+              new CoprocessorRpcUtils.BlockingRpcCallback<>();
             PingProtos.HelloRequest.Builder builder = PingProtos.HelloRequest.newBuilder();
             // Call ping on same instance.  Use result calling hello on same instance.
             builder.setName(doPing(instance));
@@ -291,7 +291,7 @@ public class TestServerCustomProtocol {
           @Override
           public String call(PingProtos.PingService instance) throws IOException {
             CoprocessorRpcUtils.BlockingRpcCallback<PingProtos.NoopResponse> rpcCallback =
-              new CoprocessorRpcUtils.BlockingRpcCallback<PingProtos.NoopResponse>();
+              new CoprocessorRpcUtils.BlockingRpcCallback<>();
             PingProtos.NoopRequest.Builder builder = PingProtos.NoopRequest.newBuilder();
             instance.noop(null, builder.build(), rpcCallback);
             rpcCallback.get();
@@ -311,7 +311,7 @@ public class TestServerCustomProtocol {
           @Override
           public String call(PingProtos.PingService instance) throws IOException {
             CoprocessorRpcUtils.BlockingRpcCallback<PingProtos.PingResponse> rpcCallback =
-              new CoprocessorRpcUtils.BlockingRpcCallback<PingProtos.PingResponse>();
+              new CoprocessorRpcUtils.BlockingRpcCallback<>();
             instance.ping(null, PingProtos.PingRequest.newBuilder().build(), rpcCallback);
             return rpcCallback.get().getPong();
           }
@@ -406,7 +406,7 @@ public class TestServerCustomProtocol {
 
   private static String doPing(PingProtos.PingService instance) throws IOException {
     CoprocessorRpcUtils.BlockingRpcCallback<PingProtos.PingResponse> rpcCallback =
-        new CoprocessorRpcUtils.BlockingRpcCallback<PingProtos.PingResponse>();
+        new CoprocessorRpcUtils.BlockingRpcCallback<>();
       instance.ping(null, PingProtos.PingRequest.newBuilder().build(), rpcCallback);
       return rpcCallback.get().getPong();
   }
diff --git a/hbase-endpoint/src/test/java/org/apache/hadoop/hbase/replication/TestReplicationSyncUpToolWithBulkLoadedData.java b/hbase-endpoint/src/test/java/org/apache/hadoop/hbase/replication/TestReplicationSyncUpToolWithBulkLoadedData.java
index f54c632..75f8ee2 100644
--- a/hbase-endpoint/src/test/java/org/apache/hadoop/hbase/replication/TestReplicationSyncUpToolWithBulkLoadedData.java
+++ b/hbase-endpoint/src/test/java/org/apache/hadoop/hbase/replication/TestReplicationSyncUpToolWithBulkLoadedData.java
@@ -71,7 +71,7 @@ public class TestReplicationSyncUpToolWithBulkLoadedData extends TestReplication
      * Prepare 16 random hfile ranges required for creating hfiles
      */
     Iterator<String> randomHFileRangeListIterator = null;
-    Set<String> randomHFileRanges = new HashSet<String>(16);
+    Set<String> randomHFileRanges = new HashSet<>(16);
     for (int i = 0; i < 16; i++) {
       randomHFileRanges.add(UUID.randomUUID().toString());
     }
diff --git a/hbase-examples/src/main/java/org/apache/hadoop/hbase/coprocessor/example/BulkDeleteEndpoint.java b/hbase-examples/src/main/java/org/apache/hadoop/hbase/coprocessor/example/BulkDeleteEndpoint.java
index 7e6c290..79ff25b 100644
--- a/hbase-examples/src/main/java/org/apache/hadoop/hbase/coprocessor/example/BulkDeleteEndpoint.java
+++ b/hbase-examples/src/main/java/org/apache/hadoop/hbase/coprocessor/example/BulkDeleteEndpoint.java
@@ -133,9 +133,9 @@ public class BulkDeleteEndpoint extends BulkDeleteService implements Coprocessor
       // filter and having necessary column(s).
       scanner = region.getScanner(scan);
       while (hasMore) {
-        List<List<Cell>> deleteRows = new ArrayList<List<Cell>>(rowBatchSize);
+        List<List<Cell>> deleteRows = new ArrayList<>(rowBatchSize);
         for (int i = 0; i < rowBatchSize; i++) {
-          List<Cell> results = new ArrayList<Cell>();
+          List<Cell> results = new ArrayList<>();
           hasMore = scanner.next(results);
           if (results.size() > 0) {
             deleteRows.add(results);
@@ -202,14 +202,14 @@ public class BulkDeleteEndpoint extends BulkDeleteService implements Coprocessor
     byte[] row = CellUtil.cloneRow(deleteRow.get(0));
     Delete delete = new Delete(row, ts);
     if (deleteType == DeleteType.FAMILY) {
-      Set<byte[]> families = new TreeSet<byte[]>(Bytes.BYTES_COMPARATOR);
+      Set<byte[]> families = new TreeSet<>(Bytes.BYTES_COMPARATOR);
       for (Cell kv : deleteRow) {
         if (families.add(CellUtil.cloneFamily(kv))) {
           delete.addFamily(CellUtil.cloneFamily(kv), ts);
         }
       }
     } else if (deleteType == DeleteType.COLUMN) {
-      Set<Column> columns = new HashSet<Column>();
+      Set<Column> columns = new HashSet<>();
       for (Cell kv : deleteRow) {
         Column column = new Column(CellUtil.cloneFamily(kv), CellUtil.cloneQualifier(kv));
         if (columns.add(column)) {
@@ -231,7 +231,7 @@ public class BulkDeleteEndpoint extends BulkDeleteService implements Coprocessor
           noOfVersionsToDelete++;
         }
       } else {
-        Set<Column> columns = new HashSet<Column>();
+        Set<Column> columns = new HashSet<>();
         for (Cell kv : deleteRow) {
           Column column = new Column(CellUtil.cloneFamily(kv), CellUtil.cloneQualifier(kv));
           // Only one version of particular column getting deleted.
diff --git a/hbase-examples/src/main/java/org/apache/hadoop/hbase/coprocessor/example/RowCountEndpoint.java b/hbase-examples/src/main/java/org/apache/hadoop/hbase/coprocessor/example/RowCountEndpoint.java
index c2387c5..36d8488 100644
--- a/hbase-examples/src/main/java/org/apache/hadoop/hbase/coprocessor/example/RowCountEndpoint.java
+++ b/hbase-examples/src/main/java/org/apache/hadoop/hbase/coprocessor/example/RowCountEndpoint.java
@@ -75,7 +75,7 @@ public class RowCountEndpoint extends ExampleProtos.RowCountService
     InternalScanner scanner = null;
     try {
       scanner = env.getRegion().getScanner(scan);
-      List<Cell> results = new ArrayList<Cell>();
+      List<Cell> results = new ArrayList<>();
       boolean hasMore = false;
       byte[] lastRow = null;
       long count = 0;
@@ -115,7 +115,7 @@ public class RowCountEndpoint extends ExampleProtos.RowCountService
     InternalScanner scanner = null;
     try {
       scanner = env.getRegion().getScanner(new Scan());
-      List<Cell> results = new ArrayList<Cell>();
+      List<Cell> results = new ArrayList<>();
       boolean hasMore = false;
       long count = 0;
       do {
diff --git a/hbase-examples/src/main/java/org/apache/hadoop/hbase/mapreduce/IndexBuilder.java b/hbase-examples/src/main/java/org/apache/hadoop/hbase/mapreduce/IndexBuilder.java
index 1dab633..01e9ef3 100644
--- a/hbase-examples/src/main/java/org/apache/hadoop/hbase/mapreduce/IndexBuilder.java
+++ b/hbase-examples/src/main/java/org/apache/hadoop/hbase/mapreduce/IndexBuilder.java
@@ -103,7 +103,7 @@ public class IndexBuilder extends Configured implements Tool {
       String[] fields = configuration.getStrings("index.fields");
       String familyName = configuration.get("index.familyname");
       family = Bytes.toBytes(familyName);
-      indexes = new TreeMap<byte[], ImmutableBytesWritable>(Bytes.BYTES_COMPARATOR);
+      indexes = new TreeMap<>(Bytes.BYTES_COMPARATOR);
       for(String field : fields) {
         // if the table is "people" and the field to index is "email", then the
         // index table will be called "people-email"
diff --git a/hbase-examples/src/main/java/org/apache/hadoop/hbase/thrift/DemoClient.java b/hbase-examples/src/main/java/org/apache/hadoop/hbase/thrift/DemoClient.java
index b16ef7b..cb0cfbb 100644
--- a/hbase-examples/src/main/java/org/apache/hadoop/hbase/thrift/DemoClient.java
+++ b/hbase-examples/src/main/java/org/apache/hadoop/hbase/thrift/DemoClient.java
@@ -114,7 +114,7 @@ public class DemoClient {
     private void run() throws Exception {
         TTransport transport = new TSocket(host, port);
         if (secure) {
-          Map<String, String> saslProperties = new HashMap<String, String>();
+          Map<String, String> saslProperties = new HashMap<>();
           saslProperties.put(Sasl.QOP, "auth-conf,auth-int,auth");
           /**
            * The Thrift server the DemoClient is trying to connect to
@@ -154,7 +154,7 @@ public class DemoClient {
         //
         // Create the demo table with two column families, entry: and unused:
         //
-        ArrayList<ColumnDescriptor> columns = new ArrayList<ColumnDescriptor>(2);
+        ArrayList<ColumnDescriptor> columns = new ArrayList<>(2);
         ColumnDescriptor col;
         col = new ColumnDescriptor();
         col.name = ByteBuffer.wrap(bytes("entry:"));
@@ -194,7 +194,7 @@ public class DemoClient {
 
         ArrayList<Mutation> mutations;
         // non-utf8 is fine for data
-        mutations = new ArrayList<Mutation>(1);
+        mutations = new ArrayList<>(1);
         mutations.add(new Mutation(false, ByteBuffer.wrap(bytes("entry:foo")),
             ByteBuffer.wrap(invalid), writeToWal));
         client.mutateRow(ByteBuffer.wrap(t), ByteBuffer.wrap(bytes("foo")),
@@ -202,19 +202,19 @@ public class DemoClient {
 
 
         // this row name is valid utf8
-        mutations = new ArrayList<Mutation>(1);
+        mutations = new ArrayList<>(1);
         mutations.add(new Mutation(false, ByteBuffer.wrap(bytes("entry:foo")), ByteBuffer.wrap(valid), writeToWal));
         client.mutateRow(ByteBuffer.wrap(t), ByteBuffer.wrap(valid), mutations, dummyAttributes);
 
         // non-utf8 is now allowed in row names because HBase stores values as binary
 
-        mutations = new ArrayList<Mutation>(1);
+        mutations = new ArrayList<>(1);
         mutations.add(new Mutation(false, ByteBuffer.wrap(bytes("entry:foo")), ByteBuffer.wrap(invalid), writeToWal));
         client.mutateRow(ByteBuffer.wrap(t), ByteBuffer.wrap(invalid), mutations, dummyAttributes);
 
 
         // Run a scanner on the rows we just created
-        ArrayList<ByteBuffer> columnNames = new ArrayList<ByteBuffer>();
+        ArrayList<ByteBuffer> columnNames = new ArrayList<>();
         columnNames.add(ByteBuffer.wrap(bytes("entry:")));
 
         System.out.println("Starting scanner...");
@@ -238,7 +238,7 @@ public class DemoClient {
             nf.setGroupingUsed(false);
             byte[] row = bytes(nf.format(i));
 
-            mutations = new ArrayList<Mutation>(1);
+            mutations = new ArrayList<>(1);
             mutations.add(new Mutation(false, ByteBuffer.wrap(bytes("unused:")), ByteBuffer.wrap(bytes("DELETE_ME")), writeToWal));
             client.mutateRow(ByteBuffer.wrap(t), ByteBuffer.wrap(row), mutations, dummyAttributes);
             printRow(client.getRow(ByteBuffer.wrap(t), ByteBuffer.wrap(row), dummyAttributes));
@@ -251,14 +251,14 @@ public class DemoClient {
                 // no-op
             }
 
-            mutations = new ArrayList<Mutation>(2);
+            mutations = new ArrayList<>(2);
             mutations.add(new Mutation(false, ByteBuffer.wrap(bytes("entry:num")), ByteBuffer.wrap(bytes("0")), writeToWal));
             mutations.add(new Mutation(false, ByteBuffer.wrap(bytes("entry:foo")), ByteBuffer.wrap(bytes("FOO")), writeToWal));
             client.mutateRow(ByteBuffer.wrap(t), ByteBuffer.wrap(row), mutations, dummyAttributes);
             printRow(client.getRow(ByteBuffer.wrap(t), ByteBuffer.wrap(row), dummyAttributes));
 
             Mutation m;
-            mutations = new ArrayList<Mutation>(2);
+            mutations = new ArrayList<>(2);
             m = new Mutation();
             m.column = ByteBuffer.wrap(bytes("entry:foo"));
             m.isDelete = true;
@@ -270,7 +270,7 @@ public class DemoClient {
             client.mutateRow(ByteBuffer.wrap(t), ByteBuffer.wrap(row), mutations, dummyAttributes);
             printRow(client.getRow(ByteBuffer.wrap(t), ByteBuffer.wrap(row), dummyAttributes));
 
-            mutations = new ArrayList<Mutation>();
+            mutations = new ArrayList<>();
             mutations.add(new Mutation(false, ByteBuffer.wrap(bytes("entry:num")), ByteBuffer.wrap(bytes(Integer.toString(i))), writeToWal));
             mutations.add(new Mutation(false, ByteBuffer.wrap(bytes("entry:sqr")), ByteBuffer.wrap(bytes(Integer.toString(i * i))), writeToWal));
             client.mutateRow(ByteBuffer.wrap(t), ByteBuffer.wrap(row), mutations, dummyAttributes);
@@ -347,7 +347,7 @@ public class DemoClient {
     private void printRow(TRowResult rowResult) {
         // copy values into a TreeMap to get them in sorted order
 
-        TreeMap<String, TCell> sorted = new TreeMap<String, TCell>();
+        TreeMap<String, TCell> sorted = new TreeMap<>();
         for (Map.Entry<ByteBuffer, TCell> column : rowResult.columns.entrySet()) {
             sorted.put(utf8(column.getKey().array()), column.getValue());
         }
@@ -379,7 +379,7 @@ public class DemoClient {
         new Configuration() {
           @Override
           public AppConfigurationEntry[] getAppConfigurationEntry(String name) {
-            Map<String, String> options = new HashMap<String, String>();
+            Map<String, String> options = new HashMap<>();
             options.put("useKeyTab", "false");
             options.put("storeKey", "false");
             options.put("doNotPrompt", "true");
diff --git a/hbase-examples/src/main/java/org/apache/hadoop/hbase/thrift/HttpDoAsClient.java b/hbase-examples/src/main/java/org/apache/hadoop/hbase/thrift/HttpDoAsClient.java
index 666891c..25fdc4a 100644
--- a/hbase-examples/src/main/java/org/apache/hadoop/hbase/thrift/HttpDoAsClient.java
+++ b/hbase-examples/src/main/java/org/apache/hadoop/hbase/thrift/HttpDoAsClient.java
@@ -151,7 +151,7 @@ public class HttpDoAsClient {
     //
     // Create the demo table with two column families, entry: and unused:
     //
-    ArrayList<ColumnDescriptor> columns = new ArrayList<ColumnDescriptor>(2);
+    ArrayList<ColumnDescriptor> columns = new ArrayList<>(2);
     ColumnDescriptor col;
     col = new ColumnDescriptor();
     col.name = ByteBuffer.wrap(bytes("entry:"));
@@ -236,7 +236,7 @@ public class HttpDoAsClient {
   private void printRow(TRowResult rowResult) {
     // copy values into a TreeMap to get them in sorted order
 
-    TreeMap<String, TCell> sorted = new TreeMap<String, TCell>();
+    TreeMap<String, TCell> sorted = new TreeMap<>();
     for (Map.Entry<ByteBuffer, TCell> column : rowResult.columns.entrySet()) {
       sorted.put(utf8(column.getKey().array()), column.getValue());
     }
@@ -261,7 +261,7 @@ public class HttpDoAsClient {
         new Configuration() {
           @Override
           public AppConfigurationEntry[] getAppConfigurationEntry(String name) {
-            Map<String, String> options = new HashMap<String, String>();
+            Map<String, String> options = new HashMap<>();
             options.put("useKeyTab", "false");
             options.put("storeKey", "false");
             options.put("doNotPrompt", "true");
diff --git a/hbase-examples/src/main/java/org/apache/hadoop/hbase/thrift2/DemoClient.java b/hbase-examples/src/main/java/org/apache/hadoop/hbase/thrift2/DemoClient.java
index 4083792..666997e 100644
--- a/hbase-examples/src/main/java/org/apache/hadoop/hbase/thrift2/DemoClient.java
+++ b/hbase-examples/src/main/java/org/apache/hadoop/hbase/thrift2/DemoClient.java
@@ -102,7 +102,7 @@ public class DemoClient {
        *
        * The HBase cluster must be secure, allow proxy user.
        */
-      Map<String, String> saslProperties = new HashMap<String, String>();
+      Map<String, String> saslProperties = new HashMap<>();
       saslProperties.put(Sasl.QOP, "auth-conf,auth-int,auth");
       transport = new TSaslClientTransport("GSSAPI", null,
         user != null ? user : "hbase",// Thrift server user name, should be an authorized proxy user
@@ -126,7 +126,7 @@ public class DemoClient {
     columnValue.setFamily("family1".getBytes());
     columnValue.setQualifier("qualifier1".getBytes());
     columnValue.setValue("value1".getBytes());
-    List<TColumnValue> columnValues = new ArrayList<TColumnValue>(1);
+    List<TColumnValue> columnValues = new ArrayList<>(1);
     columnValues.add(columnValue);
     put.setColumnValues(columnValues);
 
@@ -159,7 +159,7 @@ public class DemoClient {
       new Configuration() {
         @Override
         public AppConfigurationEntry[] getAppConfigurationEntry(String name) {
-          Map<String, String> options = new HashMap<String, String>();
+          Map<String, String> options = new HashMap<>();
           options.put("useKeyTab", "false");
           options.put("storeKey", "false");
           options.put("doNotPrompt", "true");
diff --git a/hbase-external-blockcache/src/main/java/org/apache/hadoop/hbase/io/hfile/MemcachedBlockCache.java b/hbase-external-blockcache/src/main/java/org/apache/hadoop/hbase/io/hfile/MemcachedBlockCache.java
index 69d8521..e741760 100644
--- a/hbase-external-blockcache/src/main/java/org/apache/hadoop/hbase/io/hfile/MemcachedBlockCache.java
+++ b/hbase-external-blockcache/src/main/java/org/apache/hadoop/hbase/io/hfile/MemcachedBlockCache.java
@@ -100,7 +100,7 @@ public class MemcachedBlockCache implements BlockCache {
     // case.
     String serverListString = c.get(MEMCACHED_CONFIG_KEY,"localhost:11211");
     String[] servers = serverListString.split(",");
-    List<InetSocketAddress> serverAddresses = new ArrayList<InetSocketAddress>(servers.length);
+    List<InetSocketAddress> serverAddresses = new ArrayList<>(servers.length);
     for (String s:servers) {
       serverAddresses.add(Addressing.createInetSocketAddressFromHostAndPortStr(s));
     }
diff --git a/hbase-hadoop-compat/src/main/java/org/apache/hadoop/hbase/CompatibilitySingletonFactory.java b/hbase-hadoop-compat/src/main/java/org/apache/hadoop/hbase/CompatibilitySingletonFactory.java
index 78442ba..be6d6d1 100644
--- a/hbase-hadoop-compat/src/main/java/org/apache/hadoop/hbase/CompatibilitySingletonFactory.java
+++ b/hbase-hadoop-compat/src/main/java/org/apache/hadoop/hbase/CompatibilitySingletonFactory.java
@@ -34,7 +34,7 @@ public class CompatibilitySingletonFactory extends CompatibilityFactory {
   public static enum SingletonStorage {
     INSTANCE;
     private final Object lock = new Object();
-    private final Map<Class, Object> instances = new HashMap<Class, Object>();
+    private final Map<Class, Object> instances = new HashMap<>();
   }
   private static final Log LOG = LogFactory.getLog(CompatibilitySingletonFactory.class);
 
diff --git a/hbase-hadoop-compat/src/test/java/org/apache/hadoop/hbase/TestCompatibilitySingletonFactory.java b/hbase-hadoop-compat/src/test/java/org/apache/hadoop/hbase/TestCompatibilitySingletonFactory.java
index f942059..168f6c7 100644
--- a/hbase-hadoop-compat/src/test/java/org/apache/hadoop/hbase/TestCompatibilitySingletonFactory.java
+++ b/hbase-hadoop-compat/src/test/java/org/apache/hadoop/hbase/TestCompatibilitySingletonFactory.java
@@ -55,9 +55,8 @@ public class TestCompatibilitySingletonFactory {
 
   @Test
   public void testGetInstance() throws Exception {
-    List<TestCompatibilitySingletonFactoryCallable> callables =
-        new ArrayList<TestCompatibilitySingletonFactoryCallable>(ITERATIONS);
-    List<String> resultStrings = new ArrayList<String>(ITERATIONS);
+    List<TestCompatibilitySingletonFactoryCallable> callables = new ArrayList<>(ITERATIONS);
+    List<String> resultStrings = new ArrayList<>(ITERATIONS);
 
 
     // Create the callables.
diff --git a/hbase-hadoop2-compat/src/main/java/org/apache/hadoop/hbase/ipc/MetricsHBaseServerSourceFactoryImpl.java b/hbase-hadoop2-compat/src/main/java/org/apache/hadoop/hbase/ipc/MetricsHBaseServerSourceFactoryImpl.java
index 76bbb09..78893ab 100644
--- a/hbase-hadoop2-compat/src/main/java/org/apache/hadoop/hbase/ipc/MetricsHBaseServerSourceFactoryImpl.java
+++ b/hbase-hadoop2-compat/src/main/java/org/apache/hadoop/hbase/ipc/MetricsHBaseServerSourceFactoryImpl.java
@@ -28,10 +28,7 @@ import org.apache.hadoop.hbase.classification.InterfaceAudience;
 public class MetricsHBaseServerSourceFactoryImpl extends MetricsHBaseServerSourceFactory {
   private enum SourceStorage {
     INSTANCE;
-    HashMap<String, MetricsHBaseServerSource>
-        sources =
-        new HashMap<String, MetricsHBaseServerSource>();
-
+    HashMap<String, MetricsHBaseServerSource> sources = new HashMap<>();
   }
 
   @Override
diff --git a/hbase-hadoop2-compat/src/main/java/org/apache/hadoop/hbase/master/balancer/MetricsStochasticBalancerSourceImpl.java b/hbase-hadoop2-compat/src/main/java/org/apache/hadoop/hbase/master/balancer/MetricsStochasticBalancerSourceImpl.java
index f658a27..c304fb9 100644
--- a/hbase-hadoop2-compat/src/main/java/org/apache/hadoop/hbase/master/balancer/MetricsStochasticBalancerSourceImpl.java
+++ b/hbase-hadoop2-compat/src/main/java/org/apache/hadoop/hbase/master/balancer/MetricsStochasticBalancerSourceImpl.java
@@ -46,7 +46,7 @@ public class MetricsStochasticBalancerSourceImpl extends MetricsBalancerSourceIm
           return size() > mruCap;
         }
       };
-  private Map<String, String> costFunctionDescs = new ConcurrentHashMap<String, String>();
+  private Map<String, String> costFunctionDescs = new ConcurrentHashMap<>();
 
   /**
    * Calculates the mru cache capacity from the metrics size
@@ -79,7 +79,7 @@ public class MetricsStochasticBalancerSourceImpl extends MetricsBalancerSourceIm
     synchronized (stochasticCosts) {
       Map<String, Double> costs = stochasticCosts.get(tableName);
       if (costs == null) {
-        costs = new ConcurrentHashMap<String, Double>();
+        costs = new ConcurrentHashMap<>();
       }
 
       costs.put(costFunctionName, cost);
diff --git a/hbase-hadoop2-compat/src/main/java/org/apache/hadoop/hbase/metrics/Interns.java b/hbase-hadoop2-compat/src/main/java/org/apache/hadoop/hbase/metrics/Interns.java
index 7905561..565b853 100644
--- a/hbase-hadoop2-compat/src/main/java/org/apache/hadoop/hbase/metrics/Interns.java
+++ b/hbase-hadoop2-compat/src/main/java/org/apache/hadoop/hbase/metrics/Interns.java
@@ -45,14 +45,14 @@ public final class Interns {
       CacheBuilder.newBuilder().expireAfterAccess(1, TimeUnit.DAYS)
           .build(new CacheLoader<String, ConcurrentHashMap<String, MetricsInfo>>() {
             public ConcurrentHashMap<String, MetricsInfo> load(String key) {
-              return new ConcurrentHashMap<String, MetricsInfo>();
+              return new ConcurrentHashMap<>();
             }
           });
   private static LoadingCache<MetricsInfo, ConcurrentHashMap<String, MetricsTag>> tagCache =
       CacheBuilder.newBuilder().expireAfterAccess(1, TimeUnit.DAYS)
           .build(new CacheLoader<MetricsInfo, ConcurrentHashMap<String, MetricsTag>>() {
             public ConcurrentHashMap<String, MetricsTag> load(MetricsInfo key) {
-              return new ConcurrentHashMap<String, MetricsTag>();
+              return new ConcurrentHashMap<>();
             }
           });
 
diff --git a/hbase-hadoop2-compat/src/main/java/org/apache/hadoop/metrics2/util/MetricSampleQuantiles.java b/hbase-hadoop2-compat/src/main/java/org/apache/hadoop/metrics2/util/MetricSampleQuantiles.java
index a968aca..3e4016d 100644
--- a/hbase-hadoop2-compat/src/main/java/org/apache/hadoop/metrics2/util/MetricSampleQuantiles.java
+++ b/hbase-hadoop2-compat/src/main/java/org/apache/hadoop/metrics2/util/MetricSampleQuantiles.java
@@ -73,7 +73,7 @@ public class MetricSampleQuantiles {
 
   public MetricSampleQuantiles(MetricQuantile[] quantiles) {
     this.quantiles = Arrays.copyOf(quantiles, quantiles.length);
-    this.samples = new LinkedList<SampleItem>();
+    this.samples = new LinkedList<>();
   }
 
   /**
@@ -235,7 +235,7 @@ public class MetricSampleQuantiles {
   synchronized public Map<MetricQuantile, Long> snapshot() throws IOException {
     // flush the buffer first for best results
     insertBatch();
-    Map<MetricQuantile, Long> values = new HashMap<MetricQuantile, Long>(quantiles.length);
+    Map<MetricQuantile, Long> values = new HashMap<>(quantiles.length);
     for (int i = 0; i < quantiles.length; i++) {
       values.put(quantiles[i], query(quantiles[i].quantile));
     }
diff --git a/hbase-hadoop2-compat/src/test/java/org/apache/hadoop/hbase/test/MetricsAssertHelperImpl.java b/hbase-hadoop2-compat/src/test/java/org/apache/hadoop/hbase/test/MetricsAssertHelperImpl.java
index 4291eb7..19a8ad2 100644
--- a/hbase-hadoop2-compat/src/test/java/org/apache/hadoop/hbase/test/MetricsAssertHelperImpl.java
+++ b/hbase-hadoop2-compat/src/test/java/org/apache/hadoop/hbase/test/MetricsAssertHelperImpl.java
@@ -37,9 +37,9 @@ import static org.junit.Assert.*;
  *  A helper class that will allow tests to get into hadoop2's metrics2 values.
  */
 public class MetricsAssertHelperImpl implements MetricsAssertHelper {
-  private Map<String, String> tags = new HashMap<String, String>();
-  private Map<String, Number> gauges = new HashMap<String, Number>();
-  private Map<String, Long> counters = new HashMap<String, Long>();
+  private Map<String, String> tags = new HashMap<>();
+  private Map<String, Number> gauges = new HashMap<>();
+  private Map<String, Long> counters = new HashMap<>();
 
   public class MockMetricsBuilder implements MetricsCollector {
 
diff --git a/hbase-it/src/test/java/org/apache/hadoop/hbase/DistributedHBaseCluster.java b/hbase-it/src/test/java/org/apache/hadoop/hbase/DistributedHBaseCluster.java
index d35ef84..431ba42 100644
--- a/hbase-it/src/test/java/org/apache/hadoop/hbase/DistributedHBaseCluster.java
+++ b/hbase-it/src/test/java/org/apache/hadoop/hbase/DistributedHBaseCluster.java
@@ -317,7 +317,7 @@ public class DistributedHBaseCluster extends HBaseCluster {
   }
 
   protected boolean restoreMasters(ClusterStatus initial, ClusterStatus current) {
-    List<IOException> deferred = new ArrayList<IOException>();
+    List<IOException> deferred = new ArrayList<>();
     //check whether current master has changed
     final ServerName initMaster = initial.getMaster();
     if (!ServerName.isSameHostnameAndPort(initMaster, current.getMaster())) {
@@ -371,8 +371,8 @@ public class DistributedHBaseCluster extends HBaseCluster {
       }
     } else {
       //current master has not changed, match up backup masters
-      Set<ServerName> toStart = new TreeSet<ServerName>(new ServerNameIgnoreStartCodeComparator());
-      Set<ServerName> toKill = new TreeSet<ServerName>(new ServerNameIgnoreStartCodeComparator());
+      Set<ServerName> toStart = new TreeSet<>(new ServerNameIgnoreStartCodeComparator());
+      Set<ServerName> toKill = new TreeSet<>(new ServerNameIgnoreStartCodeComparator());
       toStart.addAll(initial.getBackupMasters());
       toKill.addAll(current.getBackupMasters());
 
@@ -429,8 +429,8 @@ public class DistributedHBaseCluster extends HBaseCluster {
   }
 
   protected boolean restoreRegionServers(ClusterStatus initial, ClusterStatus current) {
-    Set<ServerName> toStart = new TreeSet<ServerName>(new ServerNameIgnoreStartCodeComparator());
-    Set<ServerName> toKill = new TreeSet<ServerName>(new ServerNameIgnoreStartCodeComparator());
+    Set<ServerName> toStart = new TreeSet<>(new ServerNameIgnoreStartCodeComparator());
+    Set<ServerName> toKill = new TreeSet<>(new ServerNameIgnoreStartCodeComparator());
     toStart.addAll(initial.getServers());
     toKill.addAll(current.getServers());
 
@@ -443,7 +443,7 @@ public class DistributedHBaseCluster extends HBaseCluster {
       toKill.remove(server);
     }
 
-    List<IOException> deferred = new ArrayList<IOException>();
+    List<IOException> deferred = new ArrayList<>();
 
     for(ServerName sn:toStart) {
       try {
diff --git a/hbase-it/src/test/java/org/apache/hadoop/hbase/HBaseClusterManager.java b/hbase-it/src/test/java/org/apache/hadoop/hbase/HBaseClusterManager.java
index b6f1aeb..07014e5 100644
--- a/hbase-it/src/test/java/org/apache/hadoop/hbase/HBaseClusterManager.java
+++ b/hbase-it/src/test/java/org/apache/hadoop/hbase/HBaseClusterManager.java
@@ -312,7 +312,7 @@ public class HBaseClusterManager extends Configured implements ClusterManager {
     LOG.info("Executed remote command, exit code:" + shell.getExitCode()
         + " , output:" + shell.getOutput());
 
-    return new Pair<Integer, String>(shell.getExitCode(), shell.getOutput());
+    return new Pair<>(shell.getExitCode(), shell.getOutput());
   }
 
   private Pair<Integer, String> execWithRetries(String hostname, ServiceType service, String... cmd)
diff --git a/hbase-it/src/test/java/org/apache/hadoop/hbase/IntegrationTestDDLMasterFailover.java b/hbase-it/src/test/java/org/apache/hadoop/hbase/IntegrationTestDDLMasterFailover.java
index 5d79722..2d3693a 100644
--- a/hbase-it/src/test/java/org/apache/hadoop/hbase/IntegrationTestDDLMasterFailover.java
+++ b/hbase-it/src/test/java/org/apache/hadoop/hbase/IntegrationTestDDLMasterFailover.java
@@ -125,17 +125,13 @@ public class IntegrationTestDDLMasterFailover extends IntegrationTestBase {
 
   protected int numThreads, numRegions;
 
-  ConcurrentHashMap<String, NamespaceDescriptor> namespaceMap =
-      new ConcurrentHashMap<String, NamespaceDescriptor>();
+  ConcurrentHashMap<String, NamespaceDescriptor> namespaceMap = new ConcurrentHashMap<>();
 
-  ConcurrentHashMap<TableName, HTableDescriptor> enabledTables =
-      new ConcurrentHashMap<TableName, HTableDescriptor>();
+  ConcurrentHashMap<TableName, HTableDescriptor> enabledTables = new ConcurrentHashMap<>();
 
-  ConcurrentHashMap<TableName, HTableDescriptor> disabledTables =
-      new ConcurrentHashMap<TableName, HTableDescriptor>();
+  ConcurrentHashMap<TableName, HTableDescriptor> disabledTables = new ConcurrentHashMap<>();
 
-  ConcurrentHashMap<TableName, HTableDescriptor> deletedTables =
-      new ConcurrentHashMap<TableName, HTableDescriptor>();
+  ConcurrentHashMap<TableName, HTableDescriptor> deletedTables = new ConcurrentHashMap<>();
 
   @Override
   public void setUpCluster() throws Exception {
@@ -256,7 +252,7 @@ public class IntegrationTestDDLMasterFailover extends IntegrationTestBase {
         if (namespaceMap.isEmpty()) {
           return null;
         }
-        ArrayList<String> namespaceList = new ArrayList<String>(namespaceMap.keySet());
+        ArrayList<String> namespaceList = new ArrayList<>(namespaceMap.keySet());
         String randomKey = namespaceList.get(RandomUtils.nextInt(namespaceList.size()));
         NamespaceDescriptor randomNsd = namespaceMap.get(randomKey);
         // remove from namespaceMap
@@ -396,7 +392,7 @@ public class IntegrationTestDDLMasterFailover extends IntegrationTestBase {
         if (tableMap.isEmpty()) {
           return null;
         }
-        ArrayList<TableName> tableList = new ArrayList<TableName>(tableMap.keySet());
+        ArrayList<TableName> tableList = new ArrayList<>(tableMap.keySet());
         TableName randomKey = tableList.get(RandomUtils.nextInt(tableList.size()));
         HTableDescriptor randomHtd = tableMap.get(randomKey);
         // remove from tableMap
@@ -770,7 +766,7 @@ public class IntegrationTestDDLMasterFailover extends IntegrationTestBase {
       Admin admin = connection.getAdmin();
       TableName tableName = selected.getTableName();
       try (Table table = connection.getTable(tableName)){
-        ArrayList<HRegionInfo> regionInfos = new ArrayList<HRegionInfo>(admin.getTableRegions(
+        ArrayList<HRegionInfo> regionInfos = new ArrayList<>(admin.getTableRegions(
             selected.getTableName()));
         int numRegions = regionInfos.size();
         // average number of rows to be added per action to each region
diff --git a/hbase-it/src/test/java/org/apache/hadoop/hbase/IntegrationTestIngest.java b/hbase-it/src/test/java/org/apache/hadoop/hbase/IntegrationTestIngest.java
index d4bcacd..7b6635e 100644
--- a/hbase-it/src/test/java/org/apache/hadoop/hbase/IntegrationTestIngest.java
+++ b/hbase-it/src/test/java/org/apache/hadoop/hbase/IntegrationTestIngest.java
@@ -207,7 +207,7 @@ public class IntegrationTestIngest extends IntegrationTestBase {
   }
 
   protected String[] getArgsForLoadTestToolInitTable() {
-    List<String> args = new ArrayList<String>();
+    List<String> args = new ArrayList<>();
     args.add("-tn");
     args.add(getTablename().getNameAsString());
     // pass all remaining args from conf with keys <test class name>.<load test tool arg>
@@ -225,7 +225,7 @@ public class IntegrationTestIngest extends IntegrationTestBase {
 
   protected String[] getArgsForLoadTestTool(String mode, String modeSpecificArg, long startKey,
       long numKeys) {
-    List<String> args = new ArrayList<String>(11);
+    List<String> args = new ArrayList<>(11);
     args.add("-tn");
     args.add(getTablename().getNameAsString());
     args.add("-families");
diff --git a/hbase-it/src/test/java/org/apache/hadoop/hbase/IntegrationTestIngestWithACL.java b/hbase-it/src/test/java/org/apache/hadoop/hbase/IntegrationTestIngestWithACL.java
index 82eef1a..d129279 100644
--- a/hbase-it/src/test/java/org/apache/hadoop/hbase/IntegrationTestIngestWithACL.java
+++ b/hbase-it/src/test/java/org/apache/hadoop/hbase/IntegrationTestIngestWithACL.java
@@ -68,7 +68,7 @@ public class IntegrationTestIngestWithACL extends IntegrationTestIngest {
   protected String[] getArgsForLoadTestTool(String mode, String modeSpecificArg, long startKey,
       long numKeys) {
     String[] args = super.getArgsForLoadTestTool(mode, modeSpecificArg, startKey, numKeys);
-    List<String> tmp = new ArrayList<String>(Arrays.asList(args));
+    List<String> tmp = new ArrayList<>(Arrays.asList(args));
     tmp.add(HYPHEN + LoadTestTool.OPT_GENERATOR);
     StringBuilder sb = new StringBuilder(LoadTestDataGeneratorWithACL.class.getName());
     sb.append(COLON);
diff --git a/hbase-it/src/test/java/org/apache/hadoop/hbase/IntegrationTestIngestWithMOB.java b/hbase-it/src/test/java/org/apache/hadoop/hbase/IntegrationTestIngestWithMOB.java
index 13a5936..cd9e355 100644
--- a/hbase-it/src/test/java/org/apache/hadoop/hbase/IntegrationTestIngestWithMOB.java
+++ b/hbase-it/src/test/java/org/apache/hadoop/hbase/IntegrationTestIngestWithMOB.java
@@ -61,7 +61,7 @@ public class IntegrationTestIngestWithMOB extends IntegrationTestIngest {
 
   @Override
   protected String[] getArgsForLoadTestToolInitTable() {
-    List<String> args = new ArrayList<String>();
+    List<String> args = new ArrayList<>();
     args.add("-tn");
     args.add(getTablename().getNameAsString());
     // pass all remaining args from conf with keys <test class name>.<load test tool arg>
@@ -133,7 +133,7 @@ public class IntegrationTestIngestWithMOB extends IntegrationTestIngest {
   protected String[] getArgsForLoadTestTool(String mode, String modeSpecificArg, long startKey,
       long numKeys) {
     String[] args = super.getArgsForLoadTestTool(mode, modeSpecificArg, startKey, numKeys);
-    List<String> tmp = new ArrayList<String>(Arrays.asList(args));
+    List<String> tmp = new ArrayList<>(Arrays.asList(args));
     // LoadTestDataGeneratorMOB:mobColumnFamily:minMobDataSize:maxMobDataSize
     tmp.add(HIPHEN + LoadTestTool.OPT_GENERATOR);
     StringBuilder sb = new StringBuilder(LoadTestDataGeneratorWithMOB.class.getName());
diff --git a/hbase-it/src/test/java/org/apache/hadoop/hbase/IntegrationTestIngestWithTags.java b/hbase-it/src/test/java/org/apache/hadoop/hbase/IntegrationTestIngestWithTags.java
index f1b2c68..08bd4e5 100644
--- a/hbase-it/src/test/java/org/apache/hadoop/hbase/IntegrationTestIngestWithTags.java
+++ b/hbase-it/src/test/java/org/apache/hadoop/hbase/IntegrationTestIngestWithTags.java
@@ -45,7 +45,7 @@ public class IntegrationTestIngestWithTags extends IntegrationTestIngest {
   protected String[] getArgsForLoadTestTool(String mode, String modeSpecificArg, long startKey,
       long numKeys) {
     String[] args = super.getArgsForLoadTestTool(mode, modeSpecificArg, startKey, numKeys);
-    List<String> tmp = new ArrayList<String>(Arrays.asList(args));
+    List<String> tmp = new ArrayList<>(Arrays.asList(args));
     // LoadTestDataGeneratorWithTags:minNumTags:maxNumTags:minTagLength:maxTagLength
     tmp.add(HIPHEN + LoadTestTool.OPT_GENERATOR);
     StringBuilder sb = new StringBuilder(LoadTestDataGeneratorWithTags.class.getName());
diff --git a/hbase-it/src/test/java/org/apache/hadoop/hbase/IntegrationTestIngestWithVisibilityLabels.java b/hbase-it/src/test/java/org/apache/hadoop/hbase/IntegrationTestIngestWithVisibilityLabels.java
index 133be1a..b7d8dad 100644
--- a/hbase-it/src/test/java/org/apache/hadoop/hbase/IntegrationTestIngestWithVisibilityLabels.java
+++ b/hbase-it/src/test/java/org/apache/hadoop/hbase/IntegrationTestIngestWithVisibilityLabels.java
@@ -42,31 +42,31 @@ public class IntegrationTestIngestWithVisibilityLabels extends IntegrationTestIn
   private static final String[] VISIBILITY_EXPS = { "secret & confidential & !private",
       "topsecret | confidential", "confidential & private", "public", "topsecret & private",
       "!public | private", "(secret | topsecret) & private" };
-  private static final List<List<String>> AUTHS = new ArrayList<List<String>>();
+  private static final List<List<String>> AUTHS = new ArrayList<>();
 
   static {
-    ArrayList<String> tmp = new ArrayList<String>(2);
+    ArrayList<String> tmp = new ArrayList<>(2);
     tmp.add("secret");
     tmp.add("confidential");
     AUTHS.add(tmp);
-    tmp = new ArrayList<String>(1);
+    tmp = new ArrayList<>(1);
     tmp.add("topsecret");
     AUTHS.add(tmp);
-    tmp = new ArrayList<String>(2);
+    tmp = new ArrayList<>(2);
     tmp.add("confidential");
     tmp.add("private");
     AUTHS.add(tmp);
-    tmp = new ArrayList<String>(1);
+    tmp = new ArrayList<>(1);
     tmp.add("public");
     AUTHS.add(tmp);
-    tmp = new ArrayList<String>(2);
+    tmp = new ArrayList<>(2);
     tmp.add("topsecret");
     tmp.add("private");
     AUTHS.add(tmp);
-    tmp = new ArrayList<String>(1);
+    tmp = new ArrayList<>(1);
     tmp.add("confidential");
     AUTHS.add(tmp);
-    tmp = new ArrayList<String>(2);
+    tmp = new ArrayList<>(2);
     tmp.add("topsecret");
     tmp.add("private");
     AUTHS.add(tmp);
@@ -88,7 +88,7 @@ public class IntegrationTestIngestWithVisibilityLabels extends IntegrationTestIn
   protected String[] getArgsForLoadTestTool(String mode, String modeSpecificArg, long startKey,
       long numKeys) {
     String[] args = super.getArgsForLoadTestTool(mode, modeSpecificArg, startKey, numKeys);
-    List<String> tmp = new ArrayList<String>(Arrays.asList(args));
+    List<String> tmp = new ArrayList<>(Arrays.asList(args));
     tmp.add(HIPHEN + LoadTestTool.OPT_GENERATOR);
     StringBuilder sb = new StringBuilder(LoadTestDataGeneratorWithVisibilityLabels.class.getName());
     sb.append(COLON);
diff --git a/hbase-it/src/test/java/org/apache/hadoop/hbase/IntegrationTestLazyCfLoading.java b/hbase-it/src/test/java/org/apache/hadoop/hbase/IntegrationTestLazyCfLoading.java
index 548ff53..6efe9d8 100644
--- a/hbase-it/src/test/java/org/apache/hadoop/hbase/IntegrationTestLazyCfLoading.java
+++ b/hbase-it/src/test/java/org/apache/hadoop/hbase/IntegrationTestLazyCfLoading.java
@@ -92,8 +92,7 @@ public class IntegrationTestLazyCfLoading {
     public static final byte[] VALUE_COLUMN = Bytes.toBytes("val");
     public static final long ACCEPTED_VALUE = 1L;
 
-    private static final Map<byte[], byte[][]> columnMap = new TreeMap<byte[], byte[][]>(
-        Bytes.BYTES_COMPARATOR);
+    private static final Map<byte[], byte[][]> columnMap = new TreeMap<>(Bytes.BYTES_COMPARATOR);
 
     private final AtomicLong expectedNumberOfKeys = new AtomicLong(0);
     private final AtomicLong totalNumberOfKeys = new AtomicLong(0);
diff --git a/hbase-it/src/test/java/org/apache/hadoop/hbase/IntegrationTestRegionReplicaPerf.java b/hbase-it/src/test/java/org/apache/hadoop/hbase/IntegrationTestRegionReplicaPerf.java
index e609f0b..c3c5df3 100644
--- a/hbase-it/src/test/java/org/apache/hadoop/hbase/IntegrationTestRegionReplicaPerf.java
+++ b/hbase-it/src/test/java/org/apache/hadoop/hbase/IntegrationTestRegionReplicaPerf.java
@@ -266,8 +266,8 @@ public class IntegrationTestRegionReplicaPerf extends IntegrationTestBase {
       format("--nomapred --table=%s --latency --sampleRate=0.1 randomRead 4", tableName);
     String replicaReadOpts = format("%s %s", replicas, readOpts);
 
-    ArrayList<TimingResult> resultsWithoutReplicas = new ArrayList<TimingResult>(maxIters);
-    ArrayList<TimingResult> resultsWithReplicas = new ArrayList<TimingResult>(maxIters);
+    ArrayList<TimingResult> resultsWithoutReplicas = new ArrayList<>(maxIters);
+    ArrayList<TimingResult> resultsWithReplicas = new ArrayList<>(maxIters);
 
     // create/populate the table, replicas disabled
     LOG.debug("Populating table.");
diff --git a/hbase-it/src/test/java/org/apache/hadoop/hbase/IntegrationTestRegionReplicaReplication.java b/hbase-it/src/test/java/org/apache/hadoop/hbase/IntegrationTestRegionReplicaReplication.java
index 98d53e9..b6cfdcd 100644
--- a/hbase-it/src/test/java/org/apache/hadoop/hbase/IntegrationTestRegionReplicaReplication.java
+++ b/hbase-it/src/test/java/org/apache/hadoop/hbase/IntegrationTestRegionReplicaReplication.java
@@ -126,7 +126,7 @@ public class IntegrationTestRegionReplicaReplication extends IntegrationTestInge
     protected BlockingQueue<Long> createWriteKeysQueue(Configuration conf) {
       this.delayMs = conf.getLong(String.format("%s.%s",
         IntegrationTestRegionReplicaReplication.class.getSimpleName(), OPT_READ_DELAY_MS), 5000);
-      return new ConstantDelayQueue<Long>(TimeUnit.MILLISECONDS, delayMs);
+      return new ConstantDelayQueue<>(TimeUnit.MILLISECONDS, delayMs);
     }
   }
 
@@ -145,7 +145,7 @@ public class IntegrationTestRegionReplicaReplication extends IntegrationTestInge
     protected BlockingQueue<Long> createWriteKeysQueue(Configuration conf) {
       this.delayMs = conf.getLong(String.format("%s.%s",
         IntegrationTestRegionReplicaReplication.class.getSimpleName(), OPT_READ_DELAY_MS), 5000);
-      return new ConstantDelayQueue<Long>(TimeUnit.MILLISECONDS, delayMs);
+      return new ConstantDelayQueue<>(TimeUnit.MILLISECONDS, delayMs);
     }
   }
 
diff --git a/hbase-it/src/test/java/org/apache/hadoop/hbase/RESTApiClusterManager.java b/hbase-it/src/test/java/org/apache/hadoop/hbase/RESTApiClusterManager.java
index 04a3b05..03ba460 100644
--- a/hbase-it/src/test/java/org/apache/hadoop/hbase/RESTApiClusterManager.java
+++ b/hbase-it/src/test/java/org/apache/hadoop/hbase/RESTApiClusterManager.java
@@ -336,7 +336,7 @@ public class RESTApiClusterManager extends Configured implements ClusterManager
   // ClusterManager methods take a "ServiceType" object (e.g. "HBASE_MASTER," "HADOOP_NAMENODE").
   // These "service types," which cluster managers call "roles" or "components," need to be mapped
   // to their corresponding service (e.g. "HBase," "HDFS") in order to be controlled.
-  private static Map<ServiceType, Service> roleServiceType = new HashMap<ServiceType, Service>();
+  private static Map<ServiceType, Service> roleServiceType = new HashMap<>();
   static {
     roleServiceType.put(ServiceType.HADOOP_NAMENODE, Service.HDFS);
     roleServiceType.put(ServiceType.HADOOP_DATANODE, Service.HDFS);
diff --git a/hbase-it/src/test/java/org/apache/hadoop/hbase/chaos/actions/Action.java b/hbase-it/src/test/java/org/apache/hadoop/hbase/chaos/actions/Action.java
index d1a32b1..4c7be8c 100644
--- a/hbase-it/src/test/java/org/apache/hadoop/hbase/chaos/actions/Action.java
+++ b/hbase-it/src/test/java/org/apache/hadoop/hbase/chaos/actions/Action.java
@@ -123,7 +123,7 @@ public class Action {
     if (count == 1) {
       return new ServerName [] {};
     }
-    ArrayList<ServerName> tmp = new ArrayList<ServerName>(count);
+    ArrayList<ServerName> tmp = new ArrayList<>(count);
     tmp.addAll(regionServers);
     tmp.remove(master);
     return tmp.toArray(new ServerName[count-1]);
@@ -192,11 +192,11 @@ public class Action {
   protected void unbalanceRegions(ClusterStatus clusterStatus,
       List<ServerName> fromServers, List<ServerName> toServers,
       double fractionOfRegions) throws Exception {
-    List<byte[]> victimRegions = new LinkedList<byte[]>();
+    List<byte[]> victimRegions = new LinkedList<>();
     for (ServerName server : fromServers) {
       ServerLoad serverLoad = clusterStatus.getLoad(server);
       // Ugh.
-      List<byte[]> regions = new LinkedList<byte[]>(serverLoad.getRegionsLoad().keySet());
+      List<byte[]> regions = new LinkedList<>(serverLoad.getRegionsLoad().keySet());
       int victimRegionCount = (int)Math.ceil(fractionOfRegions * regions.size());
       LOG.debug("Removing " + victimRegionCount + " regions from " + server.getServerName());
       for (int i = 0; i < victimRegionCount; ++i) {
diff --git a/hbase-it/src/test/java/org/apache/hadoop/hbase/chaos/actions/BatchRestartRsAction.java b/hbase-it/src/test/java/org/apache/hadoop/hbase/chaos/actions/BatchRestartRsAction.java
index ce66000..75414ae 100644
--- a/hbase-it/src/test/java/org/apache/hadoop/hbase/chaos/actions/BatchRestartRsAction.java
+++ b/hbase-it/src/test/java/org/apache/hadoop/hbase/chaos/actions/BatchRestartRsAction.java
@@ -43,7 +43,7 @@ public class BatchRestartRsAction extends RestartActionBaseAction {
     List<ServerName> selectedServers = PolicyBasedChaosMonkey.selectRandomItems(getCurrentServers(),
         ratio);
 
-    Set<ServerName> killedServers = new HashSet<ServerName>();
+    Set<ServerName> killedServers = new HashSet<>();
 
     for (ServerName server : selectedServers) {
       // Don't keep killing servers if we're
diff --git a/hbase-it/src/test/java/org/apache/hadoop/hbase/chaos/actions/RestartRandomDataNodeAction.java b/hbase-it/src/test/java/org/apache/hadoop/hbase/chaos/actions/RestartRandomDataNodeAction.java
index 7299e79..f5349dc 100644
--- a/hbase-it/src/test/java/org/apache/hadoop/hbase/chaos/actions/RestartRandomDataNodeAction.java
+++ b/hbase-it/src/test/java/org/apache/hadoop/hbase/chaos/actions/RestartRandomDataNodeAction.java
@@ -49,7 +49,7 @@ public class RestartRandomDataNodeAction extends RestartActionBaseAction {
     DistributedFileSystem fs = (DistributedFileSystem) FSUtils.getRootDir(getConf())
         .getFileSystem(getConf());
     DFSClient dfsClient = fs.getClient();
-    List<ServerName> hosts = new LinkedList<ServerName>();
+    List<ServerName> hosts = new LinkedList<>();
     for (DatanodeInfo dataNode: dfsClient.datanodeReport(HdfsConstants.DatanodeReportType.LIVE)) {
       hosts.add(ServerName.valueOf(dataNode.getHostName(), -1, -1));
     }
diff --git a/hbase-it/src/test/java/org/apache/hadoop/hbase/chaos/actions/RollingBatchRestartRsAction.java b/hbase-it/src/test/java/org/apache/hadoop/hbase/chaos/actions/RollingBatchRestartRsAction.java
index e79ff5b..ba25198 100644
--- a/hbase-it/src/test/java/org/apache/hadoop/hbase/chaos/actions/RollingBatchRestartRsAction.java
+++ b/hbase-it/src/test/java/org/apache/hadoop/hbase/chaos/actions/RollingBatchRestartRsAction.java
@@ -59,8 +59,8 @@ public class RollingBatchRestartRsAction extends BatchRestartRsAction {
         (int)(ratio * 100)));
     List<ServerName> selectedServers = selectServers();
 
-    Queue<ServerName> serversToBeKilled = new LinkedList<ServerName>(selectedServers);
-    Queue<ServerName> deadServers = new LinkedList<ServerName>();
+    Queue<ServerName> serversToBeKilled = new LinkedList<>(selectedServers);
+    Queue<ServerName> deadServers = new LinkedList<>();
 
     // loop while there are servers to be killed or dead servers to be restarted
     while ((!serversToBeKilled.isEmpty() || !deadServers.isEmpty())  && !context.isStopping()) {
@@ -123,7 +123,7 @@ public class RollingBatchRestartRsAction extends BatchRestartRsAction {
       @Override
       protected ServerName[] getCurrentServers() throws IOException {
         final int count = 4;
-        List<ServerName> serverNames = new ArrayList<ServerName>(count);
+        List<ServerName> serverNames = new ArrayList<>(count);
         for (int i = 0; i < 4; i++) {
           serverNames.add(ServerName.valueOf(i + ".example.org", i, i));
         }
diff --git a/hbase-it/src/test/java/org/apache/hadoop/hbase/chaos/actions/UnbalanceKillAndRebalanceAction.java b/hbase-it/src/test/java/org/apache/hadoop/hbase/chaos/actions/UnbalanceKillAndRebalanceAction.java
index 1ac1458..a40c8b1 100644
--- a/hbase-it/src/test/java/org/apache/hadoop/hbase/chaos/actions/UnbalanceKillAndRebalanceAction.java
+++ b/hbase-it/src/test/java/org/apache/hadoop/hbase/chaos/actions/UnbalanceKillAndRebalanceAction.java
@@ -52,13 +52,13 @@ public class UnbalanceKillAndRebalanceAction extends Action {
   @Override
   public void perform() throws Exception {
     ClusterStatus status = this.cluster.getClusterStatus();
-    List<ServerName> victimServers = new LinkedList<ServerName>(status.getServers());
-    Set<ServerName> killedServers = new HashSet<ServerName>();
+    List<ServerName> victimServers = new LinkedList<>(status.getServers());
+    Set<ServerName> killedServers = new HashSet<>();
 
     int liveCount = (int)Math.ceil(FRC_SERVERS_THAT_HOARD_AND_LIVE * victimServers.size());
     int deadCount = (int)Math.ceil(FRC_SERVERS_THAT_HOARD_AND_DIE * victimServers.size());
     Assert.assertTrue((liveCount + deadCount) < victimServers.size());
-    List<ServerName> targetServers = new ArrayList<ServerName>(liveCount);
+    List<ServerName> targetServers = new ArrayList<>(liveCount);
     for (int i = 0; i < liveCount + deadCount; ++i) {
       int victimIx = RandomUtils.nextInt(victimServers.size());
       targetServers.add(victimServers.remove(victimIx));
diff --git a/hbase-it/src/test/java/org/apache/hadoop/hbase/chaos/actions/UnbalanceRegionsAction.java b/hbase-it/src/test/java/org/apache/hadoop/hbase/chaos/actions/UnbalanceRegionsAction.java
index 2779bd1..bdffdb1 100644
--- a/hbase-it/src/test/java/org/apache/hadoop/hbase/chaos/actions/UnbalanceRegionsAction.java
+++ b/hbase-it/src/test/java/org/apache/hadoop/hbase/chaos/actions/UnbalanceRegionsAction.java
@@ -48,9 +48,9 @@ public class UnbalanceRegionsAction extends Action {
   public void perform() throws Exception {
     LOG.info("Unbalancing regions");
     ClusterStatus status = this.cluster.getClusterStatus();
-    List<ServerName> victimServers = new LinkedList<ServerName>(status.getServers());
+    List<ServerName> victimServers = new LinkedList<>(status.getServers());
     int targetServerCount = (int)Math.ceil(fractionOfServers * victimServers.size());
-    List<ServerName> targetServers = new ArrayList<ServerName>(targetServerCount);
+    List<ServerName> targetServers = new ArrayList<>(targetServerCount);
     for (int i = 0; i < targetServerCount; ++i) {
       int victimIx = RandomUtils.nextInt(victimServers.size());
       targetServers.add(victimServers.remove(victimIx));
diff --git a/hbase-it/src/test/java/org/apache/hadoop/hbase/chaos/monkies/PolicyBasedChaosMonkey.java b/hbase-it/src/test/java/org/apache/hadoop/hbase/chaos/monkies/PolicyBasedChaosMonkey.java
index 57f7c83..951f8f8 100644
--- a/hbase-it/src/test/java/org/apache/hadoop/hbase/chaos/monkies/PolicyBasedChaosMonkey.java
+++ b/hbase-it/src/test/java/org/apache/hadoop/hbase/chaos/monkies/PolicyBasedChaosMonkey.java
@@ -92,7 +92,7 @@ public class PolicyBasedChaosMonkey extends ChaosMonkey {
   public static <T> List<T> selectRandomItems(T[] items, float ratio) {
     int remaining = (int)Math.ceil(items.length * ratio);
 
-    List<T> selectedItems = new ArrayList<T>(remaining);
+    List<T> selectedItems = new ArrayList<>(remaining);
 
     for (int i=0; i<items.length && remaining > 0; i++) {
       if (RandomUtils.nextFloat() < ((float)remaining/(items.length-i))) {
diff --git a/hbase-it/src/test/java/org/apache/hadoop/hbase/chaos/policies/DoActionsOncePolicy.java b/hbase-it/src/test/java/org/apache/hadoop/hbase/chaos/policies/DoActionsOncePolicy.java
index e03816a..35f06eb 100644
--- a/hbase-it/src/test/java/org/apache/hadoop/hbase/chaos/policies/DoActionsOncePolicy.java
+++ b/hbase-it/src/test/java/org/apache/hadoop/hbase/chaos/policies/DoActionsOncePolicy.java
@@ -31,7 +31,7 @@ public class DoActionsOncePolicy extends PeriodicPolicy {
 
   public DoActionsOncePolicy(long periodMs, List<Action> actions) {
     super(periodMs);
-    this.actions = new ArrayList<Action>(actions);
+    this.actions = new ArrayList<>(actions);
   }
 
   public DoActionsOncePolicy(long periodMs, Action... actions) {
diff --git a/hbase-it/src/test/java/org/apache/hadoop/hbase/chaos/policies/PeriodicRandomActionPolicy.java b/hbase-it/src/test/java/org/apache/hadoop/hbase/chaos/policies/PeriodicRandomActionPolicy.java
index 8912467..8b76e49 100644
--- a/hbase-it/src/test/java/org/apache/hadoop/hbase/chaos/policies/PeriodicRandomActionPolicy.java
+++ b/hbase-it/src/test/java/org/apache/hadoop/hbase/chaos/policies/PeriodicRandomActionPolicy.java
@@ -46,9 +46,9 @@ public class PeriodicRandomActionPolicy extends PeriodicPolicy {
 
   public PeriodicRandomActionPolicy(long periodMs, Action... actions) {
     super(periodMs);
-    this.actions = new ArrayList<Pair<Action, Integer>>(actions.length);
+    this.actions = new ArrayList<>(actions.length);
     for (Action action : actions) {
-      this.actions.add(new Pair<Action, Integer>(action, 1));
+      this.actions.add(new Pair<>(action, 1));
     }
   }
 
diff --git a/hbase-it/src/test/java/org/apache/hadoop/hbase/mapreduce/IntegrationTestBulkLoad.java b/hbase-it/src/test/java/org/apache/hadoop/hbase/mapreduce/IntegrationTestBulkLoad.java
index fd062d1..e39d0fe 100644
--- a/hbase-it/src/test/java/org/apache/hadoop/hbase/mapreduce/IntegrationTestBulkLoad.java
+++ b/hbase-it/src/test/java/org/apache/hadoop/hbase/mapreduce/IntegrationTestBulkLoad.java
@@ -353,7 +353,7 @@ public class IntegrationTestBulkLoad extends IntegrationTestBase {
     @Override
     public List<InputSplit> getSplits(JobContext context) throws IOException, InterruptedException {
       int numSplits = context.getConfiguration().getInt(NUM_MAPS_KEY, NUM_MAPS);
-      ArrayList<InputSplit> ret = new ArrayList<InputSplit>(numSplits);
+      ArrayList<InputSplit> ret = new ArrayList<>(numSplits);
       for (int i = 0; i < numSplits; ++i) {
         ret.add(new EmptySplit());
       }
@@ -376,7 +376,7 @@ public class IntegrationTestBulkLoad extends IntegrationTestBase {
       chainId = chainId - (chainId % numMapTasks) + taskId; // ensure that chainId is unique per task and across iterations
       LongWritable[] keys = new LongWritable[] {new LongWritable(chainId)};
 
-      return new FixedRecordReader<LongWritable, LongWritable>(keys, keys);
+      return new FixedRecordReader<>(keys, keys);
     }
   }
 
diff --git a/hbase-it/src/test/java/org/apache/hadoop/hbase/mapreduce/IntegrationTestImportTsv.java b/hbase-it/src/test/java/org/apache/hadoop/hbase/mapreduce/IntegrationTestImportTsv.java
index 42b6ae7..9d04bf9 100644
--- a/hbase-it/src/test/java/org/apache/hadoop/hbase/mapreduce/IntegrationTestImportTsv.java
+++ b/hbase-it/src/test/java/org/apache/hadoop/hbase/mapreduce/IntegrationTestImportTsv.java
@@ -194,7 +194,7 @@ public class IntegrationTestImportTsv extends Configured implements Tool {
         util.getDataTestDirOnTestFS(table.getNameAsString()), "hfiles");
 
 
-    Map<String, String> args = new HashMap<String, String>();
+    Map<String, String> args = new HashMap<>();
     args.put(ImportTsv.BULK_OUTPUT_CONF_KEY, hfiles.toString());
     args.put(ImportTsv.COLUMNS_CONF_KEY,
         format("HBASE_ROW_KEY,HBASE_TS_KEY,%s:c1,%s:c2", cf, cf));
diff --git a/hbase-it/src/test/java/org/apache/hadoop/hbase/mttr/IntegrationTestMTTR.java b/hbase-it/src/test/java/org/apache/hadoop/hbase/mttr/IntegrationTestMTTR.java
index f87cc86..bd14c31 100644
--- a/hbase-it/src/test/java/org/apache/hadoop/hbase/mttr/IntegrationTestMTTR.java
+++ b/hbase-it/src/test/java/org/apache/hadoop/hbase/mttr/IntegrationTestMTTR.java
@@ -297,9 +297,9 @@ public class IntegrationTestMTTR {
     LOG.info("Starting " + testName + " with " + maxIters + " iterations.");
 
     // Array to keep track of times.
-    ArrayList<TimingResult> resultPuts = new ArrayList<TimingResult>(maxIters);
-    ArrayList<TimingResult> resultScan = new ArrayList<TimingResult>(maxIters);
-    ArrayList<TimingResult> resultAdmin = new ArrayList<TimingResult>(maxIters);
+    ArrayList<TimingResult> resultPuts = new ArrayList<>(maxIters);
+    ArrayList<TimingResult> resultScan = new ArrayList<>(maxIters);
+    ArrayList<TimingResult> resultAdmin = new ArrayList<>(maxIters);
     long start = System.nanoTime();
 
     try {
@@ -357,7 +357,7 @@ public class IntegrationTestMTTR {
    */
   private static class TimingResult {
     DescriptiveStatistics stats = new DescriptiveStatistics();
-    ArrayList<Long> traces = new ArrayList<Long>(10);
+    ArrayList<Long> traces = new ArrayList<>(10);
 
     /**
      * Add a result to this aggregate result.
diff --git a/hbase-it/src/test/java/org/apache/hadoop/hbase/test/IntegrationTestBigLinkedList.java b/hbase-it/src/test/java/org/apache/hadoop/hbase/test/IntegrationTestBigLinkedList.java
index dff1828..1b23de8 100644
--- a/hbase-it/src/test/java/org/apache/hadoop/hbase/test/IntegrationTestBigLinkedList.java
+++ b/hbase-it/src/test/java/org/apache/hadoop/hbase/test/IntegrationTestBigLinkedList.java
@@ -350,7 +350,7 @@ public class IntegrationTestBigLinkedList extends IntegrationTestBase {
       public List<InputSplit> getSplits(JobContext job) throws IOException, InterruptedException {
         int numMappers = job.getConfiguration().getInt(GENERATOR_NUM_MAPPERS_KEY, 1);
 
-        ArrayList<InputSplit> splits = new ArrayList<InputSplit>(numMappers);
+        ArrayList<InputSplit> splits = new ArrayList<>(numMappers);
 
         for (int i = 0; i < numMappers; i++) {
           splits.add(new GeneratorInputSplit());
@@ -956,7 +956,7 @@ public class IntegrationTestBigLinkedList extends IntegrationTestBase {
     throws IOException, InterruptedException {
       Path keysInputDir = new Path(conf.get(SEARCHER_INPUTDIR_KEY));
       FileSystem fs = FileSystem.get(conf);
-      SortedSet<byte []> result = new TreeSet<byte []>(Bytes.BYTES_COMPARATOR);
+      SortedSet<byte []> result = new TreeSet<>(Bytes.BYTES_COMPARATOR);
       if (!fs.exists(keysInputDir)) {
         throw new FileNotFoundException(keysInputDir.toString());
       }
@@ -977,7 +977,7 @@ public class IntegrationTestBigLinkedList extends IntegrationTestBase {
     private static SortedSet<byte[]> readFileToSearch(final Configuration conf,
         final FileSystem fs, final LocatedFileStatus keyFileStatus) throws IOException,
         InterruptedException {
-      SortedSet<byte []> result = new TreeSet<byte []>(Bytes.BYTES_COMPARATOR);
+      SortedSet<byte []> result = new TreeSet<>(Bytes.BYTES_COMPARATOR);
       // Return entries that are flagged Counts.UNDEFINED in the value. Return the row. This is
       // what is missing.
       TaskAttemptContext context = new TaskAttemptContextImpl(conf, new TaskAttemptID());
@@ -1064,7 +1064,7 @@ public class IntegrationTestBigLinkedList extends IntegrationTestBase {
      */
     public static class VerifyReducer extends
         Reducer<BytesWritable, BytesWritable, BytesWritable, BytesWritable> {
-      private ArrayList<byte[]> refs = new ArrayList<byte[]>();
+      private ArrayList<byte[]> refs = new ArrayList<>();
       private final BytesWritable UNREF = new BytesWritable(addPrefixFlag(
         Counts.UNREFERENCED.ordinal(), new byte[] {}));
       private final BytesWritable LOSTFAM = new BytesWritable(addPrefixFlag(
diff --git a/hbase-it/src/test/java/org/apache/hadoop/hbase/test/IntegrationTestLoadAndVerify.java b/hbase-it/src/test/java/org/apache/hadoop/hbase/test/IntegrationTestLoadAndVerify.java
index dec565f..9eacc5a 100644
--- a/hbase-it/src/test/java/org/apache/hadoop/hbase/test/IntegrationTestLoadAndVerify.java
+++ b/hbase-it/src/test/java/org/apache/hadoop/hbase/test/IntegrationTestLoadAndVerify.java
@@ -456,7 +456,7 @@ public void cleanUpCluster() throws Exception {
       throws IOException, InterruptedException {
     Path keysInputDir = new Path(conf.get(SEARCHER_INPUTDIR_KEY));
     FileSystem fs = FileSystem.get(conf);
-    SortedSet<byte []> result = new TreeSet<byte []>(Bytes.BYTES_COMPARATOR);
+    SortedSet<byte []> result = new TreeSet<>(Bytes.BYTES_COMPARATOR);
     if (!fs.exists(keysInputDir)) {
       throw new FileNotFoundException(keysInputDir.toString());
     }
diff --git a/hbase-it/src/test/java/org/apache/hadoop/hbase/test/IntegrationTestReplication.java b/hbase-it/src/test/java/org/apache/hadoop/hbase/test/IntegrationTestReplication.java
index 141b24d..bf534f3 100644
--- a/hbase-it/src/test/java/org/apache/hadoop/hbase/test/IntegrationTestReplication.java
+++ b/hbase-it/src/test/java/org/apache/hadoop/hbase/test/IntegrationTestReplication.java
@@ -234,7 +234,7 @@ public class IntegrationTestReplication extends IntegrationTestBigLinkedList {
 
         // set the test table to be the table to replicate
         HashMap<TableName, ArrayList<String>> toReplicate = new HashMap<>();
-        toReplicate.put(tableName, new ArrayList<String>(0));
+        toReplicate.put(tableName, new ArrayList<>(0));
 
         replicationAdmin.addPeer("TestPeer", peerConfig, toReplicate);
 
diff --git a/hbase-it/src/test/java/org/apache/hadoop/hbase/trace/IntegrationTestSendTraceRequests.java b/hbase-it/src/test/java/org/apache/hadoop/hbase/trace/IntegrationTestSendTraceRequests.java
index b7463bd..327d879 100644
--- a/hbase-it/src/test/java/org/apache/hadoop/hbase/trace/IntegrationTestSendTraceRequests.java
+++ b/hbase-it/src/test/java/org/apache/hadoop/hbase/trace/IntegrationTestSendTraceRequests.java
@@ -233,7 +233,7 @@ public class IntegrationTestSendTraceRequests extends AbstractHBaseTool {
   }
 
   private LinkedBlockingQueue<Long> insertData() throws IOException, InterruptedException {
-    LinkedBlockingQueue<Long> rowKeys = new LinkedBlockingQueue<Long>(25000);
+    LinkedBlockingQueue<Long> rowKeys = new LinkedBlockingQueue<>(25000);
     BufferedMutator ht = util.getConnection().getBufferedMutator(this.tableName);
     byte[] value = new byte[300];
     for (int x = 0; x < 5000; x++) {
diff --git a/hbase-prefix-tree/src/main/java/org/apache/hadoop/hbase/codec/prefixtree/decode/ArraySearcherPool.java b/hbase-prefix-tree/src/main/java/org/apache/hadoop/hbase/codec/prefixtree/decode/ArraySearcherPool.java
index e6df88a..5aa5d88 100644
--- a/hbase-prefix-tree/src/main/java/org/apache/hadoop/hbase/codec/prefixtree/decode/ArraySearcherPool.java
+++ b/hbase-prefix-tree/src/main/java/org/apache/hadoop/hbase/codec/prefixtree/decode/ArraySearcherPool.java
@@ -42,8 +42,7 @@ public class ArraySearcherPool {
    */
   private static final Integer MAX_POOL_SIZE = 1000;
 
-  protected Queue<PrefixTreeArraySearcher> pool
-    = new LinkedBlockingQueue<PrefixTreeArraySearcher>(MAX_POOL_SIZE);
+  protected Queue<PrefixTreeArraySearcher> pool = new LinkedBlockingQueue<>(MAX_POOL_SIZE);
 
   public PrefixTreeArraySearcher checkOut(ByteBuff buffer, boolean includesMvccVersion) {
     PrefixTreeArraySearcher searcher = pool.poll();//will return null if pool is empty
diff --git a/hbase-prefix-tree/src/main/java/org/apache/hadoop/hbase/codec/prefixtree/decode/PrefixTreeCell.java b/hbase-prefix-tree/src/main/java/org/apache/hadoop/hbase/codec/prefixtree/decode/PrefixTreeCell.java
index 3ca4236..255c8a3 100644
--- a/hbase-prefix-tree/src/main/java/org/apache/hadoop/hbase/codec/prefixtree/decode/PrefixTreeCell.java
+++ b/hbase-prefix-tree/src/main/java/org/apache/hadoop/hbase/codec/prefixtree/decode/PrefixTreeCell.java
@@ -87,7 +87,7 @@ public class PrefixTreeCell extends ByteBufferCell implements SettableSequenceId
   protected int tagsOffset;
   protected int tagsLength;
   // Pair to set the value ByteBuffer and its offset
-  protected ObjectIntPair<ByteBuffer> pair = new ObjectIntPair<ByteBuffer>();
+  protected ObjectIntPair<ByteBuffer> pair = new ObjectIntPair<>();
 
   /********************** Cell methods ******************/
 
diff --git a/hbase-prefix-tree/src/main/java/org/apache/hadoop/hbase/codec/prefixtree/encode/EncoderPoolImpl.java b/hbase-prefix-tree/src/main/java/org/apache/hadoop/hbase/codec/prefixtree/encode/EncoderPoolImpl.java
index 8a5ffba..a8f0082 100644
--- a/hbase-prefix-tree/src/main/java/org/apache/hadoop/hbase/codec/prefixtree/encode/EncoderPoolImpl.java
+++ b/hbase-prefix-tree/src/main/java/org/apache/hadoop/hbase/codec/prefixtree/encode/EncoderPoolImpl.java
@@ -26,8 +26,7 @@ import org.apache.hadoop.hbase.classification.InterfaceAudience;
 @InterfaceAudience.Private
 public class EncoderPoolImpl implements EncoderPool {
 
-  private BlockingQueue<PrefixTreeEncoder> unusedEncoders = 
-      new LinkedBlockingQueue<PrefixTreeEncoder>();
+  private BlockingQueue<PrefixTreeEncoder> unusedEncoders = new LinkedBlockingQueue<>();
 
   @Override
   public PrefixTreeEncoder checkOut(OutputStream outputStream, boolean includeMvccVersion) {
diff --git a/hbase-prefix-tree/src/main/java/org/apache/hadoop/hbase/codec/prefixtree/encode/other/LongEncoder.java b/hbase-prefix-tree/src/main/java/org/apache/hadoop/hbase/codec/prefixtree/encode/other/LongEncoder.java
index 3291d72..3597fbe 100644
--- a/hbase-prefix-tree/src/main/java/org/apache/hadoop/hbase/codec/prefixtree/encode/other/LongEncoder.java
+++ b/hbase-prefix-tree/src/main/java/org/apache/hadoop/hbase/codec/prefixtree/encode/other/LongEncoder.java
@@ -60,7 +60,7 @@ public class LongEncoder {
   /****************** construct ****************************/
 
   public LongEncoder() {
-    this.uniqueValues = new HashSet<Long>();
+    this.uniqueValues = new HashSet<>();
   }
 
   public void reset() {
diff --git a/hbase-prefix-tree/src/main/java/org/apache/hadoop/hbase/codec/prefixtree/encode/tokenize/Tokenizer.java b/hbase-prefix-tree/src/main/java/org/apache/hadoop/hbase/codec/prefixtree/encode/tokenize/Tokenizer.java
index f44017b..e2824b0 100644
--- a/hbase-prefix-tree/src/main/java/org/apache/hadoop/hbase/codec/prefixtree/encode/tokenize/Tokenizer.java
+++ b/hbase-prefix-tree/src/main/java/org/apache/hadoop/hbase/codec/prefixtree/encode/tokenize/Tokenizer.java
@@ -161,7 +161,7 @@ public class Tokenizer{
   }
 
   public List<byte[]> getArrays() {
-    List<TokenizerNode> nodes = new ArrayList<TokenizerNode>();
+    List<TokenizerNode> nodes = new ArrayList<>();
     root.appendNodesToExternalList(nodes, true, true);
     List<byte[]> byteArrays = Lists.newArrayListWithCapacity(CollectionUtils.nullSafeSize(nodes));
     for (int i = 0; i < nodes.size(); ++i) {
diff --git a/hbase-prefix-tree/src/main/java/org/apache/hadoop/hbase/util/byterange/impl/ByteRangeHashSet.java b/hbase-prefix-tree/src/main/java/org/apache/hadoop/hbase/util/byterange/impl/ByteRangeHashSet.java
index 9ce6163..dbaa508 100644
--- a/hbase-prefix-tree/src/main/java/org/apache/hadoop/hbase/util/byterange/impl/ByteRangeHashSet.java
+++ b/hbase-prefix-tree/src/main/java/org/apache/hadoop/hbase/util/byterange/impl/ByteRangeHashSet.java
@@ -39,7 +39,7 @@ public class ByteRangeHashSet extends ByteRangeSet {
   /************************ constructors *****************************/
 
   public ByteRangeHashSet() {
-    this.uniqueIndexByUniqueRange = new HashMap<ByteRange, Integer>();
+    this.uniqueIndexByUniqueRange = new HashMap<>();
   }
 
   public ByteRangeHashSet(List<ByteRange> rawByteArrays) {
diff --git a/hbase-prefix-tree/src/main/java/org/apache/hadoop/hbase/util/byterange/impl/ByteRangeTreeSet.java b/hbase-prefix-tree/src/main/java/org/apache/hadoop/hbase/util/byterange/impl/ByteRangeTreeSet.java
index b5c0b1a..4ee7b28 100644
--- a/hbase-prefix-tree/src/main/java/org/apache/hadoop/hbase/util/byterange/impl/ByteRangeTreeSet.java
+++ b/hbase-prefix-tree/src/main/java/org/apache/hadoop/hbase/util/byterange/impl/ByteRangeTreeSet.java
@@ -36,7 +36,7 @@ public class ByteRangeTreeSet extends ByteRangeSet {
   /************************ constructors *****************************/
 
   public ByteRangeTreeSet() {
-    this.uniqueIndexByUniqueRange = new TreeMap<ByteRange, Integer>();
+    this.uniqueIndexByUniqueRange = new TreeMap<>();
   }
 
   public ByteRangeTreeSet(List<ByteRange> rawByteArrays) {
diff --git a/hbase-prefix-tree/src/test/java/org/apache/hadoop/hbase/codec/prefixtree/row/data/TestRowDataExerciseFInts.java b/hbase-prefix-tree/src/test/java/org/apache/hadoop/hbase/codec/prefixtree/row/data/TestRowDataExerciseFInts.java
index 39140a3..1f9b459 100644
--- a/hbase-prefix-tree/src/test/java/org/apache/hadoop/hbase/codec/prefixtree/row/data/TestRowDataExerciseFInts.java
+++ b/hbase-prefix-tree/src/test/java/org/apache/hadoop/hbase/codec/prefixtree/row/data/TestRowDataExerciseFInts.java
@@ -43,7 +43,7 @@ public class TestRowDataExerciseFInts extends BaseTestRowData{
 
   static List<ByteRange> rows;
   static{
-    List<String> rowStrings = new ArrayList<String>(16);
+    List<String> rowStrings = new ArrayList<>(16);
     rowStrings.add("com.edsBlog/directoryAa/pageAaa");
     rowStrings.add("com.edsBlog/directoryAa/pageBbb");
     rowStrings.add("com.edsBlog/directoryAa/pageCcc");
diff --git a/hbase-prefix-tree/src/test/java/org/apache/hadoop/hbase/codec/prefixtree/row/data/TestRowDataTrivialWithTags.java b/hbase-prefix-tree/src/test/java/org/apache/hadoop/hbase/codec/prefixtree/row/data/TestRowDataTrivialWithTags.java
index 2d3901f..a7edfe7 100644
--- a/hbase-prefix-tree/src/test/java/org/apache/hadoop/hbase/codec/prefixtree/row/data/TestRowDataTrivialWithTags.java
+++ b/hbase-prefix-tree/src/test/java/org/apache/hadoop/hbase/codec/prefixtree/row/data/TestRowDataTrivialWithTags.java
@@ -46,7 +46,7 @@ public class TestRowDataTrivialWithTags extends BaseTestRowData{
 
   static List<KeyValue> d = Lists.newArrayList();
   static {
-    List<Tag> tagList = new ArrayList<Tag>(2);
+    List<Tag> tagList = new ArrayList<>(2);
     Tag t = new ArrayBackedTag((byte) 1, "visisbility");
     tagList.add(t);
     t = new ArrayBackedTag((byte) 2, "ACL");
diff --git a/hbase-prefix-tree/src/test/java/org/apache/hadoop/hbase/codec/prefixtree/row/data/TestRowDataUrls.java b/hbase-prefix-tree/src/test/java/org/apache/hadoop/hbase/codec/prefixtree/row/data/TestRowDataUrls.java
index a71daaa..0276617 100644
--- a/hbase-prefix-tree/src/test/java/org/apache/hadoop/hbase/codec/prefixtree/row/data/TestRowDataUrls.java
+++ b/hbase-prefix-tree/src/test/java/org/apache/hadoop/hbase/codec/prefixtree/row/data/TestRowDataUrls.java
@@ -41,7 +41,7 @@ public class TestRowDataUrls extends BaseTestRowData{
 
   static List<ByteRange> rows;
   static{
-    List<String> rowStrings = new ArrayList<String>(16);
+    List<String> rowStrings = new ArrayList<>(16);
     rowStrings.add("com.edsBlog/directoryAa/pageAaa");
     rowStrings.add("com.edsBlog/directoryAa/pageBbb");
     rowStrings.add("com.edsBlog/directoryAa/pageCcc");
diff --git a/hbase-prefix-tree/src/test/java/org/apache/hadoop/hbase/codec/prefixtree/timestamp/data/TestTimestampDataBasic.java b/hbase-prefix-tree/src/test/java/org/apache/hadoop/hbase/codec/prefixtree/timestamp/data/TestTimestampDataBasic.java
index bccff6d..d4fbb4d 100644
--- a/hbase-prefix-tree/src/test/java/org/apache/hadoop/hbase/codec/prefixtree/timestamp/data/TestTimestampDataBasic.java
+++ b/hbase-prefix-tree/src/test/java/org/apache/hadoop/hbase/codec/prefixtree/timestamp/data/TestTimestampDataBasic.java
@@ -27,7 +27,7 @@ public class TestTimestampDataBasic implements TestTimestampData {
 
   @Override
   public List<Long> getInputs() {
-    List<Long> d = new ArrayList<Long>(5);
+    List<Long> d = new ArrayList<>(5);
     d.add(5L);
     d.add(3L);
     d.add(0L);
@@ -43,7 +43,7 @@ public class TestTimestampDataBasic implements TestTimestampData {
 
   @Override
   public List<Long> getOutputs() {
-    List<Long> d = new ArrayList<Long>(4);
+    List<Long> d = new ArrayList<>(4);
     d.add(0L);
     d.add(1L);
     d.add(3L);
diff --git a/hbase-prefix-tree/src/test/java/org/apache/hadoop/hbase/codec/prefixtree/timestamp/data/TestTimestampDataNumbers.java b/hbase-prefix-tree/src/test/java/org/apache/hadoop/hbase/codec/prefixtree/timestamp/data/TestTimestampDataNumbers.java
index 2a5dcae..d0bc837 100644
--- a/hbase-prefix-tree/src/test/java/org/apache/hadoop/hbase/codec/prefixtree/timestamp/data/TestTimestampDataNumbers.java
+++ b/hbase-prefix-tree/src/test/java/org/apache/hadoop/hbase/codec/prefixtree/timestamp/data/TestTimestampDataNumbers.java
@@ -29,7 +29,7 @@ public class TestTimestampDataNumbers implements TestTimestampData {
 
   @Override
   public List<Long> getInputs() {
-    List<Long> d = new ArrayList<Long>(5);
+    List<Long> d = new ArrayList<>(5);
     d.add(5L << shift);
     d.add(3L << shift);
     d.add(7L << shift);
@@ -45,7 +45,7 @@ public class TestTimestampDataNumbers implements TestTimestampData {
 
   @Override
   public List<Long> getOutputs() {
-    List<Long> d = new ArrayList<Long>(4);
+    List<Long> d = new ArrayList<>(4);
     d.add(1L << shift);
     d.add(3L << shift);
     d.add(5L << shift);
diff --git a/hbase-prefix-tree/src/test/java/org/apache/hadoop/hbase/codec/prefixtree/timestamp/data/TestTimestampDataRepeats.java b/hbase-prefix-tree/src/test/java/org/apache/hadoop/hbase/codec/prefixtree/timestamp/data/TestTimestampDataRepeats.java
index 2186528..3320d66 100644
--- a/hbase-prefix-tree/src/test/java/org/apache/hadoop/hbase/codec/prefixtree/timestamp/data/TestTimestampDataRepeats.java
+++ b/hbase-prefix-tree/src/test/java/org/apache/hadoop/hbase/codec/prefixtree/timestamp/data/TestTimestampDataRepeats.java
@@ -29,7 +29,7 @@ public class TestTimestampDataRepeats implements TestTimestampData {
 
   @Override
   public List<Long> getInputs() {
-    List<Long> d = new ArrayList<Long>(5);
+    List<Long> d = new ArrayList<>(5);
     d.add(t);
     d.add(t);
     d.add(t);
@@ -45,7 +45,7 @@ public class TestTimestampDataRepeats implements TestTimestampData {
 
   @Override
   public List<Long> getOutputs() {
-    List<Long> d = new ArrayList<Long>();
+    List<Long> d = new ArrayList<>();
     return d;
   }
 
diff --git a/hbase-procedure/src/main/java/org/apache/hadoop/hbase/procedure2/ProcedureExecutor.java b/hbase-procedure/src/main/java/org/apache/hadoop/hbase/procedure2/ProcedureExecutor.java
index b38b96c..0856aa2 100644
--- a/hbase-procedure/src/main/java/org/apache/hadoop/hbase/procedure2/ProcedureExecutor.java
+++ b/hbase-procedure/src/main/java/org/apache/hadoop/hbase/procedure2/ProcedureExecutor.java
@@ -209,33 +209,28 @@ public class ProcedureExecutor<TEnvironment> {
    * Once a Root-Procedure completes (success or failure), the result will be added to this map.
    * The user of ProcedureExecutor should call getResult(procId) to get the result.
    */
-  private final ConcurrentHashMap<Long, ProcedureInfo> completed =
-    new ConcurrentHashMap<Long, ProcedureInfo>();
+  private final ConcurrentHashMap<Long, ProcedureInfo> completed = new ConcurrentHashMap<>();
 
   /**
    * Map the the procId returned by submitProcedure(), the Root-ProcID, to the RootProcedureState.
    * The RootProcedureState contains the execution stack of the Root-Procedure,
    * It is added to the map by submitProcedure() and removed on procedure completion.
    */
-  private final ConcurrentHashMap<Long, RootProcedureState> rollbackStack =
-    new ConcurrentHashMap<Long, RootProcedureState>();
+  private final ConcurrentHashMap<Long, RootProcedureState> rollbackStack = new ConcurrentHashMap<>();
 
   /**
    * Helper map to lookup the live procedures by ID.
    * This map contains every procedure. root-procedures and subprocedures.
    */
-  private final ConcurrentHashMap<Long, Procedure> procedures =
-    new ConcurrentHashMap<Long, Procedure>();
+  private final ConcurrentHashMap<Long, Procedure> procedures = new ConcurrentHashMap<>();
 
   /**
    * Helper map to lookup whether the procedure already issued from the same client.
    * This map contains every root procedure.
    */
-  private final ConcurrentHashMap<NonceKey, Long> nonceKeysToProcIdsMap =
-      new ConcurrentHashMap<NonceKey, Long>();
+  private final ConcurrentHashMap<NonceKey, Long> nonceKeysToProcIdsMap = new ConcurrentHashMap<>();
 
-  private final CopyOnWriteArrayList<ProcedureExecutorListener> listeners =
-    new CopyOnWriteArrayList<ProcedureExecutorListener>();
+  private final CopyOnWriteArrayList<ProcedureExecutorListener> listeners = new CopyOnWriteArrayList<>();
 
   private Configuration conf;
   private ThreadGroup threadGroup;
@@ -399,7 +394,7 @@ public class ProcedureExecutor<TEnvironment> {
           break;
         case WAITING_TIMEOUT:
           if (waitingSet == null) {
-            waitingSet = new HashSet<Procedure>();
+            waitingSet = new HashSet<>();
           }
           waitingSet.add(proc);
           break;
@@ -498,7 +493,7 @@ public class ProcedureExecutor<TEnvironment> {
 
     // Create the workers
     workerId.set(0);
-    workerThreads = new CopyOnWriteArrayList<WorkerThread>();
+    workerThreads = new CopyOnWriteArrayList<>();
     for (int i = 0; i < corePoolSize; ++i) {
       workerThreads.add(new WorkerThread(threadGroup));
     }
@@ -979,8 +974,7 @@ public class ProcedureExecutor<TEnvironment> {
    * @return the procedures in a list
    */
   public List<ProcedureInfo> listProcedures() {
-    final List<ProcedureInfo> procedureLists =
-        new ArrayList<ProcedureInfo>(procedures.size() + completed.size());
+    final List<ProcedureInfo> procedureLists = new ArrayList<>(procedures.size() + completed.size());
     for (Map.Entry<Long, Procedure> p: procedures.entrySet()) {
       procedureLists.add(ProcedureUtil.convertToProcedureInfo(p.getValue()));
     }
@@ -1614,7 +1608,7 @@ public class ProcedureExecutor<TEnvironment> {
   //  Timeout Thread
   // ==========================================================================
   private final class TimeoutExecutorThread extends StoppableThread {
-    private final DelayQueue<DelayedWithTimeout> queue = new DelayQueue<DelayedWithTimeout>();
+    private final DelayQueue<DelayedWithTimeout> queue = new DelayQueue<>();
 
     public TimeoutExecutorThread(final ThreadGroup group) {
       super(group, "ProcedureTimeoutExecutor");
diff --git a/hbase-procedure/src/main/java/org/apache/hadoop/hbase/procedure2/RootProcedureState.java b/hbase-procedure/src/main/java/org/apache/hadoop/hbase/procedure2/RootProcedureState.java
index 2f118b7..4f9b136 100644
--- a/hbase-procedure/src/main/java/org/apache/hadoop/hbase/procedure2/RootProcedureState.java
+++ b/hbase-procedure/src/main/java/org/apache/hadoop/hbase/procedure2/RootProcedureState.java
@@ -147,7 +147,7 @@ class RootProcedureState {
       state = State.FAILED;
     }
     if (subprocStack == null) {
-      subprocStack = new ArrayList<Procedure>();
+      subprocStack = new ArrayList<>();
     }
     proc.addStackIndex(subprocStack.size());
     subprocStack.add(proc);
@@ -156,7 +156,7 @@ class RootProcedureState {
   protected synchronized void addSubProcedure(final Procedure proc) {
     if (!proc.hasParent()) return;
     if (subprocs == null) {
-      subprocs = new HashSet<Procedure>();
+      subprocs = new HashSet<>();
     }
     subprocs.add(proc);
   }
@@ -173,7 +173,7 @@ class RootProcedureState {
     int[] stackIndexes = proc.getStackIndexes();
     if (stackIndexes != null) {
       if (subprocStack == null) {
-        subprocStack = new ArrayList<Procedure>();
+        subprocStack = new ArrayList<>();
       }
       int diff = (1 + stackIndexes[stackIndexes.length - 1]) - subprocStack.size();
       if (diff > 0) {
diff --git a/hbase-procedure/src/main/java/org/apache/hadoop/hbase/procedure2/StateMachineProcedure.java b/hbase-procedure/src/main/java/org/apache/hadoop/hbase/procedure2/StateMachineProcedure.java
index 3f9a7b7..5c3a4c7 100644
--- a/hbase-procedure/src/main/java/org/apache/hadoop/hbase/procedure2/StateMachineProcedure.java
+++ b/hbase-procedure/src/main/java/org/apache/hadoop/hbase/procedure2/StateMachineProcedure.java
@@ -130,7 +130,7 @@ public abstract class StateMachineProcedure<TEnvironment, TState>
    */
   protected void addChildProcedure(Procedure... subProcedure) {
     if (subProcList == null) {
-      subProcList = new ArrayList<Procedure>(subProcedure.length);
+      subProcList = new ArrayList<>(subProcedure.length);
     }
     for (int i = 0; i < subProcedure.length; ++i) {
       Procedure proc = subProcedure[i];
diff --git a/hbase-procedure/src/main/java/org/apache/hadoop/hbase/procedure2/store/ProcedureStoreBase.java b/hbase-procedure/src/main/java/org/apache/hadoop/hbase/procedure2/store/ProcedureStoreBase.java
index 0e0e46f..63eff37 100644
--- a/hbase-procedure/src/main/java/org/apache/hadoop/hbase/procedure2/store/ProcedureStoreBase.java
+++ b/hbase-procedure/src/main/java/org/apache/hadoop/hbase/procedure2/store/ProcedureStoreBase.java
@@ -25,8 +25,7 @@ import java.util.concurrent.atomic.AtomicBoolean;
  * Base class for {@link ProcedureStore}s.
  */
 public abstract class ProcedureStoreBase implements ProcedureStore {
-  private final CopyOnWriteArrayList<ProcedureStoreListener> listeners =
-      new CopyOnWriteArrayList<ProcedureStoreListener>();
+  private final CopyOnWriteArrayList<ProcedureStoreListener> listeners = new CopyOnWriteArrayList<>();
 
   private final AtomicBoolean running = new AtomicBoolean(false);
 
diff --git a/hbase-procedure/src/main/java/org/apache/hadoop/hbase/procedure2/store/wal/ProcedureWALPrettyPrinter.java b/hbase-procedure/src/main/java/org/apache/hadoop/hbase/procedure2/store/wal/ProcedureWALPrettyPrinter.java
index ec59607..5ad96e1 100644
--- a/hbase-procedure/src/main/java/org/apache/hadoop/hbase/procedure2/store/wal/ProcedureWALPrettyPrinter.java
+++ b/hbase-procedure/src/main/java/org/apache/hadoop/hbase/procedure2/store/wal/ProcedureWALPrettyPrinter.java
@@ -155,7 +155,7 @@ public class ProcedureWALPrettyPrinter extends Configured implements Tool {
     options.addOption("h", "help", false, "Output help message");
     options.addOption("f", "file", true, "File to print");
 
-    final List<Path> files = new ArrayList<Path>();
+    final List<Path> files = new ArrayList<>();
     try {
       CommandLine cmd = new PosixParser().parse(options, args);
 
diff --git a/hbase-procedure/src/main/java/org/apache/hadoop/hbase/procedure2/store/wal/WALProcedureStore.java b/hbase-procedure/src/main/java/org/apache/hadoop/hbase/procedure2/store/wal/WALProcedureStore.java
index 1841aa4..4712c30 100644
--- a/hbase-procedure/src/main/java/org/apache/hadoop/hbase/procedure2/store/wal/WALProcedureStore.java
+++ b/hbase-procedure/src/main/java/org/apache/hadoop/hbase/procedure2/store/wal/WALProcedureStore.java
@@ -125,7 +125,7 @@ public class WALProcedureStore extends ProcedureStoreBase {
   private final FileSystem fs;
   private final Path walDir;
 
-  private final AtomicReference<Throwable> syncException = new AtomicReference<Throwable>();
+  private final AtomicReference<Throwable> syncException = new AtomicReference<>();
   private final AtomicBoolean loading = new AtomicBoolean(true);
   private final AtomicBoolean inSync = new AtomicBoolean(false);
   private final AtomicLong totalSynced = new AtomicLong(0);
@@ -304,7 +304,7 @@ public class WALProcedureStore extends ProcedureStoreBase {
   public ArrayList<ProcedureWALFile> getActiveLogs() {
     lock.lock();
     try {
-      return new ArrayList<ProcedureWALFile>(logs);
+      return new ArrayList<>(logs);
     } finally {
       lock.unlock();
     }
@@ -395,7 +395,7 @@ public class WALProcedureStore extends ProcedureStoreBase {
         @Override
         public void markCorruptedWAL(ProcedureWALFile log, IOException e) {
           if (corruptedLogs == null) {
-            corruptedLogs = new HashSet<ProcedureWALFile>();
+            corruptedLogs = new HashSet<>();
           }
           corruptedLogs.add(log);
           // TODO: sideline corrupted log
@@ -790,7 +790,7 @@ public class WALProcedureStore extends ProcedureStoreBase {
   public ArrayList<SyncMetrics> getSyncMetrics() {
     lock.lock();
     try {
-      return new ArrayList<SyncMetrics>(syncMetricsBuffer);
+      return new ArrayList<>(syncMetricsBuffer);
     } finally {
       lock.unlock();
     }
diff --git a/hbase-procedure/src/test/java/org/apache/hadoop/hbase/procedure2/ProcedureTestingUtility.java b/hbase-procedure/src/test/java/org/apache/hadoop/hbase/procedure2/ProcedureTestingUtility.java
index c1b4e9b..226666f 100644
--- a/hbase-procedure/src/test/java/org/apache/hadoop/hbase/procedure2/ProcedureTestingUtility.java
+++ b/hbase-procedure/src/test/java/org/apache/hadoop/hbase/procedure2/ProcedureTestingUtility.java
@@ -183,7 +183,7 @@ public class ProcedureTestingUtility {
   public static <TEnv> long submitAndWait(Configuration conf, TEnv env, Procedure<TEnv> proc)
       throws IOException {
     NoopProcedureStore procStore = new NoopProcedureStore();
-    ProcedureExecutor<TEnv> procExecutor = new ProcedureExecutor<TEnv>(conf, env, procStore);
+    ProcedureExecutor<TEnv> procExecutor = new ProcedureExecutor<>(conf, env, procStore);
     procStore.start(1);
     procExecutor.start(1, false);
     try {
@@ -446,9 +446,9 @@ public class ProcedureTestingUtility {
   }
 
   public static class LoadCounter implements ProcedureStore.ProcedureLoader {
-    private final ArrayList<Procedure> corrupted = new ArrayList<Procedure>();
-    private final ArrayList<ProcedureInfo> completed = new ArrayList<ProcedureInfo>();
-    private final ArrayList<Procedure> runnable = new ArrayList<Procedure>();
+    private final ArrayList<Procedure> corrupted = new ArrayList<>();
+    private final ArrayList<ProcedureInfo> completed = new ArrayList<>();
+    private final ArrayList<Procedure> runnable = new ArrayList<>();
 
     private Set<Long> procIds;
     private long maxProcId = 0;
diff --git a/hbase-procedure/src/test/java/org/apache/hadoop/hbase/procedure2/TestProcedureExecution.java b/hbase-procedure/src/test/java/org/apache/hadoop/hbase/procedure2/TestProcedureExecution.java
index da6d960..38adbf5 100644
--- a/hbase-procedure/src/test/java/org/apache/hadoop/hbase/procedure2/TestProcedureExecution.java
+++ b/hbase-procedure/src/test/java/org/apache/hadoop/hbase/procedure2/TestProcedureExecution.java
@@ -129,7 +129,7 @@ public class TestProcedureExecution {
 
   @Test(timeout=30000)
   public void testBadSubprocList() {
-    List<String> state = new ArrayList<String>();
+    List<String> state = new ArrayList<>();
     Procedure subProc2 = new TestSequentialProcedure("subProc2", state);
     Procedure subProc1 = new TestSequentialProcedure("subProc1", state, subProc2, NULL_PROC);
     Procedure rootProc = new TestSequentialProcedure("rootProc", state, subProc1);
@@ -151,7 +151,7 @@ public class TestProcedureExecution {
 
   @Test(timeout=30000)
   public void testSingleSequentialProc() {
-    List<String> state = new ArrayList<String>();
+    List<String> state = new ArrayList<>();
     Procedure subProc2 = new TestSequentialProcedure("subProc2", state);
     Procedure subProc1 = new TestSequentialProcedure("subProc1", state, subProc2);
     Procedure rootProc = new TestSequentialProcedure("rootProc", state, subProc1);
@@ -166,7 +166,7 @@ public class TestProcedureExecution {
 
   @Test(timeout=30000)
   public void testSingleSequentialProcRollback() {
-    List<String> state = new ArrayList<String>();
+    List<String> state = new ArrayList<>();
     Procedure subProc2 = new TestSequentialProcedure("subProc2", state,
                                                      new TestProcedureException("fail test"));
     Procedure subProc1 = new TestSequentialProcedure("subProc1", state, subProc2);
@@ -295,7 +295,7 @@ public class TestProcedureExecution {
   @Test(timeout=30000)
   public void testAbortTimeout() {
     final int PROC_TIMEOUT_MSEC = 2500;
-    List<String> state = new ArrayList<String>();
+    List<String> state = new ArrayList<>();
     Procedure proc = new TestWaitingProcedure("wproc", state, false);
     proc.setTimeout(PROC_TIMEOUT_MSEC);
     long startTime = EnvironmentEdgeManager.currentTime();
@@ -313,7 +313,7 @@ public class TestProcedureExecution {
 
   @Test(timeout=30000)
   public void testAbortTimeoutWithChildren() {
-    List<String> state = new ArrayList<String>();
+    List<String> state = new ArrayList<>();
     Procedure proc = new TestWaitingProcedure("wproc", state, true);
     proc.setTimeout(2500);
     long rootId = ProcedureTestingUtility.submitAndWait(procExecutor, proc);
diff --git a/hbase-procedure/src/test/java/org/apache/hadoop/hbase/procedure2/TestProcedureReplayOrder.java b/hbase-procedure/src/test/java/org/apache/hadoop/hbase/procedure2/TestProcedureReplayOrder.java
index f838c25..bd614e3 100644
--- a/hbase-procedure/src/test/java/org/apache/hadoop/hbase/procedure2/TestProcedureReplayOrder.java
+++ b/hbase-procedure/src/test/java/org/apache/hadoop/hbase/procedure2/TestProcedureReplayOrder.java
@@ -154,7 +154,7 @@ public class TestProcedureReplayOrder {
   }
 
   private static class TestProcedureEnv {
-    private ArrayList<TestProcedure> execList = new ArrayList<TestProcedure>();
+    private ArrayList<TestProcedure> execList = new ArrayList<>();
     private AtomicLong execTimestamp = new AtomicLong(0);
 
     public long getExecId() {
diff --git a/hbase-procedure/src/test/java/org/apache/hadoop/hbase/procedure2/TestProcedureSchedulerConcurrency.java b/hbase-procedure/src/test/java/org/apache/hadoop/hbase/procedure2/TestProcedureSchedulerConcurrency.java
index b8cd8ff..4217693 100644
--- a/hbase-procedure/src/test/java/org/apache/hadoop/hbase/procedure2/TestProcedureSchedulerConcurrency.java
+++ b/hbase-procedure/src/test/java/org/apache/hadoop/hbase/procedure2/TestProcedureSchedulerConcurrency.java
@@ -79,8 +79,7 @@ public class TestProcedureSchedulerConcurrency {
     final AtomicInteger waitCount = new AtomicInteger(0);
     final AtomicInteger wakeCount = new AtomicInteger(0);
 
-    final ConcurrentSkipListSet<TestProcedureWithEvent> waitQueue =
-      new ConcurrentSkipListSet<TestProcedureWithEvent>();
+    final ConcurrentSkipListSet<TestProcedureWithEvent> waitQueue = new ConcurrentSkipListSet<>();
     threads[0] = new Thread() {
       @Override
       public void run() {
diff --git a/hbase-procedure/src/test/java/org/apache/hadoop/hbase/procedure2/TestProcedureSuspended.java b/hbase-procedure/src/test/java/org/apache/hadoop/hbase/procedure2/TestProcedureSuspended.java
index ba89768..0146bc7 100644
--- a/hbase-procedure/src/test/java/org/apache/hadoop/hbase/procedure2/TestProcedureSuspended.java
+++ b/hbase-procedure/src/test/java/org/apache/hadoop/hbase/procedure2/TestProcedureSuspended.java
@@ -161,7 +161,7 @@ public class TestProcedureSuspended {
   }
 
   public static class TestLockProcedure extends Procedure<TestProcEnv> {
-    private final ArrayList<Long> timestamps = new ArrayList<Long>();
+    private final ArrayList<Long> timestamps = new ArrayList<>();
     private final String key;
 
     private boolean triggerRollback = false;
diff --git a/hbase-procedure/src/test/java/org/apache/hadoop/hbase/procedure2/TestYieldProcedures.java b/hbase-procedure/src/test/java/org/apache/hadoop/hbase/procedure2/TestYieldProcedures.java
index 165179d..b1d0669 100644
--- a/hbase-procedure/src/test/java/org/apache/hadoop/hbase/procedure2/TestYieldProcedures.java
+++ b/hbase-procedure/src/test/java/org/apache/hadoop/hbase/procedure2/TestYieldProcedures.java
@@ -204,7 +204,7 @@ public class TestYieldProcedures {
       public boolean isRollback() { return rollback; }
     }
 
-    private final ArrayList<ExecutionInfo> executionInfo = new ArrayList<ExecutionInfo>();
+    private final ArrayList<ExecutionInfo> executionInfo = new ArrayList<>();
     private final AtomicBoolean aborted = new AtomicBoolean(false);
     private final boolean throwInterruptOnceOnEachStep;
     private final boolean abortOnFinalStep;
diff --git a/hbase-procedure/src/test/java/org/apache/hadoop/hbase/procedure2/util/TestDelayedUtil.java b/hbase-procedure/src/test/java/org/apache/hadoop/hbase/procedure2/util/TestDelayedUtil.java
index 1e2db4d..a2cd70f 100644
--- a/hbase-procedure/src/test/java/org/apache/hadoop/hbase/procedure2/util/TestDelayedUtil.java
+++ b/hbase-procedure/src/test/java/org/apache/hadoop/hbase/procedure2/util/TestDelayedUtil.java
@@ -36,13 +36,13 @@ public class TestDelayedUtil {
     Object o1 = new Object();
     Object o2 = new Object();
     ZeroDelayContainer<Long> lnull = new ZeroDelayContainer(null);
-    ZeroDelayContainer<Long> l10a = new ZeroDelayContainer<Long>(10L);
+    ZeroDelayContainer<Long> l10a = new ZeroDelayContainer<>(10L);
     ZeroDelayContainer<Long> l10b = new ZeroDelayContainer(10L);
     ZeroDelayContainer<Long> l15 = new ZeroDelayContainer(15L);
-    ZeroDelayContainer<Object> onull = new ZeroDelayContainer<Object>(null);
-    ZeroDelayContainer<Object> o1ca = new ZeroDelayContainer<Object>(o1);
-    ZeroDelayContainer<Object> o1cb = new ZeroDelayContainer<Object>(o1);
-    ZeroDelayContainer<Object> o2c = new ZeroDelayContainer<Object>(o2);
+    ZeroDelayContainer<Object> onull = new ZeroDelayContainer<>(null);
+    ZeroDelayContainer<Object> o1ca = new ZeroDelayContainer<>(o1);
+    ZeroDelayContainer<Object> o1cb = new ZeroDelayContainer<>(o1);
+    ZeroDelayContainer<Object> o2c = new ZeroDelayContainer<>(o2);
 
     ZeroDelayContainer[] items = new ZeroDelayContainer[] {
       lnull, l10a, l10b, l15, onull, o1ca, o1cb, o2c,
diff --git a/hbase-protocol-shaded/src/main/java/org/apache/hadoop/hbase/util/ForeignExceptionUtil.java b/hbase-protocol-shaded/src/main/java/org/apache/hadoop/hbase/util/ForeignExceptionUtil.java
index 0ccd9f9..28b3909 100644
--- a/hbase-protocol-shaded/src/main/java/org/apache/hadoop/hbase/util/ForeignExceptionUtil.java
+++ b/hbase-protocol-shaded/src/main/java/org/apache/hadoop/hbase/util/ForeignExceptionUtil.java
@@ -108,7 +108,7 @@ public final class ForeignExceptionUtil {
     // if there is no stack trace, ignore it and just return the message
     if (trace == null) return null;
     // build the stack trace for the message
-    List<StackTraceElementMessage> pbTrace = new ArrayList<StackTraceElementMessage>(trace.length);
+    List<StackTraceElementMessage> pbTrace = new ArrayList<>(trace.length);
     for (StackTraceElement elem : trace) {
       StackTraceElementMessage.Builder stackBuilder = StackTraceElementMessage.newBuilder();
       stackBuilder.setDeclaringClass(elem.getClassName());
diff --git a/hbase-rest/src/main/java/org/apache/hadoop/hbase/rest/RESTServer.java b/hbase-rest/src/main/java/org/apache/hadoop/hbase/rest/RESTServer.java
index 4ab194c..ba646c2 100644
--- a/hbase-rest/src/main/java/org/apache/hadoop/hbase/rest/RESTServer.java
+++ b/hbase-rest/src/main/java/org/apache/hadoop/hbase/rest/RESTServer.java
@@ -206,8 +206,7 @@ public class RESTServer implements Constants {
     }
 
     @SuppressWarnings("unchecked")
-    List<String> remainingArgs = commandLine != null ?
-        commandLine.getArgList() : new ArrayList<String>();
+    List<String> remainingArgs = commandLine != null ? commandLine.getArgList() : new ArrayList<>();
     if (remainingArgs.size() != 1) {
       printUsageAndExit(options, 1);
     }
@@ -256,7 +255,7 @@ public class RESTServer implements Constants {
     int queueSize = servlet.getConfiguration().getInt(REST_THREAD_POOL_TASK_QUEUE_SIZE, -1);
     int idleTimeout = servlet.getConfiguration().getInt(REST_THREAD_POOL_THREAD_IDLE_TIMEOUT, 60000);
     QueuedThreadPool threadPool = queueSize > 0 ?
-        new QueuedThreadPool(maxThreads, minThreads, idleTimeout, new ArrayBlockingQueue<Runnable>(queueSize)) :
+        new QueuedThreadPool(maxThreads, minThreads, idleTimeout, new ArrayBlockingQueue<>(queueSize)) :
         new QueuedThreadPool(maxThreads, minThreads, idleTimeout);
 
     Server server = new Server(threadPool);
diff --git a/hbase-rest/src/main/java/org/apache/hadoop/hbase/rest/RowResource.java b/hbase-rest/src/main/java/org/apache/hadoop/hbase/rest/RowResource.java
index de84625..7be4190 100644
--- a/hbase-rest/src/main/java/org/apache/hadoop/hbase/rest/RowResource.java
+++ b/hbase-rest/src/main/java/org/apache/hadoop/hbase/rest/RowResource.java
@@ -185,7 +185,7 @@ public class RowResource extends ResourceBase {
     Table table = null;
     try {
       List<RowModel> rows = model.getRows();
-      List<Put> puts = new ArrayList<Put>();
+      List<Put> puts = new ArrayList<>();
       for (RowModel row: rows) {
         byte[] key = row.getKey();
         if (key == null) {
diff --git a/hbase-rest/src/main/java/org/apache/hadoop/hbase/rest/RowSpec.java b/hbase-rest/src/main/java/org/apache/hadoop/hbase/rest/RowSpec.java
index cc51c85..5d25c54 100644
--- a/hbase-rest/src/main/java/org/apache/hadoop/hbase/rest/RowSpec.java
+++ b/hbase-rest/src/main/java/org/apache/hadoop/hbase/rest/RowSpec.java
@@ -44,9 +44,8 @@ public class RowSpec {
   
   private byte[] row = HConstants.EMPTY_START_ROW;
   private byte[] endRow = null;
-  private TreeSet<byte[]> columns =
-    new TreeSet<byte[]>(Bytes.BYTES_COMPARATOR);
-  private List<String> labels = new ArrayList<String>();  
+  private TreeSet<byte[]> columns = new TreeSet<>(Bytes.BYTES_COMPARATOR);
+  private List<String> labels = new ArrayList<>();
   private long startTime = DEFAULT_START_TIMESTAMP;
   private long endTime = DEFAULT_END_TIMESTAMP;
   private int maxVersions = 1;
diff --git a/hbase-rest/src/main/java/org/apache/hadoop/hbase/rest/client/Client.java b/hbase-rest/src/main/java/org/apache/hadoop/hbase/rest/client/Client.java
index 204f688..c9cf49a 100644
--- a/hbase-rest/src/main/java/org/apache/hadoop/hbase/rest/client/Client.java
+++ b/hbase-rest/src/main/java/org/apache/hadoop/hbase/rest/client/Client.java
@@ -77,7 +77,7 @@ public class Client {
   private void initialize(Cluster cluster, boolean sslEnabled) {
     this.cluster = cluster;
     this.sslEnabled = sslEnabled;
-    extraHeaders = new ConcurrentHashMap<String, String>();
+    extraHeaders = new ConcurrentHashMap<>();
     String clspath = System.getProperty("java.class.path");
     LOG.debug("classpath " + clspath);
     this.httpClient = new DefaultHttpClient();
diff --git a/hbase-rest/src/main/java/org/apache/hadoop/hbase/rest/client/RemoteHTable.java b/hbase-rest/src/main/java/org/apache/hadoop/hbase/rest/client/RemoteHTable.java
index 51a75d7..e762c31 100644
--- a/hbase-rest/src/main/java/org/apache/hadoop/hbase/rest/client/RemoteHTable.java
+++ b/hbase-rest/src/main/java/org/apache/hadoop/hbase/rest/client/RemoteHTable.java
@@ -173,9 +173,9 @@ public class RemoteHTable implements Table {
   }
 
   protected Result[] buildResultFromModel(final CellSetModel model) {
-    List<Result> results = new ArrayList<Result>();
+    List<Result> results = new ArrayList<>();
     for (RowModel row: model.getRows()) {
-      List<Cell> kvs = new ArrayList<Cell>(row.getCells().size());
+      List<Cell> kvs = new ArrayList<>(row.getCells().size());
       for (CellModel cell: row.getCells()) {
         byte[][] split = KeyValue.parseColumn(cell.getColumn());
         byte[] column = split[0];
@@ -425,13 +425,12 @@ public class RemoteHTable implements Table {
     // ignores the row specification in the URI
 
     // separate puts by row
-    TreeMap<byte[],List<Cell>> map =
-      new TreeMap<byte[],List<Cell>>(Bytes.BYTES_COMPARATOR);
+    TreeMap<byte[],List<Cell>> map = new TreeMap<>(Bytes.BYTES_COMPARATOR);
     for (Put put: puts) {
       byte[] row = put.getRow();
       List<Cell> cells = map.get(row);
       if (cells == null) {
-        cells = new ArrayList<Cell>();
+        cells = new ArrayList<>();
         map.put(row, cells);
       }
       for (List<Cell> l: put.getFamilyCellMap().values()) {
diff --git a/hbase-rest/src/main/java/org/apache/hadoop/hbase/rest/filter/GzipFilter.java b/hbase-rest/src/main/java/org/apache/hadoop/hbase/rest/filter/GzipFilter.java
index 094ae0b..626e61f 100644
--- a/hbase-rest/src/main/java/org/apache/hadoop/hbase/rest/filter/GzipFilter.java
+++ b/hbase-rest/src/main/java/org/apache/hadoop/hbase/rest/filter/GzipFilter.java
@@ -40,7 +40,7 @@ import org.apache.hadoop.hbase.HBaseInterfaceAudience;
 
 @InterfaceAudience.LimitedPrivate(HBaseInterfaceAudience.CONFIG)
 public class GzipFilter implements Filter {
-  private Set<String> mimeTypes = new HashSet<String>();
+  private Set<String> mimeTypes = new HashSet<>();
 
   @Override
   public void init(FilterConfig filterConfig) throws ServletException {
diff --git a/hbase-rest/src/main/java/org/apache/hadoop/hbase/rest/filter/RestCsrfPreventionFilter.java b/hbase-rest/src/main/java/org/apache/hadoop/hbase/rest/filter/RestCsrfPreventionFilter.java
index dbb1447..7224383 100644
--- a/hbase-rest/src/main/java/org/apache/hadoop/hbase/rest/filter/RestCsrfPreventionFilter.java
+++ b/hbase-rest/src/main/java/org/apache/hadoop/hbase/rest/filter/RestCsrfPreventionFilter.java
@@ -93,7 +93,7 @@ public class RestCsrfPreventionFilter implements Filter {
 
   void parseBrowserUserAgents(String userAgents) {
     String[] agentsArray =  userAgents.split(",");
-    browserUserAgents = new HashSet<Pattern>();
+    browserUserAgents = new HashSet<>();
     for (String patternString : agentsArray) {
       browserUserAgents.add(Pattern.compile(patternString));
     }
@@ -101,7 +101,7 @@ public class RestCsrfPreventionFilter implements Filter {
 
   void parseMethodsToIgnore(String mti) {
     String[] methods = mti.split(",");
-    methodsToIgnore = new HashSet<String>();
+    methodsToIgnore = new HashSet<>();
     for (int i = 0; i < methods.length; i++) {
       methodsToIgnore.add(methods[i]);
     }
diff --git a/hbase-rest/src/main/java/org/apache/hadoop/hbase/rest/model/CellSetModel.java b/hbase-rest/src/main/java/org/apache/hadoop/hbase/rest/model/CellSetModel.java
index 8337ffc..a754fe4 100644
--- a/hbase-rest/src/main/java/org/apache/hadoop/hbase/rest/model/CellSetModel.java
+++ b/hbase-rest/src/main/java/org/apache/hadoop/hbase/rest/model/CellSetModel.java
@@ -84,7 +84,7 @@ public class CellSetModel implements Serializable, ProtobufMessageHandler {
    * Constructor
    */
   public CellSetModel() {
-    this.rows = new ArrayList<RowModel>();
+    this.rows = new ArrayList<>();
   }
 
   /**
diff --git a/hbase-rest/src/main/java/org/apache/hadoop/hbase/rest/model/ColumnSchemaModel.java b/hbase-rest/src/main/java/org/apache/hadoop/hbase/rest/model/ColumnSchemaModel.java
index 8562cde..1b855fd 100644
--- a/hbase-rest/src/main/java/org/apache/hadoop/hbase/rest/model/ColumnSchemaModel.java
+++ b/hbase-rest/src/main/java/org/apache/hadoop/hbase/rest/model/ColumnSchemaModel.java
@@ -57,7 +57,7 @@ public class ColumnSchemaModel implements Serializable {
   private static QName VERSIONS = new QName(HConstants.VERSIONS);
 
   private String name;
-  private Map<QName,Object> attrs = new LinkedHashMap<QName,Object>();
+  private Map<QName,Object> attrs = new LinkedHashMap<>();
 
   /**
    * Default constructor
diff --git a/hbase-rest/src/main/java/org/apache/hadoop/hbase/rest/model/NamespacesInstanceModel.java b/hbase-rest/src/main/java/org/apache/hadoop/hbase/rest/model/NamespacesInstanceModel.java
index 0c5af3c..bcc1581 100644
--- a/hbase-rest/src/main/java/org/apache/hadoop/hbase/rest/model/NamespacesInstanceModel.java
+++ b/hbase-rest/src/main/java/org/apache/hadoop/hbase/rest/model/NamespacesInstanceModel.java
@@ -89,7 +89,7 @@ public class NamespacesInstanceModel implements Serializable, ProtobufMessageHan
     // For properly formed JSON, if no properties, field has to be null (not just no elements).
     if(nd.getConfiguration().isEmpty()){ return; }
 
-    properties = new HashMap<String,String>();
+    properties = new HashMap<>();
     properties.putAll(nd.getConfiguration());
   }
 
@@ -100,7 +100,7 @@ public class NamespacesInstanceModel implements Serializable, ProtobufMessageHan
    */
   public void addProperty(String key, String value) {
     if(properties == null){
-      properties = new HashMap<String,String>();
+      properties = new HashMap<>();
     }
     properties.put(key, value);
   }
@@ -110,7 +110,7 @@ public class NamespacesInstanceModel implements Serializable, ProtobufMessageHan
    */
   public Map<String,String> getProperties() {
     if(properties == null){
-      properties = new HashMap<String,String>();
+      properties = new HashMap<>();
     }
     return properties;
   }
diff --git a/hbase-rest/src/main/java/org/apache/hadoop/hbase/rest/model/NamespacesModel.java b/hbase-rest/src/main/java/org/apache/hadoop/hbase/rest/model/NamespacesModel.java
index aed80aa..4399b0b 100644
--- a/hbase-rest/src/main/java/org/apache/hadoop/hbase/rest/model/NamespacesModel.java
+++ b/hbase-rest/src/main/java/org/apache/hadoop/hbase/rest/model/NamespacesModel.java
@@ -52,7 +52,7 @@ public class NamespacesModel implements Serializable, ProtobufMessageHandler {
 
   @JsonProperty("Namespace")
   @XmlElement(name="Namespace")
-  private List<String> namespaces = new ArrayList<String>();
+  private List<String> namespaces = new ArrayList<>();
 
   /**
    * Default constructor. Do not use.
@@ -66,7 +66,7 @@ public class NamespacesModel implements Serializable, ProtobufMessageHandler {
    */
   public NamespacesModel(Admin admin) throws IOException {
     NamespaceDescriptor[] nds = admin.listNamespaceDescriptors();
-    namespaces = new ArrayList<String>(nds.length);
+    namespaces = new ArrayList<>(nds.length);
     for (NamespaceDescriptor nd : nds) {
       namespaces.add(nd.getName());
     }
diff --git a/hbase-rest/src/main/java/org/apache/hadoop/hbase/rest/model/RowModel.java b/hbase-rest/src/main/java/org/apache/hadoop/hbase/rest/model/RowModel.java
index 398d5e1..663c838 100644
--- a/hbase-rest/src/main/java/org/apache/hadoop/hbase/rest/model/RowModel.java
+++ b/hbase-rest/src/main/java/org/apache/hadoop/hbase/rest/model/RowModel.java
@@ -64,7 +64,7 @@ public class RowModel implements ProtobufMessageHandler, Serializable {
 
   @JsonProperty("Cell")
   @XmlElement(name="Cell")
-  private List<CellModel> cells = new ArrayList<CellModel>();
+  private List<CellModel> cells = new ArrayList<>();
 
 
   /**
@@ -86,7 +86,7 @@ public class RowModel implements ProtobufMessageHandler, Serializable {
    */
   public RowModel(final byte[] key) {
     this.key = key;
-    cells = new ArrayList<CellModel>();
+    cells = new ArrayList<>();
   }
 
   /**
diff --git a/hbase-rest/src/main/java/org/apache/hadoop/hbase/rest/model/ScannerModel.java b/hbase-rest/src/main/java/org/apache/hadoop/hbase/rest/model/ScannerModel.java
index 5c8d618..2098c3d 100644
--- a/hbase-rest/src/main/java/org/apache/hadoop/hbase/rest/model/ScannerModel.java
+++ b/hbase-rest/src/main/java/org/apache/hadoop/hbase/rest/model/ScannerModel.java
@@ -109,14 +109,14 @@ public class ScannerModel implements ProtobufMessageHandler, Serializable {
 
   private byte[] startRow = HConstants.EMPTY_START_ROW;
   private byte[] endRow = HConstants.EMPTY_END_ROW;;
-  private List<byte[]> columns = new ArrayList<byte[]>();
+  private List<byte[]> columns = new ArrayList<>();
   private int batch = Integer.MAX_VALUE;
   private long startTime = 0;
   private long endTime = Long.MAX_VALUE;
   private String filter = null;
   private int maxVersions = Integer.MAX_VALUE;
   private int caching = -1;
-  private List<String> labels = new ArrayList<String>();
+  private List<String> labels = new ArrayList<>();
   private boolean cacheBlocks = true;
 
   /**
@@ -287,7 +287,7 @@ public class ScannerModel implements ProtobufMessageHandler, Serializable {
         } break;
         case FilterList:
           this.op = ((FilterList)filter).getOperator().toString();
-          this.filters = new ArrayList<FilterModel>();
+          this.filters = new ArrayList<>();
           for (Filter child: ((FilterList)filter).getFilters()) {
             this.filters.add(new FilterModel(child));
           }
@@ -300,13 +300,13 @@ public class ScannerModel implements ProtobufMessageHandler, Serializable {
             Base64.encodeBytes(((InclusiveStopFilter)filter).getStopRowKey());
           break;
         case MultipleColumnPrefixFilter:
-          this.prefixes = new ArrayList<String>();
+          this.prefixes = new ArrayList<>();
           for (byte[] prefix: ((MultipleColumnPrefixFilter)filter).getPrefix()) {
             this.prefixes.add(Base64.encodeBytes(prefix));
           }
           break;
         case MultiRowRangeFilter:
-          this.ranges = new ArrayList<RowRange>();
+          this.ranges = new ArrayList<>();
           for(RowRange range : ((MultiRowRangeFilter)filter).getRowRanges()) {
             this.ranges.add(new RowRange(range.getStartRow(), range.isStartRowInclusive(),
                 range.getStopRow(), range.isStopRowInclusive()));
@@ -349,14 +349,14 @@ public class ScannerModel implements ProtobufMessageHandler, Serializable {
           }
         } break;
         case SkipFilter:
-          this.filters = new ArrayList<FilterModel>();
+          this.filters = new ArrayList<>();
           this.filters.add(new FilterModel(((SkipFilter)filter).getFilter()));
           break;
         case TimestampsFilter:
           this.timestamps = ((TimestampsFilter)filter).getTimestamps();
           break;
         case WhileMatchFilter:
-          this.filters = new ArrayList<FilterModel>();
+          this.filters = new ArrayList<>();
           this.filters.add(
             new FilterModel(((WhileMatchFilter)filter).getFilter()));
           break;
@@ -391,7 +391,7 @@ public class ScannerModel implements ProtobufMessageHandler, Serializable {
         filter = new FamilyFilter(CompareOp.valueOf(op), comparator.build());
         break;
       case FilterList: {
-        List<Filter> list = new ArrayList<Filter>(filters.size());
+        List<Filter> list = new ArrayList<>(filters.size());
         for (FilterModel model: filters) {
           list.add(model.build());
         }
diff --git a/hbase-rest/src/main/java/org/apache/hadoop/hbase/rest/model/StorageClusterStatusModel.java b/hbase-rest/src/main/java/org/apache/hadoop/hbase/rest/model/StorageClusterStatusModel.java
index c97f3e8..3c3c50e 100644
--- a/hbase-rest/src/main/java/org/apache/hadoop/hbase/rest/model/StorageClusterStatusModel.java
+++ b/hbase-rest/src/main/java/org/apache/hadoop/hbase/rest/model/StorageClusterStatusModel.java
@@ -373,7 +373,7 @@ public class StorageClusterStatusModel
     private long requests;
     private int heapSizeMB;
     private int maxHeapSizeMB;
-    private List<Region> regions = new ArrayList<Region>();
+    private List<Region> regions = new ArrayList<>();
 
     /**
      * Add a region name to the list
@@ -505,8 +505,8 @@ public class StorageClusterStatusModel
     }
   }
 
-  private List<Node> liveNodes = new ArrayList<Node>();
-  private List<String> deadNodes = new ArrayList<String>();
+  private List<Node> liveNodes = new ArrayList<>();
+  private List<String> deadNodes = new ArrayList<>();
   private int regions;
   private long requests;
   private double averageLoad;
diff --git a/hbase-rest/src/main/java/org/apache/hadoop/hbase/rest/model/TableInfoModel.java b/hbase-rest/src/main/java/org/apache/hadoop/hbase/rest/model/TableInfoModel.java
index 7336eb8..c1db1da 100644
--- a/hbase-rest/src/main/java/org/apache/hadoop/hbase/rest/model/TableInfoModel.java
+++ b/hbase-rest/src/main/java/org/apache/hadoop/hbase/rest/model/TableInfoModel.java
@@ -53,7 +53,7 @@ public class TableInfoModel implements Serializable, ProtobufMessageHandler {
   private static final long serialVersionUID = 1L;
 
   private String name;
-  private List<TableRegionModel> regions = new ArrayList<TableRegionModel>();
+  private List<TableRegionModel> regions = new ArrayList<>();
 
   /**
    * Default constructor
diff --git a/hbase-rest/src/main/java/org/apache/hadoop/hbase/rest/model/TableListModel.java b/hbase-rest/src/main/java/org/apache/hadoop/hbase/rest/model/TableListModel.java
index cc043be..f7d9a42 100644
--- a/hbase-rest/src/main/java/org/apache/hadoop/hbase/rest/model/TableListModel.java
+++ b/hbase-rest/src/main/java/org/apache/hadoop/hbase/rest/model/TableListModel.java
@@ -41,7 +41,7 @@ public class TableListModel implements Serializable, ProtobufMessageHandler {
 
   private static final long serialVersionUID = 1L;
 
-  private List<TableModel> tables = new ArrayList<TableModel>();
+  private List<TableModel> tables = new ArrayList<>();
 
   /**
    * Default constructor
diff --git a/hbase-rest/src/main/java/org/apache/hadoop/hbase/rest/model/TableSchemaModel.java b/hbase-rest/src/main/java/org/apache/hadoop/hbase/rest/model/TableSchemaModel.java
index 24fd09c..a93a3ca 100644
--- a/hbase-rest/src/main/java/org/apache/hadoop/hbase/rest/model/TableSchemaModel.java
+++ b/hbase-rest/src/main/java/org/apache/hadoop/hbase/rest/model/TableSchemaModel.java
@@ -74,8 +74,8 @@ public class TableSchemaModel implements Serializable, ProtobufMessageHandler {
     new QName(HColumnDescriptor.COMPRESSION);
 
   private String name;
-  private Map<QName,Object> attrs = new LinkedHashMap<QName,Object>();
-  private List<ColumnSchemaModel> columns = new ArrayList<ColumnSchemaModel>();
+  private Map<QName,Object> attrs = new LinkedHashMap<>();
+  private List<ColumnSchemaModel> columns = new ArrayList<>();
 
   /**
    * Default constructor.
diff --git a/hbase-rest/src/test/java/org/apache/hadoop/hbase/rest/PerformanceEvaluation.java b/hbase-rest/src/test/java/org/apache/hadoop/hbase/rest/PerformanceEvaluation.java
index 0d29159..3559ee0 100644
--- a/hbase-rest/src/test/java/org/apache/hadoop/hbase/rest/PerformanceEvaluation.java
+++ b/hbase-rest/src/test/java/org/apache/hadoop/hbase/rest/PerformanceEvaluation.java
@@ -125,7 +125,7 @@ public class PerformanceEvaluation extends Configured implements Tool {
   private TableName tableName = TABLE_NAME;
 
   protected HTableDescriptor TABLE_DESCRIPTOR;
-  protected Map<String, CmdDescriptor> commands = new TreeMap<String, CmdDescriptor>();
+  protected Map<String, CmdDescriptor> commands = new TreeMap<>();
   protected static Cluster cluster = new Cluster();
 
   volatile Configuration conf;
@@ -338,7 +338,7 @@ public class PerformanceEvaluation extends Configured implements Tool {
     @Override
     public List<InputSplit> getSplits(JobContext job) throws IOException {
       // generate splits
-      List<InputSplit> splitList = new ArrayList<InputSplit>();
+      List<InputSplit> splitList = new ArrayList<>();
 
       for (FileStatus file: listStatus(job)) {
         if (file.isDirectory()) {
@@ -601,7 +601,7 @@ public class PerformanceEvaluation extends Configured implements Tool {
    * @throws IOException
    */
   private void doMultipleClients(final Class<? extends Test> cmd) throws IOException {
-    final List<Thread> threads = new ArrayList<Thread>(this.N);
+    final List<Thread> threads = new ArrayList<>(this.N);
     final long[] timings = new long[this.N];
     final int perClientRows = R/N;
     final TableName tableName = this.tableName;
@@ -724,7 +724,7 @@ public class PerformanceEvaluation extends Configured implements Tool {
     Path inputFile = new Path(inputDir, "input.txt");
     PrintStream out = new PrintStream(fs.create(inputFile));
     // Make input random.
-    Map<Integer, String> m = new TreeMap<Integer, String>();
+    Map<Integer, String> m = new TreeMap<>();
     Hash h = MurmurHash.getInstance();
     int perClientRows = (this.R / this.N);
     try {
@@ -1039,7 +1039,7 @@ public class PerformanceEvaluation extends Configured implements Tool {
     protected Pair<byte[], byte[]> generateStartAndStopRows(int maxRange) {
       int start = this.rand.nextInt(Integer.MAX_VALUE) % totalRows;
       int stop = start + maxRange;
-      return new Pair<byte[],byte[]>(format(start), format(stop));
+      return new Pair<>(format(start), format(stop));
     }
 
     @Override
diff --git a/hbase-rest/src/test/java/org/apache/hadoop/hbase/rest/TestGetAndPutResource.java b/hbase-rest/src/test/java/org/apache/hadoop/hbase/rest/TestGetAndPutResource.java
index d6eb1b3..0f2de44 100644
--- a/hbase-rest/src/test/java/org/apache/hadoop/hbase/rest/TestGetAndPutResource.java
+++ b/hbase-rest/src/test/java/org/apache/hadoop/hbase/rest/TestGetAndPutResource.java
@@ -143,7 +143,7 @@ public class TestGetAndPutResource extends RowResourceBase {
     assertEquals(response.getCode(), 200);
     checkValuePB(TABLE, ROW_1, COLUMN_2, VALUE_2);
 
-    HashMap<String,String> otherCells = new HashMap<String, String>();
+    HashMap<String,String> otherCells = new HashMap<>();
     otherCells.put(COLUMN_2,VALUE_3);
 
     // On Success update both the cells
@@ -176,7 +176,7 @@ public class TestGetAndPutResource extends RowResourceBase {
     assertEquals(response.getCode(), 200);
     checkValueXML(TABLE, ROW_1, COLUMN_2, VALUE_2);
 
-    HashMap<String,String> otherCells = new HashMap<String, String>();
+    HashMap<String,String> otherCells = new HashMap<>();
     otherCells.put(COLUMN_2,VALUE_3);
 
     // On Success update both the cells
@@ -214,7 +214,7 @@ public class TestGetAndPutResource extends RowResourceBase {
     checkValuePB(TABLE, ROW_1, COLUMN_3, VALUE_3);
 
     // Deletes the following columns based on Column1 check
-    HashMap<String,String> cellsToDelete = new HashMap<String, String>();
+    HashMap<String,String> cellsToDelete = new HashMap<>();
     cellsToDelete.put(COLUMN_2,VALUE_2); // Value does not matter
     cellsToDelete.put(COLUMN_3,VALUE_3); // Value does not matter
 
diff --git a/hbase-rest/src/test/java/org/apache/hadoop/hbase/rest/TestMultiRowResource.java b/hbase-rest/src/test/java/org/apache/hadoop/hbase/rest/TestMultiRowResource.java
index 2ecba6a..2a0b460 100644
--- a/hbase-rest/src/test/java/org/apache/hadoop/hbase/rest/TestMultiRowResource.java
+++ b/hbase-rest/src/test/java/org/apache/hadoop/hbase/rest/TestMultiRowResource.java
@@ -82,7 +82,7 @@ public class TestMultiRowResource {
 
   @Parameterized.Parameters
   public static Collection<Object[]> data() {
-    List<Object[]> params = new ArrayList<Object[]>(2);
+    List<Object[]> params = new ArrayList<>(2);
     params.add(new Object[] {Boolean.TRUE});
     params.add(new Object[] {Boolean.FALSE});
     return params;
diff --git a/hbase-rest/src/test/java/org/apache/hadoop/hbase/rest/TestNamespacesInstanceResource.java b/hbase-rest/src/test/java/org/apache/hadoop/hbase/rest/TestNamespacesInstanceResource.java
index 2058f50..58e8ea0 100644
--- a/hbase-rest/src/test/java/org/apache/hadoop/hbase/rest/TestNamespacesInstanceResource.java
+++ b/hbase-rest/src/test/java/org/apache/hadoop/hbase/rest/TestNamespacesInstanceResource.java
@@ -61,13 +61,13 @@ import org.junit.experimental.categories.Category;
 @Category({RestTests.class, MediumTests.class})
 public class TestNamespacesInstanceResource {
   private static String NAMESPACE1 = "TestNamespacesInstanceResource1";
-  private static Map<String,String> NAMESPACE1_PROPS = new HashMap<String,String>();
+  private static Map<String,String> NAMESPACE1_PROPS = new HashMap<>();
   private static String NAMESPACE2 = "TestNamespacesInstanceResource2";
-  private static Map<String,String> NAMESPACE2_PROPS = new HashMap<String,String>();
+  private static Map<String,String> NAMESPACE2_PROPS = new HashMap<>();
   private static String NAMESPACE3 = "TestNamespacesInstanceResource3";
-  private static Map<String,String> NAMESPACE3_PROPS = new HashMap<String,String>();
+  private static Map<String,String> NAMESPACE3_PROPS = new HashMap<>();
   private static String NAMESPACE4 = "TestNamespacesInstanceResource4";
-  private static Map<String,String> NAMESPACE4_PROPS = new HashMap<String,String>();
+  private static Map<String,String> NAMESPACE4_PROPS = new HashMap<>();
 
   private static final HBaseTestingUtility TEST_UTIL = new HBaseTestingUtility();
   private static final HBaseRESTTestingUtility REST_TEST_UTIL =
@@ -189,7 +189,7 @@ public class TestNamespacesInstanceResource {
     table.addFamily(colDesc);
     admin.createTable(table);
 
-    Map<String, String> nsProperties = new HashMap<String,String>();
+    Map<String, String> nsProperties = new HashMap<>();
     nsProperties.put("key1", "value1");
     List<String> nsTables = Arrays.asList("table1", "table2");
 
@@ -230,7 +230,7 @@ public class TestNamespacesInstanceResource {
 
     response = client.get(namespacePath, Constants.MIMETYPE_PROTOBUF);
     assertEquals(200, response.getCode());
-    tablemodel.setTables(new ArrayList<TableModel>());
+    tablemodel.setTables(new ArrayList<>());
     tablemodel.getObjectFromMessage(response.getBody());
     checkNamespaceTables(tablemodel.getTables(), nsTables);
 
@@ -406,7 +406,7 @@ public class TestNamespacesInstanceResource {
     nd4 = findNamespace(admin, NAMESPACE4);
     assertNotNull(nd3);
     assertNotNull(nd4);
-    checkNamespaceProperties(nd3, new HashMap<String,String>());
+    checkNamespaceProperties(nd3, new HashMap<>());
     checkNamespaceProperties(nd4, NAMESPACE4_PROPS);
 
     // Check cannot post tables that already exist.
diff --git a/hbase-rest/src/test/java/org/apache/hadoop/hbase/rest/TestScannersWithFilters.java b/hbase-rest/src/test/java/org/apache/hadoop/hbase/rest/TestScannersWithFilters.java
index 083ddbe..6816e53 100644
--- a/hbase-rest/src/test/java/org/apache/hadoop/hbase/rest/TestScannersWithFilters.java
+++ b/hbase-rest/src/test/java/org/apache/hadoop/hbase/rest/TestScannersWithFilters.java
@@ -957,7 +957,7 @@ public class TestScannersWithFilters {
     // Test getting a single row, single key using Row, Qualifier, and Value
     // regular expression and substring filters
     // Use must pass all
-    List<Filter> filters = new ArrayList<Filter>(3);
+    List<Filter> filters = new ArrayList<>(3);
     filters.add(new RowFilter(CompareOp.EQUAL,
       new RegexStringComparator(".+-2")));
     filters.add(new QualifierFilter(CompareOp.EQUAL,
diff --git a/hbase-rest/src/test/java/org/apache/hadoop/hbase/rest/TestSchemaResource.java b/hbase-rest/src/test/java/org/apache/hadoop/hbase/rest/TestSchemaResource.java
index df920b1..f0c3d4a 100644
--- a/hbase-rest/src/test/java/org/apache/hadoop/hbase/rest/TestSchemaResource.java
+++ b/hbase-rest/src/test/java/org/apache/hadoop/hbase/rest/TestSchemaResource.java
@@ -73,7 +73,7 @@ public class TestSchemaResource {
 
   @Parameterized.Parameters
   public static Collection<Object[]> data() {
-    List<Object[]> params = new ArrayList<Object[]>(2);
+    List<Object[]> params = new ArrayList<>(2);
     params.add(new Object[] {Boolean.TRUE});
     params.add(new Object[] {Boolean.FALSE});
     return params;
diff --git a/hbase-rest/src/test/java/org/apache/hadoop/hbase/rest/client/TestRemoteTable.java b/hbase-rest/src/test/java/org/apache/hadoop/hbase/rest/client/TestRemoteTable.java
index 0310d9f..f35208a 100644
--- a/hbase-rest/src/test/java/org/apache/hadoop/hbase/rest/client/TestRemoteTable.java
+++ b/hbase-rest/src/test/java/org/apache/hadoop/hbase/rest/client/TestRemoteTable.java
@@ -262,7 +262,7 @@ public class TestRemoteTable {
 
   @Test
   public void testMultiGet() throws Exception {
-    ArrayList<Get> gets = new ArrayList<Get>(2);
+    ArrayList<Get> gets = new ArrayList<>(2);
     gets.add(new Get(ROW_1));
     gets.add(new Get(ROW_2));
     Result[] results = remoteTable.get(gets);
@@ -272,7 +272,7 @@ public class TestRemoteTable {
     assertEquals(2, results[1].size());
 
     //Test Versions
-    gets = new ArrayList<Get>(2);
+    gets = new ArrayList<>(2);
     Get g = new Get(ROW_1);
     g.setMaxVersions(3);
     gets.add(g);
@@ -284,13 +284,13 @@ public class TestRemoteTable {
     assertEquals(3, results[1].size());
 
     //404
-    gets = new ArrayList<Get>(1);
+    gets = new ArrayList<>(1);
     gets.add(new Get(Bytes.toBytes("RESALLYREALLYNOTTHERE")));
     results = remoteTable.get(gets);
     assertNotNull(results);
     assertEquals(0, results.length);
 
-    gets = new ArrayList<Get>(3);
+    gets = new ArrayList<>(3);
     gets.add(new Get(Bytes.toBytes("RESALLYREALLYNOTTHERE")));
     gets.add(new Get(ROW_1));
     gets.add(new Get(ROW_2));
@@ -314,7 +314,7 @@ public class TestRemoteTable {
 
     // multiput
 
-    List<Put> puts = new ArrayList<Put>(3);
+    List<Put> puts = new ArrayList<>(3);
     put = new Put(ROW_3);
     put.addColumn(COLUMN_2, QUALIFIER_2, VALUE_2);
     puts.add(put);
@@ -408,7 +408,7 @@ public class TestRemoteTable {
    */
   @Test
   public void testScanner() throws IOException {
-    List<Put> puts = new ArrayList<Put>(4);
+    List<Put> puts = new ArrayList<>(4);
     Put put = new Put(ROW_1);
     put.addColumn(COLUMN_1, QUALIFIER_1, VALUE_1);
     puts.add(put);
@@ -499,7 +499,7 @@ public class TestRemoteTable {
    */
   @Test
   public void testIteratorScaner() throws IOException {
-    List<Put> puts = new ArrayList<Put>(4);
+    List<Put> puts = new ArrayList<>(4);
     Put put = new Put(ROW_1);
     put.addColumn(COLUMN_1, QUALIFIER_1, VALUE_1);
     puts.add(put);
diff --git a/hbase-rest/src/test/java/org/apache/hadoop/hbase/rest/model/TestNamespacesInstanceModel.java b/hbase-rest/src/test/java/org/apache/hadoop/hbase/rest/model/TestNamespacesInstanceModel.java
index 0d4bbbd..3dee5cb 100644
--- a/hbase-rest/src/test/java/org/apache/hadoop/hbase/rest/model/TestNamespacesInstanceModel.java
+++ b/hbase-rest/src/test/java/org/apache/hadoop/hbase/rest/model/TestNamespacesInstanceModel.java
@@ -30,7 +30,7 @@ import org.junit.experimental.categories.Category;
 @Category({RestTests.class, SmallTests.class})
 public class TestNamespacesInstanceModel extends TestModelBase<NamespacesInstanceModel> {
 
-  public static final Map<String,String> NAMESPACE_PROPERTIES = new HashMap<String, String>();
+  public static final Map<String,String> NAMESPACE_PROPERTIES = new HashMap<>();
   public static final String NAMESPACE_NAME = "namespaceName";
 
   public TestNamespacesInstanceModel() throws Exception {
diff --git a/hbase-rsgroup/src/main/java/org/apache/hadoop/hbase/rsgroup/RSGroupAdminClient.java b/hbase-rsgroup/src/main/java/org/apache/hadoop/hbase/rsgroup/RSGroupAdminClient.java
index 74e91fe..dfec736 100644
--- a/hbase-rsgroup/src/main/java/org/apache/hadoop/hbase/rsgroup/RSGroupAdminClient.java
+++ b/hbase-rsgroup/src/main/java/org/apache/hadoop/hbase/rsgroup/RSGroupAdminClient.java
@@ -183,4 +183,4 @@ class RSGroupAdminClient implements RSGroupAdmin {
       throw ProtobufUtil.handleRemoteException(e);
     }
   }
-}
\ No newline at end of file
+}
diff --git a/hbase-rsgroup/src/main/java/org/apache/hadoop/hbase/rsgroup/RSGroupAdminServer.java b/hbase-rsgroup/src/main/java/org/apache/hadoop/hbase/rsgroup/RSGroupAdminServer.java
index 1f0be5a..811cf71 100644
--- a/hbase-rsgroup/src/main/java/org/apache/hadoop/hbase/rsgroup/RSGroupAdminServer.java
+++ b/hbase-rsgroup/src/main/java/org/apache/hadoop/hbase/rsgroup/RSGroupAdminServer.java
@@ -82,7 +82,7 @@ public class RSGroupAdminServer implements RSGroupAdmin {
   private void checkOnlineServersOnly(Set<Address> servers) throws ConstraintException {
     // This uglyness is because we only have Address, not ServerName.
     // Online servers are keyed by ServerName.
-    Set<Address> onlineServers = new HashSet<Address>();
+    Set<Address> onlineServers = new HashSet<>();
     for(ServerName server: master.getServerManager().getOnlineServers().keySet()) {
       onlineServers.add(server.getAddress());
     }
@@ -114,7 +114,7 @@ public class RSGroupAdminServer implements RSGroupAdmin {
    * @return List of Regions associated with this <code>server</code>.
    */
   private List<HRegionInfo> getRegions(final Address server) {
-    LinkedList<HRegionInfo> regions = new LinkedList<HRegionInfo>();
+    LinkedList<HRegionInfo> regions = new LinkedList<>();
     for (Map.Entry<HRegionInfo, ServerName> el :
         master.getAssignmentManager().getRegionStates().getRegionAssignments().entrySet()) {
       if (el.getValue().getAddress().equals(server)) {
@@ -381,7 +381,7 @@ public class RSGroupAdminServer implements RSGroupAdmin {
       }
 
       //We balance per group instead of per table
-      List<RegionPlan> plans = new ArrayList<RegionPlan>();
+      List<RegionPlan> plans = new ArrayList<>();
       for(Map.Entry<TableName, Map<ServerName, List<HRegionInfo>>> tableMap:
           getRSGroupAssignmentsByTable(groupName).entrySet()) {
         LOG.info("Creating partial plan for table " + tableMap.getKey() + ": "
diff --git a/hbase-rsgroup/src/main/java/org/apache/hadoop/hbase/rsgroup/RSGroupBasedLoadBalancer.java b/hbase-rsgroup/src/main/java/org/apache/hadoop/hbase/rsgroup/RSGroupBasedLoadBalancer.java
index b36fd21..30efc0a 100644
--- a/hbase-rsgroup/src/main/java/org/apache/hadoop/hbase/rsgroup/RSGroupBasedLoadBalancer.java
+++ b/hbase-rsgroup/src/main/java/org/apache/hadoop/hbase/rsgroup/RSGroupBasedLoadBalancer.java
@@ -120,7 +120,7 @@ public class RSGroupBasedLoadBalancer implements RSGroupableBalancer {
     }
 
     Map<ServerName,List<HRegionInfo>> correctedState = correctAssignments(clusterState);
-    List<RegionPlan> regionPlans = new ArrayList<RegionPlan>();
+    List<RegionPlan> regionPlans = new ArrayList<>();
 
     List<HRegionInfo> misplacedRegions = correctedState.get(LoadBalancer.BOGUS_SERVER_NAME);
     for (HRegionInfo regionInfo : misplacedRegions) {
@@ -129,10 +129,8 @@ public class RSGroupBasedLoadBalancer implements RSGroupableBalancer {
     try {
       List<RSGroupInfo> rsgi = rsGroupInfoManager.listRSGroups();
       for (RSGroupInfo info: rsgi) {
-        Map<ServerName, List<HRegionInfo>> groupClusterState =
-            new HashMap<ServerName, List<HRegionInfo>>();
-        Map<TableName, Map<ServerName, List<HRegionInfo>>> groupClusterLoad =
-            new HashMap<TableName, Map<ServerName, List<HRegionInfo>>>();
+        Map<ServerName, List<HRegionInfo>> groupClusterState = new HashMap<>();
+        Map<TableName, Map<ServerName, List<HRegionInfo>>> groupClusterLoad = new HashMap<>();
         for (Address sName : info.getServers()) {
           for(ServerName curr: clusterState.keySet()) {
             if(curr.getAddress().equals(sName)) {
@@ -180,7 +178,7 @@ public class RSGroupBasedLoadBalancer implements RSGroupableBalancer {
   public Map<ServerName, List<HRegionInfo>> retainAssignment(
       Map<HRegionInfo, ServerName> regions, List<ServerName> servers) throws HBaseIOException {
     try {
-      Map<ServerName, List<HRegionInfo>> assignments = new TreeMap<ServerName, List<HRegionInfo>>();
+      Map<ServerName, List<HRegionInfo>> assignments = new TreeMap<>();
       ListMultimap<String, HRegionInfo> groupToRegion = ArrayListMultimap.create();
       Set<HRegionInfo> misplacedRegions = getMisplacedRegions(regions);
       for (HRegionInfo region : regions.keySet()) {
@@ -213,13 +211,13 @@ public class RSGroupBasedLoadBalancer implements RSGroupableBalancer {
             candidateList);
         if (server != null) {
           if (!assignments.containsKey(server)) {
-            assignments.put(server, new ArrayList<HRegionInfo>());
+            assignments.put(server, new ArrayList<>());
           }
           assignments.get(server).add(region);
         } else {
           //if not server is available assign to bogus so it ends up in RIT
           if(!assignments.containsKey(LoadBalancer.BOGUS_SERVER_NAME)) {
-            assignments.put(LoadBalancer.BOGUS_SERVER_NAME, new ArrayList<HRegionInfo>());
+            assignments.put(LoadBalancer.BOGUS_SERVER_NAME, new ArrayList<>());
           }
           assignments.get(LoadBalancer.BOGUS_SERVER_NAME).add(region);
         }
@@ -299,7 +297,7 @@ public class RSGroupBasedLoadBalancer implements RSGroupableBalancer {
 
   private Set<HRegionInfo> getMisplacedRegions(
       Map<HRegionInfo, ServerName> regions) throws IOException {
-    Set<HRegionInfo> misplacedRegions = new HashSet<HRegionInfo>();
+    Set<HRegionInfo> misplacedRegions = new HashSet<>();
     for(Map.Entry<HRegionInfo, ServerName> region : regions.entrySet()) {
       HRegionInfo regionInfo = region.getKey();
       ServerName assignedServer = region.getValue();
@@ -321,13 +319,12 @@ public class RSGroupBasedLoadBalancer implements RSGroupableBalancer {
 
   private Map<ServerName, List<HRegionInfo>> correctAssignments(
        Map<ServerName, List<HRegionInfo>> existingAssignments){
-    Map<ServerName, List<HRegionInfo>> correctAssignments =
-        new TreeMap<ServerName, List<HRegionInfo>>();
-    List<HRegionInfo> misplacedRegions = new LinkedList<HRegionInfo>();
-    correctAssignments.put(LoadBalancer.BOGUS_SERVER_NAME, new LinkedList<HRegionInfo>());
+    Map<ServerName, List<HRegionInfo>> correctAssignments = new TreeMap<>();
+    List<HRegionInfo> misplacedRegions = new LinkedList<>();
+    correctAssignments.put(LoadBalancer.BOGUS_SERVER_NAME, new LinkedList<>());
     for (Map.Entry<ServerName, List<HRegionInfo>> assignments : existingAssignments.entrySet()){
       ServerName sName = assignments.getKey();
-      correctAssignments.put(sName, new LinkedList<HRegionInfo>());
+      correctAssignments.put(sName, new LinkedList<>());
       List<HRegionInfo> regions = assignments.getValue();
       for (HRegionInfo region : regions) {
         RSGroupInfo info = null;
diff --git a/hbase-rsgroup/src/test/java/org/apache/hadoop/hbase/master/balancer/TestRSGroupBasedLoadBalancer.java b/hbase-rsgroup/src/test/java/org/apache/hadoop/hbase/master/balancer/TestRSGroupBasedLoadBalancer.java
index b794084..83fe122 100644
--- a/hbase-rsgroup/src/test/java/org/apache/hadoop/hbase/master/balancer/TestRSGroupBasedLoadBalancer.java
+++ b/hbase-rsgroup/src/test/java/org/apache/hadoop/hbase/master/balancer/TestRSGroupBasedLoadBalancer.java
@@ -89,7 +89,7 @@ public class TestRSGroupBasedLoadBalancer {
     rand = new SecureRandom();
     servers = generateServers(7);
     groupMap = constructGroupInfo(servers, groups);
-    tableMap = new HashMap<TableName, String>();
+    tableMap = new HashMap<>();
     tableDescs = constructTableDesc();
     Configuration conf = HBaseConfiguration.create();
     conf.set("hbase.regions.slop", "0");
@@ -231,7 +231,7 @@ public class TestRSGroupBasedLoadBalancer {
   public void testRetainAssignment() throws Exception {
     // Test simple case where all same servers are there
     Map<ServerName, List<HRegionInfo>> currentAssignments = mockClusterServers();
-    Map<HRegionInfo, ServerName> inputForTest = new HashMap<HRegionInfo, ServerName>();
+    Map<HRegionInfo, ServerName> inputForTest = new HashMap<>();
     for (ServerName sn : currentAssignments.keySet()) {
       for (HRegionInfo region : currentAssignments.get(sn)) {
         inputForTest.put(region, sn);
@@ -264,8 +264,8 @@ public class TestRSGroupBasedLoadBalancer {
       Map<ServerName, List<HRegionInfo>> assignment)
       throws FileNotFoundException, IOException {
     // Verify condition 1, every region assigned, and to online server
-    Set<ServerName> onlineServerSet = new TreeSet<ServerName>(servers);
-    Set<HRegionInfo> assignedRegions = new TreeSet<HRegionInfo>();
+    Set<ServerName> onlineServerSet = new TreeSet<>(servers);
+    Set<HRegionInfo> assignedRegions = new TreeSet<>();
     for (Map.Entry<ServerName, List<HRegionInfo>> a : assignment.entrySet()) {
       assertTrue(
           "Region assigned to server that was not listed as online",
@@ -276,7 +276,7 @@ public class TestRSGroupBasedLoadBalancer {
     assertEquals(existing.size(), assignedRegions.size());
 
     // Verify condition 2, every region must be assigned to correct server.
-    Set<String> onlineHostNames = new TreeSet<String>();
+    Set<String> onlineHostNames = new TreeSet<>();
     for (ServerName s : servers) {
       onlineHostNames.add(s.getHostname());
     }
@@ -402,7 +402,7 @@ public class TestRSGroupBasedLoadBalancer {
 
   private Map<ServerName, List<HRegionInfo>> mockClusterServers() throws IOException {
     assertTrue(servers.size() == regionAssignment.length);
-    Map<ServerName, List<HRegionInfo>> assignment = new TreeMap<ServerName, List<HRegionInfo>>();
+    Map<ServerName, List<HRegionInfo>> assignment = new TreeMap<>();
     for (int i = 0; i < servers.size(); i++) {
       int numRegions = regionAssignment[i];
       List<HRegionInfo> regions = assignedRegions(numRegions, servers.get(i));
@@ -418,7 +418,7 @@ public class TestRSGroupBasedLoadBalancer {
    * @return List of HRegionInfo.
    */
   private List<HRegionInfo> randomRegions(int numRegions) {
-    List<HRegionInfo> regions = new ArrayList<HRegionInfo>(numRegions);
+    List<HRegionInfo> regions = new ArrayList<>(numRegions);
     byte[] start = new byte[16];
     byte[] end = new byte[16];
     rand.nextBytes(start);
@@ -444,7 +444,7 @@ public class TestRSGroupBasedLoadBalancer {
    * @throws java.io.IOException Signals that an I/O exception has occurred.
    */
   private List<HRegionInfo> assignedRegions(int numRegions, ServerName sn) throws IOException {
-    List<HRegionInfo> regions = new ArrayList<HRegionInfo>(numRegions);
+    List<HRegionInfo> regions = new ArrayList<>(numRegions);
     byte[] start = new byte[16];
     byte[] end = new byte[16];
     Bytes.putInt(start, 0, numRegions << 1);
@@ -460,7 +460,7 @@ public class TestRSGroupBasedLoadBalancer {
   }
 
   private static List<ServerName> generateServers(int numServers) {
-    List<ServerName> servers = new ArrayList<ServerName>(numServers);
+    List<ServerName> servers = new ArrayList<>(numServers);
     for (int i = 0; i < numServers; i++) {
       String host = "server" + rand.nextInt(100000);
       int port = rand.nextInt(60000);
@@ -481,7 +481,7 @@ public class TestRSGroupBasedLoadBalancer {
     assertTrue(servers != null);
     assertTrue(servers.size() >= groups.length);
     int index = 0;
-    Map<String, RSGroupInfo> groupMap = new HashMap<String, RSGroupInfo>();
+    Map<String, RSGroupInfo> groupMap = new HashMap<>();
     for (String grpName : groups) {
       RSGroupInfo RSGroupInfo = new RSGroupInfo(grpName);
       RSGroupInfo.addServer(servers.get(index).getAddress());
diff --git a/hbase-rsgroup/src/test/java/org/apache/hadoop/hbase/rsgroup/TestRSGroupsBase.java b/hbase-rsgroup/src/test/java/org/apache/hadoop/hbase/rsgroup/TestRSGroupsBase.java
index 9096dfe..5f9116b 100644
--- a/hbase-rsgroup/src/test/java/org/apache/hadoop/hbase/rsgroup/TestRSGroupsBase.java
+++ b/hbase-rsgroup/src/test/java/org/apache/hadoop/hbase/rsgroup/TestRSGroupsBase.java
@@ -95,7 +95,7 @@ public abstract class TestRSGroupsBase {
     assertTrue(defaultInfo.getServers().size() >= serverCount);
     rsGroupAdmin.addRSGroup(groupName);
 
-    Set<Address> set = new HashSet<Address>();
+    Set<Address> set = new HashSet<>();
     for(Address server: defaultInfo.getServers()) {
       if(set.size() == serverCount) {
         break;
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/HDFSBlocksDistribution.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/HDFSBlocksDistribution.java
index 3773863..b0dfd42 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/HDFSBlocksDistribution.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/HDFSBlocksDistribution.java
@@ -104,8 +104,7 @@ public class HDFSBlocksDistribution {
    * Constructor
    */
   public HDFSBlocksDistribution() {
-    this.hostAndWeights =
-      new TreeMap<String,HostAndWeight>();
+    this.hostAndWeights = new TreeMap<>();
   }
 
   /**
@@ -229,7 +228,7 @@ public class HDFSBlocksDistribution {
    */
   public List<String> getTopHosts() {
     HostAndWeight[] hostAndWeights = getTopHostsWithWeights();
-    List<String> topHosts = new ArrayList<String>(hostAndWeights.length);
+    List<String> topHosts = new ArrayList<>(hostAndWeights.length);
     for(HostAndWeight haw : hostAndWeights) {
       topHosts.add(haw.getHost());
     }
@@ -240,8 +239,7 @@ public class HDFSBlocksDistribution {
    * return the sorted list of hosts in terms of their weights
    */
   public HostAndWeight[] getTopHostsWithWeights() {
-    NavigableSet<HostAndWeight> orderedHosts = new TreeSet<HostAndWeight>(
-      new HostAndWeight.WeightComparator());
+    NavigableSet<HostAndWeight> orderedHosts = new TreeSet<>(new HostAndWeight.WeightComparator());
     orderedHosts.addAll(this.hostAndWeights.values());
     return orderedHosts.descendingSet().toArray(new HostAndWeight[orderedHosts.size()]);
   }
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/HealthChecker.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/HealthChecker.java
index 530a323..45e0f3a 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/HealthChecker.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/HealthChecker.java
@@ -59,7 +59,7 @@ class HealthChecker {
   public void init(String location, long timeout) {
     this.healthCheckScript = location;
     this.scriptTimeout = timeout;
-    ArrayList<String> execScript = new ArrayList<String>();
+    ArrayList<String> execScript = new ArrayList<>();
     execScript.add(healthCheckScript);
     this.shexec = new ShellCommandExecutor(execScript.toArray(new String[execScript.size()]), null,
         null, scriptTimeout);
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/JMXListener.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/JMXListener.java
index 9265fb8..788d25b 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/JMXListener.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/JMXListener.java
@@ -101,7 +101,7 @@ public class JMXListener implements Coprocessor {
               + ",passwordFile:" + passwordFile + ",accessFile:" + accessFile);
 
     // Environment map
-    HashMap<String, Object> jmxEnv = new HashMap<String, Object>();
+    HashMap<String, Object> jmxEnv = new HashMap<>();
 
     RMIClientSocketFactory csf = null;
     RMIServerSocketFactory ssf = null;
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/LocalHBaseCluster.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/LocalHBaseCluster.java
index 33fff97..255ca31 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/LocalHBaseCluster.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/LocalHBaseCluster.java
@@ -61,10 +61,8 @@ import org.apache.hadoop.hbase.util.JVMClusterUtil;
 @InterfaceStability.Evolving
 public class LocalHBaseCluster {
   private static final Log LOG = LogFactory.getLog(LocalHBaseCluster.class);
-  private final List<JVMClusterUtil.MasterThread> masterThreads =
-    new CopyOnWriteArrayList<JVMClusterUtil.MasterThread>();
-  private final List<JVMClusterUtil.RegionServerThread> regionThreads =
-    new CopyOnWriteArrayList<JVMClusterUtil.RegionServerThread>();
+  private final List<JVMClusterUtil.MasterThread> masterThreads = new CopyOnWriteArrayList<>();
+  private final List<JVMClusterUtil.RegionServerThread> regionThreads = new CopyOnWriteArrayList<>();
   private final static int DEFAULT_NO = 1;
   /** local mode */
   public static final String LOCAL = "local";
@@ -257,8 +255,7 @@ public class LocalHBaseCluster {
    * list).
    */
   public List<JVMClusterUtil.RegionServerThread> getLiveRegionServers() {
-    List<JVMClusterUtil.RegionServerThread> liveServers =
-      new ArrayList<JVMClusterUtil.RegionServerThread>();
+    List<JVMClusterUtil.RegionServerThread> liveServers = new ArrayList<>();
     List<RegionServerThread> list = getRegionServers();
     for (JVMClusterUtil.RegionServerThread rst: list) {
       if (rst.isAlive()) liveServers.add(rst);
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/SslRMIClientSocketFactorySecure.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/SslRMIClientSocketFactorySecure.java
index e1bc4ef..d505d6f 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/SslRMIClientSocketFactorySecure.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/SslRMIClientSocketFactorySecure.java
@@ -25,7 +25,7 @@ public class SslRMIClientSocketFactorySecure extends SslRMIClientSocketFactory {
   @Override
   public Socket createSocket(String host, int port) throws IOException {
     SSLSocket socket = (SSLSocket) super.createSocket(host, port);
-    ArrayList<String> secureProtocols = new ArrayList<String>();
+    ArrayList<String> secureProtocols = new ArrayList<>();
     for (String p : socket.getEnabledProtocols()) {
       if (!p.contains("SSLv3")) {
         secureProtocols.add(p);
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/SslRMIServerSocketFactorySecure.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/SslRMIServerSocketFactorySecure.java
index bd946252..8560ddc 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/SslRMIServerSocketFactorySecure.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/SslRMIServerSocketFactorySecure.java
@@ -42,7 +42,7 @@ public class SslRMIServerSocketFactorySecure extends SslRMIServerSocketFactory {
         sslSocket.setUseClientMode(false);
         sslSocket.setNeedClientAuth(false);
 
-        ArrayList<String> secureProtocols = new ArrayList<String>();
+        ArrayList<String> secureProtocols = new ArrayList<>();
         for (String p : sslSocket.getEnabledProtocols()) {
           if (!p.contains("SSLv3")) {
             secureProtocols.add(p);
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/ZKNamespaceManager.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/ZKNamespaceManager.java
index 36df002..6ae9637 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/ZKNamespaceManager.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/ZKNamespaceManager.java
@@ -55,7 +55,7 @@ public class ZKNamespaceManager extends ZooKeeperListener {
   public ZKNamespaceManager(ZooKeeperWatcher zkw) throws IOException {
     super(zkw);
     nsZNode = zkw.znodePaths.namespaceZNode;
-    cache = new ConcurrentSkipListMap<String, NamespaceDescriptor>();
+    cache = new ConcurrentSkipListMap<>();
   }
 
   public void start() throws IOException {
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/backup/HFileArchiver.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/backup/HFileArchiver.java
index ee32887..52185f1 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/backup/HFileArchiver.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/backup/HFileArchiver.java
@@ -126,7 +126,7 @@ public class HFileArchiver {
     // otherwise, we attempt to archive the store files
 
     // build collection of just the store directories to archive
-    Collection<File> toArchive = new ArrayList<File>();
+    Collection<File> toArchive = new ArrayList<>();
     final PathFilter dirFilter = new FSUtils.DirFilter(fs);
     PathFilter nonHidden = new PathFilter() {
       @Override
@@ -324,7 +324,7 @@ public class HFileArchiver {
       if (LOG.isTraceEnabled()) LOG.trace("Created archive directory:" + baseArchiveDir);
     }
 
-    List<File> failures = new ArrayList<File>();
+    List<File> failures = new ArrayList<>();
     String startTime = Long.toString(start);
     for (File file : toArchive) {
       // if its a file archive it
@@ -475,7 +475,7 @@ public class HFileArchiver {
   private static void deleteStoreFilesWithoutArchiving(Collection<StoreFile> compactedFiles)
       throws IOException {
     LOG.debug("Deleting store files without archiving.");
-    List<IOException> errors = new ArrayList<IOException>(0);
+    List<IOException> errors = new ArrayList<>(0);
     for (StoreFile hsf : compactedFiles) {
       try {
         hsf.deleteReader();
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/backup/example/HFileArchiveTableMonitor.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/backup/example/HFileArchiveTableMonitor.java
index 3258cbb..3a16534 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/backup/example/HFileArchiveTableMonitor.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/backup/example/HFileArchiveTableMonitor.java
@@ -32,7 +32,7 @@ import org.apache.commons.logging.LogFactory;
  */
 public class HFileArchiveTableMonitor {
   private static final Log LOG = LogFactory.getLog(HFileArchiveTableMonitor.class);
-  private final Set<String> archivedTables = new TreeSet<String>();
+  private final Set<String> archivedTables = new TreeSet<>();
 
   /**
    * Set the tables to be archived. Internally adds each table and attempts to
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/client/ClientSideRegionScanner.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/client/ClientSideRegionScanner.java
index dde2f10..8ff118e 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/client/ClientSideRegionScanner.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/client/ClientSideRegionScanner.java
@@ -61,7 +61,7 @@ public class ClientSideRegionScanner extends AbstractClientScanner {
 
     // create an internal region scanner
     this.scanner = region.getScanner(scan);
-    values = new ArrayList<Cell>();
+    values = new ArrayList<>();
 
     if (scanMetrics == null) {
       initScanMetrics(scan);
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/client/HTableWrapper.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/client/HTableWrapper.java
index 6a73261..051a8f2 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/client/HTableWrapper.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/client/HTableWrapper.java
@@ -84,7 +84,7 @@ public final class HTableWrapper implements Table {
   }
 
   public void internalClose() throws IOException {
-    List<IOException> exceptions = new ArrayList<IOException>(2);
+    List<IOException> exceptions = new ArrayList<>(2);
     try {
       table.close();
     } catch (IOException e) {
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/client/TableSnapshotScanner.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/client/TableSnapshotScanner.java
index 4601ae4..49a718c 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/client/TableSnapshotScanner.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/client/TableSnapshotScanner.java
@@ -127,7 +127,7 @@ public class TableSnapshotScanner extends AbstractClientScanner {
     final List<HRegionInfo> restoredRegions = meta.getRegionsToAdd();
 
     htd = meta.getTableDescriptor();
-    regions = new ArrayList<HRegionInfo>(restoredRegions.size());
+    regions = new ArrayList<>(restoredRegions.size());
     for (HRegionInfo hri: restoredRegions) {
       if (CellUtil.overlappingKeys(scan.getStartRow(), scan.getStopRow(),
           hri.getStartKey(), hri.getEndKey())) {
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/constraint/ConstraintProcessor.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/constraint/ConstraintProcessor.java
index 9eaecd3..f217641 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/constraint/ConstraintProcessor.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/constraint/ConstraintProcessor.java
@@ -48,7 +48,7 @@ public class ConstraintProcessor implements RegionObserver {
 
   private final ClassLoader classloader;
 
-  private List<? extends Constraint> constraints = new ArrayList<Constraint>();
+  private List<? extends Constraint> constraints = new ArrayList<>();
 
   /**
    * Create the constraint processor.
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/constraint/Constraints.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/constraint/Constraints.java
index 09c935d..5ed9aa8 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/constraint/Constraints.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/constraint/Constraints.java
@@ -120,7 +120,7 @@ public final class Constraints {
     disable(desc);
 
     // remove all the constraint settings
-    List<Bytes> keys = new ArrayList<Bytes>();
+    List<Bytes> keys = new ArrayList<>();
     // loop through all the key, values looking for constraints
     for (Map.Entry<Bytes, Bytes> e : desc
         .getValues().entrySet()) {
@@ -165,7 +165,7 @@ public final class Constraints {
     String key = serializeConstraintClass(clazz);
     String value = desc.getValue(key);
 
-    return value == null ? null : new Pair<String, String>(key, value);
+    return value == null ? null : new Pair<>(key, value);
   }
 
   /**
@@ -557,7 +557,7 @@ public final class Constraints {
    */
   static List<? extends Constraint> getConstraints(HTableDescriptor desc,
       ClassLoader classloader) throws IOException {
-    List<Constraint> constraints = new ArrayList<Constraint>();
+    List<Constraint> constraints = new ArrayList<>();
     // loop through all the key, values looking for constraints
     for (Map.Entry<Bytes, Bytes> e : desc
         .getValues().entrySet()) {
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/coordination/ZKSplitLogManagerCoordination.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/coordination/ZKSplitLogManagerCoordination.java
index a226eb6..1654c67 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/coordination/ZKSplitLogManagerCoordination.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/coordination/ZKSplitLogManagerCoordination.java
@@ -761,7 +761,7 @@ public class ZKSplitLogManagerCoordination extends ZooKeeperListener implements
     if (taskOrRescanList == null || taskOrRescanList.isEmpty()) {
       return Collections.<String> emptyList();
     }
-    List<String> taskList = new ArrayList<String>();
+    List<String> taskList = new ArrayList<>();
     for (String taskOrRescan : taskOrRescanList) {
       // Remove rescan nodes
       if (!ZKSplitLog.isRescanNode(taskOrRescan)) {
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/coordination/ZkSplitLogWorkerCoordination.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/coordination/ZkSplitLogWorkerCoordination.java
index 2bf9d78..70445bd 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/coordination/ZkSplitLogWorkerCoordination.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/coordination/ZkSplitLogWorkerCoordination.java
@@ -449,7 +449,7 @@ public class ZkSplitLogWorkerCoordination extends ZooKeeperListener implements
             if (!recoveringRegions.isEmpty()) {
               // Make a local copy to prevent ConcurrentModificationException when other threads
               // modify recoveringRegions
-              List<String> tmpCopy = new ArrayList<String>(recoveringRegions.keySet());
+              List<String> tmpCopy = new ArrayList<>(recoveringRegions.keySet());
               int listSize = tmpCopy.size();
               for (int i = 0; i < listSize; i++) {
                 String region = tmpCopy.get(i);
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/coprocessor/CoprocessorHost.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/coprocessor/CoprocessorHost.java
index 1d58bf9..bdface1 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/coprocessor/CoprocessorHost.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/coprocessor/CoprocessorHost.java
@@ -81,8 +81,7 @@ public abstract class CoprocessorHost<E extends CoprocessorEnvironment> {
   private static final Log LOG = LogFactory.getLog(CoprocessorHost.class);
   protected Abortable abortable;
   /** Ordered set of loaded coprocessors with lock */
-  protected SortedList<E> coprocessors =
-      new SortedList<E>(new EnvironmentPriorityComparator());
+  protected SortedList<E> coprocessors = new SortedList<>(new EnvironmentPriorityComparator());
   protected Configuration conf;
   // unique file prefix to use for local copies of jars when classloading
   protected String pathPrefix;
@@ -118,7 +117,7 @@ public abstract class CoprocessorHost<E extends CoprocessorEnvironment> {
    * to master).
    */
   public Set<String> getCoprocessors() {
-    Set<String> returnValue = new TreeSet<String>();
+    Set<String> returnValue = new TreeSet<>();
     for (CoprocessorEnvironment e: coprocessors) {
       returnValue.add(e.getInstance().getClass().getSimpleName());
     }
@@ -318,7 +317,7 @@ public abstract class CoprocessorHost<E extends CoprocessorEnvironment> {
    * @return the list of coprocessors, or null if not found
    */
   public <T extends Coprocessor> List<T> findCoprocessors(Class<T> cls) {
-    ArrayList<T> ret = new ArrayList<T>();
+    ArrayList<T> ret = new ArrayList<>();
 
     for (E env: coprocessors) {
       Coprocessor cp = env.getInstance();
@@ -338,7 +337,7 @@ public abstract class CoprocessorHost<E extends CoprocessorEnvironment> {
    * @return the list of CoprocessorEnvironment, or null if not found
    */
   public List<CoprocessorEnvironment> findCoprocessorEnvironment(Class<?> cls) {
-    ArrayList<CoprocessorEnvironment> ret = new ArrayList<CoprocessorEnvironment>();
+    ArrayList<CoprocessorEnvironment> ret = new ArrayList<>();
 
     for (E env: coprocessors) {
       Coprocessor cp = env.getInstance();
@@ -373,7 +372,7 @@ public abstract class CoprocessorHost<E extends CoprocessorEnvironment> {
    * @return A set of ClassLoader instances
    */
   Set<ClassLoader> getExternalClassLoaders() {
-    Set<ClassLoader> externalClassLoaders = new HashSet<ClassLoader>();
+    Set<ClassLoader> externalClassLoaders = new HashSet<>();
     final ClassLoader systemClassLoader = this.getClass().getClassLoader();
     for (E env : coprocessors) {
       ClassLoader cl = env.getInstance().getClass().getClassLoader();
@@ -664,7 +663,7 @@ public abstract class CoprocessorHost<E extends CoprocessorEnvironment> {
    * Used to limit legacy handling to once per Coprocessor class per classloader.
    */
   private static final Set<Class<? extends Coprocessor>> legacyWarning =
-      new ConcurrentSkipListSet<Class<? extends Coprocessor>>(
+      new ConcurrentSkipListSet<>(
           new Comparator<Class<? extends Coprocessor>>() {
             @Override
             public int compare(Class<? extends Coprocessor> c1, Class<? extends Coprocessor> c2) {
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/coprocessor/MultiRowMutationEndpoint.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/coprocessor/MultiRowMutationEndpoint.java
index 834b54c..3773fa6 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/coprocessor/MultiRowMutationEndpoint.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/coprocessor/MultiRowMutationEndpoint.java
@@ -86,9 +86,9 @@ CoprocessorService, Coprocessor {
     MutateRowsResponse response = MutateRowsResponse.getDefaultInstance();
     try {
       // set of rows to lock, sorted to avoid deadlocks
-      SortedSet<byte[]> rowsToLock = new TreeSet<byte[]>(Bytes.BYTES_COMPARATOR);
+      SortedSet<byte[]> rowsToLock = new TreeSet<>(Bytes.BYTES_COMPARATOR);
       List<MutationProto> mutateRequestList = request.getMutationRequestList();
-      List<Mutation> mutations = new ArrayList<Mutation>(mutateRequestList.size());
+      List<Mutation> mutations = new ArrayList<>(mutateRequestList.size());
       for (MutationProto m : mutateRequestList) {
         mutations.add(ProtobufUtil.toMutation(m));
       }
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/coprocessor/ObserverContext.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/coprocessor/ObserverContext.java
index 52f2b95..fc80768 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/coprocessor/ObserverContext.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/coprocessor/ObserverContext.java
@@ -124,7 +124,7 @@ public class ObserverContext<E extends CoprocessorEnvironment> {
   public static <T extends CoprocessorEnvironment> ObserverContext<T> createAndPrepare(
       T env, ObserverContext<T> context) {
     if (context == null) {
-      context = new ObserverContext<T>(RpcServer.getRequestUser());
+      context = new ObserverContext<>(RpcServer.getRequestUser());
     }
     context.prepare(env);
     return context;
@@ -146,7 +146,7 @@ public class ObserverContext<E extends CoprocessorEnvironment> {
   public static <T extends CoprocessorEnvironment> ObserverContext<T> createAndPrepare(
       T env, ObserverContext<T> context, User user) {
     if (context == null) {
-      context = new ObserverContext<T>(user);
+      context = new ObserverContext<>(user);
     }
     context.prepare(env);
     return context;
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/errorhandling/ForeignException.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/errorhandling/ForeignException.java
index bfcf486..a00ccd9 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/errorhandling/ForeignException.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/errorhandling/ForeignException.java
@@ -107,8 +107,7 @@ public class ForeignException extends IOException {
     // if there is no stack trace, ignore it and just return the message
     if (trace == null) return null;
     // build the stack trace for the message
-    List<StackTraceElementMessage> pbTrace =
-        new ArrayList<StackTraceElementMessage>(trace.length);
+    List<StackTraceElementMessage> pbTrace = new ArrayList<>(trace.length);
     for (StackTraceElement elem : trace) {
       StackTraceElementMessage.Builder stackBuilder = StackTraceElementMessage.newBuilder();
       stackBuilder.setDeclaringClass(elem.getClassName());
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/errorhandling/ForeignExceptionDispatcher.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/errorhandling/ForeignExceptionDispatcher.java
index f5fc979..f339e9e 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/errorhandling/ForeignExceptionDispatcher.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/errorhandling/ForeignExceptionDispatcher.java
@@ -42,8 +42,7 @@ import org.apache.hadoop.hbase.classification.InterfaceAudience;
 public class ForeignExceptionDispatcher implements ForeignExceptionListener, ForeignExceptionSnare {
   private static final Log LOG = LogFactory.getLog(ForeignExceptionDispatcher.class);
   protected final String name;
-  protected final List<ForeignExceptionListener> listeners =
-      new ArrayList<ForeignExceptionListener>();
+  protected final List<ForeignExceptionListener> listeners = new ArrayList<>();
   private ForeignException exception;
 
   public ForeignExceptionDispatcher(String name) {
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/executor/ExecutorService.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/executor/ExecutorService.java
index 403244f..df7653f 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/executor/ExecutorService.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/executor/ExecutorService.java
@@ -59,8 +59,7 @@ public class ExecutorService {
   private static final Log LOG = LogFactory.getLog(ExecutorService.class);
 
   // hold the all the executors created in a map addressable by their names
-  private final ConcurrentHashMap<String, Executor> executorMap =
-    new ConcurrentHashMap<String, Executor>();
+  private final ConcurrentHashMap<String, Executor> executorMap = new ConcurrentHashMap<>();
 
   // Name of the server hosting this executor service.
   private final String servername;
@@ -164,7 +163,7 @@ public class ExecutorService {
     // the thread pool executor that services the requests
     final TrackingThreadPoolExecutor threadPoolExecutor;
     // work queue to use - unbounded queue
-    final BlockingQueue<Runnable> q = new LinkedBlockingQueue<Runnable>();
+    final BlockingQueue<Runnable> q = new LinkedBlockingQueue<>();
     private final String name;
     private static final AtomicLong seqids = new AtomicLong(0);
     private final long id;
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/favored/FavoredNodeAssignmentHelper.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/favored/FavoredNodeAssignmentHelper.java
index 625d01f..48745ca 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/favored/FavoredNodeAssignmentHelper.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/favored/FavoredNodeAssignmentHelper.java
@@ -85,9 +85,9 @@ public class FavoredNodeAssignmentHelper {
       final RackManager rackManager) {
     this.servers = servers;
     this.rackManager = rackManager;
-    this.rackToRegionServerMap = new HashMap<String, List<ServerName>>();
-    this.regionServerToRackMap = new HashMap<String, String>();
-    this.uniqueRackList = new ArrayList<String>();
+    this.rackToRegionServerMap = new HashMap<>();
+    this.regionServerToRackMap = new HashMap<>();
+    this.uniqueRackList = new ArrayList<>();
     this.random = new Random();
   }
 
@@ -122,7 +122,7 @@ public class FavoredNodeAssignmentHelper {
   public static void updateMetaWithFavoredNodesInfo(
       Map<HRegionInfo, List<ServerName>> regionToFavoredNodes,
       Connection connection) throws IOException {
-    List<Put> puts = new ArrayList<Put>();
+    List<Put> puts = new ArrayList<>();
     for (Map.Entry<HRegionInfo, List<ServerName>> entry : regionToFavoredNodes.entrySet()) {
       Put put = makePutFromRegionInfo(entry.getKey(), entry.getValue());
       if (put != null) {
@@ -142,7 +142,7 @@ public class FavoredNodeAssignmentHelper {
   public static void updateMetaWithFavoredNodesInfo(
       Map<HRegionInfo, List<ServerName>> regionToFavoredNodes,
       Configuration conf) throws IOException {
-    List<Put> puts = new ArrayList<Put>();
+    List<Put> puts = new ArrayList<>();
     for (Map.Entry<HRegionInfo, List<ServerName>> entry : regionToFavoredNodes.entrySet()) {
       Put put = makePutFromRegionInfo(entry.getKey(), entry.getValue());
       if (put != null) {
@@ -226,7 +226,7 @@ public class FavoredNodeAssignmentHelper {
   // The regions should be distributed proportionately to the racksizes
   void placePrimaryRSAsRoundRobin(Map<ServerName, List<HRegionInfo>> assignmentMap,
       Map<HRegionInfo, ServerName> primaryRSMap, List<HRegionInfo> regions) {
-    List<String> rackList = new ArrayList<String>(rackToRegionServerMap.size());
+    List<String> rackList = new ArrayList<>(rackToRegionServerMap.size());
     rackList.addAll(rackToRegionServerMap.keySet());
     int rackIndex = random.nextInt(rackList.size());
     int maxRackSize = 0;
@@ -266,7 +266,7 @@ public class FavoredNodeAssignmentHelper {
       if (assignmentMap != null) {
         List<HRegionInfo> regionsForServer = assignmentMap.get(currentServer);
         if (regionsForServer == null) {
-          regionsForServer = new ArrayList<HRegionInfo>();
+          regionsForServer = new ArrayList<>();
           assignmentMap.put(currentServer, regionsForServer);
         }
         regionsForServer.add(regionInfo);
@@ -284,8 +284,7 @@ public class FavoredNodeAssignmentHelper {
 
   Map<HRegionInfo, ServerName[]> placeSecondaryAndTertiaryRS(
       Map<HRegionInfo, ServerName> primaryRSMap) {
-    Map<HRegionInfo, ServerName[]> secondaryAndTertiaryMap =
-        new HashMap<HRegionInfo, ServerName[]>();
+    Map<HRegionInfo, ServerName[]> secondaryAndTertiaryMap = new HashMap<>();
     for (Map.Entry<HRegionInfo, ServerName> entry : primaryRSMap.entrySet()) {
       // Get the target region and its primary region server rack
       HRegionInfo regionInfo = entry.getKey();
@@ -317,12 +316,11 @@ public class FavoredNodeAssignmentHelper {
 
   private Map<ServerName, Set<HRegionInfo>> mapRSToPrimaries(
       Map<HRegionInfo, ServerName> primaryRSMap) {
-    Map<ServerName, Set<HRegionInfo>> primaryServerMap =
-        new HashMap<ServerName, Set<HRegionInfo>>();
+    Map<ServerName, Set<HRegionInfo>> primaryServerMap = new HashMap<>();
     for (Entry<HRegionInfo, ServerName> e : primaryRSMap.entrySet()) {
       Set<HRegionInfo> currentSet = primaryServerMap.get(e.getValue());
       if (currentSet == null) {
-        currentSet = new HashSet<HRegionInfo>();
+        currentSet = new HashSet<>();
       }
       currentSet.add(e.getKey());
       primaryServerMap.put(e.getValue(), currentSet);
@@ -341,8 +339,7 @@ public class FavoredNodeAssignmentHelper {
       Map<HRegionInfo, ServerName> primaryRSMap) {
     Map<ServerName, Set<HRegionInfo>> serverToPrimaries =
         mapRSToPrimaries(primaryRSMap);
-    Map<HRegionInfo, ServerName[]> secondaryAndTertiaryMap =
-        new HashMap<HRegionInfo, ServerName[]>();
+    Map<HRegionInfo, ServerName[]> secondaryAndTertiaryMap = new HashMap<>();
 
     for (Entry<HRegionInfo, ServerName> entry : primaryRSMap.entrySet()) {
       // Get the target region and its primary region server rack
@@ -381,11 +378,11 @@ public class FavoredNodeAssignmentHelper {
     // Random to choose the secondary and tertiary region server
     // from another rack to place the secondary and tertiary
     // Random to choose one rack except for the current rack
-    Set<String> rackSkipSet = new HashSet<String>();
+    Set<String> rackSkipSet = new HashSet<>();
     rackSkipSet.add(primaryRack);
     String secondaryRack = getOneRandomRack(rackSkipSet);
     List<ServerName> serverList = getServersFromRack(secondaryRack);
-    Set<ServerName> serverSet = new HashSet<ServerName>();
+    Set<ServerName> serverSet = new HashSet<>();
     serverSet.addAll(serverList);
     ServerName[] favoredNodes;
     if (serverList.size() >= 2) {
@@ -393,7 +390,7 @@ public class FavoredNodeAssignmentHelper {
       // Skip the secondary for the tertiary placement
       // skip the servers which share the primary already
       Set<HRegionInfo> primaries = serverToPrimaries.get(primaryRS);
-      Set<ServerName> skipServerSet = new HashSet<ServerName>();
+      Set<ServerName> skipServerSet = new HashSet<>();
       while (true) {
         ServerName[] secondaryAndTertiary = null;
         if (primaries.size() > 1) {
@@ -423,7 +420,7 @@ public class FavoredNodeAssignmentHelper {
         }
         secondaryRack = getOneRandomRack(rackSkipSet);
         serverList = getServersFromRack(secondaryRack);
-        serverSet = new HashSet<ServerName>();
+        serverSet = new HashSet<>();
         serverSet.addAll(serverList);
       }
 
@@ -452,7 +449,7 @@ public class FavoredNodeAssignmentHelper {
       // Pick the tertiary
       if (getTotalNumberOfRacks() == 2) {
         // Pick the tertiary from the same rack of the primary RS
-        Set<ServerName> serverSkipSet = new HashSet<ServerName>();
+        Set<ServerName> serverSkipSet = new HashSet<>();
         serverSkipSet.add(primaryRS);
         favoredNodes[1] = getOneRandomServer(primaryRack, serverSkipSet);
       } else {
@@ -478,7 +475,7 @@ public class FavoredNodeAssignmentHelper {
     } else {
       // Randomly select two region servers from the server list and make sure
       // they are not overlap with the primary region server;
-     Set<ServerName> serverSkipSet = new HashSet<ServerName>();
+     Set<ServerName> serverSkipSet = new HashSet<>();
      serverSkipSet.add(primaryRS);
 
      // Place the secondary RS
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/favored/FavoredNodeLoadBalancer.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/favored/FavoredNodeLoadBalancer.java
index f0af0d0..6e7bf0e 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/favored/FavoredNodeLoadBalancer.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/favored/FavoredNodeLoadBalancer.java
@@ -87,7 +87,7 @@ public class FavoredNodeLoadBalancer extends BaseLoadBalancer implements Favored
   @Override
   public List<RegionPlan> balanceCluster(Map<ServerName, List<HRegionInfo>> clusterState)  {
     //TODO. Look at is whether Stochastic loadbalancer can be integrated with this
-    List<RegionPlan> plans = new ArrayList<RegionPlan>();
+    List<RegionPlan> plans = new ArrayList<>();
     //perform a scan of the meta to get the latest updates (if any)
     SnapshotOfRegionAssignmentFromMeta snaphotOfRegionAssignment =
         new SnapshotOfRegionAssignmentFromMeta(super.services.getConnection());
@@ -97,10 +97,8 @@ public class FavoredNodeLoadBalancer extends BaseLoadBalancer implements Favored
       LOG.warn("Not running balancer since exception was thrown " + ie);
       return plans;
     }
-    Map<ServerName, ServerName> serverNameToServerNameWithoutCode =
-        new HashMap<ServerName, ServerName>();
-    Map<ServerName, ServerName> serverNameWithoutCodeToServerName =
-        new HashMap<ServerName, ServerName>();
+    Map<ServerName, ServerName> serverNameToServerNameWithoutCode = new HashMap<>();
+    Map<ServerName, ServerName> serverNameWithoutCodeToServerName = new HashMap<>();
     ServerManager serverMgr = super.services.getServerManager();
     for (ServerName sn: serverMgr.getOnlineServersList()) {
       ServerName s = ServerName.valueOf(sn.getHostname(), sn.getPort(), ServerName.NON_STARTCODE);
@@ -189,7 +187,7 @@ public class FavoredNodeLoadBalancer extends BaseLoadBalancer implements Favored
           segregateRegionsAndAssignRegionsWithFavoredNodes(regions, servers);
       Map<ServerName,List<HRegionInfo>> regionsWithFavoredNodesMap = segregatedRegions.getFirst();
       List<HRegionInfo> regionsWithNoFavoredNodes = segregatedRegions.getSecond();
-      assignmentMap = new HashMap<ServerName, List<HRegionInfo>>();
+      assignmentMap = new HashMap<>();
       roundRobinAssignmentImpl(assignmentHelper, assignmentMap, regionsWithNoFavoredNodes,
           servers);
       // merge the assignment maps
@@ -225,9 +223,9 @@ public class FavoredNodeLoadBalancer extends BaseLoadBalancer implements Favored
           }
         }
       }
-      List<HRegionInfo> regions = new ArrayList<HRegionInfo>(1);
+      List<HRegionInfo> regions = new ArrayList<>(1);
       regions.add(regionInfo);
-      Map<HRegionInfo, ServerName> primaryRSMap = new HashMap<HRegionInfo, ServerName>(1);
+      Map<HRegionInfo, ServerName> primaryRSMap = new HashMap<>(1);
       primaryRSMap.put(regionInfo, primary);
       assignSecondaryAndTertiaryNodesForRegion(assignmentHelper, regions, primaryRSMap);
       return primary;
@@ -241,9 +239,8 @@ public class FavoredNodeLoadBalancer extends BaseLoadBalancer implements Favored
   private Pair<Map<ServerName, List<HRegionInfo>>, List<HRegionInfo>>
   segregateRegionsAndAssignRegionsWithFavoredNodes(List<HRegionInfo> regions,
       List<ServerName> availableServers) {
-    Map<ServerName, List<HRegionInfo>> assignmentMapForFavoredNodes =
-        new HashMap<ServerName, List<HRegionInfo>>(regions.size() / 2);
-    List<HRegionInfo> regionsWithNoFavoredNodes = new ArrayList<HRegionInfo>(regions.size()/2);
+    Map<ServerName, List<HRegionInfo>> assignmentMapForFavoredNodes = new HashMap<>(regions.size() / 2);
+    List<HRegionInfo> regionsWithNoFavoredNodes = new ArrayList<>(regions.size()/2);
     for (HRegionInfo region : regions) {
       List<ServerName> favoredNodes = fnm.getFavoredNodes(region);
       ServerName primaryHost = null;
@@ -272,8 +269,7 @@ public class FavoredNodeLoadBalancer extends BaseLoadBalancer implements Favored
         regionsWithNoFavoredNodes.add(region);
       }
     }
-    return new Pair<Map<ServerName, List<HRegionInfo>>, List<HRegionInfo>>(
-        assignmentMapForFavoredNodes, regionsWithNoFavoredNodes);
+    return new Pair<>(assignmentMapForFavoredNodes, regionsWithNoFavoredNodes);
   }
 
   // Do a check of the hostname and port and return the servername from the servers list
@@ -316,7 +312,7 @@ public class FavoredNodeLoadBalancer extends BaseLoadBalancer implements Favored
       HRegionInfo region, ServerName host) {
     List<HRegionInfo> regionsOnServer = null;
     if ((regionsOnServer = assignmentMapForFavoredNodes.get(host)) == null) {
-      regionsOnServer = new ArrayList<HRegionInfo>();
+      regionsOnServer = new ArrayList<>();
       assignmentMapForFavoredNodes.put(host, regionsOnServer);
     }
     regionsOnServer.add(region);
@@ -329,7 +325,7 @@ public class FavoredNodeLoadBalancer extends BaseLoadBalancer implements Favored
   private void roundRobinAssignmentImpl(FavoredNodeAssignmentHelper assignmentHelper,
       Map<ServerName, List<HRegionInfo>> assignmentMap,
       List<HRegionInfo> regions, List<ServerName> servers) throws IOException {
-    Map<HRegionInfo, ServerName> primaryRSMap = new HashMap<HRegionInfo, ServerName>();
+    Map<HRegionInfo, ServerName> primaryRSMap = new HashMap<>();
     // figure the primary RSs
     assignmentHelper.placePrimaryRSAsRoundRobin(assignmentMap, primaryRSMap, regions);
     assignSecondaryAndTertiaryNodesForRegion(assignmentHelper, regions, primaryRSMap);
@@ -347,7 +343,7 @@ public class FavoredNodeLoadBalancer extends BaseLoadBalancer implements Favored
     for (HRegionInfo region : regions) {
       // Store the favored nodes without startCode for the ServerName objects
       // We don't care about the startcode; but only the hostname really
-      List<ServerName> favoredNodesForRegion = new ArrayList<ServerName>(3);
+      List<ServerName> favoredNodesForRegion = new ArrayList<>(3);
       ServerName sn = primaryRSMap.get(region);
       favoredNodesForRegion.add(ServerName.valueOf(sn.getHostname(), sn.getPort(),
           ServerName.NON_STARTCODE));
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/favored/FavoredNodesPlan.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/favored/FavoredNodesPlan.java
index f24d9fc..ff6d9e1 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/favored/FavoredNodesPlan.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/favored/FavoredNodesPlan.java
@@ -46,7 +46,7 @@ public class FavoredNodesPlan {
   }
 
   public FavoredNodesPlan() {
-    favoredNodesMap = new ConcurrentHashMap<String, List<ServerName>>();
+    favoredNodesMap = new ConcurrentHashMap<>();
   }
 
   /**
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/http/HttpRequestLog.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/http/HttpRequestLog.java
index de53bd9..cfc0640 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/http/HttpRequestLog.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/http/HttpRequestLog.java
@@ -38,7 +38,7 @@ public class HttpRequestLog {
   private static final HashMap<String, String> serverToComponent;
 
   static {
-    serverToComponent = new HashMap<String, String>();
+    serverToComponent = new HashMap<>();
     serverToComponent.put("master", "master");
     serverToComponent.put("region", "regionserver");
   }
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/http/HttpServer.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/http/HttpServer.java
index 3ce2f09..c7e1153 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/http/HttpServer.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/http/HttpServer.java
@@ -163,9 +163,8 @@ public class HttpServer implements FilterContainer {
 
   protected final WebAppContext webAppContext;
   protected final boolean findPort;
-  protected final Map<ServletContextHandler, Boolean> defaultContexts =
-      new HashMap<ServletContextHandler, Boolean>();
-  protected final List<String> filterNames = new ArrayList<String>();
+  protected final Map<ServletContextHandler, Boolean> defaultContexts = new HashMap<>();
+  protected final List<String> filterNames = new ArrayList<>();
   static final String STATE_DESCRIPTION_ALIVE = " - alive";
   static final String STATE_DESCRIPTION_NOT_LIVE = " - not live";
 
@@ -555,7 +554,7 @@ public class HttpServer implements FilterContainer {
     addDefaultApps(contexts, appDir, conf);
 
     addGlobalFilter("safety", QuotingInputFilter.class.getName(), null);
-    Map<String, String> params = new HashMap<String, String>();
+    Map<String, String> params = new HashMap<>();
     params.put("xframeoptions", conf.get("hbase.http.filter.xframeoptions.mode", "DENY"));
     addGlobalFilter("clickjackingprevention",
             ClickjackingPreventionFilter.class.getName(), params);
@@ -906,7 +905,7 @@ public class HttpServer implements FilterContainer {
   private void initSpnego(Configuration conf, String hostName,
       String usernameConfKey, String keytabConfKey, String kerberosNameRuleKey,
       String signatureSecretKeyFileKey) throws IOException {
-    Map<String, String> params = new HashMap<String, String>();
+    Map<String, String> params = new HashMap<>();
     String principalInConf = getOrEmptyString(conf, usernameConfKey);
     if (!principalInConf.isEmpty()) {
       params.put(HTTP_SPNEGO_AUTHENTICATION_PRINCIPAL_SUFFIX, SecurityUtil.getServerPrincipal(
@@ -1302,7 +1301,7 @@ public class HttpServer implements FilterContainer {
 
       @Override
       public Map<String, String[]> getParameterMap() {
-        Map<String, String[]> result = new HashMap<String,String[]>();
+        Map<String, String[]> result = new HashMap<>();
         Map<String, String[]> raw = rawRequest.getParameterMap();
         for (Map.Entry<String,String[]> item: raw.entrySet()) {
           String[] rawValue = item.getValue();
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/http/lib/StaticUserWebFilter.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/http/lib/StaticUserWebFilter.java
index 710676d..7c3204b 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/http/lib/StaticUserWebFilter.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/http/lib/StaticUserWebFilter.java
@@ -124,7 +124,7 @@ public class StaticUserWebFilter extends FilterInitializer {
 
   @Override
   public void initFilter(FilterContainer container, Configuration conf) {
-    HashMap<String, String> options = new HashMap<String, String>();
+    HashMap<String, String> options = new HashMap<>();
     
     String username = getUsernameFromConf(conf);
     options.put(HBASE_HTTP_STATIC_USER, username);
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/io/FileLink.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/io/FileLink.java
index 3caf67f..ca0dfbc 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/io/FileLink.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/io/FileLink.java
@@ -426,7 +426,7 @@ public class FileLink {
   protected void setLocations(Path originPath, Path... alternativePaths) {
     assert this.locations == null : "Link locations already set";
 
-    List<Path> paths = new ArrayList<Path>(alternativePaths.length +1);
+    List<Path> paths = new ArrayList<>(alternativePaths.length +1);
     if (originPath != null) {
       paths.add(originPath);
     }
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/io/HFileLink.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/io/HFileLink.java
index 5128662..cdc5be1 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/io/HFileLink.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/io/HFileLink.java
@@ -509,7 +509,7 @@ public class HFileLink extends FileLink {
     String tableSubstr = name.substring(separatorIndex + 1)
         .replace('=', TableName.NAMESPACE_DELIM);
     TableName linkTableName = TableName.valueOf(tableSubstr);
-    return new Pair<TableName, String>(linkTableName, linkRegionName);
+    return new Pair<>(linkTableName, linkRegionName);
   }
 
   /**
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/io/asyncfs/FanOutOneBlockAsyncDFSOutput.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/io/asyncfs/FanOutOneBlockAsyncDFSOutput.java
index a6ee6da..c64cdf7 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/io/asyncfs/FanOutOneBlockAsyncDFSOutput.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/io/asyncfs/FanOutOneBlockAsyncDFSOutput.java
@@ -494,7 +494,7 @@ public class FanOutOneBlockAsyncDFSOutput implements AsyncFSOutput {
    * @return A CompletableFuture that hold the acked length after flushing.
    */
   public CompletableFuture<Long> flush(boolean syncBlock) {
-    CompletableFuture<Long> future = new CompletableFuture<Long>();
+    CompletableFuture<Long> future = new CompletableFuture<>();
     if (eventLoop.inEventLoop()) {
       flush0(future, syncBlock);
     } else {
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/io/asyncfs/FanOutOneBlockAsyncDFSOutputHelper.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/io/asyncfs/FanOutOneBlockAsyncDFSOutputHelper.java
index 875ff77..3eaacc4 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/io/asyncfs/FanOutOneBlockAsyncDFSOutputHelper.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/io/asyncfs/FanOutOneBlockAsyncDFSOutputHelper.java
@@ -682,8 +682,7 @@ public final class FanOutOneBlockAsyncDFSOutputHelper {
     try {
       stat = namenode.create(src,
         FsPermission.getFileDefault().applyUMask(FsPermission.getUMask(conf)), clientName,
-        new EnumSetWritable<CreateFlag>(
-            overwrite ? EnumSet.of(CREATE, OVERWRITE) : EnumSet.of(CREATE)),
+        new EnumSetWritable<>(overwrite ? EnumSet.of(CREATE, OVERWRITE) : EnumSet.of(CREATE)),
         createParent, replication, blockSize, CryptoProtocolVersion.supported());
     } catch (Exception e) {
       if (e instanceof RemoteException) {
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/BlockCacheUtil.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/BlockCacheUtil.java
index 9335ef6..5c306c0 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/BlockCacheUtil.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/BlockCacheUtil.java
@@ -205,8 +205,7 @@ public class BlockCacheUtil {
     /**
      * Map by filename. use concurent utils because we want our Map and contained blocks sorted.
      */
-    private NavigableMap<String, NavigableSet<CachedBlock>> cachedBlockByFile =
-      new ConcurrentSkipListMap<String, NavigableSet<CachedBlock>>();
+    private NavigableMap<String, NavigableSet<CachedBlock>> cachedBlockByFile = new ConcurrentSkipListMap<>();
     FastLongHistogram hist = new FastLongHistogram();
 
     /**
@@ -217,7 +216,7 @@ public class BlockCacheUtil {
       if (isFull()) return true;
       NavigableSet<CachedBlock> set = this.cachedBlockByFile.get(cb.getFilename());
       if (set == null) {
-        set = new ConcurrentSkipListSet<CachedBlock>();
+        set = new ConcurrentSkipListSet<>();
         this.cachedBlockByFile.put(cb.getFilename(), set);
       }
       set.add(cb);
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/CacheableDeserializerIdManager.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/CacheableDeserializerIdManager.java
index 5d2d54a..3140150 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/CacheableDeserializerIdManager.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/CacheableDeserializerIdManager.java
@@ -30,8 +30,7 @@ import org.apache.hadoop.hbase.classification.InterfaceAudience;
  */
 @InterfaceAudience.Private
 public class CacheableDeserializerIdManager {
-  private static final Map<Integer, CacheableDeserializer<Cacheable>> registeredDeserializers =
-    new HashMap<Integer, CacheableDeserializer<Cacheable>>();
+  private static final Map<Integer, CacheableDeserializer<Cacheable>> registeredDeserializers = new HashMap<>();
   private static final AtomicInteger identifier = new AtomicInteger(0);
 
   /**
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/CompoundBloomFilterWriter.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/CompoundBloomFilterWriter.java
index a50566a..96dfcbd 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/CompoundBloomFilterWriter.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/CompoundBloomFilterWriter.java
@@ -70,7 +70,7 @@ public class CompoundBloomFilterWriter extends CompoundBloomFilterBase
     BloomFilterChunk chunk;
   }
 
-  private Queue<ReadyChunk> readyChunks = new LinkedList<ReadyChunk>();
+  private Queue<ReadyChunk> readyChunks = new LinkedList<>();
 
   /** The first key in the current Bloom filter chunk. */
   private byte[] firstKeyInChunk = null;
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/HFile.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/HFile.java
index 0e07d6e..c5b334a 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/HFile.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/HFile.java
@@ -638,7 +638,7 @@ public class HFile {
     static final byte [] COMPARATOR = Bytes.toBytes(RESERVED_PREFIX + "COMPARATOR");
     static final byte [] TAGS_COMPRESSED = Bytes.toBytes(RESERVED_PREFIX + "TAGS_COMPRESSED");
     public static final byte [] MAX_TAGS_LEN = Bytes.toBytes(RESERVED_PREFIX + "MAX_TAGS_LEN");
-    private final SortedMap<byte [], byte []> map = new TreeMap<byte [], byte []>(Bytes.BYTES_COMPARATOR);
+    private final SortedMap<byte [], byte []> map = new TreeMap<>(Bytes.BYTES_COMPARATOR);
 
     public FileInfo() {
       super();
@@ -894,7 +894,7 @@ public class HFile {
    */
   static List<Path> getStoreFiles(FileSystem fs, Path regionDir)
       throws IOException {
-    List<Path> regionHFiles = new ArrayList<Path>();
+    List<Path> regionHFiles = new ArrayList<>();
     PathFilter dirFilter = new FSUtils.DirFilter(fs);
     FileStatus[] familyDirs = fs.listStatus(regionDir, dirFilter);
     for(FileStatus dir : familyDirs) {
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/HFileBlock.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/HFileBlock.java
index 1970ade..fba15ba 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/HFileBlock.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/HFileBlock.java
@@ -1406,8 +1406,7 @@ public class HFileBlock implements Cacheable {
      * next blocks header seems unnecessary given we usually get the block size
      * from the hfile index. Review!
      */
-    private AtomicReference<PrefetchedHeader> prefetchedHeader =
-        new AtomicReference<PrefetchedHeader>(new PrefetchedHeader());
+    private AtomicReference<PrefetchedHeader> prefetchedHeader = new AtomicReference<>(new PrefetchedHeader());
 
     /** The size of the file we are reading from, or -1 if unknown. */
     protected long fileSize;
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/HFileBlockIndex.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/HFileBlockIndex.java
index 575c074..b36c292 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/HFileBlockIndex.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/HFileBlockIndex.java
@@ -239,7 +239,7 @@ public class HFileBlockIndex {
 
     private Cell[] blockKeys;
     /** Pre-computed mid-key */
-    private AtomicReference<Cell> midKey = new AtomicReference<Cell>();
+    private AtomicReference<Cell> midKey = new AtomicReference<>();
     /** Needed doing lookup on blocks. */
     private CellComparator comparator;
 
@@ -741,7 +741,7 @@ public class HFileBlockIndex {
       // keys[numEntries] = Infinity, then we are maintaining an invariant that
       // keys[low - 1] < key < keys[high + 1] while narrowing down the range.
       ByteBufferKeyOnlyKeyValue nonRootIndexkeyOnlyKV = new ByteBufferKeyOnlyKeyValue();
-      ObjectIntPair<ByteBuffer> pair = new ObjectIntPair<ByteBuffer>();
+      ObjectIntPair<ByteBuffer> pair = new ObjectIntPair<>();
       while (low <= high) {
         mid = (low + high) >>> 1;
 
@@ -1402,20 +1402,20 @@ public class HFileBlockIndex {
   static class BlockIndexChunk {
 
     /** First keys of the key range corresponding to each index entry. */
-    private final List<byte[]> blockKeys = new ArrayList<byte[]>();
+    private final List<byte[]> blockKeys = new ArrayList<>();
 
     /** Block offset in backing stream. */
-    private final List<Long> blockOffsets = new ArrayList<Long>();
+    private final List<Long> blockOffsets = new ArrayList<>();
 
     /** On-disk data sizes of lower-level data or index blocks. */
-    private final List<Integer> onDiskDataSizes = new ArrayList<Integer>();
+    private final List<Integer> onDiskDataSizes = new ArrayList<>();
 
     /**
      * The cumulative number of sub-entries, i.e. entries on deeper-level block
      * index entries. numSubEntriesAt[i] is the number of sub-entries in the
      * blocks corresponding to this chunk's entries #0 through #i inclusively.
      */
-    private final List<Long> numSubEntriesAt = new ArrayList<Long>();
+    private final List<Long> numSubEntriesAt = new ArrayList<>();
 
     /**
      * The offset of the next entry to be added, relative to the end of the
@@ -1434,8 +1434,7 @@ public class HFileBlockIndex {
      * records in a "non-root" format block. These offsets are relative to the
      * end of this secondary index.
      */
-    private final List<Integer> secondaryIndexOffsetMarks =
-        new ArrayList<Integer>();
+    private final List<Integer> secondaryIndexOffsetMarks = new ArrayList<>();
 
     /**
      * Adds a new entry to this block index chunk.
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/HFilePrettyPrinter.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/HFilePrettyPrinter.java
index 1710379..030a25e 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/HFilePrettyPrinter.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/HFilePrettyPrinter.java
@@ -122,7 +122,7 @@ public class HFilePrettyPrinter extends Configured implements Tool {
    */
   private byte[] row = null;
 
-  private List<Path> files = new ArrayList<Path>();
+  private List<Path> files = new ArrayList<>();
   private int count;
 
   private static final String FOUR_SPACES = "    ";
@@ -232,7 +232,7 @@ public class HFilePrettyPrinter extends Configured implements Tool {
       if (verbose) {
         System.out.println("checkMobIntegrity is enabled");
       }
-      mobFileLocations = new HashMap<String, List<Path>>();
+      mobFileLocations = new HashMap<>();
     }
 
     cmd.getArgList().forEach((file) -> files.add(new Path(file)));
@@ -372,8 +372,8 @@ public class HFilePrettyPrinter extends Configured implements Tool {
       HFileScanner scanner,  byte[] row) throws IOException {
     Cell pCell = null;
     FileSystem fs = FileSystem.get(getConf());
-    Set<String> foundMobFiles = new LinkedHashSet<String>(FOUND_MOB_FILES_CACHE_CAPACITY);
-    Set<String> missingMobFiles = new LinkedHashSet<String>(MISSING_MOB_FILES_CACHE_CAPACITY);
+    Set<String> foundMobFiles = new LinkedHashSet<>(FOUND_MOB_FILES_CACHE_CAPACITY);
+    Set<String> missingMobFiles = new LinkedHashSet<>(MISSING_MOB_FILES_CACHE_CAPACITY);
     do {
       Cell cell = scanner.getCell();
       if (row != null && row.length != 0) {
@@ -469,7 +469,7 @@ public class HFilePrettyPrinter extends Configured implements Tool {
     String tableName = tn.getNameAsString();
     List<Path> locations = mobFileLocations.get(tableName);
     if (locations == null) {
-      locations = new ArrayList<Path>(2);
+      locations = new ArrayList<>(2);
       locations.add(MobUtils.getMobFamilyPath(getConf(), tn, family));
       locations.add(HFileArchiveUtil.getStoreArchivePath(getConf(), tn,
         MobUtils.getMobRegionInfo(tn).getEncodedName(), family));
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/HFileReaderImpl.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/HFileReaderImpl.java
index c92d77d..4e8cbaa 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/HFileReaderImpl.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/HFileReaderImpl.java
@@ -138,7 +138,7 @@ public class HFileReaderImpl implements HFile.Reader, Configurable {
    * Blocks read from the load-on-open section, excluding data root index, meta
    * index, and file info.
    */
-  private List<HFileBlock> loadOnOpenBlocks = new ArrayList<HFileBlock>();
+  private List<HFileBlock> loadOnOpenBlocks = new ArrayList<>();
 
   /** Minimum minor version supported by this HFile format */
   static final int MIN_MINOR_VERSION = 0;
@@ -493,7 +493,7 @@ public class HFileReaderImpl implements HFile.Reader, Configurable {
     // buffer backed keyonlyKV
     private ByteBufferKeyOnlyKeyValue bufBackedKeyOnlyKv = new ByteBufferKeyOnlyKeyValue();
     // A pair for reusing in blockSeek() so that we don't garbage lot of objects
-    final ObjectIntPair<ByteBuffer> pair = new ObjectIntPair<ByteBuffer>();
+    final ObjectIntPair<ByteBuffer> pair = new ObjectIntPair<>();
 
     /**
      * The next indexed key is to keep track of the indexed key of the next data block.
@@ -506,7 +506,7 @@ public class HFileReaderImpl implements HFile.Reader, Configurable {
     // Current block being used
     protected HFileBlock curBlock;
     // Previous blocks that were used in the course of the read
-    protected final ArrayList<HFileBlock> prevBlocks = new ArrayList<HFileBlock>();
+    protected final ArrayList<HFileBlock> prevBlocks = new ArrayList<>();
 
     public HFileScannerImpl(final HFile.Reader reader, final boolean cacheBlocks,
         final boolean pread, final boolean isCompaction) {
@@ -975,7 +975,7 @@ public class HFileReaderImpl implements HFile.Reader, Configurable {
     public Cell getKey() {
       assertSeeked();
       // Create a new object so that this getKey is cached as firstKey, lastKey
-      ObjectIntPair<ByteBuffer> keyPair = new ObjectIntPair<ByteBuffer>();
+      ObjectIntPair<ByteBuffer> keyPair = new ObjectIntPair<>();
       blockBuffer.asSubByteBuffer(blockBuffer.position() + KEY_VALUE_LEN_SIZE, currKeyLen, keyPair);
       ByteBuffer keyBuf = keyPair.getFirst();
       if (keyBuf.hasArray()) {
@@ -996,7 +996,7 @@ public class HFileReaderImpl implements HFile.Reader, Configurable {
     public ByteBuffer getValue() {
       assertSeeked();
       // Okie to create new Pair. Not used in hot path
-      ObjectIntPair<ByteBuffer> valuePair = new ObjectIntPair<ByteBuffer>();
+      ObjectIntPair<ByteBuffer> valuePair = new ObjectIntPair<>();
       this.blockBuffer.asSubByteBuffer(blockBuffer.position() + KEY_VALUE_LEN_SIZE + currKeyLen,
         currValueLen, valuePair);
       ByteBuffer valBuf = valuePair.getFirst().duplicate();
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/HFileWriterImpl.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/HFileWriterImpl.java
index 8a2d238..6a20b99 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/HFileWriterImpl.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/HFileWriterImpl.java
@@ -91,10 +91,10 @@ public class HFileWriterImpl implements HFile.Writer {
   protected final CellComparator comparator;
 
   /** Meta block names. */
-  protected List<byte[]> metaNames = new ArrayList<byte[]>();
+  protected List<byte[]> metaNames = new ArrayList<>();
 
   /** {@link Writable}s representing meta block data. */
-  protected List<Writable> metaData = new ArrayList<Writable>();
+  protected List<Writable> metaData = new ArrayList<>();
 
   /**
    * First cell in a block.
@@ -132,7 +132,7 @@ public class HFileWriterImpl implements HFile.Writer {
   public static final int KEY_VALUE_VER_WITH_MEMSTORE = 1;
 
   /** Inline block writers for multi-level block index and compound Blooms. */
-  private List<InlineBlockWriter> inlineBlockWriters = new ArrayList<InlineBlockWriter>();
+  private List<InlineBlockWriter> inlineBlockWriters = new ArrayList<>();
 
   /** block writer */
   protected HFileBlock.Writer blockWriter;
@@ -153,7 +153,7 @@ public class HFileWriterImpl implements HFile.Writer {
   private Cell lastCellOfPreviousBlock = null;
 
   /** Additional data items to be written to the "load-on-open" section. */
-  private List<BlockWritable> additionalLoadOnOpenData = new ArrayList<BlockWritable>();
+  private List<BlockWritable> additionalLoadOnOpenData = new ArrayList<>();
 
   protected long maxMemstoreTS = 0;
 
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/PrefetchExecutor.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/PrefetchExecutor.java
index 61deef5..838fa41 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/PrefetchExecutor.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/PrefetchExecutor.java
@@ -42,8 +42,7 @@ public class PrefetchExecutor {
   private static final Log LOG = LogFactory.getLog(PrefetchExecutor.class);
 
   /** Futures for tracking block prefetch activity */
-  private static final Map<Path,Future<?>> prefetchFutures =
-    new ConcurrentSkipListMap<Path,Future<?>>();
+  private static final Map<Path,Future<?>> prefetchFutures = new ConcurrentSkipListMap<>();
   /** Executor pool shared among all HFiles for block prefetch */
   private static final ScheduledExecutorService prefetchExecutorPool;
   /** Delay before beginning prefetch */
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/bucket/BucketCache.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/bucket/BucketCache.java
index 1bcdfc4..cb23ca9 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/bucket/BucketCache.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/bucket/BucketCache.java
@@ -142,8 +142,7 @@ public class BucketCache implements BlockCache, HeapSize {
    * to the BucketCache.  It then updates the ramCache and backingMap accordingly.
    */
   @VisibleForTesting
-  final ArrayList<BlockingQueue<RAMQueueEntry>> writerQueues =
-      new ArrayList<BlockingQueue<RAMQueueEntry>>();
+  final ArrayList<BlockingQueue<RAMQueueEntry>> writerQueues = new ArrayList<>();
   @VisibleForTesting
   final WriterThread[] writerThreads;
 
@@ -151,7 +150,7 @@ public class BucketCache implements BlockCache, HeapSize {
   private volatile boolean freeInProgress = false;
   private final Lock freeSpaceLock = new ReentrantLock();
 
-  private UniqueIndexMap<Integer> deserialiserMap = new UniqueIndexMap<Integer>();
+  private UniqueIndexMap<Integer> deserialiserMap = new UniqueIndexMap<>();
 
   private final AtomicLong realCacheSize = new AtomicLong(0);
   private final AtomicLong heapSize = new AtomicLong(0);
@@ -191,7 +190,7 @@ public class BucketCache implements BlockCache, HeapSize {
   final IdReadWriteLock offsetLock = new IdReadWriteLock();
 
   private final NavigableSet<BlockCacheKey> blocksByHFile =
-      new ConcurrentSkipListSet<BlockCacheKey>(new Comparator<BlockCacheKey>() {
+      new ConcurrentSkipListSet<>(new Comparator<BlockCacheKey>() {
         @Override
         public int compare(BlockCacheKey a, BlockCacheKey b) {
           int nameComparison = a.getHfileName().compareTo(b.getHfileName());
@@ -240,13 +239,13 @@ public class BucketCache implements BlockCache, HeapSize {
 
     bucketAllocator = new BucketAllocator(capacity, bucketSizes);
     for (int i = 0; i < writerThreads.length; ++i) {
-      writerQueues.add(new ArrayBlockingQueue<RAMQueueEntry>(writerQLen));
+      writerQueues.add(new ArrayBlockingQueue<>(writerQLen));
     }
 
     assert writerQueues.size() == writerThreads.length;
-    this.ramCache = new ConcurrentHashMap<BlockCacheKey, RAMQueueEntry>();
+    this.ramCache = new ConcurrentHashMap<>();
 
-    this.backingMap = new ConcurrentHashMap<BlockCacheKey, BucketEntry>((int) blockNumCapacity);
+    this.backingMap = new ConcurrentHashMap<>((int) blockNumCapacity);
 
     if (ioEngine.isPersistent() && persistencePath != null) {
       try {
@@ -756,7 +755,7 @@ public class BucketCache implements BlockCache, HeapSize {
         }
       }
 
-      PriorityQueue<BucketEntryGroup> bucketQueue = new PriorityQueue<BucketEntryGroup>(3);
+      PriorityQueue<BucketEntryGroup> bucketQueue = new PriorityQueue<>(3);
 
       bucketQueue.add(bucketSingle);
       bucketQueue.add(bucketMulti);
@@ -841,7 +840,7 @@ public class BucketCache implements BlockCache, HeapSize {
     }
 
     public void run() {
-      List<RAMQueueEntry> entries = new ArrayList<RAMQueueEntry>();
+      List<RAMQueueEntry> entries = new ArrayList<>();
       try {
         while (cacheEnabled && writerEnabled) {
           try {
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/bucket/UniqueIndexMap.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/bucket/UniqueIndexMap.java
index 9a72c4e..a3003c9 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/bucket/UniqueIndexMap.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/bucket/UniqueIndexMap.java
@@ -32,8 +32,8 @@ import org.apache.hadoop.hbase.classification.InterfaceAudience;
 public final class UniqueIndexMap<T> implements Serializable {
   private static final long serialVersionUID = -1145635738654002342L;
 
-  ConcurrentHashMap<T, Integer> mForwardMap = new ConcurrentHashMap<T, Integer>();
-  ConcurrentHashMap<Integer, T> mReverseMap = new ConcurrentHashMap<Integer, T>();
+  ConcurrentHashMap<T, Integer> mForwardMap = new ConcurrentHashMap<>();
+  ConcurrentHashMap<Integer, T> mReverseMap = new ConcurrentHashMap<>();
   AtomicInteger mIndex = new AtomicInteger(0);
 
   // Map a length to an index. If we can't, allocate a new mapping. We might
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/io/util/MemorySizeUtil.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/io/util/MemorySizeUtil.java
index 100f751..cf99f8b 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/io/util/MemorySizeUtil.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/io/util/MemorySizeUtil.java
@@ -169,7 +169,7 @@ public class MemorySizeUtil {
       if (MemStoreLAB.isEnabled(conf)) {
         // We are in offheap Memstore use
         long globalMemStoreLimit = (long) (offheapMSGlobal * 1024 * 1024); // Size in bytes
-        return new Pair<Long, MemoryType>(globalMemStoreLimit, MemoryType.NON_HEAP);
+        return new Pair<>(globalMemStoreLimit, MemoryType.NON_HEAP);
       } else {
         // Off heap max memstore size is configured with turning off MSLAB. It makes no sense. Do a
         // warn log and go with on heap memstore percentage. By default it will be 40% of Xmx
@@ -178,7 +178,7 @@ public class MemorySizeUtil {
             + " Going with on heap global memstore size ('" + MEMSTORE_SIZE_KEY + "')");
       }
     }
-    return new Pair<Long, MemoryType>(getOnheapGlobalMemstoreSize(conf), MemoryType.HEAP);
+    return new Pair<>(getOnheapGlobalMemstoreSize(conf), MemoryType.HEAP);
   }
 
   /**
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/ipc/FifoRpcScheduler.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/ipc/FifoRpcScheduler.java
index a9b6fd1..4ebfcd9 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/ipc/FifoRpcScheduler.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/ipc/FifoRpcScheduler.java
@@ -60,7 +60,7 @@ public class FifoRpcScheduler extends RpcScheduler {
         handlerCount,
         60,
         TimeUnit.SECONDS,
-        new ArrayBlockingQueue<Runnable>(maxQueueLength),
+        new ArrayBlockingQueue<>(maxQueueLength),
         new DaemonThreadFactory("FifoRpcScheduler.handler"),
         new ThreadPoolExecutor.CallerRunsPolicy());
   }
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/ipc/RpcServer.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/ipc/RpcServer.java
index 7813bf4..4b0c974 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/ipc/RpcServer.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/ipc/RpcServer.java
@@ -145,12 +145,10 @@ public abstract class RpcServer implements RpcServerInterface,
   /** This is set to Call object before Handler invokes an RPC and ybdie
    * after the call returns.
    */
-  protected static final ThreadLocal<RpcCall> CurCall =
-      new ThreadLocal<RpcCall>();
+  protected static final ThreadLocal<RpcCall> CurCall = new ThreadLocal<>();
 
   /** Keeps MonitoredRPCHandler per handler thread. */
-  protected static final ThreadLocal<MonitoredRPCHandler> MONITORED_RPC
-      = new ThreadLocal<MonitoredRPCHandler>();
+  protected static final ThreadLocal<MonitoredRPCHandler> MONITORED_RPC = new ThreadLocal<>();
 
   protected final InetSocketAddress bindAddress;
 
@@ -413,7 +411,7 @@ public abstract class RpcServer implements RpcServerInterface,
               this.connection.compressionCodec, cells);
           if (b != null) {
             cellBlockSize = b.remaining();
-            cellBlock = new ArrayList<ByteBuffer>(1);
+            cellBlock = new ArrayList<>(1);
             cellBlock.add(b);
           }
         }
@@ -1177,7 +1175,7 @@ public abstract class RpcServer implements RpcServerInterface,
             status.getClient(), startTime, processingTime, qTime,
             responseSize);
       }
-      return new Pair<Message, CellScanner>(result, controller.cellScanner());
+      return new Pair<>(result, controller.cellScanner());
     } catch (Throwable e) {
       // The above callBlockingMethod will always return a SE.  Strip the SE wrapper before
       // putting it on the wire.  Its needed to adhere to the pb Service Interface but we don't
@@ -1218,7 +1216,7 @@ public abstract class RpcServer implements RpcServerInterface,
       String clientAddress, long startTime, int processingTime, int qTime,
       long responseSize) throws IOException {
     // base information that is reported regardless of type of call
-    Map<String, Object> responseInfo = new HashMap<String, Object>();
+    Map<String, Object> responseInfo = new HashMap<>();
     responseInfo.put("starttimems", startTime);
     responseInfo.put("processingtimems", processingTime);
     responseInfo.put("queuetimems", qTime);
@@ -1299,7 +1297,7 @@ public abstract class RpcServer implements RpcServerInterface,
   static Pair<ByteBuff, CallCleanup> allocateByteBuffToReadInto(ByteBufferPool pool,
       int minSizeForPoolUse, int reqLen) {
     ByteBuff resultBuf;
-    List<ByteBuffer> bbs = new ArrayList<ByteBuffer>((reqLen / pool.getBufferSize()) + 1);
+    List<ByteBuffer> bbs = new ArrayList<>((reqLen / pool.getBufferSize()) + 1);
     int remain = reqLen;
     ByteBuffer buf = null;
     while (remain >= minSizeForPoolUse && (buf = pool.getBuffer()) != null) {
@@ -1325,14 +1323,14 @@ public abstract class RpcServer implements RpcServerInterface,
     resultBuf.limit(reqLen);
     if (bufsFromPool != null) {
       final ByteBuffer[] bufsFromPoolFinal = bufsFromPool;
-      return new Pair<ByteBuff, RpcServer.CallCleanup>(resultBuf, () -> {
+      return new Pair<>(resultBuf, () -> {
         // Return back all the BBs to pool
         for (int i = 0; i < bufsFromPoolFinal.length; i++) {
           pool.putbackBuffer(bufsFromPoolFinal[i]);
         }
       });
     }
-    return new Pair<ByteBuff, RpcServer.CallCleanup>(resultBuf, null);
+    return new Pair<>(resultBuf, null);
   }
 
   /**
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/ipc/SimpleRpcServer.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/ipc/SimpleRpcServer.java
index 075d8b8..9e1e81e 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/ipc/SimpleRpcServer.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/ipc/SimpleRpcServer.java
@@ -259,8 +259,7 @@ public class SimpleRpcServer extends RpcServer {
       private final Selector readSelector;
 
       Reader() throws IOException {
-        this.pendingConnections =
-          new LinkedBlockingQueue<Connection>(readerPendingConnectionQueueLength);
+        this.pendingConnections = new LinkedBlockingQueue<>(readerPendingConnectionQueueLength);
         this.readSelector = Selector.open();
       }
 
@@ -603,7 +602,7 @@ public class SimpleRpcServer extends RpcServer {
         return lastPurgeTime;
       }
 
-      ArrayList<Connection> conWithOldCalls = new ArrayList<Connection>();
+      ArrayList<Connection> conWithOldCalls = new ArrayList<>();
       // get the list of channels from list of keys.
       synchronized (writeSelector.keys()) {
         for (SelectionKey key : writeSelector.keys()) {
@@ -763,7 +762,7 @@ public class SimpleRpcServer extends RpcServer {
     protected SocketChannel channel;
     private ByteBuff data;
     private ByteBuffer dataLengthBuffer;
-    protected final ConcurrentLinkedDeque<Call> responseQueue = new ConcurrentLinkedDeque<Call>();
+    protected final ConcurrentLinkedDeque<Call> responseQueue = new ConcurrentLinkedDeque<>();
     private final Lock responseWriteLock = new ReentrantLock();
     private LongAdder rpcCount = new LongAdder(); // number of outstanding rpcs
     private long lastContact;
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/mapred/GroupingTableMap.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/mapred/GroupingTableMap.java
index ee6da75..e1ca999 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/mapred/GroupingTableMap.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/mapred/GroupingTableMap.java
@@ -116,7 +116,7 @@ implements TableMap<ImmutableBytesWritable,Result> {
    */
   protected byte[][] extractKeyValues(Result r) {
     byte[][] keyVals = null;
-    ArrayList<byte[]> foundList = new ArrayList<byte[]>();
+    ArrayList<byte[]> foundList = new ArrayList<>();
     int numCols = columns.length;
     if (numCols > 0) {
       for (Cell value: r.listCells()) {
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/mapreduce/CopyTable.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/mapreduce/CopyTable.java
index 819ef57..8f0504a 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/mapreduce/CopyTable.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/mapreduce/CopyTable.java
@@ -120,7 +120,7 @@ public class CopyTable extends Configured implements Tool {
 
     if(families != null) {
       String[] fams = families.split(",");
-      Map<String,String> cfRenameMap = new HashMap<String,String>();
+      Map<String,String> cfRenameMap = new HashMap<>();
       for(String fam : fams) {
         String sourceCf;
         if(fam.contains(":")) {
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/mapreduce/DefaultVisibilityExpressionResolver.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/mapreduce/DefaultVisibilityExpressionResolver.java
index 10e34d2..004ee5c 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/mapreduce/DefaultVisibilityExpressionResolver.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/mapreduce/DefaultVisibilityExpressionResolver.java
@@ -53,7 +53,7 @@ public class DefaultVisibilityExpressionResolver implements VisibilityExpression
   private static final Log LOG = LogFactory.getLog(DefaultVisibilityExpressionResolver.class);
 
   private Configuration conf;
-  private final Map<String, Integer> labels = new HashMap<String, Integer>();
+  private final Map<String, Integer> labels = new HashMap<>();
 
   @Override
   public Configuration getConf() {
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/mapreduce/GroupingTableMapper.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/mapreduce/GroupingTableMapper.java
index 8a9fa49..44e43c8 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/mapreduce/GroupingTableMapper.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/mapreduce/GroupingTableMapper.java
@@ -106,7 +106,7 @@ extends TableMapper<ImmutableBytesWritable,Result> implements Configurable {
    */
   protected byte[][] extractKeyValues(Result r) {
     byte[][] keyVals = null;
-    ArrayList<byte[]> foundList = new ArrayList<byte[]>();
+    ArrayList<byte[]> foundList = new ArrayList<>();
     int numCols = columns.length;
     if (numCols > 0) {
       for (Cell value: r.listCells()) {
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/mapreduce/HFileOutputFormat2.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/mapreduce/HFileOutputFormat2.java
index 13ea5c5..1ce5f60 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/mapreduce/HFileOutputFormat2.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/mapreduce/HFileOutputFormat2.java
@@ -137,7 +137,7 @@ public class HFileOutputFormat2
 
   static <V extends Cell> RecordWriter<ImmutableBytesWritable, V>
   createRecordWriter(final TaskAttemptContext context) throws IOException {
-    return new HFileRecordWriter<V>(context, null);
+    return new HFileRecordWriter<>(context, null);
   }
 
   protected static class HFileRecordWriter<V extends Cell>
@@ -211,7 +211,7 @@ public class HFileOutputFormat2
         overriddenEncoding = null;
       }
 
-      writers = new TreeMap<byte[], WriterLength>(Bytes.BYTES_COMPARATOR);
+      writers = new TreeMap<>(Bytes.BYTES_COMPARATOR);
       previousRow = HConstants.EMPTY_BYTE_ARRAY;
       now = Bytes.toBytes(EnvironmentEdgeManager.currentTime());
       rollRequested = false;
@@ -418,8 +418,7 @@ public class HFileOutputFormat2
   private static List<ImmutableBytesWritable> getRegionStartKeys(RegionLocator table)
   throws IOException {
     byte[][] byteKeys = table.getStartKeys();
-    ArrayList<ImmutableBytesWritable> ret =
-      new ArrayList<ImmutableBytesWritable>(byteKeys.length);
+    ArrayList<ImmutableBytesWritable> ret = new ArrayList<>(byteKeys.length);
     for (byte[] byteKey : byteKeys) {
       ret.add(new ImmutableBytesWritable(byteKey));
     }
@@ -442,8 +441,7 @@ public class HFileOutputFormat2
     // have keys < the first region (which has an empty start key)
     // so we need to remove it. Otherwise we would end up with an
     // empty reducer with index 0
-    TreeSet<ImmutableBytesWritable> sorted =
-      new TreeSet<ImmutableBytesWritable>(startKeys);
+    TreeSet<ImmutableBytesWritable> sorted = new TreeSet<>(startKeys);
 
     ImmutableBytesWritable first = sorted.first();
     if (!first.equals(HConstants.EMPTY_BYTE_ARRAY)) {
@@ -587,8 +585,7 @@ public class HFileOutputFormat2
       conf) {
     Map<byte[], String> stringMap = createFamilyConfValueMap(conf,
         COMPRESSION_FAMILIES_CONF_KEY);
-    Map<byte[], Algorithm> compressionMap = new TreeMap<byte[],
-        Algorithm>(Bytes.BYTES_COMPARATOR);
+    Map<byte[], Algorithm> compressionMap = new TreeMap<>(Bytes.BYTES_COMPARATOR);
     for (Map.Entry<byte[], String> e : stringMap.entrySet()) {
       Algorithm algorithm = HFileWriterImpl.compressionByName(e.getValue());
       compressionMap.put(e.getKey(), algorithm);
@@ -607,8 +604,7 @@ public class HFileOutputFormat2
   static Map<byte[], BloomType> createFamilyBloomTypeMap(Configuration conf) {
     Map<byte[], String> stringMap = createFamilyConfValueMap(conf,
         BLOOM_TYPE_FAMILIES_CONF_KEY);
-    Map<byte[], BloomType> bloomTypeMap = new TreeMap<byte[],
-        BloomType>(Bytes.BYTES_COMPARATOR);
+    Map<byte[], BloomType> bloomTypeMap = new TreeMap<>(Bytes.BYTES_COMPARATOR);
     for (Map.Entry<byte[], String> e : stringMap.entrySet()) {
       BloomType bloomType = BloomType.valueOf(e.getValue());
       bloomTypeMap.put(e.getKey(), bloomType);
@@ -627,8 +623,7 @@ public class HFileOutputFormat2
   static Map<byte[], Integer> createFamilyBlockSizeMap(Configuration conf) {
     Map<byte[], String> stringMap = createFamilyConfValueMap(conf,
         BLOCK_SIZE_FAMILIES_CONF_KEY);
-    Map<byte[], Integer> blockSizeMap = new TreeMap<byte[],
-        Integer>(Bytes.BYTES_COMPARATOR);
+    Map<byte[], Integer> blockSizeMap = new TreeMap<>(Bytes.BYTES_COMPARATOR);
     for (Map.Entry<byte[], String> e : stringMap.entrySet()) {
       Integer blockSize = Integer.parseInt(e.getValue());
       blockSizeMap.put(e.getKey(), blockSize);
@@ -649,8 +644,7 @@ public class HFileOutputFormat2
       Configuration conf) {
     Map<byte[], String> stringMap = createFamilyConfValueMap(conf,
         DATABLOCK_ENCODING_FAMILIES_CONF_KEY);
-    Map<byte[], DataBlockEncoding> encoderMap = new TreeMap<byte[],
-        DataBlockEncoding>(Bytes.BYTES_COMPARATOR);
+    Map<byte[], DataBlockEncoding> encoderMap = new TreeMap<>(Bytes.BYTES_COMPARATOR);
     for (Map.Entry<byte[], String> e : stringMap.entrySet()) {
       encoderMap.put(e.getKey(), DataBlockEncoding.valueOf((e.getValue())));
     }
@@ -667,7 +661,7 @@ public class HFileOutputFormat2
    */
   private static Map<byte[], String> createFamilyConfValueMap(
       Configuration conf, String confName) {
-    Map<byte[], String> confValMap = new TreeMap<byte[], String>(Bytes.BYTES_COMPARATOR);
+    Map<byte[], String> confValMap = new TreeMap<>(Bytes.BYTES_COMPARATOR);
     String confVal = conf.get(confName, "");
     for (String familyConf : confVal.split("&")) {
       String[] familySplit = familyConf.split("=");
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/mapreduce/HashTable.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/mapreduce/HashTable.java
index 674cb57..2834f86 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/mapreduce/HashTable.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/mapreduce/HashTable.java
@@ -213,7 +213,7 @@ public class HashTable extends Configured implements Tool {
      * into the desired number of partitions.
      */
     void selectPartitions(Pair<byte[][], byte[][]> regionStartEndKeys) {
-      List<byte[]> startKeys = new ArrayList<byte[]>();
+      List<byte[]> startKeys = new ArrayList<>();
       for (int i = 0; i < regionStartEndKeys.getFirst().length; i++) {
         byte[] regionStartKey = regionStartEndKeys.getFirst()[i];
         byte[] regionEndKey = regionStartEndKeys.getSecond()[i];
@@ -244,7 +244,7 @@ public class HashTable extends Configured implements Tool {
       }
       
       // choose a subset of start keys to group regions into ranges
-      partitions = new ArrayList<ImmutableBytesWritable>(numHashFiles - 1);
+      partitions = new ArrayList<>(numHashFiles - 1);
       // skip the first start key as it is not a partition between ranges.
       for (long i = 1; i < numHashFiles; i++) {
         int splitIndex = (int) (numRegions * i / numHashFiles);
@@ -269,7 +269,7 @@ public class HashTable extends Configured implements Tool {
       @SuppressWarnings("deprecation")
       SequenceFile.Reader reader = new SequenceFile.Reader(fs, path, conf);
       ImmutableBytesWritable key = new ImmutableBytesWritable();
-      partitions = new ArrayList<ImmutableBytesWritable>();
+      partitions = new ArrayList<>();
       while (reader.next(key)) {
         partitions.add(new ImmutableBytesWritable(key.copyBytes()));
       }
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/mapreduce/Import.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/mapreduce/Import.java
index e2693b9..d1beb8d 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/mapreduce/Import.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/mapreduce/Import.java
@@ -471,7 +471,7 @@ public class Import extends Configured implements Tool {
   }
 
   private static ArrayList<byte[]> toQuotedByteArrays(String... stringArgs) {
-    ArrayList<byte[]> quotedArgs = new ArrayList<byte[]>();
+    ArrayList<byte[]> quotedArgs = new ArrayList<>();
     for (String stringArg : stringArgs) {
       // all the filters' instantiation methods expected quoted args since they are coming from
       // the shell, so add them here, though it shouldn't really be needed :-/
@@ -536,7 +536,7 @@ public class Import extends Configured implements Tool {
       String[] allMappings = allMappingsPropVal.split(",");
       for (String mapping: allMappings) {
         if(cfRenameMap == null) {
-            cfRenameMap = new TreeMap<byte[],byte[]>(Bytes.BYTES_COMPARATOR);
+            cfRenameMap = new TreeMap<>(Bytes.BYTES_COMPARATOR);
         }
         String [] srcAndDest = mapping.split(":");
         if(srcAndDest.length != 2) {
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/mapreduce/ImportTsv.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/mapreduce/ImportTsv.java
index 39085df..a379d53 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/mapreduce/ImportTsv.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/mapreduce/ImportTsv.java
@@ -249,7 +249,7 @@ public class ImportTsv extends Configured implements Tool {
     public ParsedLine parse(byte[] lineBytes, int length)
     throws BadTsvLineException {
       // Enumerate separator offsets
-      ArrayList<Integer> tabOffsets = new ArrayList<Integer>(maxColumnCount);
+      ArrayList<Integer> tabOffsets = new ArrayList<>(maxColumnCount);
       for (int i = 0; i < length; i++) {
         if (lineBytes[i] == separatorByte) {
           tabOffsets.add(i);
@@ -448,7 +448,7 @@ public class ImportTsv extends Configured implements Tool {
                   + " are less than row key position.");
         }
       }
-      return new Pair<Integer, Integer>(startPos, endPos - startPos + 1);
+      return new Pair<>(startPos, endPos - startPos + 1);
     }
   }
 
@@ -521,7 +521,7 @@ public class ImportTsv extends Configured implements Tool {
             boolean noStrict = conf.getBoolean(NO_STRICT_COL_FAMILY, false);
             // if no.strict is false then check column family
             if(!noStrict) {
-              ArrayList<String> unmatchedFamilies = new ArrayList<String>();
+              ArrayList<String> unmatchedFamilies = new ArrayList<>();
               Set<String> cfSet = getColumnFamilies(columns);
               HTableDescriptor tDesc = table.getTableDescriptor();
               for (String cf : cfSet) {
@@ -530,7 +530,7 @@ public class ImportTsv extends Configured implements Tool {
                 }
               }
               if(unmatchedFamilies.size() > 0) {
-                ArrayList<String> familyNames = new ArrayList<String>();
+                ArrayList<String> familyNames = new ArrayList<>();
                 for (HColumnDescriptor family : table.getTableDescriptor().getFamilies()) {
                   familyNames.add(family.getNameAsString());
                 }
@@ -626,7 +626,7 @@ public class ImportTsv extends Configured implements Tool {
   }
 
   private static Set<String> getColumnFamilies(String[] columns) {
-    Set<String> cfSet = new HashSet<String>();
+    Set<String> cfSet = new HashSet<>();
     for (String aColumn : columns) {
       if (TsvParser.ROWKEY_COLUMN_SPEC.equals(aColumn)
           || TsvParser.TIMESTAMPKEY_COLUMN_SPEC.equals(aColumn)
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/mapreduce/KeyValueSortReducer.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/mapreduce/KeyValueSortReducer.java
index f6c7a90..d37ab94 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/mapreduce/KeyValueSortReducer.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/mapreduce/KeyValueSortReducer.java
@@ -40,7 +40,7 @@ public class KeyValueSortReducer extends Reducer<ImmutableBytesWritable, KeyValu
   protected void reduce(ImmutableBytesWritable row, java.lang.Iterable<KeyValue> kvs,
       org.apache.hadoop.mapreduce.Reducer<ImmutableBytesWritable, KeyValue, ImmutableBytesWritable, KeyValue>.Context context)
   throws java.io.IOException, InterruptedException {
-    TreeSet<KeyValue> map = new TreeSet<KeyValue>(CellComparator.COMPARATOR);
+    TreeSet<KeyValue> map = new TreeSet<>(CellComparator.COMPARATOR);
     for (KeyValue kv: kvs) {
       try {
         map.add(kv.clone());
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/mapreduce/LoadIncrementalHFiles.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/mapreduce/LoadIncrementalHFiles.java
index 963c4a1..718e88b 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/mapreduce/LoadIncrementalHFiles.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/mapreduce/LoadIncrementalHFiles.java
@@ -125,7 +125,7 @@ public class LoadIncrementalHFiles extends Configured implements Tool {
 
   private int maxFilesPerRegionPerFamily;
   private boolean assignSeqIds;
-  private Set<String> unmatchedFamilies = new HashSet<String>();
+  private Set<String> unmatchedFamilies = new HashSet<>();
 
   // Source filesystem
   private FileSystem fs;
@@ -630,7 +630,7 @@ public class LoadIncrementalHFiles extends Configured implements Tool {
     ThreadFactoryBuilder builder = new ThreadFactoryBuilder();
     builder.setNameFormat("LoadIncrementalHFiles-%1$d");
     ExecutorService pool = new ThreadPoolExecutor(nrThreads, nrThreads, 60, TimeUnit.SECONDS,
-        new LinkedBlockingQueue<Runnable>(), builder.build());
+        new LinkedBlockingQueue<>(), builder.build());
     ((ThreadPoolExecutor) pool).allowCoreThreadTimeOut(true);
     return pool;
   }
@@ -889,7 +889,7 @@ public class LoadIncrementalHFiles extends Configured implements Tool {
 
     // Add these back at the *front* of the queue, so there's a lower
     // chance that the region will just split again before we get there.
-    List<LoadQueueItem> lqis = new ArrayList<LoadQueueItem>(2);
+    List<LoadQueueItem> lqis = new ArrayList<>(2);
     lqis.add(new LoadQueueItem(item.family, botOut));
     lqis.add(new LoadQueueItem(item.family, topOut));
 
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/mapreduce/MultiHFileOutputFormat.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/mapreduce/MultiHFileOutputFormat.java
index 7c1ebbc..dc2fc0d 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/mapreduce/MultiHFileOutputFormat.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/mapreduce/MultiHFileOutputFormat.java
@@ -65,8 +65,7 @@ public class MultiHFileOutputFormat extends FileOutputFormat<ImmutableBytesWrita
     final FileSystem fs = outputDir.getFileSystem(conf);
 
     // Map of tables to writers
-    final Map<ImmutableBytesWritable, RecordWriter<ImmutableBytesWritable, V>> tableWriters =
-        new HashMap<ImmutableBytesWritable, RecordWriter<ImmutableBytesWritable, V>>();
+    final Map<ImmutableBytesWritable, RecordWriter<ImmutableBytesWritable, V>> tableWriters = new HashMap<>();
 
     return new RecordWriter<ImmutableBytesWritable, V>() {
       @Override
@@ -82,7 +81,7 @@ public class MultiHFileOutputFormat extends FileOutputFormat<ImmutableBytesWrita
               + tableOutputDir.toString());
 
           // Create writer for one specific table
-          tableWriter = new HFileOutputFormat2.HFileRecordWriter<V>(context, tableOutputDir);
+          tableWriter = new HFileOutputFormat2.HFileRecordWriter<>(context, tableOutputDir);
           // Put table into map
           tableWriters.put(tableName, tableWriter);
         }
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/mapreduce/MultiTableInputFormat.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/mapreduce/MultiTableInputFormat.java
index 48a982b..3099c0d 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/mapreduce/MultiTableInputFormat.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/mapreduce/MultiTableInputFormat.java
@@ -92,7 +92,7 @@ public class MultiTableInputFormat extends MultiTableInputFormatBase implements
       throw new IllegalArgumentException("There must be at least 1 scan configuration set to : "
           + SCANS);
     }
-    List<Scan> scans = new ArrayList<Scan>();
+    List<Scan> scans = new ArrayList<>();
 
     for (int i = 0; i < rawScans.length; i++) {
       try {
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/mapreduce/MultiTableInputFormatBase.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/mapreduce/MultiTableInputFormatBase.java
index 4931c3f..25ea047 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/mapreduce/MultiTableInputFormatBase.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/mapreduce/MultiTableInputFormatBase.java
@@ -163,7 +163,7 @@ public abstract class MultiTableInputFormatBase extends
       throw new IOException("No scans were provided.");
     }
 
-    Map<TableName, List<Scan>> tableMaps = new HashMap<TableName, List<Scan>>();
+    Map<TableName, List<Scan>> tableMaps = new HashMap<>();
     for (Scan scan : scans) {
       byte[] tableNameBytes = scan.getAttribute(Scan.SCAN_ATTRIBUTES_TABLE_NAME);
       if (tableNameBytes == null)
@@ -173,13 +173,13 @@ public abstract class MultiTableInputFormatBase extends
 
       List<Scan> scanList = tableMaps.get(tableName);
       if (scanList == null) {
-        scanList = new ArrayList<Scan>();
+        scanList = new ArrayList<>();
         tableMaps.put(tableName, scanList);
       }
       scanList.add(scan);
     }
 
-    List<InputSplit> splits = new ArrayList<InputSplit>();
+    List<InputSplit> splits = new ArrayList<>();
     Iterator iter = tableMaps.entrySet().iterator();
     while (iter.hasNext()) {
       Map.Entry<TableName, List<Scan>> entry = (Map.Entry<TableName, List<Scan>>) iter.next();
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/mapreduce/PutSortReducer.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/mapreduce/PutSortReducer.java
index 6657c99..b48580d 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/mapreduce/PutSortReducer.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/mapreduce/PutSortReducer.java
@@ -79,10 +79,10 @@ public class PutSortReducer extends
         "putsortreducer.row.threshold", 1L * (1<<30));
     Iterator<Put> iter = puts.iterator();
     while (iter.hasNext()) {
-      TreeSet<KeyValue> map = new TreeSet<KeyValue>(CellComparator.COMPARATOR);
+      TreeSet<KeyValue> map = new TreeSet<>(CellComparator.COMPARATOR);
       long curSize = 0;
       // stop at the end or the RAM threshold
-      List<Tag> tags = new ArrayList<Tag>();
+      List<Tag> tags = new ArrayList<>();
       while (iter.hasNext() && curSize < threshold) {
         // clear the tags
         tags.clear();
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/mapreduce/ResultSerialization.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/mapreduce/ResultSerialization.java
index 1561b3b..98c92ea 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/mapreduce/ResultSerialization.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/mapreduce/ResultSerialization.java
@@ -92,7 +92,7 @@ public class ResultSerialization extends Configured implements Serialization<Res
       }
       byte[] buf = new byte[totalBuffer];
       readChunked(in, buf, 0, totalBuffer);
-      List<Cell> kvs = new ArrayList<Cell>();
+      List<Cell> kvs = new ArrayList<>();
       int offset = 0;
       while (offset < totalBuffer) {
         int keyLength = Bytes.toInt(buf, offset);
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/mapreduce/TableInputFormatBase.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/mapreduce/TableInputFormatBase.java
index 9ebb3c1..7962a42 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/mapreduce/TableInputFormatBase.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/mapreduce/TableInputFormatBase.java
@@ -137,8 +137,7 @@ extends InputFormat<ImmutableBytesWritable, Result> {
 
   
   /** The reverse DNS lookup cache mapping: IPAddress => HostName */
-  private HashMap<InetAddress, String> reverseDNSCacheMap =
-    new HashMap<InetAddress, String>();
+  private HashMap<InetAddress, String> reverseDNSCacheMap = new HashMap<>();
 
   /**
    * Builds a {@link TableRecordReader}. If no {@link TableRecordReader} was provided, uses
@@ -262,7 +261,7 @@ extends InputFormat<ImmutableBytesWritable, Result> {
         if (null == regLoc) {
           throw new IOException("Expecting at least one region.");
         }
-        List<InputSplit> splits = new ArrayList<InputSplit>(1);
+        List<InputSplit> splits = new ArrayList<>(1);
         long regionSize = sizeCalculator.getRegionSize(regLoc.getRegionInfo().getRegionName());
         TableSplit split = new TableSplit(tableName, scan,
             HConstants.EMPTY_BYTE_ARRAY, HConstants.EMPTY_BYTE_ARRAY, regLoc
@@ -270,7 +269,7 @@ extends InputFormat<ImmutableBytesWritable, Result> {
         splits.add(split);
         return splits;
       }
-      List<InputSplit> splits = new ArrayList<InputSplit>(keys.getFirst().length);
+      List<InputSplit> splits = new ArrayList<>(keys.getFirst().length);
       for (int i = 0; i < keys.getFirst().length; i++) {
         if (!includeRegionInSplit(keys.getFirst()[i], keys.getSecond()[i])) {
           continue;
@@ -373,7 +372,7 @@ extends InputFormat<ImmutableBytesWritable, Result> {
    */
   private List<InputSplit> calculateRebalancedSplits(List<InputSplit> list, JobContext context,
                                                long average) throws IOException {
-    List<InputSplit> resultList = new ArrayList<InputSplit>();
+    List<InputSplit> resultList = new ArrayList<>();
     Configuration conf = context.getConfiguration();
     //The default data skew ratio is 3
     long dataSkewRatio = conf.getLong(INPUT_AUTOBALANCE_MAXSKEWRATIO, 3);
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/mapreduce/TableMapReduceUtil.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/mapreduce/TableMapReduceUtil.java
index 98f39da..69b486d 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/mapreduce/TableMapReduceUtil.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/mapreduce/TableMapReduceUtil.java
@@ -451,7 +451,7 @@ public class TableMapReduceUtil {
     job.setMapperClass(mapper);
     Configuration conf = job.getConfiguration();
     HBaseConfiguration.merge(conf, HBaseConfiguration.create(conf));
-    List<String> scanStrings = new ArrayList<String>();
+    List<String> scanStrings = new ArrayList<>();
 
     for (Scan scan : scans) {
       scanStrings.add(convertScanToString(scan));
@@ -807,7 +807,7 @@ public class TableMapReduceUtil {
     if (conf == null) {
       throw new IllegalArgumentException("Must provide a configuration object.");
     }
-    Set<String> paths = new HashSet<String>(conf.getStringCollection("tmpjars"));
+    Set<String> paths = new HashSet<>(conf.getStringCollection("tmpjars"));
     if (paths.isEmpty()) {
       throw new IllegalArgumentException("Configuration contains no tmpjars.");
     }
@@ -879,13 +879,13 @@ public class TableMapReduceUtil {
       Class<?>... classes) throws IOException {
 
     FileSystem localFs = FileSystem.getLocal(conf);
-    Set<String> jars = new HashSet<String>();
+    Set<String> jars = new HashSet<>();
     // Add jars that are already in the tmpjars variable
     jars.addAll(conf.getStringCollection("tmpjars"));
 
     // add jars as we find them to a map of contents jar name so that we can avoid
     // creating new jars for classes that have already been packaged.
-    Map<String, String> packagedClasses = new HashMap<String, String>();
+    Map<String, String> packagedClasses = new HashMap<>();
 
     // Add jars containing the specified classes
     for (Class<?> clazz : classes) {
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/mapreduce/TableSnapshotInputFormat.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/mapreduce/TableSnapshotInputFormat.java
index c40396f..b2db319 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/mapreduce/TableSnapshotInputFormat.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/mapreduce/TableSnapshotInputFormat.java
@@ -183,7 +183,7 @@ public class TableSnapshotInputFormat extends InputFormat<ImmutableBytesWritable
 
   @Override
   public List<InputSplit> getSplits(JobContext job) throws IOException, InterruptedException {
-    List<InputSplit> results = new ArrayList<InputSplit>();
+    List<InputSplit> results = new ArrayList<>();
     for (TableSnapshotInputFormatImpl.InputSplit split :
         TableSnapshotInputFormatImpl.getSplits(job.getConfiguration())) {
       results.add(new TableSnapshotRegionSplit(split));
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/mapreduce/TableSnapshotInputFormatImpl.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/mapreduce/TableSnapshotInputFormatImpl.java
index d52703a..69beef8 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/mapreduce/TableSnapshotInputFormatImpl.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/mapreduce/TableSnapshotInputFormatImpl.java
@@ -311,7 +311,7 @@ public class TableSnapshotInputFormatImpl {
 
     Path tableDir = FSUtils.getTableDir(restoreDir, htd.getTableName());
 
-    List<InputSplit> splits = new ArrayList<InputSplit>();
+    List<InputSplit> splits = new ArrayList<>();
     for (HRegionInfo hri : regionManifests) {
       // load region descriptor
 
@@ -346,7 +346,7 @@ public class TableSnapshotInputFormatImpl {
    */
   public static List<String> getBestLocations(
       Configuration conf, HDFSBlocksDistribution blockDistribution) {
-    List<String> locations = new ArrayList<String>(3);
+    List<String> locations = new ArrayList<>(3);
 
     HostAndWeight[] hostAndWeights = blockDistribution.getTopHostsWithWeights();
 
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/mapreduce/TextSortReducer.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/mapreduce/TextSortReducer.java
index 1e09f03..05a4820 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/mapreduce/TextSortReducer.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/mapreduce/TextSortReducer.java
@@ -146,7 +146,7 @@ public class TextSortReducer extends
         "reducer.row.threshold", 1L * (1<<30));
     Iterator<Text> iter = lines.iterator();
     while (iter.hasNext()) {
-      Set<KeyValue> kvs = new TreeSet<KeyValue>(CellComparator.COMPARATOR);
+      Set<KeyValue> kvs = new TreeSet<>(CellComparator.COMPARATOR);
       long curSize = 0;
       // stop at the end or the RAM threshold
       while (iter.hasNext() && curSize < threshold) {
@@ -160,7 +160,7 @@ public class TextSortReducer extends
           ttl = parsed.getCellTTL();
           
           // create tags for the parsed line
-          List<Tag> tags = new ArrayList<Tag>();
+          List<Tag> tags = new ArrayList<>();
           if (cellVisibilityExpr != null) {
             tags.addAll(kvCreator.getVisibilityExpressionResolver().createVisibilityExpTags(
               cellVisibilityExpr));
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/mapreduce/TsvImporterMapper.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/mapreduce/TsvImporterMapper.java
index 94bcb43..08b5aab 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/mapreduce/TsvImporterMapper.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/mapreduce/TsvImporterMapper.java
@@ -113,7 +113,7 @@ extends Mapper<LongWritable, Text, ImmutableBytesWritable, Put>
       throw new RuntimeException("No row key column specified");
     }
     this.kvCreator = new CellCreator(conf);
-    tags = new ArrayList<Tag>();
+    tags = new ArrayList<>();
   }
 
   /**
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/mapreduce/WALInputFormat.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/mapreduce/WALInputFormat.java
index 02fcbba..8514ace 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/mapreduce/WALInputFormat.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/mapreduce/WALInputFormat.java
@@ -239,7 +239,7 @@ public class WALInputFormat extends InputFormat<WALKey, WALEdit> {
     FileSystem fs = inputDir.getFileSystem(conf);
     List<FileStatus> files = getFiles(fs, inputDir, startTime, endTime);
 
-    List<InputSplit> splits = new ArrayList<InputSplit>(files.size());
+    List<InputSplit> splits = new ArrayList<>(files.size());
     for (FileStatus file : files) {
       splits.add(new WALSplit(file.getPath().toString(), file.getLen(), startTime, endTime));
     }
@@ -248,7 +248,7 @@ public class WALInputFormat extends InputFormat<WALKey, WALEdit> {
 
   private List<FileStatus> getFiles(FileSystem fs, Path dir, long startTime, long endTime)
       throws IOException {
-    List<FileStatus> result = new ArrayList<FileStatus>();
+    List<FileStatus> result = new ArrayList<>();
     LOG.debug("Scanning " + dir.toString() + " for WAL files");
 
     FileStatus[] files = fs.listStatus(dir);
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/mapreduce/WALPlayer.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/mapreduce/WALPlayer.java
index 2c67baf..cca2041 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/mapreduce/WALPlayer.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/mapreduce/WALPlayer.java
@@ -132,7 +132,7 @@ public class WALPlayer extends Configured implements Tool {
    */
   protected static class WALMapper
   extends Mapper<WALKey, WALEdit, ImmutableBytesWritable, Mutation> {
-    private Map<TableName, TableName> tables = new TreeMap<TableName, TableName>();
+    private Map<TableName, TableName> tables = new TreeMap<>();
 
     @Override
     public void map(WALKey key, WALEdit value, Context context)
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java
index 60ad545..69ebd97 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java
@@ -115,7 +115,7 @@ public class AssignmentManager {
 
   private AtomicInteger numRegionsOpened = new AtomicInteger(0);
 
-  final private KeyLocker<String> locker = new KeyLocker<String>();
+  final private KeyLocker<String> locker = new KeyLocker<>();
 
   Set<HRegionInfo> replicasToClose = Collections.synchronizedSet(new HashSet<HRegionInfo>());
 
@@ -141,8 +141,7 @@ public class AssignmentManager {
   // TODO: When do plans get cleaned out?  Ever? In server open and in server
   // shutdown processing -- St.Ack
   // All access to this Map must be synchronized.
-  final NavigableMap<String, RegionPlan> regionPlans =
-    new TreeMap<String, RegionPlan>();
+  final NavigableMap<String, RegionPlan> regionPlans = new TreeMap<>();
 
   private final TableStateManager tableStateManager;
 
@@ -183,8 +182,7 @@ public class AssignmentManager {
    * because we don't expect this to happen frequently; we don't
    * want to copy this information over during each state transition either.
    */
-  private final ConcurrentHashMap<String, AtomicInteger>
-    failedOpenTracker = new ConcurrentHashMap<String, AtomicInteger>();
+  private final ConcurrentHashMap<String, AtomicInteger> failedOpenTracker = new ConcurrentHashMap<>();
 
   // In case not using ZK for region assignment, region states
   // are persisted in meta with a state store
@@ -197,7 +195,7 @@ public class AssignmentManager {
   public static boolean TEST_SKIP_SPLIT_HANDLING = false;
 
   /** Listeners that are called on assignment events. */
-  private List<AssignmentListener> listeners = new CopyOnWriteArrayList<AssignmentListener>();
+  private List<AssignmentListener> listeners = new CopyOnWriteArrayList<>();
 
   private RegionStateListener regionStateListener;
 
@@ -382,7 +380,7 @@ public class AssignmentManager {
         pending++;
       }
     }
-    return new Pair<Integer, Integer>(pending, hris.size());
+    return new Pair<>(pending, hris.size());
   }
 
   /**
@@ -748,16 +746,16 @@ public class AssignmentManager {
         return true;
       }
       LOG.info("Assigning " + regionCount + " region(s) to " + destination.toString());
-      Set<String> encodedNames = new HashSet<String>(regionCount);
+      Set<String> encodedNames = new HashSet<>(regionCount);
       for (HRegionInfo region : regions) {
         encodedNames.add(region.getEncodedName());
       }
 
-      List<HRegionInfo> failedToOpenRegions = new ArrayList<HRegionInfo>();
+      List<HRegionInfo> failedToOpenRegions = new ArrayList<>();
       Map<String, Lock> locks = locker.acquireLocks(encodedNames);
       try {
-        Map<String, RegionPlan> plans = new HashMap<String, RegionPlan>(regionCount);
-        List<RegionState> states = new ArrayList<RegionState>(regionCount);
+        Map<String, RegionPlan> plans = new HashMap<>(regionCount);
+        List<RegionState> states = new ArrayList<>(regionCount);
         for (HRegionInfo region : regions) {
           String encodedName = region.getEncodedName();
           if (!isDisabledorDisablingRegionInRIT(region)) {
@@ -797,8 +795,7 @@ public class AssignmentManager {
         // that unnecessary timeout on RIT is reduced.
         this.addPlans(plans);
 
-        List<Pair<HRegionInfo, List<ServerName>>> regionOpenInfos =
-          new ArrayList<Pair<HRegionInfo, List<ServerName>>>(states.size());
+        List<Pair<HRegionInfo, List<ServerName>>> regionOpenInfos = new ArrayList<>(states.size());
         for (RegionState state: states) {
           HRegionInfo region = state.getRegion();
           regionStates.updateRegionState(
@@ -807,8 +804,7 @@ public class AssignmentManager {
           if (shouldAssignFavoredNodes(region)) {
             favoredNodes = server.getFavoredNodesManager().getFavoredNodesWithDNPort(region);
           }
-          regionOpenInfos.add(new Pair<HRegionInfo, List<ServerName>>(
-            region, favoredNodes));
+          regionOpenInfos.add(new Pair<>(region, favoredNodes));
         }
 
         // Move on to open regions.
@@ -908,7 +904,7 @@ public class AssignmentManager {
       }
 
       // wait for assignment completion
-      ArrayList<HRegionInfo> userRegionSet = new ArrayList<HRegionInfo>(regions.size());
+      ArrayList<HRegionInfo> userRegionSet = new ArrayList<>(regions.size());
       for (HRegionInfo region: regions) {
         if (!region.getTable().isSystemTable()) {
           userRegionSet.add(region);
@@ -1443,7 +1439,7 @@ public class AssignmentManager {
    */
   public boolean waitForAssignment(HRegionInfo regionInfo)
       throws InterruptedException {
-    ArrayList<HRegionInfo> regionSet = new ArrayList<HRegionInfo>(1);
+    ArrayList<HRegionInfo> regionSet = new ArrayList<>(1);
     regionSet.add(regionInfo);
     return waitForAssignment(regionSet, true, Long.MAX_VALUE);
   }
@@ -1588,7 +1584,7 @@ public class AssignmentManager {
       }
 
       // invoke assignment (async)
-      ArrayList<HRegionInfo> userRegionSet = new ArrayList<HRegionInfo>(regions);
+      ArrayList<HRegionInfo> userRegionSet = new ArrayList<>(regions);
       for (Map.Entry<ServerName, List<HRegionInfo>> plan: bulkPlan.entrySet()) {
         if (!assign(plan.getKey(), plan.getValue()) && !server.isStopped()) {
           for (HRegionInfo region: plan.getValue()) {
@@ -1640,7 +1636,7 @@ public class AssignmentManager {
     if (retainAssignment) {
       assign(allRegions);
     } else {
-      List<HRegionInfo> regions = new ArrayList<HRegionInfo>(regionsFromMetaScan);
+      List<HRegionInfo> regions = new ArrayList<>(regionsFromMetaScan);
       assign(regions);
     }
 
@@ -1687,7 +1683,7 @@ public class AssignmentManager {
    */
   public static List<HRegionInfo> replicaRegionsNotRecordedInMeta(
       Set<HRegionInfo> regionsRecordedInMeta, MasterServices master)throws IOException {
-    List<HRegionInfo> regionsNotRecordedInMeta = new ArrayList<HRegionInfo>();
+    List<HRegionInfo> regionsNotRecordedInMeta = new ArrayList<>();
     for (HRegionInfo hri : regionsRecordedInMeta) {
       TableName table = hri.getTable();
       if(master.getTableDescriptors().get(table) == null)
@@ -1723,7 +1719,7 @@ public class AssignmentManager {
     // Get any new but slow to checkin region server that joined the cluster
     Set<ServerName> onlineServers = serverManager.getOnlineServers().keySet();
     // Set of offline servers to be returned
-    Set<ServerName> offlineServers = new HashSet<ServerName>();
+    Set<ServerName> offlineServers = new HashSet<>();
     // Iterate regions in META
     for (Result result : results) {
       if (result == null && LOG.isDebugEnabled()){
@@ -2446,7 +2442,7 @@ public class AssignmentManager {
     threadPoolExecutorService.submit(splitReplicasCallable);
 
     // wait for assignment completion
-    ArrayList<HRegionInfo> regionAssignSet = new ArrayList<HRegionInfo>(2);
+    ArrayList<HRegionInfo> regionAssignSet = new ArrayList<>(2);
     regionAssignSet.add(daughterAHRI);
     regionAssignSet.add(daughterBHRI);
     while (!waitForAssignment(regionAssignSet, true, regionAssignSet.size(),
@@ -2558,7 +2554,7 @@ public class AssignmentManager {
 
     final HRegionInfo a = HRegionInfo.convert(transition.getRegionInfo(1));
     final HRegionInfo b = HRegionInfo.convert(transition.getRegionInfo(2));
-    Set<String> encodedNames = new HashSet<String>(2);
+    Set<String> encodedNames = new HashSet<>(2);
     encodedNames.add(a.getEncodedName());
     encodedNames.add(b.getEncodedName());
     Map<String, Lock> locks = locker.acquireLocks(encodedNames);
@@ -2645,7 +2641,7 @@ public class AssignmentManager {
     threadPoolExecutorService.submit(mergeReplicasCallable);
 
     // wait for assignment completion
-    ArrayList<HRegionInfo> regionAssignSet = new ArrayList<HRegionInfo>(1);
+    ArrayList<HRegionInfo> regionAssignSet = new ArrayList<>(1);
     regionAssignSet.add(mergedRegion);
     while (!waitForAssignment(regionAssignSet, true, regionAssignSet.size(), Long.MAX_VALUE)) {
       LOG.debug("The merged region " + mergedRegion + " is still in transition. ");
@@ -2754,7 +2750,7 @@ public class AssignmentManager {
       final HRegionInfo hri_b) {
     // Close replicas for the original unmerged regions. create/assign new replicas
     // for the merged parent.
-    List<HRegionInfo> unmergedRegions = new ArrayList<HRegionInfo>();
+    List<HRegionInfo> unmergedRegions = new ArrayList<>();
     unmergedRegions.add(hri_a);
     unmergedRegions.add(hri_b);
     Map<ServerName, List<HRegionInfo>> map = regionStates.getRegionAssignments(unmergedRegions);
@@ -2768,7 +2764,7 @@ public class AssignmentManager {
       }
     }
     int numReplicas = getNumReplicas(server, mergedHri.getTable());
-    List<HRegionInfo> regions = new ArrayList<HRegionInfo>();
+    List<HRegionInfo> regions = new ArrayList<>();
     for (int i = 1; i < numReplicas; i++) {
       regions.add(RegionReplicaUtil.getRegionInfoForReplica(mergedHri, i));
     }
@@ -2790,7 +2786,7 @@ public class AssignmentManager {
     // the replica1s of daughters will be on the same machine
     int numReplicas = getNumReplicas(server, parentHri.getTable());
     // unassign the old replicas
-    List<HRegionInfo> parentRegion = new ArrayList<HRegionInfo>();
+    List<HRegionInfo> parentRegion = new ArrayList<>();
     parentRegion.add(parentHri);
     Map<ServerName, List<HRegionInfo>> currentAssign =
         regionStates.getRegionAssignments(parentRegion);
@@ -2804,7 +2800,7 @@ public class AssignmentManager {
       }
     }
     // assign daughter replicas
-    Map<HRegionInfo, ServerName> map = new HashMap<HRegionInfo, ServerName>();
+    Map<HRegionInfo, ServerName> map = new HashMap<>();
     for (int i = 1; i < numReplicas; i++) {
       prepareDaughterReplicaForAssignment(hri_a, parentHri, i, map);
       prepareDaughterReplicaForAssignment(hri_b, parentHri, i, map);
@@ -2856,7 +2852,7 @@ public class AssignmentManager {
     sendRegionClosedNotification(regionInfo);
     // also note that all the replicas of the primary should be closed
     if (state != null && state.equals(State.SPLIT)) {
-      Collection<HRegionInfo> c = new ArrayList<HRegionInfo>(1);
+      Collection<HRegionInfo> c = new ArrayList<>(1);
       c.add(regionInfo);
       Map<ServerName, List<HRegionInfo>> map = regionStates.getRegionAssignments(c);
       Collection<List<HRegionInfo>> allReplicas = map.values();
@@ -2865,7 +2861,7 @@ public class AssignmentManager {
       }
     }
     else if (state != null && state.equals(State.MERGED)) {
-      Collection<HRegionInfo> c = new ArrayList<HRegionInfo>(1);
+      Collection<HRegionInfo> c = new ArrayList<>(1);
       c.add(regionInfo);
       Map<ServerName, List<HRegionInfo>> map = regionStates.getRegionAssignments(c);
       Collection<List<HRegionInfo>> allReplicas = map.values();
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/master/AssignmentVerificationReport.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/master/AssignmentVerificationReport.java
index d290f26..e1922af 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/master/AssignmentVerificationReport.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/master/AssignmentVerificationReport.java
@@ -51,16 +51,13 @@ public class AssignmentVerificationReport {
   private int totalRegions = 0;
   private int totalRegionServers = 0;
   // for unassigned regions
-  private List<HRegionInfo> unAssignedRegionsList =
-    new ArrayList<HRegionInfo>();
+  private List<HRegionInfo> unAssignedRegionsList = new ArrayList<>();
 
   // For regions without valid favored nodes
-  private List<HRegionInfo> regionsWithoutValidFavoredNodes =
-    new ArrayList<HRegionInfo>();
+  private List<HRegionInfo> regionsWithoutValidFavoredNodes = new ArrayList<>();
 
   // For regions not running on the favored nodes
-  private List<HRegionInfo> nonFavoredAssignedRegionList =
-    new ArrayList<HRegionInfo>();
+  private List<HRegionInfo> nonFavoredAssignedRegionList = new ArrayList<>();
 
   // For regions running on the favored nodes
   private int totalFavoredAssignments = 0;
@@ -73,26 +70,20 @@ public class AssignmentVerificationReport {
   private float avgRegionsOnRS = 0;
   private int maxRegionsOnRS = 0;
   private int minRegionsOnRS = Integer.MAX_VALUE;
-  private Set<ServerName> mostLoadedRSSet =
-    new HashSet<ServerName>();
-  private Set<ServerName> leastLoadedRSSet =
-    new HashSet<ServerName>();
+  private Set<ServerName> mostLoadedRSSet = new HashSet<>();
+  private Set<ServerName> leastLoadedRSSet = new HashSet<>();
 
   private float avgDispersionScore = 0;
   private float maxDispersionScore = 0;
-  private Set<ServerName> maxDispersionScoreServerSet =
-    new HashSet<ServerName>();
+  private Set<ServerName> maxDispersionScoreServerSet = new HashSet<>();
   private float minDispersionScore = Float.MAX_VALUE;
-  private Set<ServerName> minDispersionScoreServerSet =
-    new HashSet<ServerName>();
+  private Set<ServerName> minDispersionScoreServerSet = new HashSet<>();
 
   private float avgDispersionNum = 0;
   private float maxDispersionNum = 0;
-  private Set<ServerName> maxDispersionNumServerSet =
-    new HashSet<ServerName>();
+  private Set<ServerName> maxDispersionNumServerSet = new HashSet<>();
   private float minDispersionNum = Float.MAX_VALUE;
-  private Set<ServerName> minDispersionNumServerSet =
-    new HashSet<ServerName>();
+  private Set<ServerName> minDispersionNumServerSet = new HashSet<>();
 
   public void fillUp(TableName tableName, SnapshotOfRegionAssignmentFromMeta snapshot,
       Map<String, Map<String, Float>> regionLocalityMap) {
@@ -111,13 +102,10 @@ public class AssignmentVerificationReport {
     Map<HRegionInfo, ServerName> currentAssignment =
       snapshot.getRegionToRegionServerMap();
     // Initialize the server to its hosing region counter map
-    Map<ServerName, Integer> serverToHostingRegionCounterMap =
-      new HashMap<ServerName, Integer>();
+    Map<ServerName, Integer> serverToHostingRegionCounterMap = new HashMap<>();
 
-    Map<ServerName, Integer> primaryRSToRegionCounterMap =
-      new HashMap<ServerName, Integer>();
-    Map<ServerName, Set<ServerName>> primaryToSecTerRSMap =
-      new HashMap<ServerName, Set<ServerName>>();
+    Map<ServerName, Integer> primaryRSToRegionCounterMap = new HashMap<>();
+    Map<ServerName, Set<ServerName>> primaryToSecTerRSMap = new HashMap<>();
 
     // Check the favored nodes and its locality information
     // Also keep tracker of the most loaded and least loaded region servers
@@ -164,7 +152,7 @@ public class AssignmentVerificationReport {
         // Update the primary rs to secondary and tertiary rs map
         Set<ServerName> secAndTerSet = primaryToSecTerRSMap.get(primaryRS);
         if (secAndTerSet == null) {
-          secAndTerSet = new HashSet<ServerName>();
+          secAndTerSet = new HashSet<>();
         }
         secAndTerSet.add(secondaryRS);
         secAndTerSet.add(tertiaryRS);
@@ -340,10 +328,8 @@ public class AssignmentVerificationReport {
       plan = newPlan;
     }
     // Get the region to region server mapping
-    Map<ServerName, Integer> primaryRSToRegionCounterMap =
-        new HashMap<ServerName, Integer>();
-    Map<ServerName, Set<ServerName>> primaryToSecTerRSMap =
-        new HashMap<ServerName, Set<ServerName>>();
+    Map<ServerName, Integer> primaryRSToRegionCounterMap = new HashMap<>();
+    Map<ServerName, Set<ServerName>> primaryToSecTerRSMap = new HashMap<>();
 
     // Check the favored nodes and its locality information
     // Also keep tracker of the most loaded and least loaded region servers
@@ -375,7 +361,7 @@ public class AssignmentVerificationReport {
         // Update the primary rs to secondary and tertiary rs map
         Set<ServerName> secAndTerSet = primaryToSecTerRSMap.get(primaryRS);
         if (secAndTerSet == null) {
-          secAndTerSet = new HashSet<ServerName>();
+          secAndTerSet = new HashSet<>();
         }
         secAndTerSet.add(secondaryRS);
         secAndTerSet.add(tertiaryRS);
@@ -451,7 +437,7 @@ public class AssignmentVerificationReport {
    *
    */
   public List<Float> getDispersionInformation() {
-    List<Float> dispersion = new ArrayList<Float>();
+    List<Float> dispersion = new ArrayList<>();
     dispersion.add(avgDispersionScore);
     dispersion.add(maxDispersionScore);
     dispersion.add(minDispersionScore);
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/master/BulkReOpen.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/master/BulkReOpen.java
index 606dce4..d8c511e 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/master/BulkReOpen.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/master/BulkReOpen.java
@@ -59,7 +59,7 @@ public class BulkReOpen extends BulkAssigner {
         .entrySet()) {
       final List<HRegionInfo> hris = e.getValue();
       // add plans for the regions that need to be reopened
-      Map<String, RegionPlan> plans = new HashMap<String, RegionPlan>();
+      Map<String, RegionPlan> plans = new HashMap<>();
       for (HRegionInfo hri : hris) {
         RegionPlan reOpenPlan = assignmentManager.getRegionReopenPlan(hri);
         plans.put(hri.getEncodedName(), reOpenPlan);
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/master/CatalogJanitor.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/master/CatalogJanitor.java
index ef042af..affd44c 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/master/CatalogJanitor.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/master/CatalogJanitor.java
@@ -151,9 +151,8 @@ public class CatalogJanitor extends ScheduledChore {
     final AtomicInteger count = new AtomicInteger(0);
     // Keep Map of found split parents.  There are candidates for cleanup.
     // Use a comparator that has split parents come before its daughters.
-    final Map<HRegionInfo, Result> splitParents =
-      new TreeMap<HRegionInfo, Result>(new SplitParentFirstComparator());
-    final Map<HRegionInfo, Result> mergedRegions = new TreeMap<HRegionInfo, Result>();
+    final Map<HRegionInfo, Result> splitParents = new TreeMap<>(new SplitParentFirstComparator());
+    final Map<HRegionInfo, Result> mergedRegions = new TreeMap<>();
     // This visitor collects split parents and counts rows in the hbase:meta table
 
     MetaTableAccessor.Visitor visitor = new MetaTableAccessor.Visitor() {
@@ -181,8 +180,7 @@ public class CatalogJanitor extends ScheduledChore {
     // the start row
     MetaTableAccessor.scanMetaForTableRegions(this.connection, visitor, tableName);
 
-    return new Triple<Integer, Map<HRegionInfo, Result>, Map<HRegionInfo, Result>>(
-        count.get(), mergedRegions, splitParents);
+    return new Triple<>(count.get(), mergedRegions, splitParents);
   }
 
   /**
@@ -275,7 +273,7 @@ public class CatalogJanitor extends ScheduledChore {
       // Now work on our list of found parents. See if any we can clean up.
       int splitCleaned = 0;
       // regions whose parents are still around
-      HashSet<String> parentNotCleaned = new HashSet<String>();
+      HashSet<String> parentNotCleaned = new HashSet<>();
       for (Map.Entry<HRegionInfo, Result> e : splitParents.entrySet()) {
         if (this.services.isInMaintenanceMode()) {
           // Stop cleaning if the master is in maintenance mode
@@ -398,7 +396,7 @@ public class CatalogJanitor extends ScheduledChore {
   Pair<Boolean, Boolean> checkDaughterInFs(final HRegionInfo parent, final HRegionInfo daughter)
   throws IOException {
     if (daughter == null)  {
-      return new Pair<Boolean, Boolean>(Boolean.FALSE, Boolean.FALSE);
+      return new Pair<>(Boolean.FALSE, Boolean.FALSE);
     }
 
     FileSystem fs = this.services.getMasterFileSystem().getFileSystem();
@@ -411,12 +409,12 @@ public class CatalogJanitor extends ScheduledChore {
 
     try {
       if (!FSUtils.isExists(fs, daughterRegionDir)) {
-        return new Pair<Boolean, Boolean>(Boolean.FALSE, Boolean.FALSE);
+        return new Pair<>(Boolean.FALSE, Boolean.FALSE);
       }
     } catch (IOException ioe) {
       LOG.error("Error trying to determine if daughter region exists, " +
                "assuming exists and has references", ioe);
-      return new Pair<Boolean, Boolean>(Boolean.TRUE, Boolean.TRUE);
+      return new Pair<>(Boolean.TRUE, Boolean.TRUE);
     }
 
     boolean references = false;
@@ -433,9 +431,9 @@ public class CatalogJanitor extends ScheduledChore {
     } catch (IOException e) {
       LOG.error("Error trying to determine referenced files from : " + daughter.getEncodedName()
           + ", to: " + parent.getEncodedName() + " assuming has references", e);
-      return new Pair<Boolean, Boolean>(Boolean.TRUE, Boolean.TRUE);
+      return new Pair<>(Boolean.TRUE, Boolean.TRUE);
     }
-    return new Pair<Boolean, Boolean>(Boolean.TRUE, Boolean.valueOf(references));
+    return new Pair<>(Boolean.TRUE, Boolean.valueOf(references));
   }
 
   private HTableDescriptor getTableDescriptor(final TableName tableName)
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/master/ClusterSchemaServiceImpl.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/master/ClusterSchemaServiceImpl.java
index 52af89e..bf3ae7e 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/master/ClusterSchemaServiceImpl.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/master/ClusterSchemaServiceImpl.java
@@ -122,7 +122,7 @@ class ClusterSchemaServiceImpl implements ClusterSchemaService {
     checkIsRunning();
     Set<NamespaceDescriptor> set = getTableNamespaceManager().list();
     if (set == null || set.isEmpty()) return EMPTY_NAMESPACE_LIST;
-    List<NamespaceDescriptor> list = new ArrayList<NamespaceDescriptor>(set.size());
+    List<NamespaceDescriptor> list = new ArrayList<>(set.size());
     list.addAll(set);
     return Collections.unmodifiableList(list);
   }
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/master/ClusterStatusPublisher.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/master/ClusterStatusPublisher.java
index 3b19ada..ea5516d 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/master/ClusterStatusPublisher.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/master/ClusterStatusPublisher.java
@@ -97,8 +97,7 @@ public class ClusterStatusPublisher extends ScheduledChore {
   private long lastMessageTime = 0;
   private final HMaster master;
   private final int messagePeriod; // time between two message
-  private final ConcurrentMap<ServerName, Integer> lastSent =
-      new ConcurrentHashMap<ServerName, Integer>();
+  private final ConcurrentMap<ServerName, Integer> lastSent = new ConcurrentHashMap<>();
   private Publisher publisher;
   private boolean connected = false;
 
@@ -194,7 +193,7 @@ public class ClusterStatusPublisher extends ScheduledChore {
     }
 
     // We're sending the new deads first.
-    List<Map.Entry<ServerName, Integer>> entries = new ArrayList<Map.Entry<ServerName, Integer>>();
+    List<Map.Entry<ServerName, Integer>> entries = new ArrayList<>();
     entries.addAll(lastSent.entrySet());
     Collections.sort(entries, new Comparator<Map.Entry<ServerName, Integer>>() {
       @Override
@@ -205,7 +204,7 @@ public class ClusterStatusPublisher extends ScheduledChore {
 
     // With a limit of MAX_SERVER_PER_MESSAGE
     int max = entries.size() > MAX_SERVER_PER_MESSAGE ? MAX_SERVER_PER_MESSAGE : entries.size();
-    List<ServerName> res = new ArrayList<ServerName>(max);
+    List<ServerName> res = new ArrayList<>(max);
 
     for (int i = 0; i < max; i++) {
       Map.Entry<ServerName, Integer> toSend = entries.get(i);
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/master/DeadServer.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/master/DeadServer.java
index c33cdcc..faceba2 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/master/DeadServer.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/master/DeadServer.java
@@ -51,7 +51,7 @@ public class DeadServer {
    * and it's server logs are recovered, it will be told to call server startup
    * because by then, its regions have probably been reassigned.
    */
-  private final Map<ServerName, Long> deadServers = new HashMap<ServerName, Long>();
+  private final Map<ServerName, Long> deadServers = new HashMap<>();
 
   /**
    * Number of dead servers currently being processed
@@ -102,7 +102,7 @@ public class DeadServer {
   public synchronized boolean areDeadServersInProgress() { return processing; }
 
   public synchronized Set<ServerName> copyServerNames() {
-    Set<ServerName> clone = new HashSet<ServerName>(deadServers.size());
+    Set<ServerName> clone = new HashSet<>(deadServers.size());
     clone.addAll(deadServers.keySet());
     return clone;
   }
@@ -177,11 +177,11 @@ public class DeadServer {
    * @return a sorted array list, by death time, lowest values first.
    */
   public synchronized List<Pair<ServerName, Long>> copyDeadServersSince(long ts){
-    List<Pair<ServerName, Long>> res =  new ArrayList<Pair<ServerName, Long>>(size());
+    List<Pair<ServerName, Long>> res =  new ArrayList<>(size());
 
     for (Map.Entry<ServerName, Long> entry:deadServers.entrySet()){
       if (entry.getValue() >= ts){
-        res.add(new Pair<ServerName, Long>(entry.getKey(), entry.getValue()));
+        res.add(new Pair<>(entry.getKey(), entry.getValue()));
       }
     }
 
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/master/GeneralBulkAssigner.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/master/GeneralBulkAssigner.java
index 43ea523..fc3607f 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/master/GeneralBulkAssigner.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/master/GeneralBulkAssigner.java
@@ -43,8 +43,7 @@ import org.apache.hadoop.hbase.ServerName;
 public class GeneralBulkAssigner extends BulkAssigner {
   private static final Log LOG = LogFactory.getLog(GeneralBulkAssigner.class);
 
-  private Map<ServerName, List<HRegionInfo>> failedPlans
-    = new ConcurrentHashMap<ServerName, List<HRegionInfo>>();
+  private Map<ServerName, List<HRegionInfo>> failedPlans = new ConcurrentHashMap<>();
   private ExecutorService pool;
 
   final Map<ServerName, List<HRegionInfo>> bulkPlan;
@@ -82,7 +81,7 @@ public class GeneralBulkAssigner extends BulkAssigner {
   @Override
   protected boolean waitUntilDone(final long timeout)
   throws InterruptedException {
-    Set<HRegionInfo> regionSet = new HashSet<HRegionInfo>();
+    Set<HRegionInfo> regionSet = new HashSet<>();
     for (List<HRegionInfo> regionList : bulkPlan.values()) {
       regionSet.addAll(regionList);
     }
@@ -164,7 +163,7 @@ public class GeneralBulkAssigner extends BulkAssigner {
   }
 
   private int reassignFailedPlans() {
-    List<HRegionInfo> reassigningRegions = new ArrayList<HRegionInfo>();
+    List<HRegionInfo> reassigningRegions = new ArrayList<>();
     for (Map.Entry<ServerName, List<HRegionInfo>> e : failedPlans.entrySet()) {
       LOG.info("Failed assigning " + e.getValue().size()
           + " regions to server " + e.getKey() + ", reassigning them");
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/master/HMaster.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/master/HMaster.java
index 78f1783..501d3bd 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/master/HMaster.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/master/HMaster.java
@@ -1063,7 +1063,7 @@ public class HMaster extends HRegionServer implements MasterServices {
 
    //start the hfile archive cleaner thread
     Path archiveDir = HFileArchiveUtil.getArchivePath(conf);
-    Map<String, Object> params = new HashMap<String, Object>();
+    Map<String, Object> params = new HashMap<>();
     params.put(MASTER, this);
     this.hfileCleaner = new HFileCleaner(cleanerInterval, this, conf, getMasterFileSystem()
         .getFileSystem(), archiveDir, params);
@@ -1327,7 +1327,7 @@ public class HMaster extends HRegionServer implements MasterServices {
       Map<TableName, Map<ServerName, List<HRegionInfo>>> assignmentsByTable =
         this.assignmentManager.getRegionStates().getAssignmentsByTable();
 
-      List<RegionPlan> plans = new ArrayList<RegionPlan>();
+      List<RegionPlan> plans = new ArrayList<>();
 
       //Give the balancer the current cluster state.
       this.balancer.setClusterStatus(getClusterStatus());
@@ -2235,8 +2235,7 @@ public class HMaster extends HRegionServer implements MasterServices {
   Pair<HRegionInfo, ServerName> getTableRegionForRow(
       final TableName tableName, final byte [] rowKey)
   throws IOException {
-    final AtomicReference<Pair<HRegionInfo, ServerName>> result =
-      new AtomicReference<Pair<HRegionInfo, ServerName>>(null);
+    final AtomicReference<Pair<HRegionInfo, ServerName>> result = new AtomicReference<>(null);
 
     MetaTableAccessor.Visitor visitor = new MetaTableAccessor.Visitor() {
         @Override
@@ -2345,7 +2344,7 @@ public class HMaster extends HRegionServer implements MasterServices {
 
     List<ServerName> backupMasters = null;
     if (backupMasterStrings != null && !backupMasterStrings.isEmpty()) {
-      backupMasters = new ArrayList<ServerName>(backupMasterStrings.size());
+      backupMasters = new ArrayList<>(backupMasterStrings.size());
       for (String s: backupMasterStrings) {
         try {
           byte [] bytes;
@@ -2852,7 +2851,7 @@ public class HMaster extends HRegionServer implements MasterServices {
    */
   List<NamespaceDescriptor> getNamespaces() throws IOException {
     checkInitialized();
-    final List<NamespaceDescriptor> nsds = new ArrayList<NamespaceDescriptor>();
+    final List<NamespaceDescriptor> nsds = new ArrayList<>();
     boolean bypass = false;
     if (cpHost != null) {
       bypass = cpHost.preListNamespaceDescriptors(nsds);
@@ -2918,7 +2917,7 @@ public class HMaster extends HRegionServer implements MasterServices {
   public List<HTableDescriptor> listTableDescriptors(final String namespace, final String regex,
       final List<TableName> tableNameList, final boolean includeSysTables)
   throws IOException {
-    List<HTableDescriptor> htds = new ArrayList<HTableDescriptor>();
+    List<HTableDescriptor> htds = new ArrayList<>();
     boolean bypass = cpHost != null?
         cpHost.preGetTableDescriptors(tableNameList, htds, regex): false;
     if (!bypass) {
@@ -2939,13 +2938,13 @@ public class HMaster extends HRegionServer implements MasterServices {
    */
   public List<TableName> listTableNames(final String namespace, final String regex,
       final boolean includeSysTables) throws IOException {
-    List<HTableDescriptor> htds = new ArrayList<HTableDescriptor>();
+    List<HTableDescriptor> htds = new ArrayList<>();
     boolean bypass = cpHost != null? cpHost.preGetTableNames(htds, regex): false;
     if (!bypass) {
       htds = getTableDescriptors(htds, namespace, regex, null, includeSysTables);
       if (cpHost != null) cpHost.postGetTableNames(htds, regex);
     }
-    List<TableName> result = new ArrayList<TableName>(htds.size());
+    List<TableName> result = new ArrayList<>(htds.size());
     for (HTableDescriptor htd: htds) result.add(htd.getTableName());
     return result;
   }
@@ -3262,7 +3261,7 @@ public class HMaster extends HRegionServer implements MasterServices {
   @Override
   public List<ServerName> listDrainingRegionServers() {
     String parentZnode = getZooKeeper().znodePaths.drainingZNode;
-    List<ServerName> serverNames = new ArrayList<ServerName>();
+    List<ServerName> serverNames = new ArrayList<>();
     List<String> serverStrs = null;
     try {
       serverStrs = ZKUtil.listChildrenNoWatch(getZooKeeper(), parentZnode);
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/master/MasterMetaBootstrap.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/master/MasterMetaBootstrap.java
index 5e1917b..1988e2d 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/master/MasterMetaBootstrap.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/master/MasterMetaBootstrap.java
@@ -100,7 +100,7 @@ public class MasterMetaBootstrap {
       throws IOException, InterruptedException, KeeperException {
     int numReplicas = master.getConfiguration().getInt(HConstants.META_REPLICAS_NUM,
            HConstants.DEFAULT_META_REPLICA_NUM);
-    final Set<ServerName> EMPTY_SET = new HashSet<ServerName>();
+    final Set<ServerName> EMPTY_SET = new HashSet<>();
     for (int i = 1; i < numReplicas; i++) {
       assignMeta(EMPTY_SET, i);
     }
@@ -241,7 +241,7 @@ public class MasterMetaBootstrap {
    */
   private Set<ServerName> getPreviouselyFailedMetaServersFromZK() throws KeeperException {
     final ZooKeeperWatcher zooKeeper = master.getZooKeeper();
-    Set<ServerName> result = new HashSet<ServerName>();
+    Set<ServerName> result = new HashSet<>();
     String metaRecoveringZNode = ZKUtil.joinZNode(zooKeeper.znodePaths.recoveringRegionsZNode,
       HRegionInfo.FIRST_META_REGIONINFO.getEncodedName());
     List<String> regionFailedServers = ZKUtil.listChildrenNoWatch(zooKeeper, metaRecoveringZNode);
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/master/MasterMobCompactionThread.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/master/MasterMobCompactionThread.java
index fc0ecfb..2b1232a 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/master/MasterMobCompactionThread.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/master/MasterMobCompactionThread.java
@@ -56,7 +56,7 @@ public class MasterMobCompactionThread {
     final String n = Thread.currentThread().getName();
     // this pool is used to run the mob compaction
     this.masterMobPool = new ThreadPoolExecutor(1, 2, 60, TimeUnit.SECONDS,
-      new SynchronousQueue<Runnable>(), new ThreadFactory() {
+      new SynchronousQueue<>(), new ThreadFactory() {
         @Override
         public Thread newThread(Runnable r) {
           String name = n + "-MasterMobCompaction-" + EnvironmentEdgeManager.currentTime();
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/master/MasterRpcServices.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/master/MasterRpcServices.java
index 3beda05..177ee32 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/master/MasterRpcServices.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/master/MasterRpcServices.java
@@ -234,7 +234,7 @@ public class MasterRpcServices extends RSRpcServices
    * @return list of blocking services and their security info classes that this server supports
    */
   protected List<BlockingServiceAndInterface> getServices() {
-    List<BlockingServiceAndInterface> bssi = new ArrayList<BlockingServiceAndInterface>(5);
+    List<BlockingServiceAndInterface> bssi = new ArrayList<>(5);
     bssi.add(new BlockingServiceAndInterface(
       MasterService.newReflectiveBlockingService(this),
       MasterService.BlockingInterface.class));
@@ -1333,7 +1333,7 @@ public class MasterRpcServices extends RSRpcServices
       Pair<HRegionInfo, ServerName> pair =
         MetaTableAccessor.getRegion(master.getConnection(), regionName);
       if (Bytes.equals(HRegionInfo.FIRST_META_REGIONINFO.getRegionName(),regionName)) {
-        pair = new Pair<HRegionInfo, ServerName>(HRegionInfo.FIRST_META_REGIONINFO,
+        pair = new Pair<>(HRegionInfo.FIRST_META_REGIONINFO,
             master.getMetaTableLocator().getMetaRegionLocation(master.getZooKeeper()));
       }
       if (pair == null) {
@@ -1491,7 +1491,7 @@ public class MasterRpcServices extends RSRpcServices
       throw new DoNotRetryIOException("Table " + tableName + " is not enabled");
     }
     boolean allFiles = false;
-    List<HColumnDescriptor> compactedColumns = new ArrayList<HColumnDescriptor>();
+    List<HColumnDescriptor> compactedColumns = new ArrayList<>();
     HColumnDescriptor[] hcds = master.getTableDescriptors().get(tableName).getColumnFamilies();
     byte[] family = null;
     if (request.hasFamily()) {
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/master/MasterWalManager.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/master/MasterWalManager.java
index 1f9729c..27aca94d 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/master/MasterWalManager.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/master/MasterWalManager.java
@@ -155,7 +155,7 @@ public class MasterWalManager {
     boolean retrySplitting = !conf.getBoolean("hbase.hlog.split.skip.errors",
         WALSplitter.SPLIT_SKIP_ERRORS_DEFAULT);
 
-    Set<ServerName> serverNames = new HashSet<ServerName>();
+    Set<ServerName> serverNames = new HashSet<>();
     Path logsDirPath = new Path(this.rootDir, HConstants.HREGION_LOGDIR_NAME);
 
     do {
@@ -218,7 +218,7 @@ public class MasterWalManager {
   }
 
   public void splitLog(final ServerName serverName) throws IOException {
-    Set<ServerName> serverNames = new HashSet<ServerName>();
+    Set<ServerName> serverNames = new HashSet<>();
     serverNames.add(serverName);
     splitLog(serverNames);
   }
@@ -228,7 +228,7 @@ public class MasterWalManager {
    * @param serverName logs belonging to this server will be split
    */
   public void splitMetaLog(final ServerName serverName) throws IOException {
-    Set<ServerName> serverNames = new HashSet<ServerName>();
+    Set<ServerName> serverNames = new HashSet<>();
     serverNames.add(serverName);
     splitMetaLog(serverNames);
   }
@@ -245,7 +245,7 @@ public class MasterWalManager {
       "We only release this lock when we set it. Updates to code that uses it should verify use " +
       "of the guard boolean.")
   private List<Path> getLogDirs(final Set<ServerName> serverNames) throws IOException {
-    List<Path> logDirs = new ArrayList<Path>();
+    List<Path> logDirs = new ArrayList<>();
     boolean needReleaseLock = false;
     if (!this.services.isInitialized()) {
       // during master initialization, we could have multiple places splitting a same wal
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/master/RackManager.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/master/RackManager.java
index 2b1fb50..5c06857 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/master/RackManager.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/master/RackManager.java
@@ -80,7 +80,7 @@ public class RackManager {
   public List<String> getRack(List<ServerName> servers) {
     // just a note - switchMapping caches results (at least the implementation should unless the
     // resolution is really a lightweight process)
-    List<String> serversAsString = new ArrayList<String>(servers.size());
+    List<String> serversAsString = new ArrayList<>(servers.size());
     for (ServerName server : servers) {
       serversAsString.add(server.getHostname());
     }
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/master/RegionPlacementMaintainer.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/master/RegionPlacementMaintainer.java
index 7acf9df..ffc3e5b 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/master/RegionPlacementMaintainer.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/master/RegionPlacementMaintainer.java
@@ -106,7 +106,7 @@ public class RegionPlacementMaintainer {
     this.conf = conf;
     this.enforceLocality = enforceLocality;
     this.enforceMinAssignmentMove = enforceMinAssignmentMove;
-    this.targetTableSet = new HashSet<TableName>();
+    this.targetTableSet = new HashSet<>();
     this.rackManager = new RackManager(conf);
     try {
       this.connection = ConnectionFactory.createConnection(this.conf);
@@ -163,7 +163,7 @@ public class RegionPlacementMaintainer {
     if (this.enforceLocality == true) {
       regionLocalityMap = FSUtils.getRegionDegreeLocalityMappingFromFS(conf);
     }
-    List<AssignmentVerificationReport> reports = new ArrayList<AssignmentVerificationReport>();
+    List<AssignmentVerificationReport> reports = new ArrayList<>();
     // Iterate all the tables to fill up the verification report
     for (TableName table : tables) {
       if (!this.targetTableSet.isEmpty() &&
@@ -204,7 +204,7 @@ public class RegionPlacementMaintainer {
         assignmentSnapshot.getRegionToRegionServerMap();
 
       // Get the all the region servers
-      List<ServerName> servers = new ArrayList<ServerName>();
+      List<ServerName> servers = new ArrayList<>();
       try (Admin admin = this.connection.getAdmin()) {
         servers.addAll(admin.getClusterStatus().getServers());
       }
@@ -255,15 +255,14 @@ public class RegionPlacementMaintainer {
         // Compute the total rack locality for each region in each rack. The total
         // rack locality is the sum of the localities of a region on all servers in
         // a rack.
-        Map<String, Map<HRegionInfo, Float>> rackRegionLocality =
-            new HashMap<String, Map<HRegionInfo, Float>>();
+        Map<String, Map<HRegionInfo, Float>> rackRegionLocality = new HashMap<>();
         for (int i = 0; i < numRegions; i++) {
           HRegionInfo region = regions.get(i);
           for (int j = 0; j < regionSlots; j += slotsPerServer) {
             String rack = rackManager.getRack(servers.get(j / slotsPerServer));
             Map<HRegionInfo, Float> rackLocality = rackRegionLocality.get(rack);
             if (rackLocality == null) {
-              rackLocality = new HashMap<HRegionInfo, Float>();
+              rackLocality = new HashMap<>();
               rackRegionLocality.put(rack, rackLocality);
             }
             Float localityObj = rackLocality.get(region);
@@ -395,8 +394,7 @@ public class RegionPlacementMaintainer {
         tertiaryAssignment = randomizedMatrix.invertIndices(tertiaryAssignment);
 
         for (int i = 0; i < numRegions; i++) {
-          List<ServerName> favoredServers =
-            new ArrayList<ServerName>(FavoredNodeAssignmentHelper.FAVORED_NODES_NUM);
+          List<ServerName> favoredServers = new ArrayList<>(FavoredNodeAssignmentHelper.FAVORED_NODES_NUM);
           ServerName s = servers.get(primaryAssignment[i] / slotsPerServer);
           favoredServers.add(ServerName.valueOf(s.getHostname(), s.getPort(),
               ServerName.NON_STARTCODE));
@@ -417,7 +415,7 @@ public class RegionPlacementMaintainer {
         LOG.info("Assignment plan for secondary and tertiary generated " +
             "using MunkresAssignment");
       } else {
-        Map<HRegionInfo, ServerName> primaryRSMap = new HashMap<HRegionInfo, ServerName>();
+        Map<HRegionInfo, ServerName> primaryRSMap = new HashMap<>();
         for (int i = 0; i < numRegions; i++) {
           primaryRSMap.put(regions.get(i), servers.get(primaryAssignment[i] / slotsPerServer));
         }
@@ -427,8 +425,7 @@ public class RegionPlacementMaintainer {
         Map<HRegionInfo, ServerName[]> secondaryAndTertiaryMap =
             favoredNodeHelper.placeSecondaryAndTertiaryWithRestrictions(primaryRSMap);
         for (int i = 0; i < numRegions; i++) {
-          List<ServerName> favoredServers =
-            new ArrayList<ServerName>(FavoredNodeAssignmentHelper.FAVORED_NODES_NUM);
+          List<ServerName> favoredServers = new ArrayList<>(FavoredNodeAssignmentHelper.FAVORED_NODES_NUM);
           HRegionInfo currentRegion = regions.get(i);
           ServerName s = primaryRSMap.get(currentRegion);
           favoredServers.add(ServerName.valueOf(s.getHostname(), s.getPort(),
@@ -614,8 +611,7 @@ public class RegionPlacementMaintainer {
     if (plan == null) return;
     LOG.info("========== Start to print the assignment plan ================");
     // sort the map based on region info
-    Map<String, List<ServerName>> assignmentMap =
-      new TreeMap<String, List<ServerName>>(plan.getAssignmentMap());
+    Map<String, List<ServerName>> assignmentMap = new TreeMap<>(plan.getAssignmentMap());
 
     for (Map.Entry<String, List<ServerName>> entry : assignmentMap.entrySet()) {
 
@@ -666,13 +662,11 @@ public class RegionPlacementMaintainer {
 
     // track of the failed and succeeded updates
     int succeededNum = 0;
-    Map<ServerName, Exception> failedUpdateMap =
-      new HashMap<ServerName, Exception>();
+    Map<ServerName, Exception> failedUpdateMap = new HashMap<>();
 
     for (Map.Entry<ServerName, List<HRegionInfo>> entry :
       currentAssignment.entrySet()) {
-      List<Pair<HRegionInfo, List<ServerName>>> regionUpdateInfos =
-          new ArrayList<Pair<HRegionInfo, List<ServerName>>>();
+      List<Pair<HRegionInfo, List<ServerName>>> regionUpdateInfos = new ArrayList<>();
       try {
         // Keep track of the favored updates for the current region server
         FavoredNodesPlan singleServerPlan = null;
@@ -687,8 +681,7 @@ public class RegionPlacementMaintainer {
             }
             // Update the single server update
             singleServerPlan.updateFavoredNodesMap(region, favoredServerList);
-            regionUpdateInfos.add(
-              new Pair<HRegionInfo, List<ServerName>>(region, favoredServerList));
+            regionUpdateInfos.add(new Pair<>(region, favoredServerList));
           }
         }
         if (singleServerPlan != null) {
@@ -749,7 +742,7 @@ public class RegionPlacementMaintainer {
    */
   public Map<TableName, Integer> getRegionsMovement(FavoredNodesPlan newPlan)
       throws IOException {
-    Map<TableName, Integer> movesPerTable = new HashMap<TableName, Integer>();
+    Map<TableName, Integer> movesPerTable = new HashMap<>();
     SnapshotOfRegionAssignmentFromMeta snapshot = this.getRegionAssignmentSnapshot();
     Map<TableName, List<HRegionInfo>> tableToRegions = snapshot
         .getTableToRegionMap();
@@ -944,7 +937,7 @@ public class RegionPlacementMaintainer {
     if (favoredNodesArray == null)
       return null;
 
-    List<ServerName> serverList = new ArrayList<ServerName>();
+    List<ServerName> serverList = new ArrayList<>();
     for (String hostNameAndPort : favoredNodesArray) {
       serverList.add(ServerName.valueOf(hostNameAndPort, ServerName.NON_STARTCODE));
     }
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/master/RegionStates.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/master/RegionStates.java
index 4125eea..a1e24f2 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/master/RegionStates.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/master/RegionStates.java
@@ -80,41 +80,35 @@ public class RegionStates {
   /**
    * Regions currently in transition.
    */
-  final HashMap<String, RegionState> regionsInTransition =
-    new HashMap<String, RegionState>();
+  final HashMap<String, RegionState> regionsInTransition = new HashMap<>();
 
   /**
    * Region encoded name to state map.
    * All the regions should be in this map.
    */
-  private final Map<String, RegionState> regionStates =
-    new HashMap<String, RegionState>();
+  private final Map<String, RegionState> regionStates = new HashMap<>();
 
   /**
    * Holds mapping of table -> region state
    */
-  private final Map<TableName, Map<String, RegionState>> regionStatesTableIndex =
-      new HashMap<TableName, Map<String, RegionState>>();
+  private final Map<TableName, Map<String, RegionState>> regionStatesTableIndex = new HashMap<>();
 
   /**
    * Server to regions assignment map.
    * Contains the set of regions currently assigned to a given server.
    */
-  private final Map<ServerName, Set<HRegionInfo>> serverHoldings =
-    new HashMap<ServerName, Set<HRegionInfo>>();
+  private final Map<ServerName, Set<HRegionInfo>> serverHoldings = new HashMap<>();
 
   /**
    * Maintains the mapping from the default region to the replica regions.
    */
-  private final Map<HRegionInfo, Set<HRegionInfo>> defaultReplicaToOtherReplicas =
-    new HashMap<HRegionInfo, Set<HRegionInfo>>();
+  private final Map<HRegionInfo, Set<HRegionInfo>> defaultReplicaToOtherReplicas = new HashMap<>();
 
   /**
    * Region to server assignment map.
    * Contains the server a given region is currently assigned to.
    */
-  private final TreeMap<HRegionInfo, ServerName> regionAssignments =
-    new TreeMap<HRegionInfo, ServerName>();
+  private final TreeMap<HRegionInfo, ServerName> regionAssignments = new TreeMap<>();
 
   /**
    * Encoded region name to server assignment map for re-assignment
@@ -126,8 +120,7 @@ public class RegionStates {
    * is offline while the info in lastAssignments is cleared when
    * the region is closed or the server is dead and processed.
    */
-  private final HashMap<String, ServerName> lastAssignments =
-    new HashMap<String, ServerName>();
+  private final HashMap<String, ServerName> lastAssignments = new HashMap<>();
 
   /**
    * Encoded region name to server assignment map for the
@@ -138,16 +131,14 @@ public class RegionStates {
    * to match the meta. We need this map to find out the old server
    * whose serverHoldings needs cleanup, given a moved region.
    */
-  private final HashMap<String, ServerName> oldAssignments =
-    new HashMap<String, ServerName>();
+  private final HashMap<String, ServerName> oldAssignments = new HashMap<>();
 
   /**
    * Map a host port pair string to the latest start code
    * of a region server which is known to be dead. It is dead
    * to us, but server manager may not know it yet.
    */
-  private final HashMap<String, Long> deadServers =
-    new HashMap<String, Long>();
+  private final HashMap<String, Long> deadServers = new HashMap<>();
 
   /**
    * Map a dead servers to the time when log split is done.
@@ -156,8 +147,7 @@ public class RegionStates {
    * on a configured time. By default, we assume a dead
    * server should be done with log splitting in two hours.
    */
-  private final HashMap<ServerName, Long> processedServers =
-    new HashMap<ServerName, Long>();
+  private final HashMap<ServerName, Long> processedServers = new HashMap<>();
   private long lastProcessedServerCleanTime;
 
   private final TableStateManager tableStateManager;
@@ -181,7 +171,7 @@ public class RegionStates {
    * @return a copy of the region assignment map
    */
   public synchronized Map<HRegionInfo, ServerName> getRegionAssignments() {
-    return new TreeMap<HRegionInfo, ServerName>(regionAssignments);
+    return new TreeMap<>(regionAssignments);
   }
 
   /**
@@ -191,7 +181,7 @@ public class RegionStates {
    */
   synchronized Map<ServerName, List<HRegionInfo>> getRegionAssignments(
     Collection<HRegionInfo> regions) {
-    Map<ServerName, List<HRegionInfo>> map = new HashMap<ServerName, List<HRegionInfo>>();
+    Map<ServerName, List<HRegionInfo>> map = new HashMap<>();
     for (HRegionInfo region : regions) {
       HRegionInfo defaultReplica = RegionReplicaUtil.getRegionInfoForDefaultReplica(region);
       Set<HRegionInfo> allReplicas = defaultReplicaToOtherReplicas.get(defaultReplica);
@@ -201,7 +191,7 @@ public class RegionStates {
           if (server != null) {
             List<HRegionInfo> regionsOnServer = map.get(server);
             if (regionsOnServer == null) {
-              regionsOnServer = new ArrayList<HRegionInfo>(1);
+              regionsOnServer = new ArrayList<>(1);
               map.put(server, regionsOnServer);
             }
             regionsOnServer.add(hri);
@@ -220,11 +210,11 @@ public class RegionStates {
    * Get regions in transition and their states
    */
   public synchronized Set<RegionState> getRegionsInTransition() {
-    return new HashSet<RegionState>(regionsInTransition.values());
+    return new HashSet<>(regionsInTransition.values());
   }
 
   public synchronized SortedSet<RegionState> getRegionsInTransitionOrderedByTimestamp() {
-    final TreeSet<RegionState> rit = new TreeSet<RegionState>(REGION_STATE_COMPARATOR);
+    final TreeSet<RegionState> rit = new TreeSet<>(REGION_STATE_COMPARATOR);
     for (RegionState rs: regionsInTransition.values()) {
       rit.add(rs);
     }
@@ -404,7 +394,7 @@ public class RegionStates {
     RegionState oldState = regionStates.put(encodedName, regionState);
     Map<String, RegionState> map = regionStatesTableIndex.get(table);
     if (map == null) {
-      map = new HashMap<String, RegionState>();
+      map = new HashMap<>();
       regionStatesTableIndex.put(table, map);
     }
     map.put(encodedName, regionState);
@@ -483,7 +473,7 @@ public class RegionStates {
   private void addToServerHoldings(ServerName serverName, HRegionInfo hri) {
     Set<HRegionInfo> regions = serverHoldings.get(serverName);
     if (regions == null) {
-      regions = new HashSet<HRegionInfo>();
+      regions = new HashSet<>();
       serverHoldings.put(serverName, regions);
     }
     regions.add(hri);
@@ -494,7 +484,7 @@ public class RegionStates {
     Set<HRegionInfo> replicas =
         defaultReplicaToOtherReplicas.get(defaultReplica);
     if (replicas == null) {
-      replicas = new HashSet<HRegionInfo>();
+      replicas = new HashSet<>();
       defaultReplicaToOtherReplicas.put(defaultReplica, replicas);
     }
     replicas.add(hri);
@@ -618,16 +608,16 @@ public class RegionStates {
    */
   public List<HRegionInfo> serverOffline(final ServerName sn) {
     // Offline all regions on this server not already in transition.
-    List<HRegionInfo> rits = new ArrayList<HRegionInfo>();
-    Set<HRegionInfo> regionsToCleanIfNoMetaEntry = new HashSet<HRegionInfo>();
+    List<HRegionInfo> rits = new ArrayList<>();
+    Set<HRegionInfo> regionsToCleanIfNoMetaEntry = new HashSet<>();
     // Offline regions outside the loop and synchronized block to avoid
     // ConcurrentModificationException and deadlock in case of meta anassigned,
     // but RegionState a blocked.
-    Set<HRegionInfo> regionsToOffline = new HashSet<HRegionInfo>();
+    Set<HRegionInfo> regionsToOffline = new HashSet<>();
     synchronized (this) {
       Set<HRegionInfo> assignedRegions = serverHoldings.get(sn);
       if (assignedRegions == null) {
-        assignedRegions = new HashSet<HRegionInfo>();
+        assignedRegions = new HashSet<>();
       }
 
       for (HRegionInfo region : assignedRegions) {
@@ -711,7 +701,7 @@ public class RegionStates {
    * @return Online regions from <code>tableName</code>
    */
   public synchronized List<HRegionInfo> getRegionsOfTable(TableName tableName) {
-    List<HRegionInfo> tableRegions = new ArrayList<HRegionInfo>();
+    List<HRegionInfo> tableRegions = new ArrayList<>();
     // boundary needs to have table's name but regionID 0 so that it is sorted
     // before all table's regions.
     HRegionInfo boundary = new HRegionInfo(tableName, null, null, false, 0L);
@@ -733,10 +723,9 @@ public class RegionStates {
    */
   public synchronized Map<RegionState.State, List<HRegionInfo>>
   getRegionByStateOfTable(TableName tableName) {
-    Map<RegionState.State, List<HRegionInfo>> tableRegions =
-        new HashMap<State, List<HRegionInfo>>();
+    Map<RegionState.State, List<HRegionInfo>> tableRegions = new HashMap<>();
     for (State state : State.values()) {
-      tableRegions.put(state, new ArrayList<HRegionInfo>());
+      tableRegions.put(state, new ArrayList<>());
     }
     Map<String, RegionState> indexMap = regionStatesTableIndex.get(tableName);
     if (indexMap == null)
@@ -774,7 +763,7 @@ public class RegionStates {
    * We loop through all regions assuming we don't delete tables too much.
    */
   public void tableDeleted(final TableName tableName) {
-    Set<HRegionInfo> regionsToDelete = new HashSet<HRegionInfo>();
+    Set<HRegionInfo> regionsToDelete = new HashSet<>();
     synchronized (this) {
       for (RegionState state: regionStates.values()) {
         HRegionInfo region = state.getRegion();
@@ -794,7 +783,7 @@ public class RegionStates {
   public synchronized Set<HRegionInfo> getServerRegions(ServerName serverName) {
     Set<HRegionInfo> regions = serverHoldings.get(serverName);
     if (regions == null) return null;
-    return new HashSet<HRegionInfo>(regions);
+    return new HashSet<>(regions);
   }
 
   /**
@@ -954,7 +943,7 @@ public class RegionStates {
    */
   synchronized Map<HRegionInfo, ServerName> closeAllUserRegions(Set<TableName> excludedTables) {
     boolean noExcludeTables = excludedTables == null || excludedTables.isEmpty();
-    Set<HRegionInfo> toBeClosed = new HashSet<HRegionInfo>(regionStates.size());
+    Set<HRegionInfo> toBeClosed = new HashSet<>(regionStates.size());
     for(RegionState state: regionStates.values()) {
       HRegionInfo hri = state.getRegion();
       if (state.isSplit() || hri.isSplit()) {
@@ -966,8 +955,7 @@ public class RegionStates {
         toBeClosed.add(hri);
       }
     }
-    Map<HRegionInfo, ServerName> allUserRegions =
-      new HashMap<HRegionInfo, ServerName>(toBeClosed.size());
+    Map<HRegionInfo, ServerName> allUserRegions = new HashMap<>(toBeClosed.size());
     for (HRegionInfo hri: toBeClosed) {
       RegionState regionState = updateRegionState(hri, State.CLOSED);
       allUserRegions.put(hri, regionState.getServerName());
@@ -1032,7 +1020,7 @@ public class RegionStates {
     for (Map<ServerName, List<HRegionInfo>> map: result.values()) {
       for (ServerName svr: onlineSvrs.keySet()) {
         if (!map.containsKey(svr)) {
-          map.put(svr, new ArrayList<HRegionInfo>());
+          map.put(svr, new ArrayList<>());
         }
       }
       map.keySet().removeAll(drainingServers);
@@ -1041,20 +1029,19 @@ public class RegionStates {
   }
 
   private Map<TableName, Map<ServerName, List<HRegionInfo>>> getTableRSRegionMap(Boolean bytable){
-    Map<TableName, Map<ServerName, List<HRegionInfo>>> result =
-            new HashMap<TableName, Map<ServerName,List<HRegionInfo>>>();
+    Map<TableName, Map<ServerName, List<HRegionInfo>>> result = new HashMap<>();
     for (Map.Entry<ServerName, Set<HRegionInfo>> e: serverHoldings.entrySet()) {
       for (HRegionInfo hri: e.getValue()) {
         if (hri.isMetaRegion()) continue;
         TableName tablename = bytable ? hri.getTable() : TableName.valueOf(HConstants.ENSEMBLE_TABLE_NAME);
         Map<ServerName, List<HRegionInfo>> svrToRegions = result.get(tablename);
         if (svrToRegions == null) {
-          svrToRegions = new HashMap<ServerName, List<HRegionInfo>>(serverHoldings.size());
+          svrToRegions = new HashMap<>(serverHoldings.size());
           result.put(tablename, svrToRegions);
         }
         List<HRegionInfo> regions = svrToRegions.get(e.getKey());
         if (regions == null) {
-          regions = new ArrayList<HRegionInfo>();
+          regions = new ArrayList<>();
           svrToRegions.put(e.getKey(), regions);
         }
         regions.add(hri);
@@ -1072,10 +1059,9 @@ public class RegionStates {
    * @return a Map of ServerName to a List of HRegionInfo's
    */
   protected synchronized Map<ServerName, List<HRegionInfo>> getRegionAssignmentsByServer() {
-    Map<ServerName, List<HRegionInfo>> regionsByServer =
-        new HashMap<ServerName, List<HRegionInfo>>(serverHoldings.size());
+    Map<ServerName, List<HRegionInfo>> regionsByServer = new HashMap<>(serverHoldings.size());
     for (Map.Entry<ServerName, Set<HRegionInfo>> e: serverHoldings.entrySet()) {
-      regionsByServer.put(e.getKey(), new ArrayList<HRegionInfo>(e.getValue()));
+      regionsByServer.put(e.getKey(), new ArrayList<>(e.getValue()));
     }
     return regionsByServer;
   }
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/master/ServerManager.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/master/ServerManager.java
index eb96f97..e6b60d8 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/master/ServerManager.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/master/ServerManager.java
@@ -131,29 +131,25 @@ public class ServerManager {
    * The last flushed sequence id for a region.
    */
   private final ConcurrentNavigableMap<byte[], Long> flushedSequenceIdByRegion =
-    new ConcurrentSkipListMap<byte[], Long>(Bytes.BYTES_COMPARATOR);
+    new ConcurrentSkipListMap<>(Bytes.BYTES_COMPARATOR);
 
   /**
    * The last flushed sequence id for a store in a region.
    */
   private final ConcurrentNavigableMap<byte[], ConcurrentNavigableMap<byte[], Long>>
-    storeFlushedSequenceIdsByRegion =
-    new ConcurrentSkipListMap<byte[], ConcurrentNavigableMap<byte[], Long>>(Bytes.BYTES_COMPARATOR);
+    storeFlushedSequenceIdsByRegion = new ConcurrentSkipListMap<>(Bytes.BYTES_COMPARATOR);
 
   /** Map of registered servers to their current load */
-  private final ConcurrentNavigableMap<ServerName, ServerLoad> onlineServers =
-    new ConcurrentSkipListMap<ServerName, ServerLoad>();
+  private final ConcurrentNavigableMap<ServerName, ServerLoad> onlineServers = new ConcurrentSkipListMap<>();
 
   /**
    * Map of admin interfaces per registered regionserver; these interfaces we use to control
    * regionservers out on the cluster
    */
-  private final Map<ServerName, AdminService.BlockingInterface> rsAdmins =
-    new HashMap<ServerName, AdminService.BlockingInterface>();
+  private final Map<ServerName, AdminService.BlockingInterface> rsAdmins = new HashMap<>();
 
   /** List of region servers that should not get any more new regions. */
-  private final ArrayList<ServerName> drainingServers =
-    new ArrayList<ServerName>();
+  private final ArrayList<ServerName> drainingServers = new ArrayList<>();
 
   private final MasterServices master;
   private final ClusterConnection connection;
@@ -182,7 +178,7 @@ public class ServerManager {
    * So this is a set of region servers known to be dead but not submitted to
    * ServerShutdownHandler for processing yet.
    */
-  private Set<ServerName> queuedDeadServers = new HashSet<ServerName>();
+  private Set<ServerName> queuedDeadServers = new HashSet<>();
 
   /**
    * Set of region servers which are dead and submitted to ServerShutdownHandler to process but not
@@ -199,11 +195,10 @@ public class ServerManager {
    * is currently in startup mode. In this case, the dead server will be parked in this set
    * temporarily.
    */
-  private Map<ServerName, Boolean> requeuedDeadServers
-    = new ConcurrentHashMap<ServerName, Boolean>();
+  private Map<ServerName, Boolean> requeuedDeadServers = new ConcurrentHashMap<>();
 
   /** Listeners that are called on server events. */
-  private List<ServerListener> listeners = new CopyOnWriteArrayList<ServerListener>();
+  private List<ServerListener> listeners = new CopyOnWriteArrayList<>();
 
   /**
    * Constructor.
@@ -1111,7 +1106,7 @@ public class ServerManager {
   public List<ServerName> getOnlineServersList() {
     // TODO: optimize the load balancer call so we don't need to make a new list
     // TODO: FIX. THIS IS POPULAR CALL.
-    return new ArrayList<ServerName>(this.onlineServers.keySet());
+    return new ArrayList<>(this.onlineServers.keySet());
   }
 
   /**
@@ -1139,14 +1134,14 @@ public class ServerManager {
    * @return A copy of the internal list of draining servers.
    */
   public List<ServerName> getDrainingServersList() {
-    return new ArrayList<ServerName>(this.drainingServers);
+    return new ArrayList<>(this.drainingServers);
   }
 
   /**
    * @return A copy of the internal set of deadNotExpired servers.
    */
   Set<ServerName> getDeadNotExpiredServers() {
-    return new HashSet<ServerName>(this.queuedDeadServers);
+    return new HashSet<>(this.queuedDeadServers);
   }
 
   /**
@@ -1287,11 +1282,9 @@ public class ServerManager {
       LOG.warn("Attempting to send favored nodes update rpc to server " + server.toString()
           + " failed because no RPC connection found to this server");
     } else {
-      List<Pair<HRegionInfo, List<ServerName>>> regionUpdateInfos =
-          new ArrayList<Pair<HRegionInfo, List<ServerName>>>();
+      List<Pair<HRegionInfo, List<ServerName>>> regionUpdateInfos = new ArrayList<>();
       for (Entry<HRegionInfo, List<ServerName>> entry : favoredNodes.entrySet()) {
-        regionUpdateInfos.add(new Pair<HRegionInfo, List<ServerName>>(entry.getKey(),
-          entry.getValue()));
+        regionUpdateInfos.add(new Pair<>(entry.getKey(), entry.getValue()));
       }
       UpdateFavoredNodesRequest request =
         RequestConverter.buildUpdateFavoredNodesRequest(regionUpdateInfos);
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/master/SnapshotOfRegionAssignmentFromMeta.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/master/SnapshotOfRegionAssignmentFromMeta.java
index 8fedb40..6e477bc 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/master/SnapshotOfRegionAssignmentFromMeta.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/master/SnapshotOfRegionAssignmentFromMeta.java
@@ -80,19 +80,19 @@ public class SnapshotOfRegionAssignmentFromMeta {
   private final boolean excludeOfflinedSplitParents;
 
   public SnapshotOfRegionAssignmentFromMeta(Connection connection) {
-    this(connection, new HashSet<TableName>(), false);
+    this(connection, new HashSet<>(), false);
   }
 
   public SnapshotOfRegionAssignmentFromMeta(Connection connection, Set<TableName> disabledTables,
       boolean excludeOfflinedSplitParents) {
     this.connection = connection;
-    tableToRegionMap = new HashMap<TableName, List<HRegionInfo>>();
-    regionToRegionServerMap = new HashMap<HRegionInfo, ServerName>();
-    currentRSToRegionMap = new HashMap<ServerName, List<HRegionInfo>>();
-    primaryRSToRegionMap = new HashMap<ServerName, List<HRegionInfo>>();
-    secondaryRSToRegionMap = new HashMap<ServerName, List<HRegionInfo>>();
-    teritiaryRSToRegionMap = new HashMap<ServerName, List<HRegionInfo>>();
-    regionNameToRegionInfoMap = new TreeMap<String, HRegionInfo>();
+    tableToRegionMap = new HashMap<>();
+    regionToRegionServerMap = new HashMap<>();
+    currentRSToRegionMap = new HashMap<>();
+    primaryRSToRegionMap = new HashMap<>();
+    secondaryRSToRegionMap = new HashMap<>();
+    teritiaryRSToRegionMap = new HashMap<>();
+    regionNameToRegionInfoMap = new TreeMap<>();
     existingAssignmentPlan = new FavoredNodesPlan();
     this.disabledTables = disabledTables;
     this.excludeOfflinedSplitParents = excludeOfflinedSplitParents;
@@ -180,7 +180,7 @@ public class SnapshotOfRegionAssignmentFromMeta {
     TableName tableName = regionInfo.getTable();
     List<HRegionInfo> regionList = tableToRegionMap.get(tableName);
     if (regionList == null) {
-      regionList = new ArrayList<HRegionInfo>();
+      regionList = new ArrayList<>();
     }
     // Add the current region info into the tableToRegionMap
     regionList.add(regionInfo);
@@ -196,7 +196,7 @@ public class SnapshotOfRegionAssignmentFromMeta {
     // Process the region server to region map
     List<HRegionInfo> regionList = currentRSToRegionMap.get(server);
     if (regionList == null) {
-      regionList = new ArrayList<HRegionInfo>();
+      regionList = new ArrayList<>();
     }
     regionList.add(regionInfo);
     currentRSToRegionMap.put(server, regionList);
@@ -206,7 +206,7 @@ public class SnapshotOfRegionAssignmentFromMeta {
     // Process the region server to region map
     List<HRegionInfo> regionList = primaryRSToRegionMap.get(server);
     if (regionList == null) {
-      regionList = new ArrayList<HRegionInfo>();
+      regionList = new ArrayList<>();
     }
     regionList.add(regionInfo);
     primaryRSToRegionMap.put(server, regionList);
@@ -216,7 +216,7 @@ public class SnapshotOfRegionAssignmentFromMeta {
     // Process the region server to region map
     List<HRegionInfo> regionList = secondaryRSToRegionMap.get(server);
     if (regionList == null) {
-      regionList = new ArrayList<HRegionInfo>();
+      regionList = new ArrayList<>();
     }
     regionList.add(regionInfo);
     secondaryRSToRegionMap.put(server, regionList);
@@ -226,7 +226,7 @@ public class SnapshotOfRegionAssignmentFromMeta {
     // Process the region server to region map
     List<HRegionInfo> regionList = teritiaryRSToRegionMap.get(server);
     if (regionList == null) {
-      regionList = new ArrayList<HRegionInfo>();
+      regionList = new ArrayList<>();
     }
     regionList.add(regionInfo);
     teritiaryRSToRegionMap.put(server, regionList);
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/master/SplitLogManager.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/master/SplitLogManager.java
index 20fef35..7017d29 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/master/SplitLogManager.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/master/SplitLogManager.java
@@ -118,7 +118,7 @@ public class SplitLogManager {
   protected final ReentrantLock recoveringRegionLock = new ReentrantLock();
 
   @VisibleForTesting
-  final ConcurrentMap<String, Task> tasks = new ConcurrentHashMap<String, Task>();
+  final ConcurrentMap<String, Task> tasks = new ConcurrentHashMap<>();
   private TimeoutMonitor timeoutMonitor;
 
   private volatile Set<ServerName> deadWorkers = null;
@@ -176,7 +176,7 @@ public class SplitLogManager {
   public static FileStatus[] getFileList(final Configuration conf, final List<Path> logDirs,
       final PathFilter filter)
       throws IOException {
-    List<FileStatus> fileStatus = new ArrayList<FileStatus>();
+    List<FileStatus> fileStatus = new ArrayList<>();
     for (Path logDir : logDirs) {
       final FileSystem fs = logDir.getFileSystem(conf);
       if (!fs.exists(logDir)) {
@@ -201,7 +201,7 @@ public class SplitLogManager {
    * @throws IOException
    */
   public long splitLogDistributed(final Path logDir) throws IOException {
-    List<Path> logDirs = new ArrayList<Path>();
+    List<Path> logDirs = new ArrayList<>();
     logDirs.add(logDir);
     return splitLogDistributed(logDirs);
   }
@@ -218,7 +218,7 @@ public class SplitLogManager {
     if (logDirs.isEmpty()) {
       return 0;
     }
-    Set<ServerName> serverNames = new HashSet<ServerName>();
+    Set<ServerName> serverNames = new HashSet<>();
     for (Path logDir : logDirs) {
       try {
         ServerName serverName = AbstractFSWALProvider.getServerNameFromWALDirectoryName(logDir);
@@ -398,7 +398,7 @@ public class SplitLogManager {
     }
     if (serverNames == null || serverNames.isEmpty()) return;
 
-    Set<String> recoveredServerNameSet = new HashSet<String>();
+    Set<String> recoveredServerNameSet = new HashSet<>();
     for (ServerName tmpServerName : serverNames) {
       recoveredServerNameSet.add(tmpServerName.getServerName());
     }
@@ -410,8 +410,7 @@ public class SplitLogManager {
     } catch (IOException e) {
       LOG.warn("removeRecoveringRegions got exception. Will retry", e);
       if (serverNames != null && !serverNames.isEmpty()) {
-        this.failedRecoveringRegionDeletions.add(new Pair<Set<ServerName>, Boolean>(serverNames,
-            isMetaRecovery));
+        this.failedRecoveringRegionDeletions.add(new Pair<>(serverNames, isMetaRecovery));
       }
     } finally {
       this.recoveringRegionLock.unlock();
@@ -426,7 +425,7 @@ public class SplitLogManager {
    */
   void removeStaleRecoveringRegions(final Set<ServerName> failedServers) throws IOException,
       InterruptedIOException {
-    Set<String> knownFailedServers = new HashSet<String>();
+    Set<String> knownFailedServers = new HashSet<>();
     if (failedServers != null) {
       for (ServerName tmpServerName : failedServers) {
         knownFailedServers.add(tmpServerName.getServerName());
@@ -519,7 +518,7 @@ public class SplitLogManager {
     // to reason about concurrency. Makes it easier to retry.
     synchronized (deadWorkersLock) {
       if (deadWorkers == null) {
-        deadWorkers = new HashSet<ServerName>(100);
+        deadWorkers = new HashSet<>(100);
       }
       deadWorkers.add(workerName);
     }
@@ -529,7 +528,7 @@ public class SplitLogManager {
   void handleDeadWorkers(Set<ServerName> serverNames) {
     synchronized (deadWorkersLock) {
       if (deadWorkers == null) {
-        deadWorkers = new HashSet<ServerName>(100);
+        deadWorkers = new HashSet<>(100);
       }
       deadWorkers.addAll(serverNames);
     }
@@ -749,7 +748,7 @@ public class SplitLogManager {
         getSplitLogManagerCoordination().getDetails().getFailedDeletions();
       // Retry previously failed deletes
       if (failedDeletions.size() > 0) {
-        List<String> tmpPaths = new ArrayList<String>(failedDeletions);
+        List<String> tmpPaths = new ArrayList<>(failedDeletions);
         for (String tmpPath : tmpPaths) {
           // deleteNode is an async call
           getSplitLogManagerCoordination().deleteTask(tmpPath);
@@ -766,7 +765,7 @@ public class SplitLogManager {
         // inside the function there have more checks before GC anything
         if (!failedRecoveringRegionDeletions.isEmpty()) {
           List<Pair<Set<ServerName>, Boolean>> previouslyFailedDeletions =
-              new ArrayList<Pair<Set<ServerName>, Boolean>>(failedRecoveringRegionDeletions);
+              new ArrayList<>(failedRecoveringRegionDeletions);
           failedRecoveringRegionDeletions.removeAll(previouslyFailedDeletions);
           for (Pair<Set<ServerName>, Boolean> failedDeletion : previouslyFailedDeletions) {
             removeRecoveringRegions(failedDeletion.getFirst(), failedDeletion.getSecond());
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/master/balancer/BaseLoadBalancer.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/master/balancer/BaseLoadBalancer.java
index f6ae9af..b0e088c 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/master/balancer/BaseLoadBalancer.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/master/balancer/BaseLoadBalancer.java
@@ -73,7 +73,7 @@ public abstract class BaseLoadBalancer implements LoadBalancer {
   protected static final int MIN_SERVER_BALANCE = 2;
   private volatile boolean stopped = false;
 
-  private static final List<HRegionInfo> EMPTY_REGION_LIST = new ArrayList<HRegionInfo>(0);
+  private static final List<HRegionInfo> EMPTY_REGION_LIST = new ArrayList<>(0);
 
   static final Predicate<ServerLoad> IDLE_SERVER_PREDICATOR
     = load -> load.getNumberOfRegions() == 0;
@@ -187,19 +187,19 @@ public abstract class BaseLoadBalancer implements LoadBalancer {
         unassignedRegions = EMPTY_REGION_LIST;
       }
 
-      serversToIndex = new HashMap<String, Integer>();
-      hostsToIndex = new HashMap<String, Integer>();
-      racksToIndex = new HashMap<String, Integer>();
-      tablesToIndex = new HashMap<String, Integer>();
+      serversToIndex = new HashMap<>();
+      hostsToIndex = new HashMap<>();
+      racksToIndex = new HashMap<>();
+      tablesToIndex = new HashMap<>();
 
       //TODO: We should get the list of tables from master
-      tables = new ArrayList<String>();
+      tables = new ArrayList<>();
       this.rackManager = rackManager != null ? rackManager : new DefaultRackManager();
 
       numRegions = 0;
 
-      List<List<Integer>> serversPerHostList = new ArrayList<List<Integer>>();
-      List<List<Integer>> serversPerRackList = new ArrayList<List<Integer>>();
+      List<List<Integer>> serversPerHostList = new ArrayList<>();
+      List<List<Integer>> serversPerRackList = new ArrayList<>();
       this.clusterState = clusterState;
       this.regionFinder = regionFinder;
 
@@ -211,7 +211,7 @@ public abstract class BaseLoadBalancer implements LoadBalancer {
         }
         if (!hostsToIndex.containsKey(sn.getHostname())) {
           hostsToIndex.put(sn.getHostname(), numHosts++);
-          serversPerHostList.add(new ArrayList<Integer>(1));
+          serversPerHostList.add(new ArrayList<>(1));
         }
 
         int serverIndex = serversToIndex.get(sn.getHostAndPort());
@@ -221,7 +221,7 @@ public abstract class BaseLoadBalancer implements LoadBalancer {
         String rack = this.rackManager.getRack(sn);
         if (!racksToIndex.containsKey(rack)) {
           racksToIndex.put(rack, numRacks++);
-          serversPerRackList.add(new ArrayList<Integer>());
+          serversPerRackList.add(new ArrayList<>());
         }
         int rackIndex = racksToIndex.get(rack);
         serversPerRackList.get(rackIndex).add(serverIndex);
@@ -233,7 +233,7 @@ public abstract class BaseLoadBalancer implements LoadBalancer {
       }
       numRegions += unassignedRegions.size();
 
-      regionsToIndex = new HashMap<HRegionInfo, Integer>(numRegions);
+      regionsToIndex = new HashMap<>(numRegions);
       servers = new ServerName[numServers];
       serversPerHost = new int[numHosts][];
       serversPerRack = new int[numRacks][];
@@ -1064,7 +1064,7 @@ public abstract class BaseLoadBalancer implements LoadBalancer {
   public static final String TABLES_ON_MASTER =
     "hbase.balancer.tablesOnMaster";
 
-  protected final Set<String> tablesOnMaster = new HashSet<String>();
+  protected final Set<String> tablesOnMaster = new HashSet<>();
   protected MetricsBalancer metricsBalancer = null;
   protected ClusterStatus clusterStatus = null;
   protected ServerName masterServerName;
@@ -1170,7 +1170,7 @@ public abstract class BaseLoadBalancer implements LoadBalancer {
         // Move this region away from the master regionserver
         RegionPlan plan = new RegionPlan(region, masterServerName, dest);
         if (plans == null) {
-          plans = new ArrayList<RegionPlan>();
+          plans = new ArrayList<>();
         }
         plans.add(plan);
       }
@@ -1183,7 +1183,7 @@ public abstract class BaseLoadBalancer implements LoadBalancer {
         // Move this region to the master regionserver
         RegionPlan plan = new RegionPlan(region, server.getKey(), masterServerName);
         if (plans == null) {
-          plans = new ArrayList<RegionPlan>();
+          plans = new ArrayList<>();
         }
         plans.add(plan);
       }
@@ -1199,10 +1199,9 @@ public abstract class BaseLoadBalancer implements LoadBalancer {
     if (servers == null || regions == null || regions.isEmpty()) {
       return null;
     }
-    Map<ServerName, List<HRegionInfo>> assignments
-      = new TreeMap<ServerName, List<HRegionInfo>>();
+    Map<ServerName, List<HRegionInfo>> assignments = new TreeMap<>();
     if (masterServerName != null && servers.contains(masterServerName)) {
-      assignments.put(masterServerName, new ArrayList<HRegionInfo>());
+      assignments.put(masterServerName, new ArrayList<>());
       for (HRegionInfo region: regions) {
         if (shouldBeOnMaster(region)) {
           assignments.get(masterServerName).add(region);
@@ -1303,12 +1302,12 @@ public abstract class BaseLoadBalancer implements LoadBalancer {
     metricsBalancer.incrMiscInvocations();
     Map<ServerName, List<HRegionInfo>> assignments = assignMasterRegions(regions, servers);
     if (assignments != null && !assignments.isEmpty()) {
-      servers = new ArrayList<ServerName>(servers);
+      servers = new ArrayList<>(servers);
       // Guarantee not to put other regions on master
       servers.remove(masterServerName);
       List<HRegionInfo> masterRegions = assignments.get(masterServerName);
       if (!masterRegions.isEmpty()) {
-        regions = new ArrayList<HRegionInfo>(regions);
+        regions = new ArrayList<>(regions);
         for (HRegionInfo region: masterRegions) {
           regions.remove(region);
         }
@@ -1331,17 +1330,17 @@ public abstract class BaseLoadBalancer implements LoadBalancer {
 
     if (numServers == 1) { // Only one server, nothing fancy we can do here
       ServerName server = servers.get(0);
-      assignments.put(server, new ArrayList<HRegionInfo>(regions));
+      assignments.put(server, new ArrayList<>(regions));
       return assignments;
     }
 
     Cluster cluster = createCluster(servers, regions, false);
-    List<HRegionInfo> unassignedRegions = new ArrayList<HRegionInfo>();
+    List<HRegionInfo> unassignedRegions = new ArrayList<>();
 
     roundRobinAssignment(cluster, regions, unassignedRegions,
       servers, assignments);
 
-    List<HRegionInfo> lastFewRegions = new ArrayList<HRegionInfo>();
+    List<HRegionInfo> lastFewRegions = new ArrayList<>();
     // assign the remaining by going through the list and try to assign to servers one-by-one
     int serverIdx = RANDOM.nextInt(numServers);
     for (HRegionInfo region : unassignedRegions) {
@@ -1351,7 +1350,7 @@ public abstract class BaseLoadBalancer implements LoadBalancer {
         if (!cluster.wouldLowerAvailability(region, serverName)) {
           List<HRegionInfo> serverRegions = assignments.get(serverName);
           if (serverRegions == null) {
-            serverRegions = new ArrayList<HRegionInfo>();
+            serverRegions = new ArrayList<>();
             assignments.put(serverName, serverRegions);
           }
           serverRegions.add(region);
@@ -1372,7 +1371,7 @@ public abstract class BaseLoadBalancer implements LoadBalancer {
       ServerName server = servers.get(i);
       List<HRegionInfo> serverRegions = assignments.get(server);
       if (serverRegions == null) {
-        serverRegions = new ArrayList<HRegionInfo>();
+        serverRegions = new ArrayList<>();
         assignments.put(server, serverRegions);
       }
       serverRegions.add(region);
@@ -1416,7 +1415,7 @@ public abstract class BaseLoadBalancer implements LoadBalancer {
       if (shouldBeOnMaster(regionInfo)) {
         return masterServerName;
       }
-      servers = new ArrayList<ServerName>(servers);
+      servers = new ArrayList<>(servers);
       // Guarantee not to put other regions on master
       servers.remove(masterServerName);
     }
@@ -1465,12 +1464,12 @@ public abstract class BaseLoadBalancer implements LoadBalancer {
     Map<ServerName, List<HRegionInfo>> assignments
       = assignMasterRegions(regions.keySet(), servers);
     if (assignments != null && !assignments.isEmpty()) {
-      servers = new ArrayList<ServerName>(servers);
+      servers = new ArrayList<>(servers);
       // Guarantee not to put other regions on master
       servers.remove(masterServerName);
       List<HRegionInfo> masterRegions = assignments.get(masterServerName);
       if (!masterRegions.isEmpty()) {
-        regions = new HashMap<HRegionInfo, ServerName>(regions);
+        regions = new HashMap<>(regions);
         for (HRegionInfo region: masterRegions) {
           regions.remove(region);
         }
@@ -1487,7 +1486,7 @@ public abstract class BaseLoadBalancer implements LoadBalancer {
     }
     if (numServers == 1) { // Only one server, nothing fancy we can do here
       ServerName server = servers.get(0);
-      assignments.put(server, new ArrayList<HRegionInfo>(regions.keySet()));
+      assignments.put(server, new ArrayList<>(regions.keySet()));
       return assignments;
     }
 
@@ -1499,7 +1498,7 @@ public abstract class BaseLoadBalancer implements LoadBalancer {
     // servers on the same host on different ports.
     ArrayListMultimap<String, ServerName> serversByHostname = ArrayListMultimap.create();
     for (ServerName server : servers) {
-      assignments.put(server, new ArrayList<HRegionInfo>());
+      assignments.put(server, new ArrayList<>());
       serversByHostname.put(server.getHostname(), server);
     }
 
@@ -1516,7 +1515,7 @@ public abstract class BaseLoadBalancer implements LoadBalancer {
     for (Map.Entry<HRegionInfo, ServerName> entry : regions.entrySet()) {
       HRegionInfo region = entry.getKey();
       ServerName oldServerName = entry.getValue();
-      List<ServerName> localServers = new ArrayList<ServerName>();
+      List<ServerName> localServers = new ArrayList<>();
       if (oldServerName != null) {
         localServers = serversByHostname.get(oldServerName.getHostname());
       }
@@ -1629,7 +1628,7 @@ public abstract class BaseLoadBalancer implements LoadBalancer {
 
     for (int j = 0; j < numServers; j++) {
       ServerName server = servers.get((j + serverIdx) % numServers);
-      List<HRegionInfo> serverRegions = new ArrayList<HRegionInfo>(max);
+      List<HRegionInfo> serverRegions = new ArrayList<>(max);
       for (int i = regionIdx; i < numRegions; i += numServers) {
         HRegionInfo region = regions.get(i % numRegions);
         if (cluster.wouldLowerAvailability(region, server)) {
@@ -1649,7 +1648,7 @@ public abstract class BaseLoadBalancer implements LoadBalancer {
     if (this.services != null && this.services.getAssignmentManager() != null) {
       return this.services.getAssignmentManager().getSnapShotOfAssignment(regions);
     } else {
-      return new HashMap<ServerName, List<HRegionInfo>>();
+      return new HashMap<>();
     }
   }
 
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/master/balancer/ClusterLoadState.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/master/balancer/ClusterLoadState.java
index e5f0e3b..8c337bd 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/master/balancer/ClusterLoadState.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/master/balancer/ClusterLoadState.java
@@ -39,7 +39,7 @@ public class ClusterLoadState {
     this.numRegions = 0;
     this.numServers = clusterState.size();
     this.clusterState = clusterState;
-    serversByLoad = new TreeMap<ServerAndLoad, List<HRegionInfo>>();
+    serversByLoad = new TreeMap<>();
     // Iterate so we can count regions as we build the map
     for (Map.Entry<ServerName, List<HRegionInfo>> server : clusterState.entrySet()) {
       List<HRegionInfo> regions = server.getValue();
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/master/balancer/RegionLocationFinder.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/master/balancer/RegionLocationFinder.java
index d5edfab..f7e166d 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/master/balancer/RegionLocationFinder.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/master/balancer/RegionLocationFinder.java
@@ -179,7 +179,7 @@ class RegionLocationFinder {
    */
   protected List<ServerName> getTopBlockLocations(HRegionInfo region, String currentHost) {
     HDFSBlocksDistribution blocksDistribution = getBlockDistribution(region);
-    List<String> topHosts = new ArrayList<String>();
+    List<String> topHosts = new ArrayList<>();
     for (String host : blocksDistribution.getTopHosts()) {
       if (host.equals(currentHost)) {
         break;
@@ -250,15 +250,15 @@ class RegionLocationFinder {
       return Lists.newArrayList();
     }
 
-    List<ServerName> topServerNames = new ArrayList<ServerName>();
+    List<ServerName> topServerNames = new ArrayList<>();
     Collection<ServerName> regionServers = status.getServers();
 
     // create a mapping from hostname to ServerName for fast lookup
-    HashMap<String, List<ServerName>> hostToServerName = new HashMap<String, List<ServerName>>();
+    HashMap<String, List<ServerName>> hostToServerName = new HashMap<>();
     for (ServerName sn : regionServers) {
       String host = sn.getHostname();
       if (!hostToServerName.containsKey(host)) {
-        hostToServerName.put(host, new ArrayList<ServerName>());
+        hostToServerName.put(host, new ArrayList<>());
       }
       hostToServerName.get(host).add(sn);
     }
@@ -309,8 +309,7 @@ class RegionLocationFinder {
   }
 
   public void refreshAndWait(Collection<HRegionInfo> hris) {
-    ArrayList<ListenableFuture<HDFSBlocksDistribution>> regionLocationFutures =
-        new ArrayList<ListenableFuture<HDFSBlocksDistribution>>(hris.size());
+    ArrayList<ListenableFuture<HDFSBlocksDistribution>> regionLocationFutures = new ArrayList<>(hris.size());
     for (HRegionInfo hregionInfo : hris) {
       regionLocationFutures.add(asyncGetBlockDistribution(hregionInfo));
     }
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/master/balancer/SimpleLoadBalancer.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/master/balancer/SimpleLoadBalancer.java
index a6a0774..7e8d696 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/master/balancer/SimpleLoadBalancer.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/master/balancer/SimpleLoadBalancer.java
@@ -255,7 +255,7 @@ public class SimpleLoadBalancer extends BaseLoadBalancer {
       if (clusterMap.size() <= 2) {
         return null;
       }
-      clusterMap = new HashMap<ServerName, List<HRegionInfo>>(clusterMap);
+      clusterMap = new HashMap<>(clusterMap);
       clusterMap.remove(masterServerName);
     }
 
@@ -285,14 +285,13 @@ public class SimpleLoadBalancer extends BaseLoadBalancer {
     // TODO: Look at data block locality or a more complex load to do this
     MinMaxPriorityQueue<RegionPlan> regionsToMove =
       MinMaxPriorityQueue.orderedBy(rpComparator).create();
-    regionsToReturn = new ArrayList<RegionPlan>();
+    regionsToReturn = new ArrayList<>();
 
     // Walk down most loaded, pruning each to the max
     int serversOverloaded = 0;
     // flag used to fetch regions from head and tail of list, alternately
     boolean fetchFromTail = false;
-    Map<ServerName, BalanceInfo> serverBalanceInfo =
-      new TreeMap<ServerName, BalanceInfo>();
+    Map<ServerName, BalanceInfo> serverBalanceInfo = new TreeMap<>();
     for (Map.Entry<ServerAndLoad, List<HRegionInfo>> server:
         serversByLoad.descendingMap().entrySet()) {
       ServerAndLoad sal = server.getKey();
@@ -330,7 +329,7 @@ public class SimpleLoadBalancer extends BaseLoadBalancer {
     int neededRegions = 0; // number of regions needed to bring all up to min
     fetchFromTail = false;
 
-    Map<ServerName, Integer> underloadedServers = new HashMap<ServerName, Integer>();
+    Map<ServerName, Integer> underloadedServers = new HashMap<>();
     int maxToTake = numRegions - min;
     for (Map.Entry<ServerAndLoad, List<HRegionInfo>> server:
         serversByLoad.entrySet()) {
@@ -524,8 +523,7 @@ public class SimpleLoadBalancer extends BaseLoadBalancer {
     // A structure help to map ServerName to  it's load and index in ServerLoadList
     Map<ServerName, Pair<ServerAndLoad,Integer>> SnLoadMap = new HashMap<>();
     for (int i = 0; i < serverLoadList.size(); i++) {
-      SnLoadMap.put(serverLoadList.get(i).getServerName(),
-              new Pair<ServerAndLoad, Integer>(serverLoadList.get(i), i));
+      SnLoadMap.put(serverLoadList.get(i).getServerName(), new Pair<>(serverLoadList.get(i), i));
     }
     Pair<ServerAndLoad,Integer> shredLoad;
     // A List to help mark the plan in regionsToMove that should be removed
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/master/balancer/StochasticLoadBalancer.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/master/balancer/StochasticLoadBalancer.java
index f2329bb..f68afb6 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/master/balancer/StochasticLoadBalancer.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/master/balancer/StochasticLoadBalancer.java
@@ -121,7 +121,7 @@ public class StochasticLoadBalancer extends BaseLoadBalancer {
   private static final Random RANDOM = new Random(System.currentTimeMillis());
   private static final Log LOG = LogFactory.getLog(StochasticLoadBalancer.class);
 
-  Map<String, Deque<BalancerRegionLoad>> loads = new HashMap<String, Deque<BalancerRegionLoad>>();
+  Map<String, Deque<BalancerRegionLoad>> loads = new HashMap<>();
 
   // values are defaults
   private int maxSteps = 1000000;
@@ -332,7 +332,7 @@ public class StochasticLoadBalancer extends BaseLoadBalancer {
       if (clusterState.size() <= 2) {
         return null;
       }
-      clusterState = new HashMap<ServerName, List<HRegionInfo>>(clusterState);
+      clusterState = new HashMap<>(clusterState);
       clusterState.remove(masterServerName);
     }
 
@@ -482,7 +482,7 @@ public class StochasticLoadBalancer extends BaseLoadBalancer {
    * @return List of RegionPlan's that represent the moves needed to get to desired final state.
    */
   private List<RegionPlan> createRegionPlans(Cluster cluster) {
-    List<RegionPlan> plans = new LinkedList<RegionPlan>();
+    List<RegionPlan> plans = new LinkedList<>();
     for (int regionIndex = 0;
          regionIndex < cluster.regionIndexToServerIndex.length; regionIndex++) {
       int initialServerIndex = cluster.initialRegionIndexToServerIndex[regionIndex];
@@ -511,7 +511,7 @@ public class StochasticLoadBalancer extends BaseLoadBalancer {
     // We create a new hashmap so that regions that are no longer there are removed.
     // However we temporarily need the old loads so we can use them to keep the rolling average.
     Map<String, Deque<BalancerRegionLoad>> oldLoads = loads;
-    loads = new HashMap<String, Deque<BalancerRegionLoad>>();
+    loads = new HashMap<>();
 
     for (ServerName sn : clusterStatus.getServers()) {
       ServerLoad sl = clusterStatus.getLoad(sn);
@@ -522,7 +522,7 @@ public class StochasticLoadBalancer extends BaseLoadBalancer {
         Deque<BalancerRegionLoad> rLoads = oldLoads.get(Bytes.toString(entry.getKey()));
         if (rLoads == null) {
           // There was nothing there
-          rLoads = new ArrayDeque<BalancerRegionLoad>();
+          rLoads = new ArrayDeque<>();
         } else if (rLoads.size() >= numRegionLoadsToRemember) {
           rLoads.remove();
         }
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/master/cleaner/CleanerChore.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/master/cleaner/CleanerChore.java
index c6b6f62..dddad36 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/master/cleaner/CleanerChore.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/master/cleaner/CleanerChore.java
@@ -92,7 +92,7 @@ public abstract class CleanerChore<T extends FileCleanerDelegate> extends Schedu
    * @param confKey key to get the file cleaner classes from the configuration
    */
   private void initCleanerChain(String confKey) {
-    this.cleanersChain = new LinkedList<T>();
+    this.cleanersChain = new LinkedList<>();
     String[] logCleaners = conf.getStrings(confKey);
     if (logCleaners != null) {
       for (String className : logCleaners) {
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/master/cleaner/ReplicationMetaCleaner.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/master/cleaner/ReplicationMetaCleaner.java
index 5c56271..45b2401 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/master/cleaner/ReplicationMetaCleaner.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/master/cleaner/ReplicationMetaCleaner.java
@@ -74,7 +74,7 @@ public class ReplicationMetaCleaner extends ScheduledChore {
           }
         }
         if (hasSerialScope) {
-          serialTables.put(entry.getValue().getTableName().getNameAsString(), new HashSet<String>());
+          serialTables.put(entry.getValue().getTableName().getNameAsString(), new HashSet<>());
         }
       }
       if (serialTables.isEmpty()){
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/master/cleaner/ReplicationZKNodeCleaner.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/master/cleaner/ReplicationZKNodeCleaner.java
index c0a1b75..dafc4f8 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/master/cleaner/ReplicationZKNodeCleaner.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/master/cleaner/ReplicationZKNodeCleaner.java
@@ -82,7 +82,7 @@ public class ReplicationZKNodeCleaner {
         for (String queueId : queueIds) {
           ReplicationQueueInfo queueInfo = new ReplicationQueueInfo(queueId);
           if (!peerIds.contains(queueInfo.getPeerId())) {
-            undeletedQueues.computeIfAbsent(replicator, (key) -> new ArrayList<String>()).add(
+            undeletedQueues.computeIfAbsent(replicator, (key) -> new ArrayList<>()).add(
               queueId);
             if (LOG.isDebugEnabled()) {
               LOG.debug("Undeleted replication queue for removed peer found: "
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/master/locking/LockProcedure.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/master/locking/LockProcedure.java
index c4b49f0..8e490eb 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/master/locking/LockProcedure.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/master/locking/LockProcedure.java
@@ -477,4 +477,4 @@ public final class LockProcedure extends Procedure<MasterProcedureEnv>
       env.getProcedureScheduler().wakeRegions(LockProcedure.this, tableName, regionInfos);
     }
   }
-}
\ No newline at end of file
+}
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/master/normalizer/SimpleRegionNormalizer.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/master/normalizer/SimpleRegionNormalizer.java
index 5c67258..9c8358b 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/master/normalizer/SimpleRegionNormalizer.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/master/normalizer/SimpleRegionNormalizer.java
@@ -118,7 +118,7 @@ public class SimpleRegionNormalizer implements RegionNormalizer {
       return null;
     }
 
-    List<NormalizationPlan> plans = new ArrayList<NormalizationPlan>();
+    List<NormalizationPlan> plans = new ArrayList<>();
     List<HRegionInfo> tableRegions = masterServices.getAssignmentManager().getRegionStates().
       getRegionsOfTable(table);
 
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/master/procedure/CloneSnapshotProcedure.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/master/procedure/CloneSnapshotProcedure.java
index aefd14c..347d01d 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/master/procedure/CloneSnapshotProcedure.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/master/procedure/CloneSnapshotProcedure.java
@@ -70,8 +70,7 @@ public class CloneSnapshotProcedure
   private HTableDescriptor hTableDescriptor;
   private SnapshotDescription snapshot;
   private List<HRegionInfo> newRegions = null;
-  private Map<String, Pair<String, String> > parentsToChildrenPairMap =
-    new HashMap<String, Pair<String, String>>();
+  private Map<String, Pair<String, String> > parentsToChildrenPairMap = new HashMap<>();
 
   // Monitor
   private MonitoredTask monitorStatus = null;
@@ -264,18 +263,18 @@ public class CloneSnapshotProcedure
     if (cloneSnapshotMsg.getRegionInfoCount() == 0) {
       newRegions = null;
     } else {
-      newRegions = new ArrayList<HRegionInfo>(cloneSnapshotMsg.getRegionInfoCount());
+      newRegions = new ArrayList<>(cloneSnapshotMsg.getRegionInfoCount());
       for (HBaseProtos.RegionInfo hri: cloneSnapshotMsg.getRegionInfoList()) {
         newRegions.add(HRegionInfo.convert(hri));
       }
     }
     if (cloneSnapshotMsg.getParentToChildRegionsPairListCount() > 0) {
-      parentsToChildrenPairMap = new HashMap<String, Pair<String, String>>();
+      parentsToChildrenPairMap = new HashMap<>();
       for (MasterProcedureProtos.RestoreParentToChildRegionsPair parentToChildrenPair:
         cloneSnapshotMsg.getParentToChildRegionsPairListList()) {
         parentsToChildrenPairMap.put(
           parentToChildrenPair.getParentRegionName(),
-          new Pair<String, String>(
+          new Pair<>(
             parentToChildrenPair.getChild1RegionName(),
             parentToChildrenPair.getChild2RegionName()));
       }
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/master/procedure/CreateTableProcedure.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/master/procedure/CreateTableProcedure.java
index 2421dfc..ced7abc 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/master/procedure/CreateTableProcedure.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/master/procedure/CreateTableProcedure.java
@@ -208,7 +208,7 @@ public class CreateTableProcedure
     if (state.getRegionInfoCount() == 0) {
       newRegions = null;
     } else {
-      newRegions = new ArrayList<HRegionInfo>(state.getRegionInfoCount());
+      newRegions = new ArrayList<>(state.getRegionInfoCount());
       for (HBaseProtos.RegionInfo hri: state.getRegionInfoList()) {
         newRegions.add(HRegionInfo.convert(hri));
       }
@@ -364,8 +364,7 @@ public class CreateTableProcedure
     if (numRegionReplicas <= 0) {
       return regions;
     }
-    List<HRegionInfo> hRegionInfos =
-        new ArrayList<HRegionInfo>((numRegionReplicas+1)*regions.size());
+    List<HRegionInfo> hRegionInfos = new ArrayList<>((numRegionReplicas+1)*regions.size());
     for (int i = 0; i < regions.size(); i++) {
       for (int j = 1; j <= numRegionReplicas; j++) {
         hRegionInfos.add(RegionReplicaUtil.getRegionInfoForReplica(regions.get(i), j));
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/master/procedure/DeleteTableProcedure.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/master/procedure/DeleteTableProcedure.java
index 06b666b..9d0a283 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/master/procedure/DeleteTableProcedure.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/master/procedure/DeleteTableProcedure.java
@@ -221,7 +221,7 @@ public class DeleteTableProcedure
     if (state.getRegionInfoCount() == 0) {
       regions = null;
     } else {
-      regions = new ArrayList<HRegionInfo>(state.getRegionInfoCount());
+      regions = new ArrayList<>(state.getRegionInfoCount());
       for (HBaseProtos.RegionInfo hri: state.getRegionInfoList()) {
         regions.add(HRegionInfo.convert(hri));
       }
@@ -343,7 +343,7 @@ public class DeleteTableProcedure
     Scan tableScan = MetaTableAccessor.getScanForTableName(connection, tableName);
     try (Table metaTable =
         connection.getTable(TableName.META_TABLE_NAME)) {
-      List<Delete> deletes = new ArrayList<Delete>();
+      List<Delete> deletes = new ArrayList<>();
       try (ResultScanner resScanner = metaTable.getScanner(tableScan)) {
         for (Result result : resScanner) {
           deletes.add(new Delete(result.getRow()));
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/master/procedure/EnableTableProcedure.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/master/procedure/EnableTableProcedure.java
index f4ecf15..4d67edd 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/master/procedure/EnableTableProcedure.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/master/procedure/EnableTableProcedure.java
@@ -357,7 +357,7 @@ public class EnableTableProcedure
 
     // need to potentially create some regions for the replicas
     List<HRegionInfo> unrecordedReplicas =
-        AssignmentManager.replicaRegionsNotRecordedInMeta(new HashSet<HRegionInfo>(
+        AssignmentManager.replicaRegionsNotRecordedInMeta(new HashSet<>(
             regionsToAssign.keySet()), masterServices);
     Map<ServerName, List<HRegionInfo>> srvToUnassignedRegs =
         assignmentManager.getBalancer().roundRobinAssignment(unrecordedReplicas,
@@ -464,8 +464,7 @@ public class EnableTableProcedure
   private static Map<HRegionInfo, ServerName> regionsToAssignWithServerName(
       final MasterProcedureEnv env,
       final List<Pair<HRegionInfo, ServerName>> regionsInMeta) throws IOException {
-    Map<HRegionInfo, ServerName> regionsToAssign =
-        new HashMap<HRegionInfo, ServerName>(regionsInMeta.size());
+    Map<HRegionInfo, ServerName> regionsToAssign = new HashMap<>(regionsInMeta.size());
     RegionStates regionStates = env.getMasterServices().getAssignmentManager().getRegionStates();
     for (Pair<HRegionInfo, ServerName> regionLocation : regionsInMeta) {
       HRegionInfo hri = regionLocation.getFirst();
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/master/procedure/MasterDDLOperationHelper.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/master/procedure/MasterDDLOperationHelper.java
index 980bf94..4b9a7ab 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/master/procedure/MasterDDLOperationHelper.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/master/procedure/MasterDDLOperationHelper.java
@@ -97,12 +97,12 @@ public final class MasterDDLOperationHelper {
       regionLocations = locator.getAllRegionLocations();
     }
     // Convert List<HRegionLocation> to Map<HRegionInfo, ServerName>.
-    NavigableMap<HRegionInfo, ServerName> hri2Sn = new TreeMap<HRegionInfo, ServerName>();
+    NavigableMap<HRegionInfo, ServerName> hri2Sn = new TreeMap<>();
     for (HRegionLocation location : regionLocations) {
       hri2Sn.put(location.getRegionInfo(), location.getServerName());
     }
     TreeMap<ServerName, List<HRegionInfo>> serverToRegions = Maps.newTreeMap();
-    List<HRegionInfo> reRegions = new ArrayList<HRegionInfo>();
+    List<HRegionInfo> reRegions = new ArrayList<>();
     for (HRegionInfo hri : regionInfoList) {
       ServerName sn = hri2Sn.get(hri);
       // Skip the offlined split parent region
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/master/procedure/MergeTableRegionsProcedure.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/master/procedure/MergeTableRegionsProcedure.java
index d7fe5f6..366378a 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/master/procedure/MergeTableRegionsProcedure.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/master/procedure/MergeTableRegionsProcedure.java
@@ -678,7 +678,7 @@ public class MergeTableRegionsProcedure
     final MasterCoprocessorHost cpHost = env.getMasterCoprocessorHost();
     if (cpHost != null) {
       @MetaMutationAnnotation
-      final List<Mutation> metaEntries = new ArrayList<Mutation>();
+      final List<Mutation> metaEntries = new ArrayList<>();
       boolean ret = cpHost.preMergeRegionsCommit(regionsToMerge, metaEntries, getUser());
 
       if (ret) {
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/master/procedure/ModifyTableProcedure.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/master/procedure/ModifyTableProcedure.java
index f1b411a..6a70f62 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/master/procedure/ModifyTableProcedure.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/master/procedure/ModifyTableProcedure.java
@@ -337,7 +337,7 @@ public class ModifyTableProcedure
     final int newReplicaCount = newHTableDescriptor.getRegionReplication();
 
     if (newReplicaCount < oldReplicaCount) {
-      Set<byte[]> tableRows = new HashSet<byte[]>();
+      Set<byte[]> tableRows = new HashSet<>();
       Connection connection = env.getMasterServices().getConnection();
       Scan scan = MetaTableAccessor.getScanForTableName(connection, getTableName());
       scan.addColumn(HConstants.CATALOG_FAMILY, HConstants.REGIONINFO_QUALIFIER);
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/master/procedure/RestoreSnapshotProcedure.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/master/procedure/RestoreSnapshotProcedure.java
index d99bd6b..f8c9d8f 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/master/procedure/RestoreSnapshotProcedure.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/master/procedure/RestoreSnapshotProcedure.java
@@ -66,8 +66,7 @@ public class RestoreSnapshotProcedure
   private List<HRegionInfo> regionsToRestore = null;
   private List<HRegionInfo> regionsToRemove = null;
   private List<HRegionInfo> regionsToAdd = null;
-  private Map<String, Pair<String, String>> parentsToChildrenPairMap =
-    new HashMap<String, Pair<String, String>>();
+  private Map<String, Pair<String, String>> parentsToChildrenPairMap = new HashMap<>();
 
   private SnapshotDescription snapshot;
 
@@ -275,8 +274,7 @@ public class RestoreSnapshotProcedure
     if (restoreSnapshotMsg.getRegionInfoForRestoreCount() == 0) {
       regionsToRestore = null;
     } else {
-      regionsToRestore =
-        new ArrayList<HRegionInfo>(restoreSnapshotMsg.getRegionInfoForRestoreCount());
+      regionsToRestore = new ArrayList<>(restoreSnapshotMsg.getRegionInfoForRestoreCount());
       for (HBaseProtos.RegionInfo hri: restoreSnapshotMsg.getRegionInfoForRestoreList()) {
         regionsToRestore.add(HRegionInfo.convert(hri));
       }
@@ -284,8 +282,7 @@ public class RestoreSnapshotProcedure
     if (restoreSnapshotMsg.getRegionInfoForRemoveCount() == 0) {
       regionsToRemove = null;
     } else {
-      regionsToRemove =
-        new ArrayList<HRegionInfo>(restoreSnapshotMsg.getRegionInfoForRemoveCount());
+      regionsToRemove = new ArrayList<>(restoreSnapshotMsg.getRegionInfoForRemoveCount());
       for (HBaseProtos.RegionInfo hri: restoreSnapshotMsg.getRegionInfoForRemoveList()) {
         regionsToRemove.add(HRegionInfo.convert(hri));
       }
@@ -293,7 +290,7 @@ public class RestoreSnapshotProcedure
     if (restoreSnapshotMsg.getRegionInfoForAddCount() == 0) {
       regionsToAdd = null;
     } else {
-      regionsToAdd = new ArrayList<HRegionInfo>(restoreSnapshotMsg.getRegionInfoForAddCount());
+      regionsToAdd = new ArrayList<>(restoreSnapshotMsg.getRegionInfoForAddCount());
       for (HBaseProtos.RegionInfo hri: restoreSnapshotMsg.getRegionInfoForAddList()) {
         regionsToAdd.add(HRegionInfo.convert(hri));
       }
@@ -303,7 +300,7 @@ public class RestoreSnapshotProcedure
         restoreSnapshotMsg.getParentToChildRegionsPairListList()) {
         parentsToChildrenPairMap.put(
           parentToChildrenPair.getParentRegionName(),
-          new Pair<String, String>(
+          new Pair<>(
             parentToChildrenPair.getChild1RegionName(),
             parentToChildrenPair.getChild2RegionName()));
       }
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/master/procedure/ServerCrashProcedure.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/master/procedure/ServerCrashProcedure.java
index 7b4eb6e..484decc 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/master/procedure/ServerCrashProcedure.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/master/procedure/ServerCrashProcedure.java
@@ -99,7 +99,7 @@ implements ServerProcedureInterface {
 
   public static final int DEFAULT_WAIT_ON_RIT = 30000;
 
-  private static final Set<HRegionInfo> META_REGION_SET = new HashSet<HRegionInfo>();
+  private static final Set<HRegionInfo> META_REGION_SET = new HashSet<>();
   static {
     META_REGION_SET.add(HRegionInfo.FIRST_META_REGIONINFO);
   }
@@ -424,7 +424,7 @@ implements ServerProcedureInterface {
   private List<HRegionInfo> calcRegionsToAssign(final MasterProcedureEnv env)
   throws IOException {
     AssignmentManager am = env.getMasterServices().getAssignmentManager();
-    List<HRegionInfo> regionsToAssignAggregator = new ArrayList<HRegionInfo>();
+    List<HRegionInfo> regionsToAssignAggregator = new ArrayList<>();
     int replicaCount = env.getMasterConfiguration().getInt(HConstants.META_REPLICAS_NUM,
       HConstants.DEFAULT_META_REPLICA_NUM);
     for (int i = 1; i < replicaCount; i++) {
@@ -625,14 +625,14 @@ implements ServerProcedureInterface {
     this.shouldSplitWal = state.getShouldSplitWal();
     int size = state.getRegionsOnCrashedServerCount();
     if (size > 0) {
-      this.regionsOnCrashedServer = new HashSet<HRegionInfo>(size);
+      this.regionsOnCrashedServer = new HashSet<>(size);
       for (RegionInfo ri: state.getRegionsOnCrashedServerList()) {
         this.regionsOnCrashedServer.add(HRegionInfo.convert(ri));
       }
     }
     size = state.getRegionsAssignedCount();
     if (size > 0) {
-      this.regionsAssigned = new ArrayList<HRegionInfo>(size);
+      this.regionsAssigned = new ArrayList<>(size);
       for (RegionInfo ri: state.getRegionsOnCrashedServerList()) {
         this.regionsAssigned.add(HRegionInfo.convert(ri));
       }
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/master/procedure/SplitTableRegionProcedure.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/master/procedure/SplitTableRegionProcedure.java
index 69b89be..3cd6c66 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/master/procedure/SplitTableRegionProcedure.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/master/procedure/SplitTableRegionProcedure.java
@@ -547,7 +547,7 @@ public class SplitTableRegionProcedure
     }
     if (nbFiles == 0) {
       // no file needs to be splitted.
-      return new Pair<Integer, Integer>(0,0);
+      return new Pair<>(0,0);
     }
     // Default max #threads to use is the smaller of table's configured number of blocking store
     // files or the available number of logical cores.
@@ -561,7 +561,7 @@ public class SplitTableRegionProcedure
             " using " + maxThreads + " threads");
     ThreadPoolExecutor threadPool = (ThreadPoolExecutor) Executors.newFixedThreadPool(
       maxThreads, Threads.getNamedThreadFactory("StoreFileSplitter-%1$d"));
-    List<Future<Pair<Path,Path>>> futures = new ArrayList<Future<Pair<Path,Path>>> (nbFiles);
+    List<Future<Pair<Path,Path>>> futures = new ArrayList<>(nbFiles);
 
     // Split each store file.
     final HTableDescriptor htd = env.getMasterServices().getTableDescriptors().get(getTableName());
@@ -617,7 +617,7 @@ public class SplitTableRegionProcedure
       LOG.debug("Split storefiles for region " + parentHRI + " Daughter A: " + daughterA
           + " storefiles, Daughter B: " + daughterB + " storefiles.");
     }
-    return new Pair<Integer, Integer>(daughterA, daughterB);
+    return new Pair<>(daughterA, daughterB);
   }
 
   private void assertReferenceFileCount(
@@ -646,7 +646,7 @@ public class SplitTableRegionProcedure
     if (LOG.isDebugEnabled()) {
       LOG.debug("Splitting complete for store file: " + sf.getPath() + " for region: " + parentHRI);
     }
-    return new Pair<Path,Path>(path_first, path_second);
+    return new Pair<>(path_first, path_second);
   }
 
   /**
@@ -684,7 +684,7 @@ public class SplitTableRegionProcedure
    **/
   private void preSplitRegionBeforePONR(final MasterProcedureEnv env)
     throws IOException, InterruptedException {
-    final List<Mutation> metaEntries = new ArrayList<Mutation>();
+    final List<Mutation> metaEntries = new ArrayList<>();
     final MasterCoprocessorHost cpHost = env.getMasterCoprocessorHost();
     if (cpHost != null) {
       if (cpHost.preSplitBeforePONRAction(getSplitRow(), metaEntries, getUser())) {
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/master/procedure/TruncateTableProcedure.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/master/procedure/TruncateTableProcedure.java
index 7482831..2ab142a 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/master/procedure/TruncateTableProcedure.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/master/procedure/TruncateTableProcedure.java
@@ -250,7 +250,7 @@ public class TruncateTableProcedure
     if (state.getRegionInfoCount() == 0) {
       regions = null;
     } else {
-      regions = new ArrayList<HRegionInfo>(state.getRegionInfoCount());
+      regions = new ArrayList<>(state.getRegionInfoCount());
       for (HBaseProtos.RegionInfo hri: state.getRegionInfoList()) {
         regions.add(HRegionInfo.convert(hri));
       }
@@ -258,7 +258,7 @@ public class TruncateTableProcedure
   }
 
   private static List<HRegionInfo> recreateRegionInfo(final List<HRegionInfo> regions) {
-    ArrayList<HRegionInfo> newRegions = new ArrayList<HRegionInfo>(regions.size());
+    ArrayList<HRegionInfo> newRegions = new ArrayList<>(regions.size());
     for (HRegionInfo hri: regions) {
       newRegions.add(new HRegionInfo(hri.getTable(), hri.getStartKey(), hri.getEndKey()));
     }
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/master/snapshot/DisabledTableSnapshotHandler.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/master/snapshot/DisabledTableSnapshotHandler.java
index 113ce58..5f86e08 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/master/snapshot/DisabledTableSnapshotHandler.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/master/snapshot/DisabledTableSnapshotHandler.java
@@ -74,7 +74,7 @@ public class DisabledTableSnapshotHandler extends TakeSnapshotHandler {
       // 1. get all the regions hosting this table.
 
       // extract each pair to separate lists
-      Set<HRegionInfo> regions = new HashSet<HRegionInfo>();
+      Set<HRegionInfo> regions = new HashSet<>();
       for (Pair<HRegionInfo, ServerName> p : regionsAndLocations) {
         // Don't include non-default regions
         HRegionInfo hri = p.getFirst();
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/master/snapshot/EnabledTableSnapshotHandler.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/master/snapshot/EnabledTableSnapshotHandler.java
index e63727a..73cd4d7 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/master/snapshot/EnabledTableSnapshotHandler.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/master/snapshot/EnabledTableSnapshotHandler.java
@@ -71,7 +71,7 @@ public class EnabledTableSnapshotHandler extends TakeSnapshotHandler {
   @Override
   protected void snapshotRegions(List<Pair<HRegionInfo, ServerName>> regions)
       throws HBaseSnapshotException, IOException {
-    Set<String> regionServers = new HashSet<String>(regions.size());
+    Set<String> regionServers = new HashSet<>(regions.size());
     for (Pair<HRegionInfo, ServerName> region : regions) {
       if (region != null && region.getFirst() != null && region.getSecond() != null) {
         HRegionInfo hri = region.getFirst();
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/master/snapshot/SnapshotFileCache.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/master/snapshot/SnapshotFileCache.java
index f03344c..b6641de 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/master/snapshot/SnapshotFileCache.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/master/snapshot/SnapshotFileCache.java
@@ -90,13 +90,12 @@ public class SnapshotFileCache implements Stoppable {
   private final FileSystem fs;
   private final SnapshotFileInspector fileInspector;
   private final Path snapshotDir;
-  private final Set<String> cache = new HashSet<String>();
+  private final Set<String> cache = new HashSet<>();
   /**
    * This is a helper map of information about the snapshot directories so we don't need to rescan
    * them if they haven't changed since the last time we looked.
    */
-  private final Map<String, SnapshotDirectoryInfo> snapshots =
-      new HashMap<String, SnapshotDirectoryInfo>();
+  private final Map<String, SnapshotDirectoryInfo> snapshots = new HashMap<>();
   private final Timer refreshTimer;
 
   private long lastModifiedTime = Long.MIN_VALUE;
@@ -229,7 +228,7 @@ public class SnapshotFileCache implements Stoppable {
 
     // 2.clear the cache
     this.cache.clear();
-    Map<String, SnapshotDirectoryInfo> known = new HashMap<String, SnapshotDirectoryInfo>();
+    Map<String, SnapshotDirectoryInfo> known = new HashMap<>();
 
     // 3. check each of the snapshot directories
     FileStatus[] snapshots = FSUtils.listStatus(fs, snapshotDir);
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/master/snapshot/SnapshotManager.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/master/snapshot/SnapshotManager.java
index b950079..54b68d3 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/master/snapshot/SnapshotManager.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/master/snapshot/SnapshotManager.java
@@ -147,8 +147,7 @@ public class SnapshotManager extends MasterProcedureManager implements Stoppable
   // The map is always accessed and modified under the object lock using synchronized.
   // snapshotTable() will insert an Handler in the table.
   // isSnapshotDone() will remove the handler requested if the operation is finished.
-  private Map<TableName, SnapshotSentinel> snapshotHandlers =
-      new HashMap<TableName, SnapshotSentinel>();
+  private Map<TableName, SnapshotSentinel> snapshotHandlers = new HashMap<>();
 
   // Restore map, with table name as key, procedure ID as value.
   // The map is always accessed and modified under the object lock using synchronized.
@@ -157,7 +156,7 @@ public class SnapshotManager extends MasterProcedureManager implements Stoppable
   // TODO: just as the Apache HBase 1.x implementation, this map would not survive master
   // restart/failover. This is just a stopgap implementation until implementation of taking
   // snapshot using Procedure-V2.
-  private Map<TableName, Long> restoreTableToProcIdMap = new HashMap<TableName, Long>();
+  private Map<TableName, Long> restoreTableToProcIdMap = new HashMap<>();
 
   private Path rootDir;
   private ExecutorService executorService;
@@ -168,7 +167,7 @@ public class SnapshotManager extends MasterProcedureManager implements Stoppable
    *    - create snapshot
    *    - SnapshotCleaner
    * */
-  private KeyLocker<String> locks = new KeyLocker<String>();
+  private KeyLocker<String> locks = new KeyLocker<>();
 
 
 
@@ -209,7 +208,7 @@ public class SnapshotManager extends MasterProcedureManager implements Stoppable
    * @throws IOException File system exception
    */
   private List<SnapshotDescription> getCompletedSnapshots(Path snapshotDir) throws IOException {
-    List<SnapshotDescription> snapshotDescs = new ArrayList<SnapshotDescription>();
+    List<SnapshotDescription> snapshotDescs = new ArrayList<>();
     // first create the snapshot root path and check to see if it exists
     FileSystem fs = master.getMasterFileSystem().getFileSystem();
     if (snapshotDir == null) snapshotDir = SnapshotDescriptionUtils.getSnapshotsDir(rootDir);
@@ -1032,11 +1031,11 @@ public class SnapshotManager extends MasterProcedureManager implements Stoppable
     boolean userDisabled = (enabled != null && enabled.trim().length() > 0 && !snapshotEnabled);
 
     // Extract cleaners from conf
-    Set<String> hfileCleaners = new HashSet<String>();
+    Set<String> hfileCleaners = new HashSet<>();
     String[] cleaners = conf.getStrings(HFileCleaner.MASTER_HFILE_CLEANER_PLUGINS);
     if (cleaners != null) Collections.addAll(hfileCleaners, cleaners);
 
-    Set<String> logCleaners = new HashSet<String>();
+    Set<String> logCleaners = new HashSet<>();
     cleaners = conf.getStrings(HConstants.HBASE_MASTER_LOGCLEANER_PLUGINS);
     if (cleaners != null) Collections.addAll(logCleaners, cleaners);
 
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/master/snapshot/TakeSnapshotHandler.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/master/snapshot/TakeSnapshotHandler.java
index 992f28e..123758f 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/master/snapshot/TakeSnapshotHandler.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/master/snapshot/TakeSnapshotHandler.java
@@ -183,7 +183,7 @@ public abstract class TakeSnapshotHandler extends EventHandler implements Snapsh
       monitor.rethrowException();
 
       // extract each pair to separate lists
-      Set<String> serverNames = new HashSet<String>();
+      Set<String> serverNames = new HashSet<>();
       for (Pair<HRegionInfo, ServerName> p : regionsAndLocations) {
         if (p != null && p.getFirst() != null && p.getSecond() != null) {
           HRegionInfo hri = p.getFirst();
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/mob/DefaultMobStoreCompactor.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/mob/DefaultMobStoreCompactor.java
index 697286c..56c0242 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/mob/DefaultMobStoreCompactor.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/mob/DefaultMobStoreCompactor.java
@@ -174,7 +174,7 @@ public class DefaultMobStoreCompactor extends DefaultCompactor {
     long bytesWrittenProgressForShippedCall = 0;
     // Since scanner.next() can return 'false' but still be delivering data,
     // we have to use a do/while loop.
-    List<Cell> cells = new ArrayList<Cell>();
+    List<Cell> cells = new ArrayList<>();
     // Limit to "hbase.hstore.compaction.kv.max" (default 10) to avoid OOME
     int closeCheckSizeLimit = HStore.getCloseCheckInterval();
     long lastMillis = 0;
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/mob/DefaultMobStoreFlusher.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/mob/DefaultMobStoreFlusher.java
index 3c6a071..2456a41 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/mob/DefaultMobStoreFlusher.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/mob/DefaultMobStoreFlusher.java
@@ -98,7 +98,7 @@ public class DefaultMobStoreFlusher extends DefaultStoreFlusher {
   @Override
   public List<Path> flushSnapshot(MemStoreSnapshot snapshot, long cacheFlushId,
       MonitoredTask status, ThroughputController throughputController) throws IOException {
-    ArrayList<Path> result = new ArrayList<Path>();
+    ArrayList<Path> result = new ArrayList<>();
     long cellsCount = snapshot.getCellsCount();
     if (cellsCount == 0) return result; // don't flush if there are no entries
 
@@ -179,7 +179,7 @@ public class DefaultMobStoreFlusher extends DefaultStoreFlusher {
     byte[] fileName = Bytes.toBytes(mobFileWriter.getPath().getName());
     ScannerContext scannerContext =
         ScannerContext.newBuilder().setBatchLimit(compactionKVMax).build();
-    List<Cell> cells = new ArrayList<Cell>();
+    List<Cell> cells = new ArrayList<>();
     boolean hasMore;
     String flushName = ThroughputControlUtil.getNameForThrottling(store, "flush");
     boolean control = throughputController != null && !store.getRegionInfo().isSystemTable();
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/mob/MobFile.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/mob/MobFile.java
index 8c760e6..cd4c079 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/mob/MobFile.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/mob/MobFile.java
@@ -55,7 +55,7 @@ public class MobFile {
    * @throws IOException
    */
   public StoreFileScanner getScanner() throws IOException {
-    List<StoreFile> sfs = new ArrayList<StoreFile>();
+    List<StoreFile> sfs = new ArrayList<>();
     sfs.add(sf);
     List<StoreFileScanner> sfScanners = StoreFileScanner.getScannersForStoreFiles(sfs, false, true,
         false, false, sf.getMaxMemstoreTS());
@@ -85,7 +85,7 @@ public class MobFile {
   public Cell readCell(Cell search, boolean cacheMobBlocks, long readPt) throws IOException {
     Cell result = null;
     StoreFileScanner scanner = null;
-    List<StoreFile> sfs = new ArrayList<StoreFile>();
+    List<StoreFile> sfs = new ArrayList<>();
     sfs.add(sf);
     try {
       List<StoreFileScanner> sfScanners = StoreFileScanner.getScannersForStoreFiles(sfs,
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/mob/MobFileCache.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/mob/MobFileCache.java
index fd62340..25328b1 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/mob/MobFileCache.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/mob/MobFileCache.java
@@ -102,7 +102,7 @@ public class MobFileCache {
     this.mobFileMaxCacheSize = conf.getInt(MobConstants.MOB_FILE_CACHE_SIZE_KEY,
         MobConstants.DEFAULT_MOB_FILE_CACHE_SIZE);
     isCacheEnabled = (mobFileMaxCacheSize > 0);
-    map = new ConcurrentHashMap<String, CachedMobFile>(mobFileMaxCacheSize);
+    map = new ConcurrentHashMap<>(mobFileMaxCacheSize);
     if (isCacheEnabled) {
       long period = conf.getLong(MobConstants.MOB_CACHE_EVICT_PERIOD,
           MobConstants.DEFAULT_MOB_CACHE_EVICT_PERIOD); // in seconds
@@ -136,12 +136,12 @@ public class MobFileCache {
         return;
       }
       printStatistics();
-      List<CachedMobFile> evictedFiles = new ArrayList<CachedMobFile>();
+      List<CachedMobFile> evictedFiles = new ArrayList<>();
       try {
         if (map.size() <= mobFileMaxCacheSize) {
           return;
         }
-        List<CachedMobFile> files = new ArrayList<CachedMobFile>(map.values());
+        List<CachedMobFile> files = new ArrayList<>(map.values());
         Collections.sort(files);
         int start = (int) (mobFileMaxCacheSize * evictRemainRatio);
         if (start >= 0) {
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/mob/MobUtils.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/mob/MobUtils.java
index 8191828..eb75120 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/mob/MobUtils.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/mob/MobUtils.java
@@ -314,7 +314,7 @@ public final class MobUtils {
       // no file found
       return;
     }
-    List<StoreFile> filesToClean = new ArrayList<StoreFile>();
+    List<StoreFile> filesToClean = new ArrayList<>();
     int deletedFileCount = 0;
     for (FileStatus file : stats) {
       String fileName = file.getPath().getName();
@@ -482,7 +482,7 @@ public final class MobUtils {
   public static Cell createMobRefCell(Cell cell, byte[] fileName, Tag tableNameTag) {
     // Append the tags to the KeyValue.
     // The key is same, the value is the filename of the mob file
-    List<Tag> tags = new ArrayList<Tag>();
+    List<Tag> tags = new ArrayList<>();
     // Add the ref tag as the 1st one.
     tags.add(MobConstants.MOB_REF_TAG);
     // Add the tag of the source table name, this table is where this mob file is flushed
@@ -832,7 +832,7 @@ public final class MobUtils {
     if (maxThreads == 0) {
       maxThreads = 1;
     }
-    final SynchronousQueue<Runnable> queue = new SynchronousQueue<Runnable>();
+    final SynchronousQueue<Runnable> queue = new SynchronousQueue<>();
     ThreadPoolExecutor pool = new ThreadPoolExecutor(1, maxThreads, 60, TimeUnit.SECONDS, queue,
       Threads.newDaemonThreadFactory("MobCompactor"), new RejectedExecutionHandler() {
         @Override
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/mob/compactions/PartitionedMobCompactionRequest.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/mob/compactions/PartitionedMobCompactionRequest.java
index b6cf814..f1dcaee 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/mob/compactions/PartitionedMobCompactionRequest.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/mob/compactions/PartitionedMobCompactionRequest.java
@@ -71,7 +71,7 @@ public class PartitionedMobCompactionRequest extends MobCompactionRequest {
    * the same partition.
    */
   protected static class CompactionPartition {
-    private List<FileStatus> files = new ArrayList<FileStatus>();
+    private List<FileStatus> files = new ArrayList<>();
     private CompactionPartitionId partitionId;
 
     // The startKey and endKey of this partition, both are inclusive.
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/monitoring/MonitoredRPCHandlerImpl.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/monitoring/MonitoredRPCHandlerImpl.java
index 7ff7db6..b49df28 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/monitoring/MonitoredRPCHandlerImpl.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/monitoring/MonitoredRPCHandlerImpl.java
@@ -224,7 +224,7 @@ public class MonitoredRPCHandlerImpl extends MonitoredTaskImpl
     if (getState() != State.RUNNING) {
       return map;
     }
-    Map<String, Object> rpcJSON = new HashMap<String, Object>();
+    Map<String, Object> rpcJSON = new HashMap<>();
     ArrayList paramList = new ArrayList();
     map.put("rpcCall", rpcJSON);
     rpcJSON.put("queuetimems", getRPCQueueTime());
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/monitoring/MonitoredTaskImpl.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/monitoring/MonitoredTaskImpl.java
index 27aaceb..dda77ac 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/monitoring/MonitoredTaskImpl.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/monitoring/MonitoredTaskImpl.java
@@ -148,7 +148,7 @@ class MonitoredTaskImpl implements MonitoredTask {
 
   @Override
   public Map<String, Object> toMap() {
-    Map<String, Object> map = new HashMap<String, Object>();
+    Map<String, Object> map = new HashMap<>();
     map.put("description", getDescription());
     map.put("status", getStatus());
     map.put("state", getState());
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/monitoring/TaskMonitor.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/monitoring/TaskMonitor.java
index 949b036..ff92704 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/monitoring/TaskMonitor.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/monitoring/TaskMonitor.java
@@ -72,7 +72,7 @@ public class TaskMonitor {
     MonitoredTask proxy = (MonitoredTask) Proxy.newProxyInstance(
         stat.getClass().getClassLoader(),
         new Class<?>[] { MonitoredTask.class },
-        new PassthroughInvocationHandler<MonitoredTask>(stat));
+        new PassthroughInvocationHandler<>(stat));
     TaskAndWeakRefPair pair = new TaskAndWeakRefPair(stat, proxy);
     if (tasks.isFull()) {
       purgeExpiredTasks();
@@ -87,7 +87,7 @@ public class TaskMonitor {
     MonitoredRPCHandler proxy = (MonitoredRPCHandler) Proxy.newProxyInstance(
         stat.getClass().getClassLoader(),
         new Class<?>[] { MonitoredRPCHandler.class },
-        new PassthroughInvocationHandler<MonitoredRPCHandler>(stat));
+        new PassthroughInvocationHandler<>(stat));
     TaskAndWeakRefPair pair = new TaskAndWeakRefPair(stat, proxy);
     rpcTasks.add(pair);
     return proxy;
@@ -189,7 +189,7 @@ public class TaskMonitor {
     public TaskAndWeakRefPair(MonitoredTask stat,
         MonitoredTask proxy) {
       this.impl = stat;
-      this.weakProxy = new WeakReference<MonitoredTask>(proxy);
+      this.weakProxy = new WeakReference<>(proxy);
     }
     
     public MonitoredTask get() {
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/namespace/NamespaceStateManager.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/namespace/NamespaceStateManager.java
index 523b056..604f211 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/namespace/NamespaceStateManager.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/namespace/NamespaceStateManager.java
@@ -47,7 +47,7 @@ class NamespaceStateManager {
   private volatile boolean initialized = false;
 
   public NamespaceStateManager(MasterServices masterServices) {
-    nsStateCache = new ConcurrentHashMap<String, NamespaceTableAndRegionInfo>();
+    nsStateCache = new ConcurrentHashMap<>();
     master = masterServices;
   }
 
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/namespace/NamespaceTableAndRegionInfo.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/namespace/NamespaceTableAndRegionInfo.java
index 86651e4..d30de6e 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/namespace/NamespaceTableAndRegionInfo.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/namespace/NamespaceTableAndRegionInfo.java
@@ -40,7 +40,7 @@ class NamespaceTableAndRegionInfo {
 
   public NamespaceTableAndRegionInfo(String namespace) {
     this.name = namespace;
-    this.tableAndRegionInfo = new HashMap<TableName, AtomicInteger>();
+    this.tableAndRegionInfo = new HashMap<>();
   }
 
   /**
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/procedure/MasterProcedureManagerHost.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/procedure/MasterProcedureManagerHost.java
index 8161ffe..222c933 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/procedure/MasterProcedureManagerHost.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/procedure/MasterProcedureManagerHost.java
@@ -33,8 +33,7 @@ import org.apache.zookeeper.KeeperException;
 public class MasterProcedureManagerHost extends
     ProcedureManagerHost<MasterProcedureManager> {
 
-  private Hashtable<String, MasterProcedureManager> procedureMgrMap
-      = new Hashtable<String, MasterProcedureManager>();
+  private Hashtable<String, MasterProcedureManager> procedureMgrMap = new Hashtable<>();
 
   @Override
   public void loadProcedures(Configuration conf) {
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/procedure/Procedure.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/procedure/Procedure.java
index 0279a60..1d20ba5 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/procedure/Procedure.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/procedure/Procedure.java
@@ -125,9 +125,9 @@ public class Procedure implements Callable<Void>, ForeignExceptionListener {
   public Procedure(ProcedureCoordinator coord, ForeignExceptionDispatcher monitor, long wakeFreq,
       long timeout, String procName, byte[] args, List<String> expectedMembers) {
     this.coord = coord;
-    this.acquiringMembers = new ArrayList<String>(expectedMembers);
-    this.inBarrierMembers = new ArrayList<String>(acquiringMembers.size());
-    this.dataFromFinishedMembers = new HashMap<String, byte[]>();
+    this.acquiringMembers = new ArrayList<>(expectedMembers);
+    this.inBarrierMembers = new ArrayList<>(acquiringMembers.size());
+    this.dataFromFinishedMembers = new HashMap<>();
     this.procName = procName;
     this.args = args;
     this.monitor = monitor;
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/procedure/ProcedureCoordinator.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/procedure/ProcedureCoordinator.java
index b7e0c04..8a64cc8 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/procedure/ProcedureCoordinator.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/procedure/ProcedureCoordinator.java
@@ -113,7 +113,7 @@ public class ProcedureCoordinator {
   public static ThreadPoolExecutor defaultPool(String coordName, int opThreads,
       long keepAliveMillis) {
     return new ThreadPoolExecutor(1, opThreads, keepAliveMillis, TimeUnit.MILLISECONDS,
-        new SynchronousQueue<Runnable>(),
+        new SynchronousQueue<>(),
         new DaemonThreadFactory("(" + coordName + ")-proc-coordinator-pool"));
   }
 
@@ -325,6 +325,6 @@ public class ProcedureCoordinator {
    * @return Return set of all procedure names.
    */
   public Set<String> getProcedureNames() {
-    return new HashSet<String>(procedures.keySet());
+    return new HashSet<>(procedures.keySet());
   }
 }
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/procedure/ProcedureManagerHost.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/procedure/ProcedureManagerHost.java
index 3ab4ac5..f61ce14 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/procedure/ProcedureManagerHost.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/procedure/ProcedureManagerHost.java
@@ -46,7 +46,7 @@ public abstract class ProcedureManagerHost<E extends ProcedureManager> {
 
   private static final Log LOG = LogFactory.getLog(ProcedureManagerHost.class);
 
-  protected Set<E> procedures = new HashSet<E>();
+  protected Set<E> procedures = new HashSet<>();
 
   /**
    * Load system procedures. Read the class names from configuration.
@@ -60,7 +60,7 @@ public abstract class ProcedureManagerHost<E extends ProcedureManager> {
     if (defaultProcClasses == null || defaultProcClasses.length == 0)
       return;
 
-    List<E> configured = new ArrayList<E>();
+    List<E> configured = new ArrayList<>();
     for (String className : defaultProcClasses) {
       className = className.trim();
       ClassLoader cl = this.getClass().getClassLoader();
@@ -105,7 +105,7 @@ public abstract class ProcedureManagerHost<E extends ProcedureManager> {
   }
 
   public Set<E> getProcedureManagers() {
-    Set<E> returnValue = new HashSet<E>();
+    Set<E> returnValue = new HashSet<>();
     for (E e: procedures) {
       returnValue.add(e);
     }
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/procedure/ProcedureMember.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/procedure/ProcedureMember.java
index 485821e..baed1f3 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/procedure/ProcedureMember.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/procedure/ProcedureMember.java
@@ -86,7 +86,7 @@ public class ProcedureMember implements Closeable {
   public static ThreadPoolExecutor defaultPool(String memberName, int procThreads,
       long keepAliveMillis) {
     return new ThreadPoolExecutor(1, procThreads, keepAliveMillis, TimeUnit.MILLISECONDS,
-        new SynchronousQueue<Runnable>(),
+        new SynchronousQueue<>(),
         new DaemonThreadFactory("member: '" + memberName + "' subprocedure-pool"));
   }
 
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/procedure/flush/MasterFlushTableProcedureManager.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/procedure/flush/MasterFlushTableProcedureManager.java
index 802a5ab..7b624a5 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/procedure/flush/MasterFlushTableProcedureManager.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/procedure/flush/MasterFlushTableProcedureManager.java
@@ -68,7 +68,7 @@ public class MasterFlushTableProcedureManager extends MasterProcedureManager {
 
   private MasterServices master;
   private ProcedureCoordinator coordinator;
-  private Map<TableName, Procedure> procMap = new HashMap<TableName, Procedure>();
+  private Map<TableName, Procedure> procMap = new HashMap<>();
   private boolean stopped;
 
   public MasterFlushTableProcedureManager() {};
@@ -135,7 +135,7 @@ public class MasterFlushTableProcedureManager extends MasterProcedureManager {
         master.getConnection(), tableName, false);
     }
 
-    Set<String> regionServers = new HashSet<String>(regionsAndLocations.size());
+    Set<String> regionServers = new HashSet<>(regionsAndLocations.size());
     for (Pair<HRegionInfo, ServerName> region : regionsAndLocations) {
       if (region != null && region.getFirst() != null && region.getSecond() != null) {
         HRegionInfo hri = region.getFirst();
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/procedure/flush/RegionServerFlushTableProcedureManager.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/procedure/flush/RegionServerFlushTableProcedureManager.java
index 1aa959c..147c013 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/procedure/flush/RegionServerFlushTableProcedureManager.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/procedure/flush/RegionServerFlushTableProcedureManager.java
@@ -201,7 +201,7 @@ public class RegionServerFlushTableProcedureManager extends RegionServerProcedur
     private final ExecutorCompletionService<Void> taskPool;
     private final ThreadPoolExecutor executor;
     private volatile boolean stopped;
-    private final List<Future<Void>> futures = new ArrayList<Future<Void>>();
+    private final List<Future<Void>> futures = new ArrayList<>();
     private final String name;
 
     FlushTableSubprocedurePool(String name, Configuration conf, Abortable abortable) {
@@ -213,10 +213,10 @@ public class RegionServerFlushTableProcedureManager extends RegionServerProcedur
       int threads = conf.getInt(CONCURENT_FLUSH_TASKS_KEY, DEFAULT_CONCURRENT_FLUSH_TASKS);
       this.name = name;
       executor = new ThreadPoolExecutor(threads, threads, keepAlive, TimeUnit.MILLISECONDS,
-          new LinkedBlockingQueue<Runnable>(), new DaemonThreadFactory("rs("
+          new LinkedBlockingQueue<>(), new DaemonThreadFactory("rs("
               + name + ")-flush-proc-pool"));
       executor.allowCoreThreadTimeOut(true);
-      taskPool = new ExecutorCompletionService<Void>(executor);
+      taskPool = new ExecutorCompletionService<>(executor);
     }
 
     boolean hasTasks() {
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/protobuf/ReplicationProtbufUtil.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/protobuf/ReplicationProtbufUtil.java
index c301759..8f681f0 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/protobuf/ReplicationProtbufUtil.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/protobuf/ReplicationProtbufUtil.java
@@ -99,7 +99,7 @@ public class ReplicationProtbufUtil {
       buildReplicateWALEntryRequest(final Entry[] entries, byte[] encodedRegionName,
           String replicationClusterId, Path sourceBaseNamespaceDir, Path sourceHFileArchiveDir) {
     // Accumulate all the Cells seen in here.
-    List<List<? extends Cell>> allCells = new ArrayList<List<? extends Cell>>(entries.length);
+    List<List<? extends Cell>> allCells = new ArrayList<>(entries.length);
     int size = 0;
     WALProtos.FamilyScope.Builder scopeBuilder = WALProtos.FamilyScope.newBuilder();
     AdminProtos.WALEntry.Builder entryBuilder = AdminProtos.WALEntry.newBuilder();
@@ -165,7 +165,7 @@ public class ReplicationProtbufUtil {
       builder.setSourceHFileArchiveDirPath(sourceHFileArchiveDir.toString());
     }
 
-    return new Pair<AdminProtos.ReplicateWALEntryRequest, CellScanner>(builder.build(),
+    return new Pair<>(builder.build(),
       getCellScanner(allCells, size));
   }
 
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/quotas/MasterQuotaManager.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/quotas/MasterQuotaManager.java
index 647a770..5dab2e3 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/quotas/MasterQuotaManager.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/quotas/MasterQuotaManager.java
@@ -80,9 +80,9 @@ public class MasterQuotaManager implements RegionStateListener {
     }
 
     LOG.info("Initializing quota support");
-    namespaceLocks = new NamedLock<String>();
-    tableLocks = new NamedLock<TableName>();
-    userLocks = new NamedLock<String>();
+    namespaceLocks = new NamedLock<>();
+    tableLocks = new NamedLock<>();
+    userLocks = new NamedLock<>();
 
     namespaceQuotaManager = new NamespaceAuditor(masterServices);
     namespaceQuotaManager.start();
@@ -460,7 +460,7 @@ public class MasterQuotaManager implements RegionStateListener {
   }
 
   private static class NamedLock<T> {
-    private HashSet<T> locks = new HashSet<T>();
+    private HashSet<T> locks = new HashSet<>();
 
     public void lock(final T name) throws InterruptedException {
       synchronized (locks) {
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/quotas/QuotaCache.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/quotas/QuotaCache.java
index 1451052..ad91617 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/quotas/QuotaCache.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/quotas/QuotaCache.java
@@ -65,12 +65,9 @@ public class QuotaCache implements Stoppable {
   // for testing purpose only, enforce the cache to be always refreshed
   static boolean TEST_FORCE_REFRESH = false;
 
-  private final ConcurrentHashMap<String, QuotaState> namespaceQuotaCache =
-      new ConcurrentHashMap<String, QuotaState>();
-  private final ConcurrentHashMap<TableName, QuotaState> tableQuotaCache =
-      new ConcurrentHashMap<TableName, QuotaState>();
-  private final ConcurrentHashMap<String, UserQuotaState> userQuotaCache =
-      new ConcurrentHashMap<String, UserQuotaState>();
+  private final ConcurrentHashMap<String, QuotaState> namespaceQuotaCache = new ConcurrentHashMap<>();
+  private final ConcurrentHashMap<TableName, QuotaState> tableQuotaCache = new ConcurrentHashMap<>();
+  private final ConcurrentHashMap<String, UserQuotaState> userQuotaCache = new ConcurrentHashMap<>();
   private final RegionServerServices rsServices;
 
   private QuotaRefresherChore refreshChore;
@@ -262,8 +259,8 @@ public class QuotaCache implements Stoppable {
       long evictPeriod = refreshPeriod * EVICT_PERIOD_FACTOR;
 
       // Find the quota entries to update
-      List<Get> gets = new ArrayList<Get>();
-      List<K> toRemove = new ArrayList<K>();
+      List<Get> gets = new ArrayList<>();
+      List<K> toRemove = new ArrayList<>();
       for (Map.Entry<K, V> entry: quotasMap.entrySet()) {
         long lastUpdate = entry.getValue().getLastUpdate();
         long lastQuery = entry.getValue().getLastQuery();
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/quotas/QuotaUtil.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/quotas/QuotaUtil.java
index ab646b9..fd12fa1 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/quotas/QuotaUtil.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/quotas/QuotaUtil.java
@@ -167,7 +167,7 @@ public class QuotaUtil extends QuotaTableUtil {
     long nowTs = EnvironmentEdgeManager.currentTime();
     Result[] results = doGet(connection, gets);
 
-    Map<String, UserQuotaState> userQuotas = new HashMap<String, UserQuotaState>(results.length);
+    Map<String, UserQuotaState> userQuotas = new HashMap<>(results.length);
     for (int i = 0; i < results.length; ++i) {
       byte[] key = gets.get(i).getRow();
       assert isUserRowKey(key);
@@ -232,7 +232,7 @@ public class QuotaUtil extends QuotaTableUtil {
     long nowTs = EnvironmentEdgeManager.currentTime();
     Result[] results = doGet(connection, gets);
 
-    Map<K, QuotaState> globalQuotas = new HashMap<K, QuotaState>(results.length);
+    Map<K, QuotaState> globalQuotas = new HashMap<>(results.length);
     for (int i = 0; i < results.length; ++i) {
       byte[] row = gets.get(i).getRow();
       K key = kfr.getKeyFromRow(row);
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/quotas/UserQuotaState.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/quotas/UserQuotaState.java
index cb00c34..21b4768 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/quotas/UserQuotaState.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/quotas/UserQuotaState.java
@@ -123,7 +123,7 @@ public class UserQuotaState extends QuotaState {
   private <K> Map<K, QuotaLimiter> setLimiter(Map<K, QuotaLimiter> limiters,
       final K key, final Quotas quotas) {
     if (limiters == null) {
-      limiters = new HashMap<K, QuotaLimiter>();
+      limiters = new HashMap<>();
     }
 
     QuotaLimiter limiter = quotas.hasThrottle() ?
@@ -164,7 +164,7 @@ public class UserQuotaState extends QuotaState {
 
     if (otherMap != null) {
       // To Remove
-      Set<K> toRemove = new HashSet<K>(map.keySet());
+      Set<K> toRemove = new HashSet<>(map.keySet());
       toRemove.removeAll(otherMap.keySet());
       map.keySet().removeAll(toRemove);
 
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/AbstractMultiFileWriter.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/AbstractMultiFileWriter.java
index 0735629..91c0050 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/AbstractMultiFileWriter.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/AbstractMultiFileWriter.java
@@ -70,7 +70,7 @@ public abstract class AbstractMultiFileWriter implements CellSink, ShipperListen
       LOG.debug("Commit " + writers.size() + " writers, maxSeqId=" + maxSeqId
           + ", majorCompaction=" + majorCompaction);
     }
-    List<Path> paths = new ArrayList<Path>();
+    List<Path> paths = new ArrayList<>();
     for (StoreFileWriter writer : writers) {
       if (writer == null) {
         continue;
@@ -87,7 +87,7 @@ public abstract class AbstractMultiFileWriter implements CellSink, ShipperListen
    * Close all writers without throwing any exceptions. This is used when compaction failed usually.
    */
   public List<Path> abortWriters() {
-    List<Path> paths = new ArrayList<Path>();
+    List<Path> paths = new ArrayList<>();
     for (StoreFileWriter writer : writers()) {
       try {
         if (writer != null) {
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/AnnotationReadingPriorityFunction.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/AnnotationReadingPriorityFunction.java
index c492180..6c98c1d 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/AnnotationReadingPriorityFunction.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/AnnotationReadingPriorityFunction.java
@@ -92,10 +92,8 @@ public class AnnotationReadingPriorityFunction implements PriorityFunction {
   };
 
   // Some caches for helping performance
-  private final Map<String, Class<? extends Message>> argumentToClassMap =
-    new HashMap<String, Class<? extends Message>>();
-  private final Map<String, Map<Class<? extends Message>, Method>> methodMap =
-    new HashMap<String, Map<Class<? extends Message>, Method>>();
+  private final Map<String, Class<? extends Message>> argumentToClassMap = new HashMap<>();
+  private final Map<String, Map<Class<? extends Message>, Method>> methodMap = new HashMap<>();
 
   private final float scanVirtualTimeWeight;
 
@@ -121,7 +119,7 @@ public class AnnotationReadingPriorityFunction implements PriorityFunction {
    */
   public AnnotationReadingPriorityFunction(final RSRpcServices rpcServices,
       Class<? extends RSRpcServices> clz) {
-    Map<String,Integer> qosMap = new HashMap<String,Integer>();
+    Map<String,Integer> qosMap = new HashMap<>();
     for (Method m : clz.getMethods()) {
       QosPriority p = m.getAnnotation(QosPriority.class);
       if (p != null) {
@@ -137,8 +135,8 @@ public class AnnotationReadingPriorityFunction implements PriorityFunction {
     this.rpcServices = rpcServices;
     this.annotatedQos = qosMap;
     if (methodMap.get("getRegion") == null) {
-      methodMap.put("hasRegion", new HashMap<Class<? extends Message>, Method>());
-      methodMap.put("getRegion", new HashMap<Class<? extends Message>, Method>());
+      methodMap.put("hasRegion", new HashMap<>());
+      methodMap.put("getRegion", new HashMap<>());
     }
     for (Class<? extends Message> cls : knownArgumentClasses) {
       argumentToClassMap.put(cls.getName(), cls);
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/BaseRowProcessor.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/BaseRowProcessor.java
index be2bd91..0b1ab18 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/BaseRowProcessor.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/BaseRowProcessor.java
@@ -57,7 +57,7 @@ implements RowProcessor<S,T> {
 
   @Override
   public List<UUID> getClusterIds() {
-    return new ArrayList<UUID>();
+    return new ArrayList<>();
   }
 
   @Override
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/CellSet.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/CellSet.java
index 531bf66..9f08712 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/CellSet.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/CellSet.java
@@ -48,7 +48,7 @@ public class CellSet implements NavigableSet<Cell>  {
   private final NavigableMap<Cell, Cell> delegatee; ///
 
   CellSet(final CellComparator c) {
-    this.delegatee = new ConcurrentSkipListMap<Cell, Cell>(c);
+    this.delegatee = new ConcurrentSkipListMap<>(c);
   }
 
   CellSet(final NavigableMap<Cell, Cell> m) {
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/CompactSplitThread.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/CompactSplitThread.java
index 6870445..eba984a 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/CompactSplitThread.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/CompactSplitThread.java
@@ -285,14 +285,14 @@ public class CompactSplitThread implements CompactionRequestor, PropagatingConfi
     // not a special compaction request, so make our own list
     List<CompactionRequest> ret = null;
     if (requests == null) {
-      ret = selectNow ? new ArrayList<CompactionRequest>(r.getStores().size()) : null;
+      ret = selectNow ? new ArrayList<>(r.getStores().size()) : null;
       for (Store s : r.getStores()) {
         CompactionRequest cr = requestCompactionInternal(r, s, why, p, null, selectNow, user);
         if (selectNow) ret.add(cr);
       }
     } else {
       Preconditions.checkArgument(selectNow); // only system requests have selectNow == false
-      ret = new ArrayList<CompactionRequest>(requests.size());
+      ret = new ArrayList<>(requests.size());
       for (Pair<CompactionRequest, Store> pair : requests) {
         ret.add(requestCompaction(r, pair.getSecond(), why, p, pair.getFirst(), user));
       }
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/CompactingMemStore.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/CompactingMemStore.java
index 312e9fc..511bd80 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/CompactingMemStore.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/CompactingMemStore.java
@@ -322,7 +322,7 @@ public class CompactingMemStore extends AbstractMemStore {
     // The list of elements in pipeline + the active element + the snapshot segment
     // TODO : This will change when the snapshot is made of more than one element
     // The order is the Segment ordinal
-    List<KeyValueScanner> list = new ArrayList<KeyValueScanner>(order+1);
+    List<KeyValueScanner> list = new ArrayList<>(order+1);
     list.add(this.active.getScanner(readPt, order + 1));
     for (Segment item : pipelineList) {
       list.add(item.getScanner(readPt, order));
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/CompactionTool.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/CompactionTool.java
index b037c89..bea3e7f 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/CompactionTool.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/CompactionTool.java
@@ -248,7 +248,7 @@ public class CompactionTool extends Configured implements Tool {
      */
     @Override
     public List<InputSplit> getSplits(JobContext job) throws IOException {
-      List<InputSplit> splits = new ArrayList<InputSplit>();
+      List<InputSplit> splits = new ArrayList<>();
       List<FileStatus> files = listStatus(job);
 
       Text key = new Text();
@@ -301,7 +301,7 @@ public class CompactionTool extends Configured implements Tool {
     public static void createInputFile(final FileSystem fs, final Path path,
         final Set<Path> toCompactDirs) throws IOException {
       // Extract the list of store dirs
-      List<Path> storeDirs = new LinkedList<Path>();
+      List<Path> storeDirs = new LinkedList<>();
       for (Path compactDir: toCompactDirs) {
         if (isFamilyDir(fs, compactDir)) {
           storeDirs.add(compactDir);
@@ -389,7 +389,7 @@ public class CompactionTool extends Configured implements Tool {
 
   @Override
   public int run(String[] args) throws Exception {
-    Set<Path> toCompactDirs = new HashSet<Path>();
+    Set<Path> toCompactDirs = new HashSet<>();
     boolean compactOnce = false;
     boolean major = false;
     boolean mapred = false;
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/CompositeImmutableSegment.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/CompositeImmutableSegment.java
index 30d17fb..73556bd 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/CompositeImmutableSegment.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/CompositeImmutableSegment.java
@@ -65,7 +65,7 @@ public class CompositeImmutableSegment extends ImmutableSegment {
 
   @VisibleForTesting
   public List<Segment> getAllSegments() {
-    return new LinkedList<Segment>(segments);
+    return new LinkedList<>(segments);
   }
 
   public int getNumOfSegments() {
@@ -150,7 +150,7 @@ public class CompositeImmutableSegment extends ImmutableSegment {
    */
   public KeyValueScanner getScanner(long readPoint, long order) {
     KeyValueScanner resultScanner;
-    List<KeyValueScanner> list = new ArrayList<KeyValueScanner>(segments.size());
+    List<KeyValueScanner> list = new ArrayList<>(segments.size());
     for (ImmutableSegment s : segments) {
       list.add(s.getScanner(readPoint, order));
     }
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/DateTieredMultiFileWriter.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/DateTieredMultiFileWriter.java
index 2cea92f..e682597 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/DateTieredMultiFileWriter.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/DateTieredMultiFileWriter.java
@@ -34,8 +34,7 @@ import org.apache.hadoop.hbase.classification.InterfaceAudience;
 @InterfaceAudience.Private
 public class DateTieredMultiFileWriter extends AbstractMultiFileWriter {
 
-  private final NavigableMap<Long, StoreFileWriter> lowerBoundary2Writer
-    = new TreeMap<Long, StoreFileWriter>();
+  private final NavigableMap<Long, StoreFileWriter> lowerBoundary2Writer = new TreeMap<>();
 
   private final boolean needEmptyFile;
 
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/DefaultMemStore.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/DefaultMemStore.java
index a31c2c3..4757e1d 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/DefaultMemStore.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/DefaultMemStore.java
@@ -129,7 +129,7 @@ public class DefaultMemStore extends AbstractMemStore {
    * Scanners are ordered from 0 (oldest) to newest in increasing order.
    */
   public List<KeyValueScanner> getScanners(long readPt) throws IOException {
-    List<KeyValueScanner> list = new ArrayList<KeyValueScanner>(2);
+    List<KeyValueScanner> list = new ArrayList<>(2);
     list.add(this.active.getScanner(readPt, 1));
     list.add(this.snapshot.getScanner(readPt, 0));
     return Collections.<KeyValueScanner> singletonList(
@@ -138,7 +138,7 @@ public class DefaultMemStore extends AbstractMemStore {
 
   @Override
   protected List<Segment> getSegments() throws IOException {
-    List<Segment> list = new ArrayList<Segment>(2);
+    List<Segment> list = new ArrayList<>(2);
     list.add(this.active);
     list.add(this.snapshot);
     return list;
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/DefaultStoreFileManager.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/DefaultStoreFileManager.java
index db0ad01..c37ae99 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/DefaultStoreFileManager.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/DefaultStoreFileManager.java
@@ -91,7 +91,7 @@ class DefaultStoreFileManager implements StoreFileManager {
 
   @Override
   public void insertNewFiles(Collection<StoreFile> sfs) throws IOException {
-    ArrayList<StoreFile> newFiles = new ArrayList<StoreFile>(storefiles);
+    ArrayList<StoreFile> newFiles = new ArrayList<>(storefiles);
     newFiles.addAll(sfs);
     sortAndSetStoreFiles(newFiles);
   }
@@ -106,7 +106,7 @@ class DefaultStoreFileManager implements StoreFileManager {
   @Override
   public Collection<StoreFile> clearCompactedFiles() {
     List<StoreFile> result = compactedfiles;
-    compactedfiles = new ArrayList<StoreFile>();
+    compactedfiles = new ArrayList<>();
     return result;
   }
 
@@ -126,10 +126,10 @@ class DefaultStoreFileManager implements StoreFileManager {
     sortAndSetStoreFiles(newStoreFiles);
     ArrayList<StoreFile> updatedCompactedfiles = null;
     if (this.compactedfiles != null) {
-      updatedCompactedfiles = new ArrayList<StoreFile>(this.compactedfiles);
+      updatedCompactedfiles = new ArrayList<>(this.compactedfiles);
       updatedCompactedfiles.addAll(newCompactedfiles);
     } else {
-      updatedCompactedfiles = new ArrayList<StoreFile>(newCompactedfiles);
+      updatedCompactedfiles = new ArrayList<>(newCompactedfiles);
     }
     markCompactedAway(newCompactedfiles);
     this.compactedfiles = sortCompactedfiles(updatedCompactedfiles);
@@ -149,7 +149,7 @@ class DefaultStoreFileManager implements StoreFileManager {
   public void removeCompactedFiles(Collection<StoreFile> removedCompactedfiles) throws IOException {
     ArrayList<StoreFile> updatedCompactedfiles = null;
     if (this.compactedfiles != null) {
-      updatedCompactedfiles = new ArrayList<StoreFile>(this.compactedfiles);
+      updatedCompactedfiles = new ArrayList<>(this.compactedfiles);
       updatedCompactedfiles.removeAll(removedCompactedfiles);
       this.compactedfiles = sortCompactedfiles(updatedCompactedfiles);
     }
@@ -157,7 +157,7 @@ class DefaultStoreFileManager implements StoreFileManager {
 
   @Override
   public final Iterator<StoreFile> getCandidateFilesForRowKeyBefore(final KeyValue targetKey) {
-    return new ArrayList<StoreFile>(Lists.reverse(this.storefiles)).iterator();
+    return new ArrayList<>(Lists.reverse(this.storefiles)).iterator();
   }
 
   @Override
@@ -204,7 +204,7 @@ class DefaultStoreFileManager implements StoreFileManager {
         LOG.info("Found an expired store file: " + sf.getPath()
             + " whose maxTimeStamp is " + fileTs + ", which is below " + maxTs);
         if (expiredStoreFiles == null) {
-          expiredStoreFiles = new ArrayList<StoreFile>();
+          expiredStoreFiles = new ArrayList<>();
         }
         expiredStoreFiles.add(sf);
       }
@@ -220,7 +220,7 @@ class DefaultStoreFileManager implements StoreFileManager {
   private List<StoreFile> sortCompactedfiles(List<StoreFile> storefiles) {
     // Sorting may not be really needed here for the compacted files?
     Collections.sort(storefiles, storeFileComparator);
-    return new ArrayList<StoreFile>(storefiles);
+    return new ArrayList<>(storefiles);
   }
 
   @Override
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/DefaultStoreFlusher.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/DefaultStoreFlusher.java
index 93837b7..8cb3a1d 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/DefaultStoreFlusher.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/DefaultStoreFlusher.java
@@ -46,7 +46,7 @@ public class DefaultStoreFlusher extends StoreFlusher {
   @Override
   public List<Path> flushSnapshot(MemStoreSnapshot snapshot, long cacheFlushId,
       MonitoredTask status, ThroughputController throughputController) throws IOException {
-    ArrayList<Path> result = new ArrayList<Path>();
+    ArrayList<Path> result = new ArrayList<>();
     int cellsCount = snapshot.getCellsCount();
     if (cellsCount == 0) return result; // don't flush if there are no entries
 
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/FlushAllLargeStoresPolicy.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/FlushAllLargeStoresPolicy.java
index bb57869..6138f5f 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/FlushAllLargeStoresPolicy.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/FlushAllLargeStoresPolicy.java
@@ -55,7 +55,7 @@ public class FlushAllLargeStoresPolicy extends FlushLargeStoresPolicy{
     }
     // start selection
     Collection<Store> stores = region.stores.values();
-    Set<Store> specificStoresToFlush = new HashSet<Store>();
+    Set<Store> specificStoresToFlush = new HashSet<>();
     for (Store store : stores) {
       if (shouldFlush(store)) {
         specificStoresToFlush.add(store);
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/FlushNonSloppyStoresFirstPolicy.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/FlushNonSloppyStoresFirstPolicy.java
index 61f5882..4318dce 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/FlushNonSloppyStoresFirstPolicy.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/FlushNonSloppyStoresFirstPolicy.java
@@ -39,7 +39,7 @@ public class FlushNonSloppyStoresFirstPolicy extends FlushLargeStoresPolicy {
    * @return the stores need to be flushed.
    */
   @Override public Collection<Store> selectStoresToFlush() {
-    Collection<Store> specificStoresToFlush = new HashSet<Store>();
+    Collection<Store> specificStoresToFlush = new HashSet<>();
     for(Store store : regularStores) {
       if(shouldFlush(store) || region.shouldFlushStore(store)) {
         specificStoresToFlush.add(store);
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HMobStore.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HMobStore.java
index a990ceb..b021430 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HMobStore.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HMobStore.java
@@ -91,7 +91,7 @@ public class HMobStore extends HStore {
   private volatile long mobScanCellsCount = 0;
   private volatile long mobScanCellsSize = 0;
   private HColumnDescriptor family;
-  private Map<String, List<Path>> map = new ConcurrentHashMap<String, List<Path>>();
+  private Map<String, List<Path>> map = new ConcurrentHashMap<>();
   private final IdLock keyLock = new IdLock();
   // When we add a MOB reference cell to the HFile, we will add 2 tags along with it
   // 1. A ref tag with type TagType.MOB_REFERENCE_TAG_TYPE. This just denote this this cell is not
@@ -109,7 +109,7 @@ public class HMobStore extends HStore {
     this.homePath = MobUtils.getMobHome(conf);
     this.mobFamilyPath = MobUtils.getMobFamilyPath(conf, this.getTableName(),
         family.getNameAsString());
-    List<Path> locations = new ArrayList<Path>(2);
+    List<Path> locations = new ArrayList<>(2);
     locations.add(mobFamilyPath);
     TableName tn = region.getTableDesc().getTableName();
     locations.add(HFileArchiveUtil.getStoreArchivePath(conf, tn, MobUtils.getMobRegionInfo(tn)
@@ -341,7 +341,7 @@ public class HMobStore extends HStore {
           try {
             locations = map.get(tableNameString);
             if (locations == null) {
-              locations = new ArrayList<Path>(2);
+              locations = new ArrayList<>(2);
               TableName tn = TableName.valueOf(tableNameString);
               locations.add(MobUtils.getMobFamilyPath(conf, tn, family.getNameAsString()));
               locations.add(HFileArchiveUtil.getStoreArchivePath(conf, tn, MobUtils
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java
index cc32179..40dc31b 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java
@@ -250,11 +250,9 @@ public class HRegion implements HeapSize, PropagatingConfigurationObserver, Regi
   // - the thread that owns the lock (allow reentrancy)
   // - reference count of (reentrant) locks held by the thread
   // - the row itself
-  private final ConcurrentHashMap<HashedBytes, RowLockContext> lockedRows =
-      new ConcurrentHashMap<HashedBytes, RowLockContext>();
+  private final ConcurrentHashMap<HashedBytes, RowLockContext> lockedRows = new ConcurrentHashMap<>();
 
-  protected final Map<byte[], Store> stores = new ConcurrentSkipListMap<byte[], Store>(
-      Bytes.BYTES_RAWCOMPARATOR);
+  protected final Map<byte[], Store> stores = new ConcurrentSkipListMap<>(Bytes.BYTES_RAWCOMPARATOR);
 
   // TODO: account for each registered handler in HeapSize computation
   private Map<String, com.google.protobuf.Service> coprocessorServiceHandlers = Maps.newHashMap();
@@ -336,7 +334,7 @@ public class HRegion implements HeapSize, PropagatingConfigurationObserver, Regi
   // the maxSeqId up to which the store was flushed. And, skip the edits which
   // are equal to or lower than maxSeqId for each store.
   // The following map is populated when opening the region
-  Map<byte[], Long> maxSeqIdInStores = new TreeMap<byte[], Long>(Bytes.BYTES_COMPARATOR);
+  Map<byte[], Long> maxSeqIdInStores = new TreeMap<>(Bytes.BYTES_COMPARATOR);
 
   /** Saved state from replaying prepare flush cache */
   private PrepareFlushResult prepareFlushResult = null;
@@ -609,8 +607,7 @@ public class HRegion implements HeapSize, PropagatingConfigurationObserver, Regi
   final long rowProcessorTimeout;
 
   // Last flush time for each Store. Useful when we are flushing for each column
-  private final ConcurrentMap<Store, Long> lastStoreFlushTimeMap =
-      new ConcurrentHashMap<Store, Long>();
+  private final ConcurrentMap<Store, Long> lastStoreFlushTimeMap = new ConcurrentHashMap<>();
 
   final RegionServerServices rsServices;
   private RegionServerAccounting rsAccounting;
@@ -642,7 +639,7 @@ public class HRegion implements HeapSize, PropagatingConfigurationObserver, Regi
   private final boolean regionStatsEnabled;
   // Stores the replication scope of the various column families of the table
   // that has non-default scope
-  private final NavigableMap<byte[], Integer> replicationScope = new TreeMap<byte[], Integer>(
+  private final NavigableMap<byte[], Integer> replicationScope = new TreeMap<>(
       Bytes.BYTES_COMPARATOR);
 
   /**
@@ -736,7 +733,7 @@ public class HRegion implements HeapSize, PropagatingConfigurationObserver, Regi
     this.rsServices = rsServices;
     this.threadWakeFrequency = conf.getLong(HConstants.THREAD_WAKE_FREQUENCY, 10 * 1000);
     setHTableSpecificConf();
-    this.scannerReadPoints = new ConcurrentHashMap<RegionScanner, Long>();
+    this.scannerReadPoints = new ConcurrentHashMap<>();
 
     this.busyWaitDuration = conf.getLong(
       "hbase.busy.wait.duration", DEFAULT_BUSY_WAIT_DURATION);
@@ -976,8 +973,7 @@ public class HRegion implements HeapSize, PropagatingConfigurationObserver, Regi
       // initialize the thread pool for opening stores in parallel.
       ThreadPoolExecutor storeOpenerThreadPool =
         getStoreOpenAndCloseThreadPool("StoreOpener-" + this.getRegionInfo().getShortNameToLog());
-      CompletionService<HStore> completionService =
-        new ExecutorCompletionService<HStore>(storeOpenerThreadPool);
+      CompletionService<HStore> completionService = new ExecutorCompletionService<>(storeOpenerThreadPool);
 
       // initialize each store in parallel
       for (final HColumnDescriptor family : htableDescriptor.getFamilies()) {
@@ -1054,12 +1050,11 @@ public class HRegion implements HeapSize, PropagatingConfigurationObserver, Regi
    * @return Map of StoreFiles by column family
    */
   private NavigableMap<byte[], List<Path>> getStoreFiles() {
-    NavigableMap<byte[], List<Path>> allStoreFiles =
-      new TreeMap<byte[], List<Path>>(Bytes.BYTES_COMPARATOR);
+    NavigableMap<byte[], List<Path>> allStoreFiles = new TreeMap<>(Bytes.BYTES_COMPARATOR);
     for (Store store: getStores()) {
       Collection<StoreFile> storeFiles = store.getStorefiles();
       if (storeFiles == null) continue;
-      List<Path> storeFileNames = new ArrayList<Path>();
+      List<Path> storeFileNames = new ArrayList<>();
       for (StoreFile storeFile: storeFiles) {
         storeFileNames.add(storeFile.getPath());
       }
@@ -1626,15 +1621,14 @@ public class HRegion implements HeapSize, PropagatingConfigurationObserver, Regi
         }
       }
 
-      Map<byte[], List<StoreFile>> result =
-        new TreeMap<byte[], List<StoreFile>>(Bytes.BYTES_COMPARATOR);
+      Map<byte[], List<StoreFile>> result = new TreeMap<>(Bytes.BYTES_COMPARATOR);
       if (!stores.isEmpty()) {
         // initialize the thread pool for closing stores in parallel.
         ThreadPoolExecutor storeCloserThreadPool =
           getStoreOpenAndCloseThreadPool("StoreCloserThread-" +
             getRegionInfo().getRegionNameAsString());
         CompletionService<Pair<byte[], Collection<StoreFile>>> completionService =
-          new ExecutorCompletionService<Pair<byte[], Collection<StoreFile>>>(storeCloserThreadPool);
+          new ExecutorCompletionService<>(storeCloserThreadPool);
 
         // close each store in parallel
         for (final Store store : stores.values()) {
@@ -1652,8 +1646,7 @@ public class HRegion implements HeapSize, PropagatingConfigurationObserver, Regi
               .submit(new Callable<Pair<byte[], Collection<StoreFile>>>() {
                 @Override
                 public Pair<byte[], Collection<StoreFile>> call() throws IOException {
-                  return new Pair<byte[], Collection<StoreFile>>(
-                    store.getFamily().getName(), store.close());
+                  return new Pair<>(store.getFamily().getName(), store.close());
                 }
               });
         }
@@ -1663,7 +1656,7 @@ public class HRegion implements HeapSize, PropagatingConfigurationObserver, Regi
             Pair<byte[], Collection<StoreFile>> storeFiles = future.get();
             List<StoreFile> familyFiles = result.get(storeFiles.getFirst());
             if (familyFiles == null) {
-              familyFiles = new ArrayList<StoreFile>();
+              familyFiles = new ArrayList<>();
               result.put(storeFiles.getFirst(), familyFiles);
             }
             familyFiles.addAll(storeFiles.getSecond());
@@ -2418,12 +2411,9 @@ public class HRegion implements HeapSize, PropagatingConfigurationObserver, Regi
           ((HStore) store).preFlushSeqIDEstimation());
     }
 
-    TreeMap<byte[], StoreFlushContext> storeFlushCtxs
-      = new TreeMap<byte[], StoreFlushContext>(Bytes.BYTES_COMPARATOR);
-    TreeMap<byte[], List<Path>> committedFiles = new TreeMap<byte[], List<Path>>(
-        Bytes.BYTES_COMPARATOR);
-    TreeMap<byte[], MemstoreSize> storeFlushableSize
-        = new TreeMap<byte[], MemstoreSize>(Bytes.BYTES_COMPARATOR);
+    TreeMap<byte[], StoreFlushContext> storeFlushCtxs = new TreeMap<>(Bytes.BYTES_COMPARATOR);
+    TreeMap<byte[], List<Path>> committedFiles = new TreeMap<>(Bytes.BYTES_COMPARATOR);
+    TreeMap<byte[], MemstoreSize> storeFlushableSize = new TreeMap<>(Bytes.BYTES_COMPARATOR);
     // The sequence id of this flush operation which is used to log FlushMarker and pass to
     // createFlushContext to use as the store file's sequence id. It can be in advance of edits
     // still in the memstore, edits that are in other column families yet to be flushed.
@@ -2561,7 +2551,7 @@ public class HRegion implements HeapSize, PropagatingConfigurationObserver, Regi
   private boolean writeFlushRequestMarkerToWAL(WAL wal, boolean writeFlushWalMarker) {
     if (writeFlushWalMarker && wal != null && !writestate.readOnly) {
       FlushDescriptor desc = ProtobufUtil.toFlushDescriptor(FlushAction.CANNOT_FLUSH,
-        getRegionInfo(), -1, new TreeMap<byte[], List<Path>>(Bytes.BYTES_COMPARATOR));
+        getRegionInfo(), -1, new TreeMap<>(Bytes.BYTES_COMPARATOR));
       try {
         WALUtil.writeFlushMarker(wal, this.getReplicationScope(), getRegionInfo(), desc, true,
             mvcc);
@@ -2842,7 +2832,7 @@ public class HRegion implements HeapSize, PropagatingConfigurationObserver, Regi
       List<Cell> cells = e.getValue();
       assert cells instanceof RandomAccess;
 
-      Map<byte[], Integer> kvCount = new TreeMap<byte[], Integer>(Bytes.BYTES_COMPARATOR);
+      Map<byte[], Integer> kvCount = new TreeMap<>(Bytes.BYTES_COMPARATOR);
       int listSize = cells.size();
       for (int i=0; i < listSize; i++) {
         Cell cell = cells.get(i);
@@ -3247,7 +3237,7 @@ public class HRegion implements HeapSize, PropagatingConfigurationObserver, Regi
       // calling the pre CP hook for batch mutation
       if (!replay && coprocessorHost != null) {
         MiniBatchOperationInProgress<Mutation> miniBatchOp =
-          new MiniBatchOperationInProgress<Mutation>(batchOp.getMutationsForCoprocs(),
+          new MiniBatchOperationInProgress<>(batchOp.getMutationsForCoprocs(),
           batchOp.retCodeDetails, batchOp.walEditsFromCoprocessors, firstIndex, lastIndexExclusive);
         if (coprocessorHost.preBatchMutate(miniBatchOp)) {
           return;
@@ -3401,7 +3391,7 @@ public class HRegion implements HeapSize, PropagatingConfigurationObserver, Regi
       // calling the post CP hook for batch mutation
       if (!replay && coprocessorHost != null) {
         MiniBatchOperationInProgress<Mutation> miniBatchOp =
-          new MiniBatchOperationInProgress<Mutation>(batchOp.getMutationsForCoprocs(),
+          new MiniBatchOperationInProgress<>(batchOp.getMutationsForCoprocs(),
           batchOp.retCodeDetails, batchOp.walEditsFromCoprocessors, firstIndex, lastIndexExclusive);
         coprocessorHost.postBatchMutate(miniBatchOp);
       }
@@ -3485,7 +3475,7 @@ public class HRegion implements HeapSize, PropagatingConfigurationObserver, Regi
         // call the coprocessor hook to do any finalization steps
         // after the put is done
         MiniBatchOperationInProgress<Mutation> miniBatchOp =
-          new MiniBatchOperationInProgress<Mutation>(batchOp.getMutationsForCoprocs(),
+          new MiniBatchOperationInProgress<>(batchOp.getMutationsForCoprocs(),
           batchOp.retCodeDetails, batchOp.walEditsFromCoprocessors, firstIndex, lastIndexExclusive);
         coprocessorHost.postBatchMutateIndispensably(miniBatchOp, success);
       }
@@ -3599,7 +3589,7 @@ public class HRegion implements HeapSize, PropagatingConfigurationObserver, Regi
     for (byte[] family : familyMap.keySet()) {
       if (!this.htableDescriptor.hasFamily(family)) {
         if (nonExistentList == null) {
-          nonExistentList = new ArrayList<byte[]>();
+          nonExistentList = new ArrayList<>();
         }
         nonExistentList.add(family);
       }
@@ -3915,7 +3905,7 @@ public class HRegion implements HeapSize, PropagatingConfigurationObserver, Regi
   private void put(final byte [] row, byte [] family, List<Cell> edits)
   throws IOException {
     NavigableMap<byte[], List<Cell>> familyMap;
-    familyMap = new TreeMap<byte[], List<Cell>>(Bytes.BYTES_COMPARATOR);
+    familyMap = new TreeMap<>(Bytes.BYTES_COMPARATOR);
 
     familyMap.put(family, edits);
     Put p = new Put(row);
@@ -4164,7 +4154,7 @@ public class HRegion implements HeapSize, PropagatingConfigurationObserver, Regi
       // If this flag is set, make use of the hfile archiving by making recovered.edits a fake
       // column family. Have to fake out file type too by casting our recovered.edits as storefiles
       String fakeFamilyName = WALSplitter.getRegionDirRecoveredEditsDir(regiondir).getName();
-      Set<StoreFile> fakeStoreFiles = new HashSet<StoreFile>(files.size());
+      Set<StoreFile> fakeStoreFiles = new HashSet<>(files.size());
       for (Path file: files) {
         fakeStoreFiles.add(new StoreFile(getRegionFileSystem().getFileSystem(), file, this.conf,
           null, null));
@@ -4506,7 +4496,7 @@ public class HRegion implements HeapSize, PropagatingConfigurationObserver, Regi
   PrepareFlushResult replayWALFlushStartMarker(FlushDescriptor flush) throws IOException {
     long flushSeqId = flush.getFlushSequenceNumber();
 
-    HashSet<Store> storesToFlush = new HashSet<Store>();
+    HashSet<Store> storesToFlush = new HashSet<>();
     for (StoreFlushDescriptor storeFlush : flush.getStoreFlushesList()) {
       byte[] family = storeFlush.getFamilyName().toByteArray();
       Store store = getStore(family);
@@ -5103,7 +5093,7 @@ public class HRegion implements HeapSize, PropagatingConfigurationObserver, Regi
 
     startRegionOperation(); // obtain region close lock
     try {
-      Map<Store, Long> map = new HashMap<Store, Long>();
+      Map<Store, Long> map = new HashMap<>();
       synchronized (writestate) {
         for (Store store : getStores()) {
           // TODO: some stores might see new data from flush, while others do not which
@@ -5280,7 +5270,7 @@ public class HRegion implements HeapSize, PropagatingConfigurationObserver, Regi
 
   @Override
   public List<Store> getStores() {
-    List<Store> list = new ArrayList<Store>(stores.size());
+    List<Store> list = new ArrayList<>(stores.size());
     list.addAll(stores.values());
     return list;
   }
@@ -5288,7 +5278,7 @@ public class HRegion implements HeapSize, PropagatingConfigurationObserver, Regi
   @Override
   public List<String> getStoreFileList(final byte [][] columns)
     throws IllegalArgumentException {
-    List<String> storeFileNames = new ArrayList<String>();
+    List<String> storeFileNames = new ArrayList<>();
     synchronized(closeLock) {
       for(byte[] column : columns) {
         Store store = this.stores.get(column);
@@ -5560,8 +5550,8 @@ public class HRegion implements HeapSize, PropagatingConfigurationObserver, Regi
   public Map<byte[], List<Path>> bulkLoadHFiles(Collection<Pair<byte[], String>> familyPaths,
       boolean assignSeqId, BulkLoadListener bulkLoadListener, boolean copyFile) throws IOException {
     long seqId = -1;
-    Map<byte[], List<Path>> storeFiles = new TreeMap<byte[], List<Path>>(Bytes.BYTES_COMPARATOR);
-    Map<String, Long> storeFilesSizes = new HashMap<String, Long>();
+    Map<byte[], List<Path>> storeFiles = new TreeMap<>(Bytes.BYTES_COMPARATOR);
+    Map<String, Long> storeFilesSizes = new HashMap<>();
     Preconditions.checkNotNull(familyPaths);
     // we need writeLock for multi-family bulk load
     startBulkRegionOperation(hasMultipleColumnFamilies(familyPaths));
@@ -5572,8 +5562,8 @@ public class HRegion implements HeapSize, PropagatingConfigurationObserver, Regi
       // There possibly was a split that happened between when the split keys
       // were gathered and before the HRegion's write lock was taken.  We need
       // to validate the HFile region before attempting to bulk load all of them
-      List<IOException> ioes = new ArrayList<IOException>();
-      List<Pair<byte[], String>> failures = new ArrayList<Pair<byte[], String>>();
+      List<IOException> ioes = new ArrayList<>();
+      List<Pair<byte[], String>> failures = new ArrayList<>();
       for (Pair<byte[], String> p : familyPaths) {
         byte[] familyName = p.getFirst();
         String path = p.getSecond();
@@ -5694,7 +5684,7 @@ public class HRegion implements HeapSize, PropagatingConfigurationObserver, Regi
             if(storeFiles.containsKey(familyName)) {
               storeFiles.get(familyName).add(commitedStoreFile);
             } else {
-              List<Path> storeFileNames = new ArrayList<Path>();
+              List<Path> storeFileNames = new ArrayList<>();
               storeFileNames.add(commitedStoreFile);
               storeFiles.put(familyName, storeFileNames);
             }
@@ -5841,11 +5831,10 @@ public class HRegion implements HeapSize, PropagatingConfigurationObserver, Regi
         throws IOException {
       // Here we separate all scanners into two lists - scanner that provide data required
       // by the filter to operate (scanners list) and all others (joinedScanners list).
-      List<KeyValueScanner> scanners = new ArrayList<KeyValueScanner>(scan.getFamilyMap().size());
-      List<KeyValueScanner> joinedScanners =
-          new ArrayList<KeyValueScanner>(scan.getFamilyMap().size());
+      List<KeyValueScanner> scanners = new ArrayList<>(scan.getFamilyMap().size());
+      List<KeyValueScanner> joinedScanners = new ArrayList<>(scan.getFamilyMap().size());
       // Store all already instantiated scanners for exception handling
-      List<KeyValueScanner> instantiatedScanners = new ArrayList<KeyValueScanner>();
+      List<KeyValueScanner> instantiatedScanners = new ArrayList<>();
       // handle additionalScanners
       if (additionalScanners != null && !additionalScanners.isEmpty()) {
         scanners.addAll(additionalScanners);
@@ -5973,7 +5962,7 @@ public class HRegion implements HeapSize, PropagatingConfigurationObserver, Regi
         // to handle scan or get operation.
         moreValues = nextInternal(outResults, scannerContext);
       } else {
-        List<Cell> tmpList = new ArrayList<Cell>();
+        List<Cell> tmpList = new ArrayList<>();
         moreValues = nextInternal(tmpList, scannerContext);
         outResults.addAll(tmpList);
       }
@@ -6838,7 +6827,7 @@ public class HRegion implements HeapSize, PropagatingConfigurationObserver, Regi
     // The row key is the region name
     byte[] row = r.getRegionInfo().getRegionName();
     final long now = EnvironmentEdgeManager.currentTime();
-    final List<Cell> cells = new ArrayList<Cell>(2);
+    final List<Cell> cells = new ArrayList<>(2);
     cells.add(new KeyValue(row, HConstants.CATALOG_FAMILY,
       HConstants.REGIONINFO_QUALIFIER, now,
       r.getRegionInfo().toByteArray()));
@@ -6931,7 +6920,7 @@ public class HRegion implements HeapSize, PropagatingConfigurationObserver, Regi
   @Override
   public List<Cell> get(Get get, boolean withCoprocessor, long nonceGroup, long nonce)
       throws IOException {
-    List<Cell> results = new ArrayList<Cell>();
+    List<Cell> results = new ArrayList<>();
 
     // pre-get CP hook
     if (withCoprocessor && (coprocessorHost != null)) {
@@ -7069,7 +7058,7 @@ public class HRegion implements HeapSize, PropagatingConfigurationObserver, Regi
 
     boolean locked = false;
     List<RowLock> acquiredRowLocks = null;
-    List<Mutation> mutations = new ArrayList<Mutation>();
+    List<Mutation> mutations = new ArrayList<>();
     Collection<byte[]> rowsToLock = processor.getRowsToLock();
     // This is assigned by mvcc either explicity in the below or in the guts of the WAL append
     // when it assigns the edit a sequencedid (A.K.A the mvcc write number).
@@ -7191,8 +7180,7 @@ public class HRegion implements HeapSize, PropagatingConfigurationObserver, Regi
     }
 
     // Case with time bound
-    FutureTask<Void> task =
-      new FutureTask<Void>(new Callable<Void>() {
+    FutureTask<Void> task = new FutureTask<>(new Callable<Void>() {
         @Override
         public Void call() throws IOException {
           try {
@@ -7281,7 +7269,7 @@ public class HRegion implements HeapSize, PropagatingConfigurationObserver, Regi
     this.writeRequestsCount.increment();
     WriteEntry writeEntry = null;
     startRegionOperation(op);
-    List<Cell> results = returnResults? new ArrayList<Cell>(mutation.size()): null;
+    List<Cell> results = returnResults? new ArrayList<>(mutation.size()): null;
     RowLock rowLock = null;
     MemstoreSize memstoreSize = new MemstoreSize();
     try {
@@ -7293,8 +7281,7 @@ public class HRegion implements HeapSize, PropagatingConfigurationObserver, Regi
           return returnResults? cpResult: null;
         }
         Durability effectiveDurability = getEffectiveDurability(mutation.getDurability());
-        Map<Store, List<Cell>> forMemStore =
-            new HashMap<Store, List<Cell>>(mutation.getFamilyCellMap().size());
+        Map<Store, List<Cell>> forMemStore = new HashMap<>(mutation.getFamilyCellMap().size());
         // Reckon Cells to apply to WAL --  in returned walEdit -- and what to add to memstore and
         // what to return back to the client (in 'forMemStore' and 'results' respectively).
         WALEdit walEdit = reckonDeltas(op, mutation, effectiveDurability, forMemStore, results);
@@ -7469,7 +7456,7 @@ public class HRegion implements HeapSize, PropagatingConfigurationObserver, Regi
       final List<Cell> deltas, final List<Cell> results)
   throws IOException {
     byte [] columnFamily = store.getFamily().getName();
-    List<Cell> toApply = new ArrayList<Cell>(deltas.size());
+    List<Cell> toApply = new ArrayList<>(deltas.size());
     // Get previous values for all columns in this family.
     List<Cell> currentValues = get(mutation, store, deltas,
         null/*Default IsolationLevel*/,
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegionFileSystem.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegionFileSystem.java
index 97cc126..144f43b 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegionFileSystem.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegionFileSystem.java
@@ -241,7 +241,7 @@ public class HRegionFileSystem {
       return null;
     }
 
-    ArrayList<StoreFileInfo> storeFiles = new ArrayList<StoreFileInfo>(files.length);
+    ArrayList<StoreFileInfo> storeFiles = new ArrayList<>(files.length);
     for (FileStatus status: files) {
       if (validate && !StoreFileInfo.isValid(status)) {
         LOG.warn("Invalid StoreFile: " + status.getPath());
@@ -355,7 +355,7 @@ public class HRegionFileSystem {
     FileStatus[] fds = FSUtils.listStatus(fs, getRegionDir(), new FSUtils.FamilyDirFilter(fs));
     if (fds == null) return null;
 
-    ArrayList<String> families = new ArrayList<String>(fds.length);
+    ArrayList<String> families = new ArrayList<>(fds.length);
     for (FileStatus status: fds) {
       families.add(status.getPath().getName());
     }
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java
index 66d2d4d..cbf6561 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java
@@ -237,7 +237,7 @@ public class HRegionServer extends HasThread implements
   //true - if open region action in progress
   //false - if close region action in progress
   protected final ConcurrentMap<byte[], Boolean> regionsInTransitionInRS =
-    new ConcurrentSkipListMap<byte[], Boolean>(Bytes.BYTES_COMPARATOR);
+    new ConcurrentSkipListMap<>(Bytes.BYTES_COMPARATOR);
 
   // Cache flushing
   protected MemStoreFlusher cacheFlusher;
@@ -280,7 +280,7 @@ public class HRegionServer extends HasThread implements
    * Map of regions currently being served by this region server. Key is the
    * encoded region name.  All access should be synchronized.
    */
-  protected final Map<String, Region> onlineRegions = new ConcurrentHashMap<String, Region>();
+  protected final Map<String, Region> onlineRegions = new ConcurrentHashMap<>();
 
   /**
    * Map of encoded region names to the DataNode locations they should be hosted on
@@ -292,7 +292,7 @@ public class HRegionServer extends HasThread implements
    * and here we really mean DataNode locations.
    */
   protected final Map<String, InetSocketAddress[]> regionFavoredNodesMap =
-      new ConcurrentHashMap<String, InetSocketAddress[]>();
+      new ConcurrentHashMap<>();
 
   /**
    * Set of regions currently being in recovering state which means it can accept writes(edits from
@@ -321,7 +321,7 @@ public class HRegionServer extends HasThread implements
   // debugging and unit tests.
   private volatile boolean abortRequested;
 
-  ConcurrentMap<String, Integer> rowlocks = new ConcurrentHashMap<String, Integer>();
+  ConcurrentMap<String, Integer> rowlocks = new ConcurrentHashMap<>();
 
   // A state before we go into stopped state.  At this stage we're closing user
   // space regions.
@@ -1323,7 +1323,7 @@ public class HRegionServer extends HasThread implements
     // Wait till all regions are closed before going out.
     int lastCount = -1;
     long previousLogTime = 0;
-    Set<String> closedRegions = new HashSet<String>();
+    Set<String> closedRegions = new HashSet<>();
     boolean interrupted = false;
     try {
       while (!isOnlineRegionsEmpty()) {
@@ -1746,7 +1746,7 @@ public class HRegionServer extends HasThread implements
     createNewReplicationInstance(conf, this, this.walFs, logDir, oldLogDir);
 
     // listeners the wal factory will add to wals it creates.
-    final List<WALActionsListener> listeners = new ArrayList<WALActionsListener>();
+    final List<WALActionsListener> listeners = new ArrayList<>();
     listeners.add(new MetricsWAL());
     if (this.replicationSourceHandler != null &&
         this.replicationSourceHandler.getWALActionsListener() != null) {
@@ -2657,7 +2657,7 @@ public class HRegionServer extends HasThread implements
    */
   SortedMap<Long, Region> getCopyOfOnlineRegionsSortedBySize() {
     // we'll sort the regions in reverse
-    SortedMap<Long, Region> sortedRegions = new TreeMap<Long, Region>(
+    SortedMap<Long, Region> sortedRegions = new TreeMap<>(
         new Comparator<Long>() {
           @Override
           public int compare(Long a, Long b) {
@@ -2691,7 +2691,7 @@ public class HRegionServer extends HasThread implements
    * the first N regions being served regardless of load.)
    */
   protected HRegionInfo[] getMostLoadedRegions() {
-    ArrayList<HRegionInfo> regions = new ArrayList<HRegionInfo>();
+    ArrayList<HRegionInfo> regions = new ArrayList<>();
     for (Region r : onlineRegions.values()) {
       if (!r.isAvailable()) {
         continue;
@@ -2903,7 +2903,7 @@ public class HRegionServer extends HasThread implements
    */
   @Override
   public List<Region> getOnlineRegions(TableName tableName) {
-     List<Region> tableRegions = new ArrayList<Region>();
+     List<Region> tableRegions = new ArrayList<>();
      synchronized (this.onlineRegions) {
        for (Region region: this.onlineRegions.values()) {
          HRegionInfo regionInfo = region.getRegionInfo();
@@ -2917,7 +2917,7 @@ public class HRegionServer extends HasThread implements
 
   @Override
   public List<Region> getOnlineRegions() {
-    List<Region> allRegions = new ArrayList<Region>();
+    List<Region> allRegions = new ArrayList<>();
     synchronized (this.onlineRegions) {
       // Return a clone copy of the onlineRegions
       allRegions.addAll(onlineRegions.values());
@@ -2931,7 +2931,7 @@ public class HRegionServer extends HasThread implements
    */
   @Override
   public Set<TableName> getOnlineTables() {
-    Set<TableName> tables = new HashSet<TableName>();
+    Set<TableName> tables = new HashSet<>();
     synchronized (this.onlineRegions) {
       for (Region region: this.onlineRegions.values()) {
         tables.add(region.getTableDesc().getTableName());
@@ -2942,7 +2942,7 @@ public class HRegionServer extends HasThread implements
 
   // used by org/apache/hbase/tmpl/regionserver/RSStatusTmpl.jamon (HBASE-4070).
   public String[] getRegionServerCoprocessors() {
-    TreeSet<String> coprocessors = new TreeSet<String>();
+    TreeSet<String> coprocessors = new TreeSet<>();
     try {
       coprocessors.addAll(getWAL(null).getCoprocessorHost().getCoprocessors());
     } catch (IOException exception) {
@@ -3306,7 +3306,7 @@ public class HRegionServer extends HasThread implements
   // This map will contains all the regions that we closed for a move.
   //  We add the time it was moved as we don't want to keep too old information
   protected Map<String, MovedRegionInfo> movedRegions =
-      new ConcurrentHashMap<String, MovedRegionInfo>(3000);
+      new ConcurrentHashMap<>(3000);
 
   // We need a timeout. If not there is a risk of giving a wrong information: this would double
   //  the number of network calls instead of reducing them.
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HStore.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HStore.java
index b74e635..8a66c3a 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HStore.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HStore.java
@@ -509,14 +509,13 @@ public class HStore implements Store {
 
   private List<StoreFile> openStoreFiles(Collection<StoreFileInfo> files) throws IOException {
     if (files == null || files.isEmpty()) {
-      return new ArrayList<StoreFile>();
+      return new ArrayList<>();
     }
     // initialize the thread pool for opening store files in parallel..
     ThreadPoolExecutor storeFileOpenerThreadPool =
       this.region.getStoreFileOpenAndCloseThreadPool("StoreFileOpenerThread-" +
           this.getColumnFamilyName());
-    CompletionService<StoreFile> completionService =
-      new ExecutorCompletionService<StoreFile>(storeFileOpenerThreadPool);
+    CompletionService<StoreFile> completionService = new ExecutorCompletionService<>(storeFileOpenerThreadPool);
 
     int totalValidStoreFile = 0;
     for (final StoreFileInfo storeFileInfo: files) {
@@ -531,7 +530,7 @@ public class HStore implements Store {
       totalValidStoreFile++;
     }
 
-    ArrayList<StoreFile> results = new ArrayList<StoreFile>(files.size());
+    ArrayList<StoreFile> results = new ArrayList<>(files.size());
     IOException ioe = null;
     try {
       for (int i = 0; i < totalValidStoreFile; i++) {
@@ -588,7 +587,7 @@ public class HStore implements Store {
 
   @Override
   public void refreshStoreFiles(Collection<String> newFiles) throws IOException {
-    List<StoreFileInfo> storeFiles = new ArrayList<StoreFileInfo>(newFiles.size());
+    List<StoreFileInfo> storeFiles = new ArrayList<>(newFiles.size());
     for (String file : newFiles) {
       storeFiles.add(fs.getStoreFileInfo(getColumnFamilyName(), file));
     }
@@ -605,16 +604,15 @@ public class HStore implements Store {
   private void refreshStoreFilesInternal(Collection<StoreFileInfo> newFiles) throws IOException {
     StoreFileManager sfm = storeEngine.getStoreFileManager();
     Collection<StoreFile> currentFiles = sfm.getStorefiles();
-    if (currentFiles == null) currentFiles = new ArrayList<StoreFile>(0);
+    if (currentFiles == null) currentFiles = new ArrayList<>(0);
 
-    if (newFiles == null) newFiles = new ArrayList<StoreFileInfo>(0);
+    if (newFiles == null) newFiles = new ArrayList<>(0);
 
-    HashMap<StoreFileInfo, StoreFile> currentFilesSet =
-        new HashMap<StoreFileInfo, StoreFile>(currentFiles.size());
+    HashMap<StoreFileInfo, StoreFile> currentFilesSet = new HashMap<>(currentFiles.size());
     for (StoreFile sf : currentFiles) {
       currentFilesSet.put(sf.getFileInfo(), sf);
     }
-    HashSet<StoreFileInfo> newFilesSet = new HashSet<StoreFileInfo>(newFiles);
+    HashSet<StoreFileInfo> newFilesSet = new HashSet<>(newFiles);
 
     Set<StoreFileInfo> toBeAddedFiles = Sets.difference(newFilesSet, currentFilesSet.keySet());
     Set<StoreFileInfo> toBeRemovedFiles = Sets.difference(currentFilesSet.keySet(), newFilesSet);
@@ -626,7 +624,7 @@ public class HStore implements Store {
     LOG.info("Refreshing store files for region " + this.getRegionInfo().getRegionNameAsString()
       + " files to add: " + toBeAddedFiles + " files to remove: " + toBeRemovedFiles);
 
-    Set<StoreFile> toBeRemovedStoreFiles = new HashSet<StoreFile>(toBeRemovedFiles.size());
+    Set<StoreFile> toBeRemovedStoreFiles = new HashSet<>(toBeRemovedFiles.size());
     for (StoreFileInfo sfi : toBeRemovedFiles) {
       toBeRemovedStoreFiles.add(currentFilesSet.get(sfi));
     }
@@ -879,7 +877,7 @@ public class HStore implements Store {
 
         // close each store file in parallel
         CompletionService<Void> completionService =
-          new ExecutorCompletionService<Void>(storeFileCloserThreadPool);
+          new ExecutorCompletionService<>(storeFileCloserThreadPool);
         for (final StoreFile f : result) {
           completionService.submit(new Callable<Void>() {
             @Override
@@ -1183,7 +1181,7 @@ public class HStore implements Store {
     // actually more correct, since memstore get put at the end.
     List<StoreFileScanner> sfScanners = StoreFileScanner.getScannersForStoreFiles(storeFilesToScan,
       cacheBlocks, usePread, isCompaction, false, matcher, readPt, isPrimaryReplicaStore());
-    List<KeyValueScanner> scanners = new ArrayList<KeyValueScanner>(sfScanners.size() + 1);
+    List<KeyValueScanner> scanners = new ArrayList<>(sfScanners.size() + 1);
     scanners.addAll(sfScanners);
     // Then the memstore scanners
     scanners.addAll(memStoreScanners);
@@ -1206,7 +1204,7 @@ public class HStore implements Store {
     }
     List<StoreFileScanner> sfScanners = StoreFileScanner.getScannersForStoreFiles(files,
       cacheBlocks, usePread, isCompaction, false, matcher, readPt, isPrimaryReplicaStore());
-    List<KeyValueScanner> scanners = new ArrayList<KeyValueScanner>(sfScanners.size() + 1);
+    List<KeyValueScanner> scanners = new ArrayList<>(sfScanners.size() + 1);
     scanners.addAll(sfScanners);
     // Then the memstore scanners
     if (memStoreScanners != null) {
@@ -1312,7 +1310,7 @@ public class HStore implements Store {
       // TODO: get rid of this!
       if (!this.conf.getBoolean("hbase.hstore.compaction.complete", true)) {
         LOG.warn("hbase.hstore.compaction.complete is set to false");
-        sfs = new ArrayList<StoreFile>(newFiles.size());
+        sfs = new ArrayList<>(newFiles.size());
         final boolean evictOnClose =
             cacheConf != null? cacheConf.shouldEvictOnClose(): true;
         for (Path newFile : newFiles) {
@@ -1359,7 +1357,7 @@ public class HStore implements Store {
 
   private List<StoreFile> moveCompatedFilesIntoPlace(
       final CompactionRequest cr, List<Path> newFiles, User user) throws IOException {
-    List<StoreFile> sfs = new ArrayList<StoreFile>(newFiles.size());
+    List<StoreFile> sfs = new ArrayList<>(newFiles.size());
     for (Path newFile : newFiles) {
       assert newFile != null;
       final StoreFile sf = moveFileIntoPlace(newFile);
@@ -1389,11 +1387,11 @@ public class HStore implements Store {
   private void writeCompactionWalRecord(Collection<StoreFile> filesCompacted,
       Collection<StoreFile> newFiles) throws IOException {
     if (region.getWAL() == null) return;
-    List<Path> inputPaths = new ArrayList<Path>(filesCompacted.size());
+    List<Path> inputPaths = new ArrayList<>(filesCompacted.size());
     for (StoreFile f : filesCompacted) {
       inputPaths.add(f.getPath());
     }
-    List<Path> outputPaths = new ArrayList<Path>(newFiles.size());
+    List<Path> outputPaths = new ArrayList<>(newFiles.size());
     for (StoreFile f : newFiles) {
       outputPaths.add(f.getPath());
     }
@@ -1489,14 +1487,14 @@ public class HStore implements Store {
     // being in the store's folder) or they may be missing due to a compaction.
 
     String familyName = this.getColumnFamilyName();
-    List<String> inputFiles = new ArrayList<String>(compactionInputs.size());
+    List<String> inputFiles = new ArrayList<>(compactionInputs.size());
     for (String compactionInput : compactionInputs) {
       Path inputPath = fs.getStoreFilePath(familyName, compactionInput);
       inputFiles.add(inputPath.getName());
     }
 
     //some of the input files might already be deleted
-    List<StoreFile> inputStoreFiles = new ArrayList<StoreFile>(compactionInputs.size());
+    List<StoreFile> inputStoreFiles = new ArrayList<>(compactionInputs.size());
     for (StoreFile sf : this.getStorefiles()) {
       if (inputFiles.contains(sf.getPath().getName())) {
         inputStoreFiles.add(sf);
@@ -1504,7 +1502,7 @@ public class HStore implements Store {
     }
 
     // check whether we need to pick up the new files
-    List<StoreFile> outputStoreFiles = new ArrayList<StoreFile>(compactionOutputs.size());
+    List<StoreFile> outputStoreFiles = new ArrayList<>(compactionOutputs.size());
 
     if (pickCompactionFiles) {
       for (StoreFile sf : this.getStorefiles()) {
@@ -1738,7 +1736,7 @@ public class HStore implements Store {
     }
     if (delSfs == null || delSfs.isEmpty()) return;
 
-    Collection<StoreFile> newFiles = new ArrayList<StoreFile>(); // No new files.
+    Collection<StoreFile> newFiles = new ArrayList<>(); // No new files.
     writeCompactionWalRecord(delSfs, newFiles);
     replaceStoreFiles(delSfs, newFiles);
     completeCompaction(delSfs);
@@ -2167,7 +2165,7 @@ public class HStore implements Store {
       this.snapshot = memstore.snapshot();
       this.cacheFlushCount = snapshot.getCellsCount();
       this.cacheFlushSize = snapshot.getDataSize();
-      committedFiles = new ArrayList<Path>(1);
+      committedFiles = new ArrayList<>(1);
     }
 
     @Override
@@ -2183,7 +2181,7 @@ public class HStore implements Store {
       if (this.tempFiles == null || this.tempFiles.isEmpty()) {
         return false;
       }
-      List<StoreFile> storeFiles = new ArrayList<StoreFile>(this.tempFiles.size());
+      List<StoreFile> storeFiles = new ArrayList<>(this.tempFiles.size());
       for (Path storeFilePath : tempFiles) {
         try {
           StoreFile sf = HStore.this.commitFile(storeFilePath, cacheFlushSeqNum, status);
@@ -2241,7 +2239,7 @@ public class HStore implements Store {
     @Override
     public void replayFlush(List<String> fileNames, boolean dropMemstoreSnapshot)
         throws IOException {
-      List<StoreFile> storeFiles = new ArrayList<StoreFile>(fileNames.size());
+      List<StoreFile> storeFiles = new ArrayList<>(fileNames.size());
       for (String file : fileNames) {
         // open the file as a store file (hfile link, etc)
         StoreFileInfo storeFileInfo = fs.getStoreFileInfo(getColumnFamilyName(), file);
@@ -2273,7 +2271,7 @@ public class HStore implements Store {
       if (snapshot == null) {
         return;
       }
-      HStore.this.updateStorefiles(new ArrayList<StoreFile>(0), snapshot.getId());
+      HStore.this.updateStorefiles(new ArrayList<>(0), snapshot.getId());
     }
   }
 
@@ -2424,7 +2422,7 @@ public class HStore implements Store {
             this.getStoreEngine().getStoreFileManager().getCompactedfiles();
         if (compactedfiles != null && compactedfiles.size() != 0) {
           // Do a copy under read lock
-          copyCompactedfiles = new ArrayList<StoreFile>(compactedfiles);
+          copyCompactedfiles = new ArrayList<>(compactedfiles);
         } else {
           if (LOG.isTraceEnabled()) {
             LOG.trace("No compacted files to archive");
@@ -2449,7 +2447,7 @@ public class HStore implements Store {
    */
   private void removeCompactedfiles(Collection<StoreFile> compactedfiles)
       throws IOException {
-    final List<StoreFile> filesToRemove = new ArrayList<StoreFile>(compactedfiles.size());
+    final List<StoreFile> filesToRemove = new ArrayList<>(compactedfiles.size());
     for (final StoreFile file : compactedfiles) {
       synchronized (file) {
         try {
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HeapMemoryManager.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HeapMemoryManager.java
index c7099a5..e834306 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HeapMemoryManager.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HeapMemoryManager.java
@@ -106,7 +106,7 @@ public class HeapMemoryManager {
 
   private MetricsHeapMemoryManager metricsHeapMemoryManager;
 
-  private List<HeapMemoryTuneObserver> tuneObservers = new ArrayList<HeapMemoryTuneObserver>();
+  private List<HeapMemoryTuneObserver> tuneObservers = new ArrayList<>();
 
   public static HeapMemoryManager create(Configuration conf, FlushRequester memStoreFlusher,
       Server server, RegionServerAccounting regionServerAccounting) {
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/ImmutableSegment.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/ImmutableSegment.java
index faa9b67..501c1e9 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/ImmutableSegment.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/ImmutableSegment.java
@@ -157,7 +157,7 @@ public class ImmutableSegment extends Segment {
   }
 
   public List<Segment> getAllSegments() {
-    List<Segment> res = new ArrayList<Segment>(Arrays.asList(this));
+    List<Segment> res = new ArrayList<>(Arrays.asList(this));
     return res;
   }
 
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/KeyValueHeap.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/KeyValueHeap.java
index ff76d20..195e8f7 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/KeyValueHeap.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/KeyValueHeap.java
@@ -88,11 +88,9 @@ public class KeyValueHeap extends NonReversedNonLazyKeyValueScanner
   KeyValueHeap(List<? extends KeyValueScanner> scanners,
       KVScannerComparator comparator) throws IOException {
     this.comparator = comparator;
-    this.scannersForDelayedClose = new ArrayList<KeyValueScanner>(
-        scanners.size());
+    this.scannersForDelayedClose = new ArrayList<>(scanners.size());
     if (!scanners.isEmpty()) {
-      this.heap = new PriorityQueue<KeyValueScanner>(scanners.size(),
-          this.comparator);
+      this.heap = new PriorityQueue<>(scanners.size(), this.comparator);
       for (KeyValueScanner scanner : scanners) {
         if (scanner.peek() != null) {
           this.heap.add(scanner);
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/Leases.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/Leases.java
index 4af703c..b12b7b5 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/Leases.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/Leases.java
@@ -56,7 +56,7 @@ import java.io.IOException;
 public class Leases extends HasThread {
   private static final Log LOG = LogFactory.getLog(Leases.class.getName());
   public static final int MIN_WAIT_TIME = 100;
-  private final Map<String, Lease> leases = new ConcurrentHashMap<String, Lease>();
+  private final Map<String, Lease> leases = new ConcurrentHashMap<>();
 
   protected final int leaseCheckFrequency;
   protected volatile boolean stopRequested = false;
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/LogRoller.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/LogRoller.java
index 24f0d1a..9d1bc4b 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/LogRoller.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/LogRoller.java
@@ -54,8 +54,7 @@ public class LogRoller extends HasThread implements Closeable {
   private static final Log LOG = LogFactory.getLog(LogRoller.class);
   private final ReentrantLock rollLock = new ReentrantLock();
   private final AtomicBoolean rollLog = new AtomicBoolean(false);
-  private final ConcurrentHashMap<WAL, Boolean> walNeedsRoll =
-      new ConcurrentHashMap<WAL, Boolean>();
+  private final ConcurrentHashMap<WAL, Boolean> walNeedsRoll = new ConcurrentHashMap<>();
   private final Server server;
   protected final RegionServerServices services;
   private volatile long lastrolltime = System.currentTimeMillis();
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/LruHashMap.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/LruHashMap.java
index 8975ac7..a339abf 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/LruHashMap.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/LruHashMap.java
@@ -627,7 +627,7 @@ implements HeapSize, Map<K,V> {
   */
   private long addEntry(int hash, K key, V value, int bucketIndex) {
     Entry<K,V> e = entries[bucketIndex];
-    Entry<K,V> newE = new Entry<K,V>(hash, key, value, e, tailPtr);
+    Entry<K,V> newE = new Entry<>(hash, key, value, e, tailPtr);
     entries[bucketIndex] = newE;
     // add as most recently used in lru
     if (size == 0) {
@@ -810,7 +810,7 @@ implements HeapSize, Map<K,V> {
    * @return Sorted list of entries
    */
   public List<Entry<K,V>> entryLruList() {
-    List<Entry<K,V>> entryList = new ArrayList<Entry<K,V>>();
+    List<Entry<K,V>> entryList = new ArrayList<>();
     Entry<K,V> entry = headPtr;
     while(entry != null) {
       entryList.add(entry);
@@ -827,7 +827,7 @@ implements HeapSize, Map<K,V> {
   @edu.umd.cs.findbugs.annotations.SuppressWarnings(value="IS2_INCONSISTENT_SYNC",
       justification="Unused debugging function that reads only")
   public Set<Entry<K,V>> entryTableSet() {
-    Set<Entry<K,V>> entrySet = new HashSet<Entry<K,V>>();
+    Set<Entry<K,V>> entrySet = new HashSet<>();
     Entry [] table = entries; // FindBugs IS2_INCONSISTENT_SYNC
     for(int i=0;i<table.length;i++) {
       for(Entry e = table[i]; e != null; e = e.next) {
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/MemStoreCompactorSegmentsIterator.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/MemStoreCompactorSegmentsIterator.java
index f31c973..6a30eac 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/MemStoreCompactorSegmentsIterator.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/MemStoreCompactorSegmentsIterator.java
@@ -38,7 +38,7 @@ import java.util.List;
 @InterfaceAudience.Private
 public class MemStoreCompactorSegmentsIterator extends MemStoreSegmentsIterator {
 
-  private List<Cell> kvs = new ArrayList<Cell>();
+  private List<Cell> kvs = new ArrayList<>();
   private boolean hasMore;
   private Iterator<Cell> kvsIterator;
 
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/MemStoreFlusher.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/MemStoreFlusher.java
index fd77cf9..174d3ca 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/MemStoreFlusher.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/MemStoreFlusher.java
@@ -77,10 +77,8 @@ class MemStoreFlusher implements FlushRequester {
   private Configuration conf;
   // These two data members go together.  Any entry in the one must have
   // a corresponding entry in the other.
-  private final BlockingQueue<FlushQueueEntry> flushQueue =
-    new DelayQueue<FlushQueueEntry>();
-  private final Map<Region, FlushRegionEntry> regionsInQueue =
-    new HashMap<Region, FlushRegionEntry>();
+  private final BlockingQueue<FlushQueueEntry> flushQueue = new DelayQueue<>();
+  private final Map<Region, FlushRegionEntry> regionsInQueue = new HashMap<>();
   private AtomicBoolean wakeupPending = new AtomicBoolean();
 
   private final long threadWakeFrequency;
@@ -92,7 +90,7 @@ class MemStoreFlusher implements FlushRequester {
   private final LongAdder updatesBlockedMsHighWater = new LongAdder();
 
   private final FlushHandler[] flushHandlers;
-  private List<FlushRequestListener> flushRequestListeners = new ArrayList<FlushRequestListener>(1);
+  private List<FlushRequestListener> flushRequestListeners = new ArrayList<>(1);
 
   /**
    * @param conf
@@ -131,7 +129,7 @@ class MemStoreFlusher implements FlushRequester {
    */
   private boolean flushOneForGlobalPressure() {
     SortedMap<Long, Region> regionsBySize = server.getCopyOfOnlineRegionsSortedBySize();
-    Set<Region> excludedRegions = new HashSet<Region>();
+    Set<Region> excludedRegions = new HashSet<>();
 
     double secondaryMultiplier
       = ServerRegionReplicaUtil.getRegionReplicaStoreFileRefreshMultiplier(conf);
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/MemStoreLABImpl.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/MemStoreLABImpl.java
index 30e4311..4e87135 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/MemStoreLABImpl.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/MemStoreLABImpl.java
@@ -65,7 +65,7 @@ public class MemStoreLABImpl implements MemStoreLAB {
 
   static final Log LOG = LogFactory.getLog(MemStoreLABImpl.class);
 
-  private AtomicReference<Chunk> curChunk = new AtomicReference<Chunk>();
+  private AtomicReference<Chunk> curChunk = new AtomicReference<>();
   // A queue of chunks from pool contained by this memstore LAB
   // TODO: in the future, it would be better to have List implementation instead of Queue,
   // as FIFO order is not so important here
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/MemStoreSegmentsIterator.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/MemStoreSegmentsIterator.java
index e2f4ebb..7728534 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/MemStoreSegmentsIterator.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/MemStoreSegmentsIterator.java
@@ -49,7 +49,7 @@ public abstract class MemStoreSegmentsIterator implements Iterator<Cell> {
     this.scannerContext = ScannerContext.newBuilder().setBatchLimit(compactionKVMax).build();
 
     // list of Scanners of segments in the pipeline, when compaction starts
-    List<KeyValueScanner> scanners = new ArrayList<KeyValueScanner>();
+    List<KeyValueScanner> scanners = new ArrayList<>();
 
     // create the list of scanners to traverse over all the data
     // no dirty reads here as these are immutable segments
@@ -61,4 +61,4 @@ public abstract class MemStoreSegmentsIterator implements Iterator<Cell> {
   }
 
   public abstract void close();
-}
\ No newline at end of file
+}
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/MultiRowMutationProcessor.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/MultiRowMutationProcessor.java
index 995ea93..a0cd79d 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/MultiRowMutationProcessor.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/MultiRowMutationProcessor.java
@@ -131,7 +131,7 @@ MultiRowMutationProcessorResponse> {
     Arrays.fill(opStatus, OperationStatus.NOT_RUN);
     WALEdit[] walEditsFromCP = new WALEdit[mutations.size()];
     if (coprocessorHost != null) {
-      miniBatch = new MiniBatchOperationInProgress<Mutation>(
+      miniBatch = new MiniBatchOperationInProgress<>(
           mutations.toArray(new Mutation[mutations.size()]), opStatus, walEditsFromCP, 0,
           mutations.size());
       coprocessorHost.preBatchMutate(miniBatch);
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/MultiVersionConcurrencyControl.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/MultiVersionConcurrencyControl.java
index ee4fbb9..ffcc834 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/MultiVersionConcurrencyControl.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/MultiVersionConcurrencyControl.java
@@ -54,7 +54,7 @@ public class MultiVersionConcurrencyControl {
   // reduce the number of allocations on the write path?
   // This could be equal to the number of handlers + a small number.
   // TODO: St.Ack 20150903 Sounds good to me.
-  private final LinkedList<WriteEntry> writeQueue = new LinkedList<WriteEntry>();
+  private final LinkedList<WriteEntry> writeQueue = new LinkedList<>();
 
   public MultiVersionConcurrencyControl() {
     super();
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/RSRpcServices.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/RSRpcServices.java
index e6c2a49..dc0d7f1 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/RSRpcServices.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/RSRpcServices.java
@@ -323,7 +323,7 @@ public class RSRpcServices implements HBaseRPCErrorHandler,
    * completion of multiGets.
    */
    static class RegionScannersCloseCallBack implements RpcCallback {
-    private final List<RegionScanner> scanners = new ArrayList<RegionScanner>();
+    private final List<RegionScanner> scanners = new ArrayList<>();
 
     public void addScanner(RegionScanner scanner) {
       this.scanners.add(scanner);
@@ -818,7 +818,7 @@ public class RSRpcServices implements HBaseRPCErrorHandler,
             case DELETE:
               // Collect the individual mutations and apply in a batch
               if (mutations == null) {
-                mutations = new ArrayList<ClientProtos.Action>(actions.getActionCount());
+                mutations = new ArrayList<>(actions.getActionCount());
               }
               mutations.add(action);
               break;
@@ -834,7 +834,7 @@ public class RSRpcServices implements HBaseRPCErrorHandler,
             pbResult = ProtobufUtil.toResultNoData(r);
             //  Hard to guess the size here.  Just make a rough guess.
             if (cellsToReturn == null) {
-              cellsToReturn = new ArrayList<CellScannable>();
+              cellsToReturn = new ArrayList<>();
             }
             cellsToReturn.add(r);
           } else {
@@ -1301,7 +1301,7 @@ public class RSRpcServices implements HBaseRPCErrorHandler,
    * @return list of blocking services and their security info classes that this server supports
    */
   protected List<BlockingServiceAndInterface> getServices() {
-    List<BlockingServiceAndInterface> bssi = new ArrayList<BlockingServiceAndInterface>(2);
+    List<BlockingServiceAndInterface> bssi = new ArrayList<>(2);
     bssi.add(new BlockingServiceAndInterface(
       ClientService.newReflectiveBlockingService(this),
       ClientService.BlockingInterface.class));
@@ -1543,7 +1543,7 @@ public class RSRpcServices implements HBaseRPCErrorHandler,
       checkOpen();
       requestCount.increment();
       Map<String, Region> onlineRegions = regionServer.onlineRegions;
-      List<HRegionInfo> list = new ArrayList<HRegionInfo>(onlineRegions.size());
+      List<HRegionInfo> list = new ArrayList<>(onlineRegions.size());
       for (Region region: onlineRegions.values()) {
         list.add(region.getRegionInfo());
       }
@@ -1587,7 +1587,7 @@ public class RSRpcServices implements HBaseRPCErrorHandler,
     } else {
       regions = regionServer.getOnlineRegions();
     }
-    List<RegionLoad> rLoads = new ArrayList<RegionLoad>(regions.size());
+    List<RegionLoad> rLoads = new ArrayList<>(regions.size());
     RegionLoad.Builder regionLoadBuilder = ClusterStatusProtos.RegionLoad.newBuilder();
     RegionSpecifier.Builder regionSpecifier = RegionSpecifier.newBuilder();
 
@@ -1636,7 +1636,7 @@ public class RSRpcServices implements HBaseRPCErrorHandler,
       if (request.getFamilyCount() == 0) {
         columnFamilies = region.getTableDesc().getFamiliesKeys();
       } else {
-        columnFamilies = new TreeSet<byte[]>(Bytes.BYTES_RAWCOMPARATOR);
+        columnFamilies = new TreeSet<>(Bytes.BYTES_RAWCOMPARATOR);
         for (ByteString cf: request.getFamilyList()) {
           columnFamilies.add(cf.toByteArray());
         }
@@ -1692,8 +1692,7 @@ public class RSRpcServices implements HBaseRPCErrorHandler,
 
     OpenRegionResponse.Builder builder = OpenRegionResponse.newBuilder();
     final int regionCount = request.getOpenInfoCount();
-    final Map<TableName, HTableDescriptor> htds =
-        new HashMap<TableName, HTableDescriptor>(regionCount);
+    final Map<TableName, HTableDescriptor> htds = new HashMap<>(regionCount);
     final boolean isBulkAssign = regionCount > 1;
     try {
       checkOpen();
@@ -1783,7 +1782,7 @@ public class RSRpcServices implements HBaseRPCErrorHandler,
             } else {
               // Remove stale recovery region from ZK when we open region not for recovering which
               // could happen when turn distributedLogReplay off from on.
-              List<String> tmpRegions = new ArrayList<String>();
+              List<String> tmpRegions = new ArrayList<>();
               tmpRegions.add(region.getEncodedName());
               ZKSplitLog.deleteRecoveringRegionZNodes(regionServer.getZooKeeper(),
                 tmpRegions);
@@ -1914,7 +1913,7 @@ public class RSRpcServices implements HBaseRPCErrorHandler,
           ServerRegionReplicaUtil.isDefaultReplica(region.getRegionInfo())
             ? region.getCoprocessorHost()
             : null; // do not invoke coprocessors if this is a secondary region replica
-      List<Pair<WALKey, WALEdit>> walEntries = new ArrayList<Pair<WALKey, WALEdit>>();
+      List<Pair<WALKey, WALEdit>> walEntries = new ArrayList<>();
 
       // Skip adding the edits to WAL if this is a secondary region replica
       boolean isPrimary = RegionReplicaUtil.isDefaultReplica(region.getRegionInfo());
@@ -1935,8 +1934,7 @@ public class RSRpcServices implements HBaseRPCErrorHandler,
               nonce,
               entry.getKey().getWriteTime());
         }
-        Pair<WALKey, WALEdit> walEntry = (coprocessorHost == null) ? null :
-          new Pair<WALKey, WALEdit>();
+        Pair<WALKey, WALEdit> walEntry = (coprocessorHost == null) ? null : new Pair<>();
         List<WALSplitter.MutationReplay> edits = WALSplitter.getMutationsFromWALEntry(entry,
           cells, walEntry, durability);
         if (coprocessorHost != null) {
@@ -2132,11 +2130,9 @@ public class RSRpcServices implements HBaseRPCErrorHandler,
 
       if (!request.hasBulkToken()) {
         // Old style bulk load. This will not be supported in future releases
-        List<Pair<byte[], String>> familyPaths =
-            new ArrayList<Pair<byte[], String>>(request.getFamilyPathCount());
+        List<Pair<byte[], String>> familyPaths = new ArrayList<>(request.getFamilyPathCount());
         for (FamilyPath familyPath : request.getFamilyPathList()) {
-          familyPaths.add(new Pair<byte[], String>(familyPath.getFamily().toByteArray(), familyPath
-              .getPath()));
+          familyPaths.add(new Pair<>(familyPath.getFamily().toByteArray(), familyPath.getPath()));
         }
         if (region.getCoprocessorHost() != null) {
           bypass = region.getCoprocessorHost().preBulkLoadHFile(familyPaths);
@@ -2317,7 +2313,7 @@ public class RSRpcServices implements HBaseRPCErrorHandler,
   private Result get(Get get, HRegion region, RegionScannersCloseCallBack closeCallBack,
       RpcCallContext context) throws IOException {
     region.prepareGet(get);
-    List<Cell> results = new ArrayList<Cell>();
+    List<Cell> results = new ArrayList<>();
     boolean stale = region.getRegionInfo().getReplicaId() != 0;
     // pre-get CP hook
     if (region.getCoprocessorHost() != null) {
@@ -2789,7 +2785,7 @@ public class RSRpcServices implements HBaseRPCErrorHandler,
     // This is cells inside a row. Default size is 10 so if many versions or many cfs,
     // then we'll resize. Resizings show in profiler. Set it higher than 10. For now
     // arbitrary 32. TODO: keep record of general size of results being returned.
-    List<Cell> values = new ArrayList<Cell>(32);
+    List<Cell> values = new ArrayList<>(32);
     region.startRegionOperation(Operation.SCAN);
     try {
       int i = 0;
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/RegionCoprocessorHost.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/RegionCoprocessorHost.java
index 649273d..925e349 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/RegionCoprocessorHost.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/RegionCoprocessorHost.java
@@ -361,7 +361,7 @@ public class RegionCoprocessorHost
 
     // scan the table attributes for coprocessor load specifications
     // initialize the coprocessors
-    List<RegionEnvironment> configured = new ArrayList<RegionEnvironment>();
+    List<RegionEnvironment> configured = new ArrayList<>();
     for (TableCoprocessorAttribute attr: getTableCoprocessorAttrsFromSchema(conf, 
         region.getTableDesc())) {
       // Load encompasses classloading and coprocessor initialization
@@ -405,7 +405,7 @@ public class RegionCoprocessorHost
       // remain in this map
       classData = (ConcurrentMap<String, Object>)sharedDataMap.get(implClass.getName());
       if (classData == null) {
-        classData = new ConcurrentHashMap<String, Object>();
+        classData = new ConcurrentHashMap<>();
         sharedDataMap.put(implClass.getName(), classData);
       }
     }
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/RegionServerAccounting.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/RegionServerAccounting.java
index 3904393..91e28fd 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/RegionServerAccounting.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/RegionServerAccounting.java
@@ -45,7 +45,7 @@ public class RegionServerAccounting {
   // Store the edits size during replaying WAL. Use this to roll back the
   // global memstore size once a region opening failed.
   private final ConcurrentMap<byte[], MemstoreSize> replayEditsPerRegion =
-    new ConcurrentSkipListMap<byte[], MemstoreSize>(Bytes.BYTES_COMPARATOR);
+    new ConcurrentSkipListMap<>(Bytes.BYTES_COMPARATOR);
 
   private final Configuration conf;
 
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/RegionServicesForStores.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/RegionServicesForStores.java
index 82e6778..ea346ea 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/RegionServicesForStores.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/RegionServicesForStores.java
@@ -40,7 +40,7 @@ public class RegionServicesForStores {
   private static final int POOL_SIZE = 10;
   private static final ThreadPoolExecutor INMEMORY_COMPACTION_POOL =
       new ThreadPoolExecutor(POOL_SIZE, POOL_SIZE, 60, TimeUnit.SECONDS,
-          new LinkedBlockingQueue<Runnable>(),
+          new LinkedBlockingQueue<>(),
           new ThreadFactory() {
             @Override
             public Thread newThread(Runnable r) {
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/SecureBulkLoadManager.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/SecureBulkLoadManager.java
index 1accae1..b1473cb 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/SecureBulkLoadManager.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/SecureBulkLoadManager.java
@@ -140,8 +140,7 @@ public class SecureBulkLoadManager {
     List<BulkLoadObserver> bulkLoadObservers = getBulkLoadObservers(region);
 
     if (bulkLoadObservers != null && bulkLoadObservers.size() != 0) {
-      ObserverContext<RegionCoprocessorEnvironment> ctx =
-          new ObserverContext<RegionCoprocessorEnvironment>(getActiveUser());
+      ObserverContext<RegionCoprocessorEnvironment> ctx = new ObserverContext<>(getActiveUser());
       ctx.prepare((RegionCoprocessorEnvironment) region.getCoprocessorHost()
           .findCoprocessorEnvironment(BulkLoadObserver.class).get(0));
 
@@ -162,8 +161,7 @@ public class SecureBulkLoadManager {
     List<BulkLoadObserver> bulkLoadObservers = getBulkLoadObservers(region);
 
     if (bulkLoadObservers != null && bulkLoadObservers.size() != 0) {
-      ObserverContext<RegionCoprocessorEnvironment> ctx =
-          new ObserverContext<RegionCoprocessorEnvironment>(getActiveUser());
+      ObserverContext<RegionCoprocessorEnvironment> ctx = new ObserverContext<>(getActiveUser());
       ctx.prepare((RegionCoprocessorEnvironment) region.getCoprocessorHost()
         .findCoprocessorEnvironment(BulkLoadObserver.class).get(0));
 
@@ -177,9 +175,9 @@ public class SecureBulkLoadManager {
 
   public Map<byte[], List<Path>> secureBulkLoadHFiles(final Region region,
       final BulkLoadHFileRequest request) throws IOException {
-    final List<Pair<byte[], String>> familyPaths = new ArrayList<Pair<byte[], String>>(request.getFamilyPathCount());
+    final List<Pair<byte[], String>> familyPaths = new ArrayList<>(request.getFamilyPathCount());
     for(ClientProtos.BulkLoadHFileRequest.FamilyPath el : request.getFamilyPathList()) {
-      familyPaths.add(new Pair<byte[], String>(el.getFamily().toByteArray(), el.getPath()));
+      familyPaths.add(new Pair<>(el.getFamily().toByteArray(), el.getPath()));
     }
 
     Token userToken = null;
@@ -324,7 +322,7 @@ public class SecureBulkLoadManager {
       this.fs = fs;
       this.stagingDir = stagingDir;
       this.conf = conf;
-      this.origPermissions = new HashMap<String, FsPermission>();
+      this.origPermissions = new HashMap<>();
     }
 
     @Override
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/Segment.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/Segment.java
index 8581517..11d51d8 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/Segment.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/Segment.java
@@ -55,7 +55,7 @@ public abstract class Segment {
   public final static long DEEP_OVERHEAD = FIXED_OVERHEAD + ClassSize.ATOMIC_REFERENCE
       + ClassSize.CELL_SET + ClassSize.ATOMIC_LONG + ClassSize.TIMERANGE_TRACKER;
 
-  private AtomicReference<CellSet> cellSet= new AtomicReference<CellSet>();
+  private AtomicReference<CellSet> cellSet= new AtomicReference<>();
   private final CellComparator comparator;
   protected long minSequenceId;
   private MemStoreLAB memStoreLAB;
@@ -115,7 +115,7 @@ public abstract class Segment {
   }
 
   public List<KeyValueScanner> getScanners(long readPoint, long order) {
-    List<KeyValueScanner> scanners = new ArrayList<KeyValueScanner>(1);
+    List<KeyValueScanner> scanners = new ArrayList<>(1);
     scanners.add(getScanner(readPoint, order));
     return scanners;
   }
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/SegmentFactory.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/SegmentFactory.java
index 7e53026..1a8b89d 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/SegmentFactory.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/SegmentFactory.java
@@ -108,7 +108,7 @@ public final class SegmentFactory {
   }
 
   private MemStoreLAB getMergedMemStoreLAB(Configuration conf, List<ImmutableSegment> segments) {
-    List<MemStoreLAB> mslabs = new ArrayList<MemStoreLAB>();
+    List<MemStoreLAB> mslabs = new ArrayList<>();
     if (!conf.getBoolean(MemStoreLAB.USEMSLAB_KEY, MemStoreLAB.USEMSLAB_DEFAULT)) {
       return null;
     }
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/ServerNonceManager.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/ServerNonceManager.java
index 11e46a4..874ca44 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/ServerNonceManager.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/ServerNonceManager.java
@@ -121,8 +121,7 @@ public class ServerNonceManager {
    * which is a realistic worst case. If it's much worse, we could use some sort of memory
    * limit and cleanup.
    */
-  private ConcurrentHashMap<NonceKey, OperationContext> nonces =
-      new ConcurrentHashMap<NonceKey, OperationContext>();
+  private ConcurrentHashMap<NonceKey, OperationContext> nonces = new ConcurrentHashMap<>();
 
   private int deleteNonceGracePeriod;
 
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/ShutdownHook.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/ShutdownHook.java
index f19f26f..bdae05a 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/ShutdownHook.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/ShutdownHook.java
@@ -58,7 +58,7 @@ public class ShutdownHook {
    * to be executed after the last regionserver referring to a given filesystem
    * stops. We keep track of the # of regionserver references in values of the map.
    */
-  private final static Map<Runnable, Integer> fsShutdownHooks = new HashMap<Runnable, Integer>();
+  private final static Map<Runnable, Integer> fsShutdownHooks = new HashMap<>();
 
   /**
    * Install a shutdown hook that calls stop on the passed Stoppable
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/StoreFileScanner.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/StoreFileScanner.java
index 52811f6..ca7dfd4 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/StoreFileScanner.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/StoreFileScanner.java
@@ -125,7 +125,7 @@ public class StoreFileScanner implements KeyValueScanner {
   public static List<StoreFileScanner> getScannersForStoreFiles(Collection<StoreFile> files,
       boolean cacheBlocks, boolean usePread, boolean isCompaction, boolean canUseDrop,
       ScanQueryMatcher matcher, long readPt, boolean isPrimaryReplica) throws IOException {
-    List<StoreFileScanner> scanners = new ArrayList<StoreFileScanner>(files.size());
+    List<StoreFileScanner> scanners = new ArrayList<>(files.size());
     List<StoreFile> sorted_files = new ArrayList<>(files);
     Collections.sort(sorted_files, StoreFile.Comparators.SEQ_ID);
     for (int i = 0; i < sorted_files.size(); i++) {
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/StoreFlusher.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/StoreFlusher.java
index abfd3fc..23fae6a 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/StoreFlusher.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/StoreFlusher.java
@@ -119,7 +119,7 @@ abstract class StoreFlusher {
     ScannerContext scannerContext =
         ScannerContext.newBuilder().setBatchLimit(compactionKVMax).build();
 
-    List<Cell> kvs = new ArrayList<Cell>();
+    List<Cell> kvs = new ArrayList<>();
     boolean hasMore;
     String flushName = ThroughputControlUtil.getNameForThrottling(store, "flush");
     // no control on system table (such as meta, namespace, etc) flush
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/StoreScanner.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/StoreScanner.java
index 5c21a41..99ec30e 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/StoreScanner.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/StoreScanner.java
@@ -92,7 +92,7 @@ public class StoreScanner extends NonReversedNonLazyKeyValueScanner
 
   // Collects all the KVHeap that are eagerly getting closed during the
   // course of a scan
-  protected List<KeyValueHeap> heapsForDelayedClose = new ArrayList<KeyValueHeap>();
+  protected List<KeyValueHeap> heapsForDelayedClose = new ArrayList<>();
 
   /**
    * The number of KVs seen by the scanner. Includes explicitly skipped KVs, but not
@@ -131,9 +131,9 @@ public class StoreScanner extends NonReversedNonLazyKeyValueScanner
   // Indicates whether there was flush during the course of the scan
   protected volatile boolean flushed = false;
   // generally we get one file from a flush
-  protected List<StoreFile> flushedStoreFiles = new ArrayList<StoreFile>(1);
+  protected List<StoreFile> flushedStoreFiles = new ArrayList<>(1);
   // The current list of scanners
-  protected List<KeyValueScanner> currentScanners = new ArrayList<KeyValueScanner>();
+  protected List<KeyValueScanner> currentScanners = new ArrayList<>();
   // flush update lock
   private ReentrantLock flushLock = new ReentrantLock();
 
@@ -428,8 +428,7 @@ public class StoreScanner extends NonReversedNonLazyKeyValueScanner
       filesOnly = false;
     }
 
-    List<KeyValueScanner> scanners =
-        new ArrayList<KeyValueScanner>(allScanners.size());
+    List<KeyValueScanner> scanners = new ArrayList<>(allScanners.size());
 
     // We can only exclude store files based on TTL if minVersions is set to 0.
     // Otherwise, we might have to return KVs that have technically expired.
@@ -940,8 +939,7 @@ public class StoreScanner extends NonReversedNonLazyKeyValueScanner
     if (scanners.isEmpty()) return;
     int storeFileScannerCount = scanners.size();
     CountDownLatch latch = new CountDownLatch(storeFileScannerCount);
-    List<ParallelSeekHandler> handlers =
-        new ArrayList<ParallelSeekHandler>(storeFileScannerCount);
+    List<ParallelSeekHandler> handlers = new ArrayList<>(storeFileScannerCount);
     for (KeyValueScanner scanner : scanners) {
       if (scanner instanceof StoreFileScanner) {
         ParallelSeekHandler seekHandler = new ParallelSeekHandler(scanner, kv,
@@ -972,7 +970,7 @@ public class StoreScanner extends NonReversedNonLazyKeyValueScanner
    * @return all scanners in no particular order
    */
   List<KeyValueScanner> getAllScannersForTesting() {
-    List<KeyValueScanner> allScanners = new ArrayList<KeyValueScanner>();
+    List<KeyValueScanner> allScanners = new ArrayList<>();
     KeyValueScanner current = heap.getCurrentForTesting();
     if (current != null)
       allScanners.add(current);
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/StorefileRefresherChore.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/StorefileRefresherChore.java
index a2a0dcc..0ec41b3 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/StorefileRefresherChore.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/StorefileRefresherChore.java
@@ -79,7 +79,7 @@ public class StorefileRefresherChore extends ScheduledChore {
       throw new RuntimeException(REGIONSERVER_STOREFILE_REFRESH_PERIOD +
         " should be set smaller than half of " + TimeToLiveHFileCleaner.TTL_CONF_KEY);
     }
-    lastRefreshTimes = new HashMap<String, Long>();
+    lastRefreshTimes = new HashMap<>();
   }
 
   @Override
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/StripeMultiFileWriter.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/StripeMultiFileWriter.java
index 2662dd1..7392492 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/StripeMultiFileWriter.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/StripeMultiFileWriter.java
@@ -143,7 +143,7 @@ public abstract class StripeMultiFileWriter extends AbstractMultiFileWriter {
         byte[] majorRangeFrom, byte[] majorRangeTo) throws IOException {
       super(comparator);
       this.boundaries = targetBoundaries;
-      this.existingWriters = new ArrayList<StoreFileWriter>(this.boundaries.size() - 1);
+      this.existingWriters = new ArrayList<>(this.boundaries.size() - 1);
       // "major" range (range for which all files are included) boundaries, if any,
       // must match some target boundaries, let's find them.
       assert (majorRangeFrom == null) == (majorRangeTo == null);
@@ -283,8 +283,8 @@ public abstract class StripeMultiFileWriter extends AbstractMultiFileWriter {
       this.left = left;
       this.right = right;
       int preallocate = Math.min(this.targetCount, 64);
-      this.existingWriters = new ArrayList<StoreFileWriter>(preallocate);
-      this.boundaries = new ArrayList<byte[]>(preallocate + 1);
+      this.existingWriters = new ArrayList<>(preallocate);
+      this.boundaries = new ArrayList<>(preallocate + 1);
     }
 
     @Override
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/StripeStoreEngine.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/StripeStoreEngine.java
index 9255634..1e78ab2 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/StripeStoreEngine.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/StripeStoreEngine.java
@@ -84,7 +84,7 @@ public class StripeStoreEngine extends StoreEngine<StripeStoreFlusher,
       this.stripeRequest = compactionPolicy.selectCompaction(
           storeFileManager, filesCompacting, mayUseOffPeak);
       this.request = (this.stripeRequest == null)
-          ? new CompactionRequest(new ArrayList<StoreFile>()) : this.stripeRequest.getRequest();
+          ? new CompactionRequest(new ArrayList<>()) : this.stripeRequest.getRequest();
       return this.stripeRequest != null;
     }
 
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/StripeStoreFileManager.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/StripeStoreFileManager.java
index 1b3c9f8..4a719f3 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/StripeStoreFileManager.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/StripeStoreFileManager.java
@@ -100,8 +100,7 @@ public class StripeStoreFileManager
      * same index, except the last one. Inside each list, the files are in reverse order by
      * seqNum. Note that the length of this is one higher than that of stripeEndKeys.
      */
-    public ArrayList<ImmutableList<StoreFile>> stripeFiles
-      = new ArrayList<ImmutableList<StoreFile>>();
+    public ArrayList<ImmutableList<StoreFile>> stripeFiles = new ArrayList<>();
     /** Level 0. The files are in reverse order by seqNum. */
     public ImmutableList<StoreFile> level0Files = ImmutableList.<StoreFile>of();
 
@@ -112,8 +111,8 @@ public class StripeStoreFileManager
   private State state = null;
 
   /** Cached file metadata (or overrides as the case may be) */
-  private HashMap<StoreFile, byte[]> fileStarts = new HashMap<StoreFile, byte[]>();
-  private HashMap<StoreFile, byte[]> fileEnds = new HashMap<StoreFile, byte[]>();
+  private HashMap<StoreFile, byte[]> fileStarts = new HashMap<>();
+  private HashMap<StoreFile, byte[]> fileEnds = new HashMap<>();
   /** Normally invalid key is null, but in the map null is the result for "no key"; so use
    * the following constant value in these maps instead. Note that this is a constant and
    * we use it to compare by reference when we read from the map. */
@@ -277,7 +276,7 @@ public class StripeStoreFileManager
   }
 
   private byte[] getSplitPointFromAllFiles() throws IOException {
-    ConcatenatedLists<StoreFile> sfs = new ConcatenatedLists<StoreFile>();
+    ConcatenatedLists<StoreFile> sfs = new ConcatenatedLists<>();
     sfs.addSublist(state.level0Files);
     sfs.addAllSublists(state.stripeFiles);
     if (sfs.isEmpty()) return null;
@@ -305,7 +304,7 @@ public class StripeStoreFileManager
       return state.allFilesCached; // We need to read all files.
     }
 
-    ConcatenatedLists<StoreFile> result = new ConcatenatedLists<StoreFile>();
+    ConcatenatedLists<StoreFile> result = new ConcatenatedLists<>();
     result.addAllSublists(state.stripeFiles.subList(firstStripe, lastStripe + 1));
     result.addSublist(state.level0Files);
     return result;
@@ -385,9 +384,8 @@ public class StripeStoreFileManager
    */
   private void loadUnclassifiedStoreFiles(List<StoreFile> storeFiles) {
     LOG.debug("Attempting to load " + storeFiles.size() + " store files.");
-    TreeMap<byte[], ArrayList<StoreFile>> candidateStripes =
-        new TreeMap<byte[], ArrayList<StoreFile>>(MAP_COMPARATOR);
-    ArrayList<StoreFile> level0Files = new ArrayList<StoreFile>();
+    TreeMap<byte[], ArrayList<StoreFile>> candidateStripes = new TreeMap<>(MAP_COMPARATOR);
+    ArrayList<StoreFile> level0Files = new ArrayList<>();
     // Separate the files into tentative stripes; then validate. Currently, we rely on metadata.
     // If needed, we could dynamically determine the stripes in future.
     for (StoreFile sf : storeFiles) {
@@ -405,7 +403,7 @@ public class StripeStoreFileManager
       } else {
         ArrayList<StoreFile> stripe = candidateStripes.get(endRow);
         if (stripe == null) {
-          stripe = new ArrayList<StoreFile>();
+          stripe = new ArrayList<>();
           candidateStripes.put(endRow, stripe);
         }
         insertFileIntoStripe(stripe, sf);
@@ -477,9 +475,9 @@ public class StripeStoreFileManager
     // Copy the results into the fields.
     State state = new State();
     state.level0Files = ImmutableList.copyOf(level0Files);
-    state.stripeFiles = new ArrayList<ImmutableList<StoreFile>>(candidateStripes.size());
+    state.stripeFiles = new ArrayList<>(candidateStripes.size());
     state.stripeEndRows = new byte[Math.max(0, candidateStripes.size() - 1)][];
-    ArrayList<StoreFile> newAllFiles = new ArrayList<StoreFile>(level0Files);
+    ArrayList<StoreFile> newAllFiles = new ArrayList<>(level0Files);
     int i = candidateStripes.size() - 1;
     for (Map.Entry<byte[], ArrayList<StoreFile>> entry : candidateStripes.entrySet()) {
       state.stripeFiles.add(ImmutableList.copyOf(entry.getValue()));
@@ -685,7 +683,7 @@ public class StripeStoreFileManager
         this.nextWasCalled = false;
         List<StoreFile> src = components.get(currentComponent);
         if (src instanceof ImmutableList<?>) {
-          src = new ArrayList<StoreFile>(src);
+          src = new ArrayList<>(src);
           components.set(currentComponent, src);
         }
         src.remove(indexWithinComponent);
@@ -711,13 +709,12 @@ public class StripeStoreFileManager
     private Collection<StoreFile> compactedFiles = null;
     private Collection<StoreFile> results = null;
 
-    private List<StoreFile> l0Results = new ArrayList<StoreFile>();
+    private List<StoreFile> l0Results = new ArrayList<>();
     private final boolean isFlush;
 
     public CompactionOrFlushMergeCopy(boolean isFlush) {
       // Create a lazy mutable copy (other fields are so lazy they start out as nulls).
-      this.stripeFiles = new ArrayList<List<StoreFile>>(
-          StripeStoreFileManager.this.state.stripeFiles);
+      this.stripeFiles = new ArrayList<>(StripeStoreFileManager.this.state.stripeFiles);
       this.isFlush = isFlush;
     }
 
@@ -755,15 +752,14 @@ public class StripeStoreFileManager
           : ImmutableList.copyOf(this.level0Files);
       newState.stripeEndRows = (this.stripeEndRows == null) ? oldState.stripeEndRows
           : this.stripeEndRows.toArray(new byte[this.stripeEndRows.size()][]);
-      newState.stripeFiles = new ArrayList<ImmutableList<StoreFile>>(this.stripeFiles.size());
+      newState.stripeFiles = new ArrayList<>(this.stripeFiles.size());
       for (List<StoreFile> newStripe : this.stripeFiles) {
         newState.stripeFiles.add(newStripe instanceof ImmutableList<?>
             ? (ImmutableList<StoreFile>)newStripe : ImmutableList.copyOf(newStripe));
       }
 
-      List<StoreFile> newAllFiles = new ArrayList<StoreFile>(oldState.allFilesCached);
-      List<StoreFile> newAllCompactedFiles =
-          new ArrayList<StoreFile>(oldState.allCompactedFilesCached);
+      List<StoreFile> newAllFiles = new ArrayList<>(oldState.allFilesCached);
+      List<StoreFile> newAllCompactedFiles = new ArrayList<>(oldState.allCompactedFilesCached);
       if (!isFlush) {
         newAllFiles.removeAll(compactedFiles);
         if (delCompactedFiles) {
@@ -803,7 +799,7 @@ public class StripeStoreFileManager
       List<StoreFile> stripeCopy = this.stripeFiles.get(index);
       ArrayList<StoreFile> result = null;
       if (stripeCopy instanceof ImmutableList<?>) {
-        result = new ArrayList<StoreFile>(stripeCopy);
+        result = new ArrayList<>(stripeCopy);
         this.stripeFiles.set(index, result);
       } else {
         result = (ArrayList<StoreFile>)stripeCopy;
@@ -816,7 +812,7 @@ public class StripeStoreFileManager
      */
     private final ArrayList<StoreFile> getLevel0Copy() {
       if (this.level0Files == null) {
-        this.level0Files = new ArrayList<StoreFile>(StripeStoreFileManager.this.state.level0Files);
+        this.level0Files = new ArrayList<>(StripeStoreFileManager.this.state.level0Files);
       }
       return this.level0Files;
     }
@@ -849,7 +845,7 @@ public class StripeStoreFileManager
 
         // Make a new candidate stripe.
         if (newStripes == null) {
-          newStripes = new TreeMap<byte[], StoreFile>(MAP_COMPARATOR);
+          newStripes = new TreeMap<>(MAP_COMPARATOR);
         }
         StoreFile oldSf = newStripes.put(endRow, sf);
         if (oldSf != null) {
@@ -893,8 +889,7 @@ public class StripeStoreFileManager
         TreeMap<byte[], StoreFile> newStripes) throws IOException {
       // Validate that the removed and added aggregate ranges still make for a full key space.
       boolean hasStripes = !this.stripeFiles.isEmpty();
-      this.stripeEndRows = new ArrayList<byte[]>(
-          Arrays.asList(StripeStoreFileManager.this.state.stripeEndRows));
+      this.stripeEndRows = new ArrayList<>(Arrays.asList(StripeStoreFileManager.this.state.stripeEndRows));
       int removeFrom = 0;
       byte[] firstStartRow = startOf(newStripes.firstEntry().getValue());
       byte[] lastEndRow = newStripes.lastKey();
@@ -917,7 +912,7 @@ public class StripeStoreFileManager
         int removeTo = findStripeIndexByEndRow(lastEndRow);
         if (removeTo < 0) throw new IOException("Compaction is trying to add a bad range.");
         // See if there are files in the stripes we are trying to replace.
-        ArrayList<StoreFile> conflictingFiles = new ArrayList<StoreFile>();
+        ArrayList<StoreFile> conflictingFiles = new ArrayList<>();
         for (int removeIndex = removeTo; removeIndex >= removeFrom; --removeIndex) {
           conflictingFiles.addAll(this.stripeFiles.get(removeIndex));
         }
@@ -973,7 +968,7 @@ public class StripeStoreFileManager
           }
         }
         // Add the new stripe.
-        ArrayList<StoreFile> tmp = new ArrayList<StoreFile>();
+        ArrayList<StoreFile> tmp = new ArrayList<>();
         tmp.add(newStripe.getValue());
         stripeFiles.add(insertAt, tmp);
         previousEndRow = newStripe.getKey();
@@ -992,8 +987,8 @@ public class StripeStoreFileManager
 
   @Override
   public List<byte[]> getStripeBoundaries() {
-    if (this.state.stripeFiles.isEmpty()) return new ArrayList<byte[]>();
-    ArrayList<byte[]> result = new ArrayList<byte[]>(this.state.stripeEndRows.length + 2);
+    if (this.state.stripeFiles.isEmpty()) return new ArrayList<>();
+    ArrayList<byte[]> result = new ArrayList<>(this.state.stripeEndRows.length + 2);
     result.add(OPEN_KEY);
     Collections.addAll(result, this.state.stripeEndRows);
     result.add(OPEN_KEY);
@@ -1033,7 +1028,7 @@ public class StripeStoreFileManager
           LOG.info("Found an expired store file: " + sf.getPath() + " whose maxTimeStamp is "
               + fileTs + ", which is below " + maxTs);
           if (expiredStoreFiles == null) {
-            expiredStoreFiles = new ArrayList<StoreFile>();
+            expiredStoreFiles = new ArrayList<>();
           }
           expiredStoreFiles.add(sf);
         }
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/StripeStoreFlusher.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/StripeStoreFlusher.java
index 22c3ce7..85bae9d 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/StripeStoreFlusher.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/StripeStoreFlusher.java
@@ -57,7 +57,7 @@ public class StripeStoreFlusher extends StoreFlusher {
   @Override
   public List<Path> flushSnapshot(MemStoreSnapshot snapshot, long cacheFlushSeqNum,
       MonitoredTask status, ThroughputController throughputController) throws IOException {
-    List<Path> result = new ArrayList<Path>();
+    List<Path> result = new ArrayList<>();
     int cellsCount = snapshot.getCellsCount();
     if (cellsCount == 0) return result; // don't flush if there are no entries
 
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/compactions/CompactionRequest.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/compactions/CompactionRequest.java
index 6a3ff4a..3d4f9a1 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/compactions/CompactionRequest.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/compactions/CompactionRequest.java
@@ -96,7 +96,7 @@ public class CompactionRequest implements Comparable<CompactionRequest> {
    * @return The result (may be "this" or "other").
    */
   public CompactionRequest combineWith(CompactionRequest other) {
-    this.filesToCompact = new ArrayList<StoreFile>(other.getFiles());
+    this.filesToCompact = new ArrayList<>(other.getFiles());
     this.isOffPeak = other.isOffPeak;
     this.isMajor = other.isMajor;
     this.priority = other.priority;
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/compactions/Compactor.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/compactions/Compactor.java
index 1fe5077..d72529a 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/compactions/Compactor.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/compactions/Compactor.java
@@ -294,7 +294,7 @@ public abstract class Compactor<T extends CellSink> {
     if (this.conf.getBoolean("hbase.regionserver.compaction.private.readers", true)) {
       // clone all StoreFiles, so we'll do the compaction on a independent copy of StoreFiles,
       // HFiles, and their readers
-      readersToClose = new ArrayList<StoreFile>(request.getFiles().size());
+      readersToClose = new ArrayList<>(request.getFiles().size());
       for (StoreFile f : request.getFiles()) {
         StoreFile clonedStoreFile = f.cloneForReader();
         // create the reader after the store file is cloned in case
@@ -320,7 +320,7 @@ public abstract class Compactor<T extends CellSink> {
       scanner = postCreateCoprocScanner(request, scanType, scanner, user);
       if (scanner == null) {
         // NULL scanner returned from coprocessor hooks means skip normal processing.
-        return new ArrayList<Path>();
+        return new ArrayList<>();
       }
       boolean cleanSeqId = false;
       if (fd.minSeqIdToKeep > 0) {
@@ -413,7 +413,7 @@ public abstract class Compactor<T extends CellSink> {
     long bytesWrittenProgressForShippedCall = 0;
     // Since scanner.next() can return 'false' but still be delivering data,
     // we have to use a do/while loop.
-    List<Cell> cells = new ArrayList<Cell>();
+    List<Cell> cells = new ArrayList<>();
     long closeCheckSizeLimit = HStore.getCloseCheckInterval();
     long lastMillis = 0;
     if (LOG.isDebugEnabled()) {
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/compactions/DateTieredCompactionPolicy.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/compactions/DateTieredCompactionPolicy.java
index e37a7fe..6413ee6 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/compactions/DateTieredCompactionPolicy.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/compactions/DateTieredCompactionPolicy.java
@@ -99,7 +99,7 @@ public class DateTieredCompactionPolicy extends SortedCompactionPolicy {
   @VisibleForTesting
   public boolean needsCompaction(final Collection<StoreFile> storeFiles,
       final List<StoreFile> filesCompacting) {
-    ArrayList<StoreFile> candidates = new ArrayList<StoreFile>(storeFiles);
+    ArrayList<StoreFile> candidates = new ArrayList<>(storeFiles);
     try {
       return !selectMinorCompaction(candidates, false, true).getFiles().isEmpty();
     } catch (Exception e) {
@@ -222,7 +222,7 @@ public class DateTieredCompactionPolicy extends SortedCompactionPolicy {
       // we put them in the same window as the last file in increasing order
       maxTimestampSeen = Math.max(maxTimestampSeen,
         storeFile.getMaximumTimestamp() == null? Long.MIN_VALUE : storeFile.getMaximumTimestamp());
-      storefileMaxTimestampPairs.add(new Pair<StoreFile, Long>(storeFile, maxTimestampSeen));
+      storefileMaxTimestampPairs.add(new Pair<>(storeFile, maxTimestampSeen));
     }
     Collections.reverse(storefileMaxTimestampPairs);
 
@@ -299,7 +299,7 @@ public class DateTieredCompactionPolicy extends SortedCompactionPolicy {
           file.getMinimumTimestamp() == null ? Long.MAX_VALUE : file.getMinimumTimestamp());
     }
 
-    List<Long> boundaries = new ArrayList<Long>();
+    List<Long> boundaries = new ArrayList<>();
 
     // Add startMillis of all windows between now and min timestamp
     for (CompactionWindow window = getIncomingWindow(now);
@@ -317,7 +317,7 @@ public class DateTieredCompactionPolicy extends SortedCompactionPolicy {
    */
   private static List<Long> getCompactionBoundariesForMinor(CompactionWindow window,
       boolean singleOutput) {
-    List<Long> boundaries = new ArrayList<Long>();
+    List<Long> boundaries = new ArrayList<>();
     boundaries.add(Long.MIN_VALUE);
     if (!singleOutput) {
       boundaries.add(window.startMillis());
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/compactions/ExploringCompactionPolicy.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/compactions/ExploringCompactionPolicy.java
index 8b5aa31..0bd917a 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/compactions/ExploringCompactionPolicy.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/compactions/ExploringCompactionPolicy.java
@@ -53,7 +53,7 @@ public class ExploringCompactionPolicy extends RatioBasedCompactionPolicy {
   @Override
   protected final ArrayList<StoreFile> applyCompactionPolicy(final ArrayList<StoreFile> candidates,
     final boolean mayUseOffPeak, final boolean mightBeStuck) throws IOException {
-    return new ArrayList<StoreFile>(applyCompactionPolicy(candidates, mightBeStuck,
+    return new ArrayList<>(applyCompactionPolicy(candidates, mightBeStuck,
         mayUseOffPeak, comConf.getMinFilesToCompact(), comConf.getMaxFilesToCompact()));
   }
 
@@ -64,8 +64,8 @@ public class ExploringCompactionPolicy extends RatioBasedCompactionPolicy {
         ? comConf.getCompactionRatioOffPeak() : comConf.getCompactionRatio();
 
     // Start off choosing nothing.
-    List<StoreFile> bestSelection = new ArrayList<StoreFile>(0);
-    List<StoreFile> smallest = mightBeStuck ? new ArrayList<StoreFile>(0) : null;
+    List<StoreFile> bestSelection = new ArrayList<>(0);
+    List<StoreFile> smallest = mightBeStuck ? new ArrayList<>(0) : null;
     long bestSize = 0;
     long smallestSize = Long.MAX_VALUE;
 
@@ -117,12 +117,12 @@ public class ExploringCompactionPolicy extends RatioBasedCompactionPolicy {
     if (bestSelection.isEmpty() && mightBeStuck) {
       LOG.debug("Exploring compaction algorithm has selected " + smallest.size()
           + " files of size "+ smallestSize + " because the store might be stuck");
-      return new ArrayList<StoreFile>(smallest);
+      return new ArrayList<>(smallest);
     }
     LOG.debug("Exploring compaction algorithm has selected " + bestSelection.size()
         + " files of size " + bestSize + " starting at candidate #" + bestStart +
         " after considering " + opts + " permutations with " + optsInRatio + " in ratio");
-    return new ArrayList<StoreFile>(bestSelection);
+    return new ArrayList<>(bestSelection);
   }
 
   private boolean isBetterSelection(List<StoreFile> bestSelection,
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/compactions/FIFOCompactionPolicy.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/compactions/FIFOCompactionPolicy.java
index d339898..97b8387 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/compactions/FIFOCompactionPolicy.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/compactions/FIFOCompactionPolicy.java
@@ -117,7 +117,7 @@ public class FIFOCompactionPolicy extends ExploringCompactionPolicy {
   private  Collection<StoreFile> getExpiredStores(Collection<StoreFile> files,
     Collection<StoreFile> filesCompacting) {
     long currentTime = EnvironmentEdgeManager.currentTime();
-    Collection<StoreFile> expiredStores = new ArrayList<StoreFile>();    
+    Collection<StoreFile> expiredStores = new ArrayList<>();
     for(StoreFile sf: files){
       // Check MIN_VERSIONS is in HStore removeUnneededFiles
       Long maxTs = sf.getReader().getMaxTimestamp();
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/compactions/SortedCompactionPolicy.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/compactions/SortedCompactionPolicy.java
index 77b0af8..42b57a4 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/compactions/SortedCompactionPolicy.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/compactions/SortedCompactionPolicy.java
@@ -43,7 +43,7 @@ public abstract class SortedCompactionPolicy extends CompactionPolicy {
 
   public List<StoreFile> preSelectCompactionForCoprocessor(final Collection<StoreFile> candidates,
       final List<StoreFile> filesCompacting) {
-    return getCurrentEligibleFiles(new ArrayList<StoreFile>(candidates), filesCompacting);
+    return getCurrentEligibleFiles(new ArrayList<>(candidates), filesCompacting);
   }
 
   /**
@@ -56,7 +56,7 @@ public abstract class SortedCompactionPolicy extends CompactionPolicy {
       final List<StoreFile> filesCompacting, final boolean isUserCompaction,
       final boolean mayUseOffPeak, final boolean forceMajor) throws IOException {
     // Preliminary compaction subject to filters
-    ArrayList<StoreFile> candidateSelection = new ArrayList<StoreFile>(candidateFiles);
+    ArrayList<StoreFile> candidateSelection = new ArrayList<>(candidateFiles);
     // Stuck and not compacting enough (estimate). It is not guaranteed that we will be
     // able to compact more if stuck and compacting, because ratio policy excludes some
     // non-compacting files from consideration during compaction (see getCurrentEligibleFiles).
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/compactions/StripeCompactionPolicy.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/compactions/StripeCompactionPolicy.java
index a553cf6..0b66d3d 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/compactions/StripeCompactionPolicy.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/compactions/StripeCompactionPolicy.java
@@ -68,7 +68,7 @@ public class StripeCompactionPolicy extends CompactionPolicy {
     // We sincerely hope nobody is messing with us with their coprocessors.
     // If they do, they are very likely to shoot themselves in the foot.
     // We'll just exclude all the filesCompacting from the list.
-    ArrayList<StoreFile> candidateFiles = new ArrayList<StoreFile>(si.getStorefiles());
+    ArrayList<StoreFile> candidateFiles = new ArrayList<>(si.getStorefiles());
     candidateFiles.removeAll(filesCompacting);
     return candidateFiles;
   }
@@ -217,7 +217,7 @@ public class StripeCompactionPolicy extends CompactionPolicy {
       LOG.debug("No good compaction is possible in any stripe");
       return null;
     }
-    List<StoreFile> filesToCompact = new ArrayList<StoreFile>(bqSelection);
+    List<StoreFile> filesToCompact = new ArrayList<>(bqSelection);
     // See if we can, and need to, split this stripe.
     int targetCount = 1;
     long targetKvs = Long.MAX_VALUE;
@@ -246,7 +246,7 @@ public class StripeCompactionPolicy extends CompactionPolicy {
       assert hasAllFiles;
       List<StoreFile> l0Files = si.getLevel0Files();
       LOG.debug("Adding " + l0Files.size() + " files to compaction to be able to drop deletes");
-      ConcatenatedLists<StoreFile> sfs = new ConcatenatedLists<StoreFile>();
+      ConcatenatedLists<StoreFile> sfs = new ConcatenatedLists<>();
       sfs.addSublist(filesToCompact);
       sfs.addSublist(l0Files);
       req = new BoundaryStripeCompactionRequest(sfs, si.getStripeBoundaries());
@@ -345,7 +345,7 @@ public class StripeCompactionPolicy extends CompactionPolicy {
     }
     LOG.debug("Merging " + bestLength + " stripes to delete expired store files");
     int endIndex = bestStart + bestLength - 1;
-    ConcatenatedLists<StoreFile> sfs = new ConcatenatedLists<StoreFile>();
+    ConcatenatedLists<StoreFile> sfs = new ConcatenatedLists<>();
     sfs.addAllSublists(stripes.subList(bestStart, endIndex + 1));
     SplitStripeCompactionRequest result = new SplitStripeCompactionRequest(sfs,
         si.getStartRow(bestStart), si.getEndRow(endIndex), 1, Long.MAX_VALUE);
@@ -388,7 +388,7 @@ public class StripeCompactionPolicy extends CompactionPolicy {
       splitCount += 1.0;
     }
     long kvCount = (long)(getTotalKvCount(files) / splitCount);
-    return new Pair<Long, Integer>(kvCount, (int)Math.ceil(splitCount));
+    return new Pair<>(kvCount, (int)Math.ceil(splitCount));
   }
 
   /** Stripe compaction request wrapper. */
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/querymatcher/ScanDeleteTracker.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/querymatcher/ScanDeleteTracker.java
index 450a30e..467fb2c 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/querymatcher/ScanDeleteTracker.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/querymatcher/ScanDeleteTracker.java
@@ -47,7 +47,7 @@ public class ScanDeleteTracker implements DeleteTracker {
 
   protected boolean hasFamilyStamp = false;
   protected long familyStamp = 0L;
-  protected SortedSet<Long> familyVersionStamps = new TreeSet<Long>();
+  protected SortedSet<Long> familyVersionStamps = new TreeSet<>();
   protected byte[] deleteBuffer = null;
   protected int deleteOffset = 0;
   protected int deleteLength = 0;
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/snapshot/RegionServerSnapshotManager.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/snapshot/RegionServerSnapshotManager.java
index aa1205a..7b43c3d 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/snapshot/RegionServerSnapshotManager.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/snapshot/RegionServerSnapshotManager.java
@@ -271,7 +271,7 @@ public class RegionServerSnapshotManager extends RegionServerProcedureManager {
     private final ExecutorCompletionService<Void> taskPool;
     private final ThreadPoolExecutor executor;
     private volatile boolean stopped;
-    private final List<Future<Void>> futures = new ArrayList<Future<Void>>();
+    private final List<Future<Void>> futures = new ArrayList<>();
     private final String name;
 
     SnapshotSubprocedurePool(String name, Configuration conf, Abortable abortable) {
@@ -283,10 +283,10 @@ public class RegionServerSnapshotManager extends RegionServerProcedureManager {
       int threads = conf.getInt(CONCURENT_SNAPSHOT_TASKS_KEY, DEFAULT_CONCURRENT_SNAPSHOT_TASKS);
       this.name = name;
       executor = new ThreadPoolExecutor(threads, threads, keepAlive, TimeUnit.MILLISECONDS,
-          new LinkedBlockingQueue<Runnable>(), new DaemonThreadFactory("rs("
+          new LinkedBlockingQueue<>(), new DaemonThreadFactory("rs("
               + name + ")-snapshot-pool"));
       executor.allowCoreThreadTimeOut(true);
-      taskPool = new ExecutorCompletionService<Void>(executor);
+      taskPool = new ExecutorCompletionService<>(executor);
     }
 
     boolean hasTasks() {
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/throttle/PressureAwareThroughputController.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/throttle/PressureAwareThroughputController.java
index 8867611..ca76ad5 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/throttle/PressureAwareThroughputController.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/throttle/PressureAwareThroughputController.java
@@ -76,8 +76,7 @@ public abstract class PressureAwareThroughputController extends Configured imple
   private volatile double maxThroughput;
   private volatile double maxThroughputPerOperation;
 
-  protected final ConcurrentMap<String, ActiveOperation> activeOperations =
-      new ConcurrentHashMap<String, ActiveOperation>();
+  protected final ConcurrentMap<String, ActiveOperation> activeOperations = new ConcurrentHashMap<>();
 
   @Override
   public abstract void setup(final RegionServerServices server);
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/wal/AbstractFSWAL.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/wal/AbstractFSWAL.java
index bf283f8..f32d0ed 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/wal/AbstractFSWAL.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/wal/AbstractFSWAL.java
@@ -155,7 +155,7 @@ public abstract class AbstractFSWAL<W> implements WAL {
   protected final Configuration conf;
 
   /** Listeners that are called on WAL events. */
-  protected final List<WALActionsListener> listeners = new CopyOnWriteArrayList<WALActionsListener>();
+  protected final List<WALActionsListener> listeners = new CopyOnWriteArrayList<>();
 
   /**
    * Class that does accounting of sequenceids in WAL subsystem. Holds oldest outstanding sequence
@@ -413,7 +413,7 @@ public abstract class AbstractFSWAL<W> implements WAL {
         .toNanos(conf.getLong("hbase.regionserver.hlog.sync.timeout", DEFAULT_WAL_SYNC_TIMEOUT_MS));
     int maxHandlersCount = conf.getInt(HConstants.REGION_SERVER_HANDLER_COUNT, 200);
     // Presize our map of SyncFutures by handler objects.
-    this.syncFuturesByHandler = new ConcurrentHashMap<Thread, SyncFuture>(maxHandlersCount);
+    this.syncFuturesByHandler = new ConcurrentHashMap<>(maxHandlersCount);
     this.implClassName = getClass().getSimpleName();
   }
 
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/wal/AsyncFSWAL.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/wal/AsyncFSWAL.java
index 83d93fe..c3e96cf 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/wal/AsyncFSWAL.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/wal/AsyncFSWAL.java
@@ -180,7 +180,7 @@ public class AsyncFSWAL extends AbstractFSWAL<AsyncWriter> {
 
   private final Deque<FSWALEntry> unackedAppends = new ArrayDeque<>();
 
-  private final SortedSet<SyncFuture> syncFutures = new TreeSet<SyncFuture>(SEQ_COMPARATOR);
+  private final SortedSet<SyncFuture> syncFutures = new TreeSet<>(SEQ_COMPARATOR);
 
   // the highest txid of WAL entries being processed
   private long highestProcessedAppendTxid;
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/wal/AsyncProtobufLogWriter.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/wal/AsyncProtobufLogWriter.java
index a0ac8a2..e1f7b8f 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/wal/AsyncProtobufLogWriter.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/wal/AsyncProtobufLogWriter.java
@@ -156,7 +156,7 @@ public class AsyncProtobufLogWriter extends AbstractProtobufLogWriter
   }
 
   private long write(Consumer<CompletableFuture<Long>> action) throws IOException {
-    CompletableFuture<Long> future = new CompletableFuture<Long>();
+    CompletableFuture<Long> future = new CompletableFuture<>();
     eventLoop.execute(() -> action.accept(future));
     try {
       return future.get().longValue();
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/wal/FSHLog.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/wal/FSHLog.java
index f5a3382..f0e29c1 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/wal/FSHLog.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/wal/FSHLog.java
@@ -226,7 +226,7 @@ public class FSHLog extends AbstractFSWAL<Writer> {
     String hostingThreadName = Thread.currentThread().getName();
     // Using BlockingWaitStrategy. Stuff that is going on here takes so long it makes no sense
     // spinning as other strategies do.
-    this.disruptor = new Disruptor<RingBufferTruck>(RingBufferTruck::new,
+    this.disruptor = new Disruptor<>(RingBufferTruck::new,
         getPreallocatedEventCount(), Threads.getNamedThreadFactory(hostingThreadName + ".append"),
         ProducerType.MULTI, new BlockingWaitStrategy());
     // Advance the ring buffer sequence so that it starts from 1 instead of 0,
@@ -489,7 +489,7 @@ public class FSHLog extends AbstractFSWAL<Writer> {
       // the meta table when succesful (i.e. sync), closing handlers -- etc. These are usually
       // much fewer in number than the user-space handlers so Q-size should be user handlers plus
       // some space for these other handlers. Lets multiply by 3 for good-measure.
-      this.syncFutures = new LinkedBlockingQueue<SyncFuture>(maxHandlersCount * 3);
+      this.syncFutures = new LinkedBlockingQueue<>(maxHandlersCount * 3);
     }
 
     void offer(final long sequence, final SyncFuture[] syncFutures, final int syncFutureCount) {
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/wal/ProtobufLogReader.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/wal/ProtobufLogReader.java
index d10220d..f445059 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/wal/ProtobufLogReader.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/wal/ProtobufLogReader.java
@@ -88,7 +88,7 @@ public class ProtobufLogReader extends ReaderBase {
   // maximum size of the wal Trailer in bytes. If a user writes/reads a trailer with size larger
   // than this size, it is written/read respectively, with a WARN message in the log.
   protected int trailerWarnSize;
-  private static List<String> writerClsNames = new ArrayList<String>();
+  private static List<String> writerClsNames = new ArrayList<>();
   static {
     writerClsNames.add(ProtobufLogWriter.class.getSimpleName());
     writerClsNames.add(AsyncProtobufLogWriter.class.getSimpleName());
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/wal/SecureProtobufLogReader.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/wal/SecureProtobufLogReader.java
index 62bc96e..f9ebed7 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/wal/SecureProtobufLogReader.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/wal/SecureProtobufLogReader.java
@@ -44,7 +44,7 @@ public class SecureProtobufLogReader extends ProtobufLogReader {
   private static final Log LOG = LogFactory.getLog(SecureProtobufLogReader.class);
 
   private Decryptor decryptor = null;
-  private static List<String> writerClsNames = new ArrayList<String>();
+  private static List<String> writerClsNames = new ArrayList<>();
   static {
     writerClsNames.add(ProtobufLogWriter.class.getSimpleName());
     writerClsNames.add(SecureProtobufLogWriter.class.getSimpleName());
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/wal/SequenceIdAccounting.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/wal/SequenceIdAccounting.java
index 8226b82..cd73eb3 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/wal/SequenceIdAccounting.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/wal/SequenceIdAccounting.java
@@ -154,7 +154,7 @@ class SequenceIdAccounting {
    */
   Map<byte[], Long> resetHighest() {
     Map<byte[], Long> old = this.highestSequenceIds;
-    this.highestSequenceIds = new HashMap<byte[], Long>();
+    this.highestSequenceIds = new HashMap<>();
     return old;
   }
 
@@ -422,7 +422,7 @@ class SequenceIdAccounting {
         long lowest = getLowestSequenceId(m);
         if (lowest != HConstants.NO_SEQNUM && lowest <= e.getValue()) {
           if (toFlush == null) {
-            toFlush = new ArrayList<byte[]>();
+            toFlush = new ArrayList<>();
           }
           toFlush.add(e.getKey());
         }
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/wal/WALEdit.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/wal/WALEdit.java
index f79fa01..7a8b3d5 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/wal/WALEdit.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/wal/WALEdit.java
@@ -130,7 +130,7 @@ public class WALEdit implements Writable, HeapSize {
 
   public WALEdit(int cellCount, boolean isReplay) {
     this.isReplay = isReplay;
-    cells = new ArrayList<Cell>(cellCount);
+    cells = new ArrayList<>(cellCount);
   }
 
   /**
@@ -222,7 +222,7 @@ public class WALEdit implements Writable, HeapSize {
       int numFamilies = in.readInt();
       if (numFamilies > 0) {
         if (scopes == null) {
-          scopes = new TreeMap<byte[], Integer>(Bytes.BYTES_COMPARATOR);
+          scopes = new TreeMap<>(Bytes.BYTES_COMPARATOR);
         }
         for (int i = 0; i < numFamilies; i++) {
           byte[] fam = Bytes.readByteArray(in);
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/wal/WALEditsReplaySink.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/wal/WALEditsReplaySink.java
index 4dee9f1..f451207 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/wal/WALEditsReplaySink.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/wal/WALEditsReplaySink.java
@@ -111,7 +111,7 @@ public class WALEditsReplaySink {
       if (entriesByRegion.containsKey(loc.getRegionInfo())) {
         regionEntries = entriesByRegion.get(loc.getRegionInfo());
       } else {
-        regionEntries = new ArrayList<Entry>();
+        regionEntries = new ArrayList<>();
         entriesByRegion.put(loc.getRegionInfo(), regionEntries);
       }
       regionEntries.add(entry);
@@ -160,7 +160,7 @@ public class WALEditsReplaySink {
     try {
       RpcRetryingCallerFactory factory = RpcRetryingCallerFactory.instantiate(conf, null);
       ReplayServerCallable<ReplicateWALEntryResponse> callable =
-          new ReplayServerCallable<ReplicateWALEntryResponse>(this.conn, this.rpcControllerFactory,
+          new ReplayServerCallable<>(this.conn, this.rpcControllerFactory,
               this.tableName, regionLoc, entries);
       factory.<ReplicateWALEntryResponse> newCaller().callWithRetries(callable, this.replayTimeout);
     } catch (IOException ie) {
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/replication/BulkLoadCellFilter.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/replication/BulkLoadCellFilter.java
index 1045c1d..86fc1fa 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/replication/BulkLoadCellFilter.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/replication/BulkLoadCellFilter.java
@@ -52,7 +52,7 @@ public class BulkLoadCellFilter {
     }
     List<StoreDescriptor> storesList = bld.getStoresList();
     // Copy the StoreDescriptor list and update it as storesList is a unmodifiableList
-    List<StoreDescriptor> copiedStoresList = new ArrayList<StoreDescriptor>(storesList);
+    List<StoreDescriptor> copiedStoresList = new ArrayList<>(storesList);
     Iterator<StoreDescriptor> copiedStoresListIterator = copiedStoresList.iterator();
     boolean anyStoreRemoved = false;
     while (copiedStoresListIterator.hasNext()) {
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/replication/ChainWALEntryFilter.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/replication/ChainWALEntryFilter.java
index 1d67faa..f858e5d 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/replication/ChainWALEntryFilter.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/replication/ChainWALEntryFilter.java
@@ -43,7 +43,7 @@ public class ChainWALEntryFilter implements WALEntryFilter {
   }
 
   public ChainWALEntryFilter(List<WALEntryFilter> filters) {
-    ArrayList<WALEntryFilter> rawFilters = new ArrayList<WALEntryFilter>(filters.size());
+    ArrayList<WALEntryFilter> rawFilters = new ArrayList<>(filters.size());
     // flatten the chains
     for (WALEntryFilter filter : filters) {
       if (filter instanceof ChainWALEntryFilter) {
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/replication/HBaseReplicationEndpoint.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/replication/HBaseReplicationEndpoint.java
index 1a603e0..23df804 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/replication/HBaseReplicationEndpoint.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/replication/HBaseReplicationEndpoint.java
@@ -52,7 +52,7 @@ public abstract class HBaseReplicationEndpoint extends BaseReplicationEndpoint
 
   private ZooKeeperWatcher zkw = null; // FindBugs: MT_CORRECTNESS
 
-  private List<ServerName> regionServers = new ArrayList<ServerName>(0);
+  private List<ServerName> regionServers = new ArrayList<>(0);
   private long lastRegionServerUpdate;
 
   protected void disconnect() {
@@ -151,7 +151,7 @@ public abstract class HBaseReplicationEndpoint extends BaseReplicationEndpoint
     if (children == null) {
       return Collections.emptyList();
     }
-    List<ServerName> addresses = new ArrayList<ServerName>(children.size());
+    List<ServerName> addresses = new ArrayList<>(children.size());
     for (String child : children) {
       addresses.add(ServerName.parseServerName(child));
     }
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/DumpReplicationQueues.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/DumpReplicationQueues.java
index 9a1e2bc..2bedbfd 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/DumpReplicationQueues.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/DumpReplicationQueues.java
@@ -80,8 +80,8 @@ public class DumpReplicationQueues extends Configured implements Tool {
   private long numWalsNotFound;
 
   public DumpReplicationQueues() {
-    deadRegionServers = new ArrayList<String>();
-    deletedQueues = new ArrayList<String>();
+    deadRegionServers = new ArrayList<>();
+    deletedQueues = new ArrayList<>();
     peersQueueSize = AtomicLongMap.create();
     totalSizeOfWALs = 0;
     numWalsNotFound = 0;
@@ -162,7 +162,7 @@ public class DumpReplicationQueues extends Configured implements Tool {
   public int run(String[] args) throws Exception {
 
     int errCode = -1;
-    LinkedList<String> argv = new LinkedList<String>();
+    LinkedList<String> argv = new LinkedList<>();
     argv.addAll(Arrays.asList(args));
     DumpOptions opts = parseOpts(argv);
 
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/HBaseInterClusterReplicationEndpoint.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/HBaseInterClusterReplicationEndpoint.java
index de3159f..ba12d53 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/HBaseInterClusterReplicationEndpoint.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/HBaseInterClusterReplicationEndpoint.java
@@ -127,7 +127,7 @@ public class HBaseInterClusterReplicationEndpoint extends HBaseReplicationEndpoi
     this.maxThreads = this.conf.getInt(HConstants.REPLICATION_SOURCE_MAXTHREADS_KEY,
       HConstants.REPLICATION_SOURCE_MAXTHREADS_DEFAULT);
     this.exec = new ThreadPoolExecutor(maxThreads, maxThreads, 60, TimeUnit.SECONDS,
-        new LinkedBlockingQueue<Runnable>());
+        new LinkedBlockingQueue<>());
     this.exec.allowCoreThreadTimeOut(true);
     this.abortable = ctx.getAbortable();
 
@@ -190,7 +190,7 @@ public class HBaseInterClusterReplicationEndpoint extends HBaseReplicationEndpoi
    */
   @Override
   public boolean replicate(ReplicateContext replicateContext) {
-    CompletionService<Integer> pool = new ExecutorCompletionService<Integer>(this.exec);
+    CompletionService<Integer> pool = new ExecutorCompletionService<>(this.exec);
     List<Entry> entries = replicateContext.getEntries();
     String walGroupId = replicateContext.getWalGroupId();
     int sleepMultiplier = 1;
@@ -212,12 +212,12 @@ public class HBaseInterClusterReplicationEndpoint extends HBaseReplicationEndpoi
     //  and number of current sinks
     int n = Math.min(Math.min(this.maxThreads, entries.size()/100+1), numSinks);
 
-    List<List<Entry>> entryLists = new ArrayList<List<Entry>>(n);
+    List<List<Entry>> entryLists = new ArrayList<>(n);
     if (n == 1) {
       entryLists.add(entries);
     } else {
       for (int i=0; i<n; i++) {
-        entryLists.add(new ArrayList<Entry>(entries.size()/n+1));
+        entryLists.add(new ArrayList<>(entries.size()/n+1));
       }
       // now group by region
       for (Entry e : entries) {
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/HFileReplicator.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/HFileReplicator.java
index 35aa1fb..c091b44 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/HFileReplicator.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/HFileReplicator.java
@@ -109,7 +109,7 @@ public class HFileReplicator {
     builder.setNameFormat("HFileReplicationCallable-%1$d");
     this.exec =
         new ThreadPoolExecutor(maxCopyThreads, maxCopyThreads, 60, TimeUnit.SECONDS,
-            new LinkedBlockingQueue<Runnable>(), builder.build());
+            new LinkedBlockingQueue<>(), builder.build());
     this.exec.allowCoreThreadTimeOut(true);
     this.copiesPerThread =
         conf.getInt(REPLICATION_BULKLOAD_COPY_HFILES_PERTHREAD_KEY,
@@ -144,7 +144,7 @@ public class HFileReplicator {
       Table table = this.connection.getTable(tableName);
 
       // Prepare collection of queue of hfiles to be loaded(replicated)
-      Deque<LoadQueueItem> queue = new LinkedList<LoadQueueItem>();
+      Deque<LoadQueueItem> queue = new LinkedList<>();
       loadHFiles.prepareHFileQueue(stagingDir, table, queue, false);
 
       if (queue.isEmpty()) {
@@ -221,7 +221,7 @@ public class HFileReplicator {
   }
 
   private Map<String, Path> copyHFilesToStagingDir() throws IOException {
-    Map<String, Path> mapOfCopiedHFiles = new HashMap<String, Path>();
+    Map<String, Path> mapOfCopiedHFiles = new HashMap<>();
     Pair<byte[], List<String>> familyHFilePathsPair;
     List<String> hfilePaths;
     byte[] family;
@@ -270,7 +270,7 @@ public class HFileReplicator {
           totalNoOfHFiles = hfilePaths.size();
 
           // For each list of hfile paths for the family
-          List<Future<Void>> futures = new ArrayList<Future<Void>>();
+          List<Future<Void>> futures = new ArrayList<>();
           Callable<Void> c;
           Future<Void> future;
           int currentCopied = 0;
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/MetricsSource.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/MetricsSource.java
index a647d03..7a9ef9f 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/MetricsSource.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/MetricsSource.java
@@ -39,7 +39,7 @@ public class MetricsSource implements BaseSource {
   private static final Log LOG = LogFactory.getLog(MetricsSource.class);
 
   // tracks last shipped timestamp for each wal group
-  private Map<String, Long> lastTimeStamps = new HashMap<String, Long>();
+  private Map<String, Long> lastTimeStamps = new HashMap<>();
   private int lastQueueSize = 0;
   private long lastHFileRefsQueueSize = 0;
   private String id;
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/RegionReplicaReplicationEndpoint.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/RegionReplicaReplicationEndpoint.java
index dc4fad0..3e0de45 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/RegionReplicaReplicationEndpoint.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/RegionReplicaReplicationEndpoint.java
@@ -238,7 +238,7 @@ public class RegionReplicaReplicationEndpoint extends HBaseReplicationEndpoint {
     }
     long keepAliveTime = conf.getLong("hbase.region.replica.replication.threads.keepalivetime", 60);
     LinkedBlockingQueue<Runnable> workQueue =
-        new LinkedBlockingQueue<Runnable>(maxThreads *
+        new LinkedBlockingQueue<>(maxThreads *
             conf.getInt(HConstants.HBASE_CLIENT_MAX_TOTAL_TASKS,
               HConstants.DEFAULT_HBASE_CLIENT_MAX_TOTAL_TASKS));
     ThreadPoolExecutor tpe = new ThreadPoolExecutor(
@@ -527,8 +527,7 @@ public class RegionReplicaReplicationEndpoint extends HBaseReplicationEndpoint {
         return;
       }
 
-      ArrayList<Future<ReplicateWALEntryResponse>> tasks
-        = new ArrayList<Future<ReplicateWALEntryResponse>>(locations.size() - 1);
+      ArrayList<Future<ReplicateWALEntryResponse>> tasks = new ArrayList<>(locations.size() - 1);
 
       // All passed entries should belong to one region because it is coming from the EntryBuffers
       // split per region. But the regions might split and merge (unlike log recovery case).
@@ -543,8 +542,7 @@ public class RegionReplicaReplicationEndpoint extends HBaseReplicationEndpoint {
             rpcControllerFactory, tableName, location, regionInfo, row, entries,
             sink.getSkippedEditsCounter());
            Future<ReplicateWALEntryResponse> task = pool.submit(
-             new RetryingRpcCallable<ReplicateWALEntryResponse>(rpcRetryingCallerFactory,
-                 callable, operationTimeout));
+             new RetryingRpcCallable<>(rpcRetryingCallerFactory, callable, operationTimeout));
            tasks.add(task);
         }
       }
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/Replication.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/Replication.java
index d3f9ba2..9cc9c7c 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/Replication.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/Replication.java
@@ -363,7 +363,7 @@ public class Replication extends WALActionsListener.Base implements
   }
 
   private void buildReplicationLoad() {
-    List<MetricsSource> sourceMetricsList = new ArrayList<MetricsSource>();
+    List<MetricsSource> sourceMetricsList = new ArrayList<>();
 
     // get source
     List<ReplicationSourceInterface> sources = this.replicationManager.getSources();
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationLoad.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationLoad.java
index b02b212..ef97687 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationLoad.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationLoad.java
@@ -68,8 +68,7 @@ public class ReplicationLoad {
     this.replicationLoadSink = rLoadSinkBuild.build();
 
     // build the SourceLoad List
-    Map<String, ClusterStatusProtos.ReplicationLoadSource> replicationLoadSourceMap =
-        new HashMap<String, ClusterStatusProtos.ReplicationLoadSource>();
+    Map<String, ClusterStatusProtos.ReplicationLoadSource> replicationLoadSourceMap = new HashMap<>();
     for (MetricsSource sm : this.sourceMetricsList) {
       // Get the actual peer id
       String peerId = sm.getPeerID();
@@ -111,8 +110,7 @@ public class ReplicationLoad {
 
       replicationLoadSourceMap.put(peerId, rLoadSourceBuild.build());
     }
-    this.replicationLoadSourceList = new ArrayList<ClusterStatusProtos.ReplicationLoadSource>(
-        replicationLoadSourceMap.values());
+    this.replicationLoadSourceList = new ArrayList<>(replicationLoadSourceMap.values());
   }
 
   /**
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSink.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSink.java
index 71f9f3d..a3d6d13 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSink.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSink.java
@@ -152,8 +152,7 @@ public class ReplicationSink {
       long totalReplicated = 0;
       // Map of table => list of Rows, grouped by cluster id, we only want to flushCommits once per
       // invocation of this method per table and cluster id.
-      Map<TableName, Map<List<UUID>, List<Row>>> rowMap =
-          new TreeMap<TableName, Map<List<UUID>, List<Row>>>();
+      Map<TableName, Map<List<UUID>, List<Row>>> rowMap = new TreeMap<>();
 
       // Map of table name Vs list of pair of family and list of hfile paths from its namespace
       Map<String, List<Pair<byte[], List<String>>>> bulkLoadHFileMap = null;
@@ -173,7 +172,7 @@ public class ReplicationSink {
           // Handle bulk load hfiles replication
           if (CellUtil.matchingQualifier(cell, WALEdit.BULK_LOAD)) {
             if (bulkLoadHFileMap == null) {
-              bulkLoadHFileMap = new HashMap<String, List<Pair<byte[], List<String>>>>();
+              bulkLoadHFileMap = new HashMap<>();
             }
             buildBulkLoadHFileMap(bulkLoadHFileMap, table, cell);
           } else {
@@ -184,7 +183,7 @@ public class ReplicationSink {
                   CellUtil.isDelete(cell) ? new Delete(cell.getRowArray(), cell.getRowOffset(),
                       cell.getRowLength()) : new Put(cell.getRowArray(), cell.getRowOffset(),
                       cell.getRowLength());
-              List<UUID> clusterIds = new ArrayList<UUID>(entry.getKey().getClusterIdsList().size());
+              List<UUID> clusterIds = new ArrayList<>(entry.getKey().getClusterIdsList().size());
               for (HBaseProtos.UUID clusterId : entry.getKey().getClusterIdsList()) {
                 clusterIds.add(toUUID(clusterId));
               }
@@ -275,20 +274,18 @@ public class ReplicationSink {
 
   private void addFamilyAndItsHFilePathToTableInMap(byte[] family, String pathToHfileFromNS,
       List<Pair<byte[], List<String>>> familyHFilePathsList) {
-    List<String> hfilePaths = new ArrayList<String>(1);
+    List<String> hfilePaths = new ArrayList<>(1);
     hfilePaths.add(pathToHfileFromNS);
-    familyHFilePathsList.add(new Pair<byte[], List<String>>(family, hfilePaths));
+    familyHFilePathsList.add(new Pair<>(family, hfilePaths));
   }
 
   private void addNewTableEntryInMap(
       final Map<String, List<Pair<byte[], List<String>>>> bulkLoadHFileMap, byte[] family,
       String pathToHfileFromNS, String tableName) {
-    List<String> hfilePaths = new ArrayList<String>(1);
+    List<String> hfilePaths = new ArrayList<>(1);
     hfilePaths.add(pathToHfileFromNS);
-    Pair<byte[], List<String>> newFamilyHFilePathsPair =
-        new Pair<byte[], List<String>>(family, hfilePaths);
-    List<Pair<byte[], List<String>>> newFamilyHFilePathsList =
-        new ArrayList<Pair<byte[], List<String>>>();
+    Pair<byte[], List<String>> newFamilyHFilePathsPair = new Pair<>(family, hfilePaths);
+    List<Pair<byte[], List<String>>> newFamilyHFilePathsList = new ArrayList<>();
     newFamilyHFilePathsList.add(newFamilyHFilePathsPair);
     bulkLoadHFileMap.put(tableName, newFamilyHFilePathsList);
   }
@@ -327,12 +324,12 @@ public class ReplicationSink {
   private <K1, K2, V> List<V> addToHashMultiMap(Map<K1, Map<K2,List<V>>> map, K1 key1, K2 key2, V value) {
     Map<K2,List<V>> innerMap = map.get(key1);
     if (innerMap == null) {
-      innerMap = new HashMap<K2, List<V>>();
+      innerMap = new HashMap<>();
       map.put(key1, innerMap);
     }
     List<V> values = innerMap.get(key2);
     if (values == null) {
-      values = new ArrayList<V>();
+      values = new ArrayList<>();
       innerMap.put(key2, values);
     }
     values.add(value);
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSource.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSource.java
index d3f6d35..72da9bd 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSource.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSource.java
@@ -94,8 +94,7 @@ public class ReplicationSource extends Thread implements ReplicationSourceInterf
   private static final Log LOG = LogFactory.getLog(ReplicationSource.class);
   // Queues of logs to process, entry in format of walGroupId->queue,
   // each presents a queue for one wal group
-  private Map<String, PriorityBlockingQueue<Path>> queues =
-      new HashMap<String, PriorityBlockingQueue<Path>>();
+  private Map<String, PriorityBlockingQueue<Path>> queues = new HashMap<>();
   // per group queue size, keep no more than this number of logs in each wal group
   private int queueSizePerGroup;
   private ReplicationQueues replicationQueues;
@@ -140,8 +139,7 @@ public class ReplicationSource extends Thread implements ReplicationSourceInterf
   private ReplicationThrottler throttler;
   private long defaultBandwidth;
   private long currentBandwidth;
-  private ConcurrentHashMap<String, ReplicationSourceShipperThread> workerThreads =
-      new ConcurrentHashMap<String, ReplicationSourceShipperThread>();
+  private ConcurrentHashMap<String, ReplicationSourceShipperThread> workerThreads = new ConcurrentHashMap<>();
 
   private AtomicLong totalBufferUsed;
 
@@ -209,7 +207,7 @@ public class ReplicationSource extends Thread implements ReplicationSourceInterf
     String logPrefix = AbstractFSWALProvider.getWALPrefixFromWALName(log.getName());
     PriorityBlockingQueue<Path> queue = queues.get(logPrefix);
     if (queue == null) {
-      queue = new PriorityBlockingQueue<Path>(queueSizePerGroup, new LogsComparator());
+      queue = new PriorityBlockingQueue<>(queueSizePerGroup, new LogsComparator());
       queues.put(logPrefix, queue);
       if (this.sourceRunning) {
         // new wal group observed after source startup, start a new worker thread to track it
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSourceManager.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSourceManager.java
index 5cb7d75..a38e264 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSourceManager.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSourceManager.java
@@ -147,14 +147,14 @@ public class ReplicationSourceManager implements ReplicationListener {
       final Path oldLogDir, final UUID clusterId) throws IOException {
     //CopyOnWriteArrayList is thread-safe.
     //Generally, reading is more than modifying.
-    this.sources = new CopyOnWriteArrayList<ReplicationSourceInterface>();
+    this.sources = new CopyOnWriteArrayList<>();
     this.replicationQueues = replicationQueues;
     this.replicationPeers = replicationPeers;
     this.replicationTracker = replicationTracker;
     this.server = server;
-    this.walsById = new HashMap<String, Map<String, SortedSet<String>>>();
-    this.walsByIdRecoveredQueues = new ConcurrentHashMap<String, Map<String, SortedSet<String>>>();
-    this.oldsources = new CopyOnWriteArrayList<ReplicationSourceInterface>();
+    this.walsById = new HashMap<>();
+    this.walsByIdRecoveredQueues = new ConcurrentHashMap<>();
+    this.oldsources = new CopyOnWriteArrayList<>();
     this.conf = conf;
     this.fs = fs;
     this.logDir = logDir;
@@ -170,8 +170,7 @@ public class ReplicationSourceManager implements ReplicationListener {
     // use a short 100ms sleep since this could be done inline with a RS startup
     // even if we fail, other region servers can take care of it
     this.executor = new ThreadPoolExecutor(nbWorkers, nbWorkers,
-        100, TimeUnit.MILLISECONDS,
-        new LinkedBlockingQueue<Runnable>());
+        100, TimeUnit.MILLISECONDS, new LinkedBlockingQueue<>());
     ThreadFactoryBuilder tfb = new ThreadFactoryBuilder();
     tfb.setNameFormat("ReplicationExecutor-%d");
     tfb.setDaemon(true);
@@ -277,7 +276,7 @@ public class ReplicationSourceManager implements ReplicationListener {
           this.replicationPeers, server, id, this.clusterId, peerConfig, peer);
     synchronized (this.walsById) {
       this.sources.add(src);
-      Map<String, SortedSet<String>> walsByGroup = new HashMap<String, SortedSet<String>>();
+      Map<String, SortedSet<String>> walsByGroup = new HashMap<>();
       this.walsById.put(id, walsByGroup);
       // Add the latest wal to that source's queue
       synchronized (latestPaths) {
@@ -285,7 +284,7 @@ public class ReplicationSourceManager implements ReplicationListener {
           for (Path logPath : latestPaths) {
             String name = logPath.getName();
             String walPrefix = AbstractFSWALProvider.getWALPrefixFromWALName(name);
-            SortedSet<String> logs = new TreeSet<String>();
+            SortedSet<String> logs = new TreeSet<>();
             logs.add(name);
             walsByGroup.put(walPrefix, logs);
             try {
@@ -423,7 +422,7 @@ public class ReplicationSourceManager implements ReplicationListener {
         if (!existingPrefix) {
           // The new log belongs to a new group, add it into this peer
           LOG.debug("Start tracking logs for wal group " + logPrefix + " for peer " + peerId);
-          SortedSet<String> wals = new TreeSet<String>();
+          SortedSet<String> wals = new TreeSet<>();
           wals.add(logName);
           walsByPrefix.put(logPrefix, wals);
         }
@@ -570,8 +569,7 @@ public class ReplicationSourceManager implements ReplicationListener {
         + sources.size() + " and another "
         + oldsources.size() + " that were recovered");
     String terminateMessage = "Replication stream was removed by a user";
-    List<ReplicationSourceInterface> oldSourcesToDelete =
-        new ArrayList<ReplicationSourceInterface>();
+    List<ReplicationSourceInterface> oldSourcesToDelete = new ArrayList<>();
     // synchronized on oldsources to avoid adding recovered source for the to-be-removed peer
     // see NodeFailoverWorker.run
     synchronized (oldsources) {
@@ -589,7 +587,7 @@ public class ReplicationSourceManager implements ReplicationListener {
     LOG.info("Number of deleted recovered sources for " + id + ": "
         + oldSourcesToDelete.size());
     // Now look for the one on this cluster
-    List<ReplicationSourceInterface> srcToRemove = new ArrayList<ReplicationSourceInterface>();
+    List<ReplicationSourceInterface> srcToRemove = new ArrayList<>();
     // synchronize on replicationPeers to avoid adding source for the to-be-removed peer
     synchronized (this.replicationPeers) {
       for (ReplicationSourceInterface src : this.sources) {
@@ -735,13 +733,13 @@ public class ReplicationSourceManager implements ReplicationListener {
             continue;
           }
           // track sources in walsByIdRecoveredQueues
-          Map<String, SortedSet<String>> walsByGroup = new HashMap<String, SortedSet<String>>();
+          Map<String, SortedSet<String>> walsByGroup = new HashMap<>();
           walsByIdRecoveredQueues.put(peerId, walsByGroup);
           for (String wal : walsSet) {
             String walPrefix = AbstractFSWALProvider.getWALPrefixFromWALName(wal);
             SortedSet<String> wals = walsByGroup.get(walPrefix);
             if (wals == null) {
-              wals = new TreeSet<String>();
+              wals = new TreeSet<>();
               walsByGroup.put(walPrefix, wals);
             }
             wals.add(wal);
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/security/access/AccessControlLists.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/security/access/AccessControlLists.java
index f06330c..c1aad93 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/security/access/AccessControlLists.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/security/access/AccessControlLists.java
@@ -286,7 +286,7 @@ public class AccessControlLists {
             ACL_KEY_DELIMITER, columnName, ACL_KEY_DELIMITER,
             ACL_KEY_DELIMITER, columnName))));
 
-    Set<byte[]> qualifierSet = new TreeSet<byte[]>(Bytes.BYTES_COMPARATOR);
+    Set<byte[]> qualifierSet = new TreeSet<>(Bytes.BYTES_COMPARATOR);
     ResultScanner scanner = null;
     try {
       scanner = table.getScanner(scan);
@@ -384,8 +384,7 @@ public class AccessControlLists {
       throw new IOException("Can only load permissions from "+ACL_TABLE_NAME);
     }
 
-    Map<byte[], ListMultimap<String, TablePermission>> allPerms =
-        new TreeMap<byte[], ListMultimap<String, TablePermission>>(Bytes.BYTES_RAWCOMPARATOR);
+    Map<byte[], ListMultimap<String, TablePermission>> allPerms = new TreeMap<>(Bytes.BYTES_RAWCOMPARATOR);
 
     // do a full scan of _acl_ table
 
@@ -397,7 +396,7 @@ public class AccessControlLists {
       iScanner = aclRegion.getScanner(scan);
 
       while (true) {
-        List<Cell> row = new ArrayList<Cell>();
+        List<Cell> row = new ArrayList<>();
 
         boolean hasNext = iScanner.next(row);
         ListMultimap<String,TablePermission> perms = ArrayListMultimap.create();
@@ -436,8 +435,7 @@ public class AccessControlLists {
    */
   static Map<byte[], ListMultimap<String,TablePermission>> loadAll(
       Configuration conf) throws IOException {
-    Map<byte[], ListMultimap<String,TablePermission>> allPerms =
-        new TreeMap<byte[], ListMultimap<String,TablePermission>>(Bytes.BYTES_RAWCOMPARATOR);
+    Map<byte[], ListMultimap<String,TablePermission>> allPerms = new TreeMap<>(Bytes.BYTES_RAWCOMPARATOR);
 
     // do a full scan of _acl_, filtering on only first table region rows
 
@@ -530,7 +528,7 @@ public class AccessControlLists {
     ListMultimap<String,TablePermission> allPerms = getPermissions(
         conf, entryName, null);
 
-    List<UserPermission> perms = new ArrayList<UserPermission>();
+    List<UserPermission> perms = new ArrayList<>();
 
     if(isNamespaceEntry(entryName)) {  // Namespace
       for (Map.Entry<String, TablePermission> entry : allPerms.entries()) {
@@ -591,8 +589,7 @@ public class AccessControlLists {
 
     //Handle namespace entry
     if(isNamespaceEntry(entryName)) {
-      return new Pair<String, TablePermission>(username,
-          new TablePermission(Bytes.toString(fromNamespaceEntry(entryName)), value));
+      return new Pair<>(username, new TablePermission(Bytes.toString(fromNamespaceEntry(entryName)), value));
     }
 
     //Handle table and global entry
@@ -612,8 +609,7 @@ public class AccessControlLists {
       }
     }
 
-    return new Pair<String,TablePermission>(username,
-        new TablePermission(TableName.valueOf(entryName), permFamily, permQualifier, value));
+    return new Pair<>(username, new TablePermission(TableName.valueOf(entryName), permFamily, permQualifier, value));
   }
 
   /**
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/security/access/AccessController.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/security/access/AccessController.java
index 814f209..64ac900 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/security/access/AccessController.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/security/access/AccessController.java
@@ -259,8 +259,7 @@ public class AccessController implements MasterObserver, RegionObserver, RegionS
    */
   void updateACL(RegionCoprocessorEnvironment e,
       final Map<byte[], List<Cell>> familyMap) {
-    Set<byte[]> entries =
-        new TreeSet<byte[]>(Bytes.BYTES_RAWCOMPARATOR);
+    Set<byte[]> entries = new TreeSet<>(Bytes.BYTES_RAWCOMPARATOR);
     for (Map.Entry<byte[], List<Cell>> f : familyMap.entrySet()) {
       List<Cell> cells = f.getValue();
       for (Cell cell: cells) {
@@ -793,7 +792,7 @@ public class AccessController implements MasterObserver, RegionObserver, RegionS
     // This Map is identical to familyMap. The key is a BR rather than byte[].
     // It will be easy to do gets over this new Map as we can create get keys over the Cell cf by
     // new SimpleByteRange(cell.familyArray, cell.familyOffset, cell.familyLen)
-    Map<ByteRange, List<Cell>> familyMap1 = new HashMap<ByteRange, List<Cell>>();
+    Map<ByteRange, List<Cell>> familyMap1 = new HashMap<>();
     for (Entry<byte[], ? extends Collection<?>> entry : familyMap.entrySet()) {
       if (entry.getValue() instanceof List) {
         familyMap1.put(new SimpleMutableByteRange(entry.getKey()), (List<Cell>) entry.getValue());
@@ -882,7 +881,7 @@ public class AccessController implements MasterObserver, RegionObserver, RegionS
       List<Cell> newCells = Lists.newArrayList();
       for (Cell cell: e.getValue()) {
         // Prepend the supplied perms in a new ACL tag to an update list of tags for the cell
-        List<Tag> tags = new ArrayList<Tag>();
+        List<Tag> tags = new ArrayList<>();
         tags.add(new ArrayBackedTag(AccessControlLists.ACL_TAG_TYPE, perms));
         Iterator<Tag> tagIterator = CellUtil.tagsIterator(cell);
         while (tagIterator.hasNext()) {
@@ -990,7 +989,7 @@ public class AccessController implements MasterObserver, RegionObserver, RegionS
   public void preCreateTable(ObserverContext<MasterCoprocessorEnvironment> c,
       HTableDescriptor desc, HRegionInfo[] regions) throws IOException {
     Set<byte[]> families = desc.getFamiliesKeys();
-    Map<byte[], Set<byte[]>> familyMap = new TreeMap<byte[], Set<byte[]>>(Bytes.BYTES_COMPARATOR);
+    Map<byte[], Set<byte[]>> familyMap = new TreeMap<>(Bytes.BYTES_COMPARATOR);
     for (byte[] family: families) {
       familyMap.put(family, null);
     }
@@ -2407,8 +2406,7 @@ public class AccessController implements MasterObserver, RegionObserver, RegionS
                   tperm.getTableName()));
             }
 
-            Map<byte[], Set<byte[]>> familyMap =
-                new TreeMap<byte[], Set<byte[]>>(Bytes.BYTES_COMPARATOR);
+            Map<byte[], Set<byte[]>> familyMap = new TreeMap<>(Bytes.BYTES_COMPARATOR);
             if (tperm.getFamily() != null) {
               if (tperm.getQualifier() != null) {
                 Set<byte[]> qualifiers = Sets.newTreeSet(Bytes.BYTES_COMPARATOR);
@@ -2515,7 +2513,7 @@ public class AccessController implements MasterObserver, RegionObserver, RegionS
       return null;
     }
 
-    Map<byte[], Collection<byte[]>> familyMap = new TreeMap<byte[], Collection<byte[]>>(Bytes.BYTES_COMPARATOR);
+    Map<byte[], Collection<byte[]>> familyMap = new TreeMap<>(Bytes.BYTES_COMPARATOR);
     familyMap.put(family, qualifier != null ? ImmutableSet.of(qualifier) : null);
     return familyMap;
   }
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/security/access/TableAuthManager.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/security/access/TableAuthManager.java
index eae9e4e..0d539ce 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/security/access/TableAuthManager.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/security/access/TableAuthManager.java
@@ -103,10 +103,10 @@ public class TableAuthManager implements Closeable {
   private volatile PermissionCache<Permission> globalCache;
 
   private ConcurrentSkipListMap<TableName, PermissionCache<TablePermission>> tableCache =
-      new ConcurrentSkipListMap<TableName, PermissionCache<TablePermission>>();
+      new ConcurrentSkipListMap<>();
 
   private ConcurrentSkipListMap<String, PermissionCache<TablePermission>> nsCache =
-    new ConcurrentSkipListMap<String, PermissionCache<TablePermission>>();
+    new ConcurrentSkipListMap<>();
 
   private Configuration conf;
   private ZKPermissionWatcher zkperms;
@@ -143,7 +143,7 @@ public class TableAuthManager implements Closeable {
       throw new IOException("Unable to obtain the current user, " +
           "authorization checks for internal operations will not work correctly!");
     }
-    PermissionCache<Permission> newCache = new PermissionCache<Permission>();
+    PermissionCache<Permission> newCache = new PermissionCache<>();
     String currentUser = user.getShortName();
 
     // the system user is always included
@@ -239,7 +239,7 @@ public class TableAuthManager implements Closeable {
    */
   private void updateTableCache(TableName table,
                                 ListMultimap<String,TablePermission> tablePerms) {
-    PermissionCache<TablePermission> newTablePerms = new PermissionCache<TablePermission>();
+    PermissionCache<TablePermission> newTablePerms = new PermissionCache<>();
 
     for (Map.Entry<String,TablePermission> entry : tablePerms.entries()) {
       if (AuthUtil.isGroupPrincipal(entry.getKey())) {
@@ -263,7 +263,7 @@ public class TableAuthManager implements Closeable {
    */
   private void updateNsCache(String namespace,
                              ListMultimap<String, TablePermission> tablePerms) {
-    PermissionCache<TablePermission> newTablePerms = new PermissionCache<TablePermission>();
+    PermissionCache<TablePermission> newTablePerms = new PermissionCache<>();
 
     for (Map.Entry<String, TablePermission> entry : tablePerms.entries()) {
       if (AuthUtil.isGroupPrincipal(entry.getKey())) {
@@ -734,8 +734,7 @@ public class TableAuthManager implements Closeable {
     return mtime.get();
   }
 
-  private static Map<ZooKeeperWatcher,TableAuthManager> managerMap =
-    new HashMap<ZooKeeperWatcher,TableAuthManager>();
+  private static Map<ZooKeeperWatcher,TableAuthManager> managerMap = new HashMap<>();
 
   private static Map<TableAuthManager, Integer> refCount = new HashMap<>();
 
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/security/access/ZKPermissionWatcher.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/security/access/ZKPermissionWatcher.java
index f21e877..3324b90 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/security/access/ZKPermissionWatcher.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/security/access/ZKPermissionWatcher.java
@@ -58,8 +58,7 @@ public class ZKPermissionWatcher extends ZooKeeperListener implements Closeable
   TableAuthManager authManager;
   String aclZNode;
   CountDownLatch initialized = new CountDownLatch(1);
-  AtomicReference<List<ZKUtil.NodeAndData>> nodes =
-      new AtomicReference<List<ZKUtil.NodeAndData>>(null);
+  AtomicReference<List<ZKUtil.NodeAndData>> nodes = new AtomicReference<>(null);
   ExecutorService executor;
 
   public ZKPermissionWatcher(ZooKeeperWatcher watcher,
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/security/token/AuthenticationTokenSecretManager.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/security/token/AuthenticationTokenSecretManager.java
index 26448b1..a569cf3 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/security/token/AuthenticationTokenSecretManager.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/security/token/AuthenticationTokenSecretManager.java
@@ -70,8 +70,7 @@ public class AuthenticationTokenSecretManager
   private LeaderElector leaderElector;
   private ZKClusterId clusterId;
 
-  private Map<Integer,AuthenticationKey> allKeys =
-      new ConcurrentHashMap<Integer, AuthenticationKey>();
+  private Map<Integer,AuthenticationKey> allKeys = new ConcurrentHashMap<>();
   private AuthenticationKey currentKey;
 
   private int idSeq;
@@ -181,8 +180,7 @@ public class AuthenticationTokenSecretManager
   public Token<AuthenticationTokenIdentifier> generateToken(String username) {
     AuthenticationTokenIdentifier ident =
         new AuthenticationTokenIdentifier(username);
-    Token<AuthenticationTokenIdentifier> token =
-        new Token<AuthenticationTokenIdentifier>(ident, this);
+    Token<AuthenticationTokenIdentifier> token = new Token<>(ident, this);
     if (clusterId.hasId()) {
       token.setService(new Text(clusterId.getId()));
     }
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/security/token/TokenUtil.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/security/token/TokenUtil.java
index f767ed3..1d42450 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/security/token/TokenUtil.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/security/token/TokenUtil.java
@@ -314,7 +314,7 @@ public class TokenUtil {
    * @return the Token instance
    */
   public static Token<AuthenticationTokenIdentifier> toToken(AuthenticationProtos.Token proto) {
-    return new Token<AuthenticationTokenIdentifier>(
+    return new Token<>(
         proto.hasIdentifier() ? proto.getIdentifier().toByteArray() : null,
         proto.hasPassword() ? proto.getPassword().toByteArray() : null,
         AuthenticationTokenIdentifier.AUTH_TOKEN_TYPE,
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/security/visibility/DefaultVisibilityLabelServiceImpl.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/security/visibility/DefaultVisibilityLabelServiceImpl.java
index 9abb3a2..d4a5627 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/security/visibility/DefaultVisibilityLabelServiceImpl.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/security/visibility/DefaultVisibilityLabelServiceImpl.java
@@ -151,10 +151,10 @@ public class DefaultVisibilityLabelServiceImpl implements VisibilityLabelService
   protected List<List<Cell>> getExistingLabelsWithAuths() throws IOException {
     Scan scan = new Scan();
     RegionScanner scanner = labelsRegion.getScanner(scan);
-    List<List<Cell>> existingLabels = new ArrayList<List<Cell>>();
+    List<List<Cell>> existingLabels = new ArrayList<>();
     try {
       while (true) {
-        List<Cell> cells = new ArrayList<Cell>();
+        List<Cell> cells = new ArrayList<>();
         scanner.next(cells);
         if (cells.isEmpty()) {
           break;
@@ -169,8 +169,8 @@ public class DefaultVisibilityLabelServiceImpl implements VisibilityLabelService
 
   protected Pair<Map<String, Integer>, Map<String, List<Integer>>> extractLabelsAndAuths(
       List<List<Cell>> labelDetails) {
-    Map<String, Integer> labels = new HashMap<String, Integer>();
-    Map<String, List<Integer>> userAuths = new HashMap<String, List<Integer>>();
+    Map<String, Integer> labels = new HashMap<>();
+    Map<String, List<Integer>> userAuths = new HashMap<>();
     for (List<Cell> cells : labelDetails) {
       for (Cell cell : cells) {
         if (CellUtil.matchingQualifier(cell, LABEL_QUALIFIER)) {
@@ -183,14 +183,14 @@ public class DefaultVisibilityLabelServiceImpl implements VisibilityLabelService
               cell.getQualifierLength());
           List<Integer> auths = userAuths.get(user);
           if (auths == null) {
-            auths = new ArrayList<Integer>();
+            auths = new ArrayList<>();
             userAuths.put(user, auths);
           }
           auths.add(CellUtil.getRowAsInt(cell));
         }
       }
     }
-    return new Pair<Map<String, Integer>, Map<String, List<Integer>>>(labels, userAuths);
+    return new Pair<>(labels, userAuths);
   }
 
   protected void addSystemLabel(Region region, Map<String, Integer> labels,
@@ -207,7 +207,7 @@ public class DefaultVisibilityLabelServiceImpl implements VisibilityLabelService
   public OperationStatus[] addLabels(List<byte[]> labels) throws IOException {
     assert labelsRegion != null;
     OperationStatus[] finalOpStatus = new OperationStatus[labels.size()];
-    List<Mutation> puts = new ArrayList<Mutation>(labels.size());
+    List<Mutation> puts = new ArrayList<>(labels.size());
     int i = 0;
     for (byte[] label : labels) {
       String labelStr = Bytes.toString(label);
@@ -235,7 +235,7 @@ public class DefaultVisibilityLabelServiceImpl implements VisibilityLabelService
   public OperationStatus[] setAuths(byte[] user, List<byte[]> authLabels) throws IOException {
     assert labelsRegion != null;
     OperationStatus[] finalOpStatus = new OperationStatus[authLabels.size()];
-    List<Mutation> puts = new ArrayList<Mutation>(authLabels.size());
+    List<Mutation> puts = new ArrayList<>(authLabels.size());
     int i = 0;
     for (byte[] auth : authLabels) {
       String authStr = Bytes.toString(auth);
@@ -269,7 +269,7 @@ public class DefaultVisibilityLabelServiceImpl implements VisibilityLabelService
     else {
       currentAuths = this.getUserAuths(user, true);
     }
-    List<Mutation> deletes = new ArrayList<Mutation>(authLabels.size());
+    List<Mutation> deletes = new ArrayList<>(authLabels.size());
     int i = 0;
     for (byte[] authLabel : authLabels) {
       String authLabelStr = Bytes.toString(authLabel);
@@ -334,10 +334,10 @@ public class DefaultVisibilityLabelServiceImpl implements VisibilityLabelService
     Filter filter = VisibilityUtils.createVisibilityLabelFilter(this.labelsRegion,
         new Authorizations(SYSTEM_LABEL));
     s.setFilter(filter);
-    ArrayList<String> auths = new ArrayList<String>();
+    ArrayList<String> auths = new ArrayList<>();
     RegionScanner scanner = this.labelsRegion.getScanner(s);
     try {
-      List<Cell> results = new ArrayList<Cell>(1);
+      List<Cell> results = new ArrayList<>(1);
       while (true) {
         scanner.next(results);
         if (results.isEmpty()) break;
@@ -371,10 +371,10 @@ public class DefaultVisibilityLabelServiceImpl implements VisibilityLabelService
     Filter filter = VisibilityUtils.createVisibilityLabelFilter(this.labelsRegion,
         new Authorizations(SYSTEM_LABEL));
     s.setFilter(filter);
-    Set<String> auths = new HashSet<String>();
+    Set<String> auths = new HashSet<>();
     RegionScanner scanner = this.labelsRegion.getScanner(s);
     try {
-      List<Cell> results = new ArrayList<Cell>(1);
+      List<Cell> results = new ArrayList<>(1);
       while (true) {
         scanner.next(results);
         if (results.isEmpty()) break;
@@ -389,7 +389,7 @@ public class DefaultVisibilityLabelServiceImpl implements VisibilityLabelService
     } finally {
       scanner.close();
     }
-    return new ArrayList<String>(auths);
+    return new ArrayList<>(auths);
   }
 
   @Override
@@ -401,7 +401,7 @@ public class DefaultVisibilityLabelServiceImpl implements VisibilityLabelService
     labels.remove(SYSTEM_LABEL);
     if (regex != null) {
       Pattern pattern = Pattern.compile(regex);
-      ArrayList<String> matchedLabels = new ArrayList<String>();
+      ArrayList<String> matchedLabels = new ArrayList<>();
       for (String label : labels.keySet()) {
         if (pattern.matcher(label).matches()) {
           matchedLabels.add(label);
@@ -409,13 +409,13 @@ public class DefaultVisibilityLabelServiceImpl implements VisibilityLabelService
       }
       return matchedLabels;
     }
-    return new ArrayList<String>(labels.keySet());
+    return new ArrayList<>(labels.keySet());
   }
 
   @Override
   public List<Tag> createVisibilityExpTags(String visExpression, boolean withSerializationFormat,
       boolean checkAuths) throws IOException {
-    Set<Integer> auths = new HashSet<Integer>();
+    Set<Integer> auths = new HashSet<>();
     if (checkAuths) {
       User user = VisibilityUtils.getActiveUser();
       auths.addAll(this.labelsCache.getUserAuthsAsOrdinals(user.getShortName()));
@@ -461,7 +461,7 @@ public class DefaultVisibilityLabelServiceImpl implements VisibilityLabelService
       try {
         // null authorizations to be handled inside SLG impl.
         authLabels = scanLabelGenerator.getLabels(VisibilityUtils.getActiveUser(), authorizations);
-        authLabels = (authLabels == null) ? new ArrayList<String>() : authLabels;
+        authLabels = (authLabels == null) ? new ArrayList<>() : authLabels;
         authorizations = new Authorizations(authLabels);
       } catch (Throwable t) {
         LOG.error(t);
@@ -605,7 +605,7 @@ public class DefaultVisibilityLabelServiceImpl implements VisibilityLabelService
   }
 
   private static List<List<Integer>> sortTagsBasedOnOrdinal(List<Tag> tags) throws IOException {
-    List<List<Integer>> fullTagsList = new ArrayList<List<Integer>>();
+    List<List<Integer>> fullTagsList = new ArrayList<>();
     for (Tag tag : tags) {
       if (tag.getType() == VISIBILITY_TAG_TYPE) {
         getSortedTagOrdinals(fullTagsList, tag);
@@ -616,7 +616,7 @@ public class DefaultVisibilityLabelServiceImpl implements VisibilityLabelService
 
   private static void getSortedTagOrdinals(List<List<Integer>> fullTagsList, Tag tag)
       throws IOException {
-    List<Integer> tagsOrdinalInSortedOrder = new ArrayList<Integer>();
+    List<Integer> tagsOrdinalInSortedOrder = new ArrayList<>();
     int offset = tag.getValueOffset();
     int endOffset = offset + tag.getValueLength();
     while (offset < endOffset) {
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/security/visibility/DefinedSetFilterScanLabelGenerator.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/security/visibility/DefinedSetFilterScanLabelGenerator.java
index 2c7d253..2126ee7 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/security/visibility/DefinedSetFilterScanLabelGenerator.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/security/visibility/DefinedSetFilterScanLabelGenerator.java
@@ -62,18 +62,18 @@ public class DefinedSetFilterScanLabelGenerator implements ScanLabelGenerator {
     if (authorizations != null) {
       List<String> labels = authorizations.getLabels();
       String userName = user.getShortName();
-      Set<String> auths = new HashSet<String>();
+      Set<String> auths = new HashSet<>();
       auths.addAll(this.labelsCache.getUserAuths(userName));
       auths.addAll(this.labelsCache.getGroupAuths(user.getGroupNames()));
-      return dropLabelsNotInUserAuths(labels, new ArrayList<String>(auths), userName);
+      return dropLabelsNotInUserAuths(labels, new ArrayList<>(auths), userName);
     }
     return null;
   }
 
   private List<String> dropLabelsNotInUserAuths(List<String> labels, List<String> auths,
       String userName) {
-    List<String> droppedLabels = new ArrayList<String>();
-    List<String> passedLabels = new ArrayList<String>(labels.size());
+    List<String> droppedLabels = new ArrayList<>();
+    List<String> passedLabels = new ArrayList<>(labels.size());
     for (String label : labels) {
       if (auths.contains(label)) {
         passedLabels.add(label);
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/security/visibility/EnforcingScanLabelGenerator.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/security/visibility/EnforcingScanLabelGenerator.java
index dd0497c..177f4d2 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/security/visibility/EnforcingScanLabelGenerator.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/security/visibility/EnforcingScanLabelGenerator.java
@@ -62,10 +62,10 @@ public class EnforcingScanLabelGenerator implements ScanLabelGenerator {
     if (authorizations != null) {
       LOG.warn("Dropping authorizations requested by user " + userName + ": " + authorizations);
     }
-    Set<String> auths = new HashSet<String>();
+    Set<String> auths = new HashSet<>();
     auths.addAll(this.labelsCache.getUserAuths(userName));
     auths.addAll(this.labelsCache.getGroupAuths(user.getGroupNames()));
-    return new ArrayList<String>(auths);
+    return new ArrayList<>(auths);
   }
 
 }
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/security/visibility/ExpressionParser.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/security/visibility/ExpressionParser.java
index db3caff..2b9a56e 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/security/visibility/ExpressionParser.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/security/visibility/ExpressionParser.java
@@ -40,7 +40,7 @@ public class ExpressionParser {
   private static final char DOUBLE_QUOTES = '"';
   public ExpressionNode parse(String expS) throws ParseException {
     expS = expS.trim();
-    Stack<ExpressionNode> expStack = new Stack<ExpressionNode>();
+    Stack<ExpressionNode> expStack = new Stack<>();
     int index = 0;
     byte[] exp = Bytes.toBytes(expS);
     int endPos = exp.length;
@@ -68,7 +68,7 @@ public class ExpressionParser {
           // We have to rewrite the expression within double quotes as incase of expressions 
           // with escape characters we may have to avoid them as the original expression did
           // not have them
-          List<Byte> list = new ArrayList<Byte>();
+          List<Byte> list = new ArrayList<>();
           while (index < endPos && !endDoubleQuotesFound(exp[index])) {
             if (exp[index] == '\\') {
               index++;
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/security/visibility/FeedUserAuthScanLabelGenerator.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/security/visibility/FeedUserAuthScanLabelGenerator.java
index 1f90682..f4cf762 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/security/visibility/FeedUserAuthScanLabelGenerator.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/security/visibility/FeedUserAuthScanLabelGenerator.java
@@ -65,10 +65,10 @@ public class FeedUserAuthScanLabelGenerator implements ScanLabelGenerator {
     if (authorizations == null || authorizations.getLabels() == null
         || authorizations.getLabels().isEmpty()) {
       String userName = user.getShortName();
-      Set<String> auths = new HashSet<String>();
+      Set<String> auths = new HashSet<>();
       auths.addAll(this.labelsCache.getUserAuths(userName));
       auths.addAll(this.labelsCache.getGroupAuths(user.getGroupNames()));
-      return new ArrayList<String>(auths);
+      return new ArrayList<>(auths);
     }
     return authorizations.getLabels();
   }
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/security/visibility/VisibilityController.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/security/visibility/VisibilityController.java
index fb685bc..476921b 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/security/visibility/VisibilityController.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/security/visibility/VisibilityController.java
@@ -145,7 +145,7 @@ public class VisibilityController implements MasterObserver, RegionObserver,
   boolean authorizationEnabled;
 
   // Add to this list if there are any reserved tag types
-  private static ArrayList<Byte> RESERVED_VIS_TAG_TYPES = new ArrayList<Byte>();
+  private static ArrayList<Byte> RESERVED_VIS_TAG_TYPES = new ArrayList<>();
   static {
     RESERVED_VIS_TAG_TYPES.add(TagType.VISIBILITY_TAG_TYPE);
     RESERVED_VIS_TAG_TYPES.add(TagType.VISIBILITY_EXP_SERIALIZATION_FORMAT_TAG_TYPE);
@@ -328,7 +328,7 @@ public class VisibilityController implements MasterObserver, RegionObserver,
       return;
     }
     // TODO this can be made as a global LRU cache at HRS level?
-    Map<String, List<Tag>> labelCache = new HashMap<String, List<Tag>>();
+    Map<String, List<Tag>> labelCache = new HashMap<>();
     for (int i = 0; i < miniBatchOp.size(); i++) {
       Mutation m = miniBatchOp.getOperation(i);
       CellVisibility cellVisibility = null;
@@ -341,7 +341,7 @@ public class VisibilityController implements MasterObserver, RegionObserver,
       }
       boolean sanityFailure = false;
       boolean modifiedTagFound = false;
-      Pair<Boolean, Tag> pair = new Pair<Boolean, Tag>(false, null);
+      Pair<Boolean, Tag> pair = new Pair<>(false, null);
       for (CellScanner cellScanner = m.cellScanner(); cellScanner.advance();) {
         pair = checkForReservedVisibilityTagPresence(cellScanner.current(), pair);
         if (!pair.getFirst()) {
@@ -381,7 +381,7 @@ public class VisibilityController implements MasterObserver, RegionObserver,
             }
           }
           if (visibilityTags != null) {
-            List<Cell> updatedCells = new ArrayList<Cell>();
+            List<Cell> updatedCells = new ArrayList<>();
             for (CellScanner cellScanner = m.cellScanner(); cellScanner.advance();) {
               Cell cell = cellScanner.current();
               List<Tag> tags = CellUtil.getTags(cell);
@@ -427,7 +427,7 @@ public class VisibilityController implements MasterObserver, RegionObserver,
     }
     // The check for checkForReservedVisibilityTagPresence happens in preBatchMutate happens.
     // It happens for every mutation and that would be enough.
-    List<Tag> visibilityTags = new ArrayList<Tag>();
+    List<Tag> visibilityTags = new ArrayList<>();
     if (cellVisibility != null) {
       String labelsExp = cellVisibility.getExpression();
       try {
@@ -474,7 +474,7 @@ public class VisibilityController implements MasterObserver, RegionObserver,
   private Pair<Boolean, Tag> checkForReservedVisibilityTagPresence(Cell cell,
       Pair<Boolean, Tag> pair) throws IOException {
     if (pair == null) {
-      pair = new Pair<Boolean, Tag>(false, null);
+      pair = new Pair<>(false, null);
     } else {
       pair.setFirst(false);
       pair.setSecond(null);
@@ -782,7 +782,7 @@ public class VisibilityController implements MasterObserver, RegionObserver,
         new VisibilityControllerNotReadyException("VisibilityController not yet initialized!"),
         response);
     } else {
-      List<byte[]> labels = new ArrayList<byte[]>(visLabels.size());
+      List<byte[]> labels = new ArrayList<>(visLabels.size());
       try {
         if (authorizationEnabled) {
           checkCallingUserAuth();
@@ -844,7 +844,7 @@ public class VisibilityController implements MasterObserver, RegionObserver,
         response);
     } else {
       byte[] user = request.getUser().toByteArray();
-      List<byte[]> labelAuths = new ArrayList<byte[]>(auths.size());
+      List<byte[]> labelAuths = new ArrayList<>(auths.size());
       try {
         if (authorizationEnabled) {
           checkCallingUserAuth();
@@ -959,7 +959,7 @@ public class VisibilityController implements MasterObserver, RegionObserver,
           "VisibilityController not yet initialized"), response);
     } else {
       byte[] requestUser = request.getUser().toByteArray();
-      List<byte[]> labelAuths = new ArrayList<byte[]>(auths.size());
+      List<byte[]> labelAuths = new ArrayList<>(auths.size());
       try {
         // When AC is ON, do AC based user auth check
         if (authorizationEnabled && accessControllerAvailable && !isSystemOrSuperUser()) {
@@ -1071,7 +1071,7 @@ public class VisibilityController implements MasterObserver, RegionObserver,
 
     @Override
     public ReturnCode filterKeyValue(Cell cell) throws IOException {
-      List<Tag> putVisTags = new ArrayList<Tag>();
+      List<Tag> putVisTags = new ArrayList<>();
       Byte putCellVisTagsFormat = VisibilityUtils.extractVisibilityTags(cell, putVisTags);
       boolean matchFound = VisibilityLabelServiceManager
           .getInstance().getVisibilityLabelService()
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/security/visibility/VisibilityLabelsCache.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/security/visibility/VisibilityLabelsCache.java
index 0948520..e27a4f8 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/security/visibility/VisibilityLabelsCache.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/security/visibility/VisibilityLabelsCache.java
@@ -54,10 +54,10 @@ public class VisibilityLabelsCache implements VisibilityLabelOrdinalProvider {
   private static VisibilityLabelsCache instance;
 
   private ZKVisibilityLabelWatcher zkVisibilityWatcher;
-  private Map<String, Integer> labels = new HashMap<String, Integer>();
-  private Map<Integer, String> ordinalVsLabels = new HashMap<Integer, String>();
-  private Map<String, Set<Integer>> userAuths = new HashMap<String, Set<Integer>>();
-  private Map<String, Set<Integer>> groupAuths = new HashMap<String, Set<Integer>>();
+  private Map<String, Integer> labels = new HashMap<>();
+  private Map<Integer, String> ordinalVsLabels = new HashMap<>();
+  private Map<String, Set<Integer>> userAuths = new HashMap<>();
+  private Map<String, Set<Integer>> groupAuths = new HashMap<>();
 
   /**
    * This covers the members labels, ordinalVsLabels and userAuths
@@ -145,10 +145,9 @@ public class VisibilityLabelsCache implements VisibilityLabelOrdinalProvider {
       for (UserAuthorizations userAuths : multiUserAuths.getUserAuthsList()) {
         String user = Bytes.toString(userAuths.getUser().toByteArray());
         if (AuthUtil.isGroupPrincipal(user)) {
-          this.groupAuths.put(AuthUtil.getGroupName(user),
-            new HashSet<Integer>(userAuths.getAuthList()));
+          this.groupAuths.put(AuthUtil.getGroupName(user), new HashSet<>(userAuths.getAuthList()));
         } else {
-          this.userAuths.put(user, new HashSet<Integer>(userAuths.getAuthList()));
+          this.userAuths.put(user, new HashSet<>(userAuths.getAuthList()));
         }
       }
     } finally {
@@ -210,7 +209,7 @@ public class VisibilityLabelsCache implements VisibilityLabelOrdinalProvider {
       List<String> auths = EMPTY_LIST;
       Set<Integer> authOrdinals = getUserAuthsAsOrdinals(user);
       if (!authOrdinals.equals(EMPTY_SET)) {
-        auths = new ArrayList<String>(authOrdinals.size());
+        auths = new ArrayList<>(authOrdinals.size());
         for (Integer authOrdinal : authOrdinals) {
           auths.add(ordinalVsLabels.get(authOrdinal));
         }
@@ -227,7 +226,7 @@ public class VisibilityLabelsCache implements VisibilityLabelOrdinalProvider {
       List<String> auths = EMPTY_LIST;
       Set<Integer> authOrdinals = getGroupAuthsAsOrdinals(groups);
       if (!authOrdinals.equals(EMPTY_SET)) {
-        auths = new ArrayList<String>(authOrdinals.size());
+        auths = new ArrayList<>(authOrdinals.size());
         for (Integer authOrdinal : authOrdinals) {
           auths.add(ordinalVsLabels.get(authOrdinal));
         }
@@ -263,7 +262,7 @@ public class VisibilityLabelsCache implements VisibilityLabelOrdinalProvider {
   public Set<Integer> getGroupAuthsAsOrdinals(String[] groups) {
     this.lock.readLock().lock();
     try {
-      Set<Integer> authOrdinals = new HashSet<Integer>();
+      Set<Integer> authOrdinals = new HashSet<>();
       if (groups != null && groups.length > 0) {
         Set<Integer> groupAuthOrdinals = null;
         for (String group : groups) {
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/security/visibility/VisibilityReplicationEndpoint.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/security/visibility/VisibilityReplicationEndpoint.java
index c1c3852..c77b776 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/security/visibility/VisibilityReplicationEndpoint.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/security/visibility/VisibilityReplicationEndpoint.java
@@ -69,9 +69,9 @@ public class VisibilityReplicationEndpoint implements ReplicationEndpoint {
       // string based tags. But for intra cluster replication like region
       // replicas it is not needed.
       List<Entry> entries = replicateContext.getEntries();
-      List<Tag> visTags = new ArrayList<Tag>();
-      List<Tag> nonVisTags = new ArrayList<Tag>();
-      List<Entry> newEntries = new ArrayList<Entry>(entries.size());
+      List<Tag> visTags = new ArrayList<>();
+      List<Tag> nonVisTags = new ArrayList<>();
+      List<Entry> newEntries = new ArrayList<>(entries.size());
       for (Entry entry : entries) {
         WALEdit newEdit = new WALEdit();
         ArrayList<Cell> cells = entry.getEdit().getCells();
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/security/visibility/VisibilityScanDeleteTracker.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/security/visibility/VisibilityScanDeleteTracker.java
index 4e27bbf..4fcedcf 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/security/visibility/VisibilityScanDeleteTracker.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/security/visibility/VisibilityScanDeleteTracker.java
@@ -53,17 +53,14 @@ public class VisibilityScanDeleteTracker extends ScanDeleteTracker {
   // type would solve this problem and also ensure that the combination of different type
   // of deletes with diff ts would also work fine
   // Track per TS
-  private List<Triple<List<Tag>, Byte, Long>> visibilityTagsDeleteFamily =
-      new ArrayList<Triple<List<Tag>, Byte, Long>>();
+  private List<Triple<List<Tag>, Byte, Long>> visibilityTagsDeleteFamily = new ArrayList<>();
   // Delete family version with different ts and different visibility expression could come.
   // Need to track it per ts.
-  private List<Triple<List<Tag>, Byte, Long>> visibilityTagsDeleteFamilyVersion =
-      new ArrayList<Triple<List<Tag>, Byte, Long>>();
+  private List<Triple<List<Tag>, Byte, Long>> visibilityTagsDeleteFamilyVersion = new ArrayList<>();
   private List<Pair<List<Tag>, Byte>> visibilityTagsDeleteColumns;
   // Tracking as List<List> is to handle same ts cell but different visibility tag. 
   // TODO : Need to handle puts with same ts but different vis tags.
-  private List<Pair<List<Tag>, Byte>> visiblityTagsDeleteColumnVersion =
-      new ArrayList<Pair<List<Tag>, Byte>>();
+  private List<Pair<List<Tag>, Byte>> visiblityTagsDeleteColumnVersion = new ArrayList<>();
 
   public VisibilityScanDeleteTracker() {
     super();
@@ -119,50 +116,46 @@ public class VisibilityScanDeleteTracker extends ScanDeleteTracker {
       Byte deleteCellVisTagsFormat = null;
       switch (type) {
       case DeleteFamily:
-        List<Tag> delTags = new ArrayList<Tag>();
+        List<Tag> delTags = new ArrayList<>();
         if (visibilityTagsDeleteFamily == null) {
-          visibilityTagsDeleteFamily = new ArrayList<Triple<List<Tag>, Byte, Long>>();
+          visibilityTagsDeleteFamily = new ArrayList<>();
         }
         deleteCellVisTagsFormat = VisibilityUtils.extractVisibilityTags(delCell, delTags);
         if (!delTags.isEmpty()) {
-          visibilityTagsDeleteFamily.add(new Triple<List<Tag>, Byte, Long>(delTags,
-              deleteCellVisTagsFormat, delCell.getTimestamp()));
+          visibilityTagsDeleteFamily.add(new Triple<>(delTags, deleteCellVisTagsFormat, delCell.getTimestamp()));
           hasVisTag = true;
         }
         break;
       case DeleteFamilyVersion:
         if(visibilityTagsDeleteFamilyVersion == null) {
-          visibilityTagsDeleteFamilyVersion = new ArrayList<Triple<List<Tag>, Byte, Long>>();
+          visibilityTagsDeleteFamilyVersion = new ArrayList<>();
         }
-        delTags = new ArrayList<Tag>();
+        delTags = new ArrayList<>();
         deleteCellVisTagsFormat = VisibilityUtils.extractVisibilityTags(delCell, delTags);
         if (!delTags.isEmpty()) {
-          visibilityTagsDeleteFamilyVersion.add(new Triple<List<Tag>, Byte, Long>(delTags,
-              deleteCellVisTagsFormat, delCell.getTimestamp()));
+          visibilityTagsDeleteFamilyVersion.add(new Triple<>(delTags, deleteCellVisTagsFormat, delCell.getTimestamp()));
           hasVisTag = true;
         }
         break;
       case DeleteColumn:
         if (visibilityTagsDeleteColumns == null) {
-          visibilityTagsDeleteColumns = new ArrayList<Pair<List<Tag>, Byte>>();
+          visibilityTagsDeleteColumns = new ArrayList<>();
         }
-        delTags = new ArrayList<Tag>();
+        delTags = new ArrayList<>();
         deleteCellVisTagsFormat = VisibilityUtils.extractVisibilityTags(delCell, delTags);
         if (!delTags.isEmpty()) {
-          visibilityTagsDeleteColumns.add(new Pair<List<Tag>, Byte>(delTags,
-              deleteCellVisTagsFormat));
+          visibilityTagsDeleteColumns.add(new Pair<>(delTags, deleteCellVisTagsFormat));
           hasVisTag = true;
         }
         break;
       case Delete:
         if (visiblityTagsDeleteColumnVersion == null) {
-          visiblityTagsDeleteColumnVersion = new ArrayList<Pair<List<Tag>, Byte>>();
+          visiblityTagsDeleteColumnVersion = new ArrayList<>();
         }
-        delTags = new ArrayList<Tag>();
+        delTags = new ArrayList<>();
         deleteCellVisTagsFormat = VisibilityUtils.extractVisibilityTags(delCell, delTags);
         if (!delTags.isEmpty()) {
-          visiblityTagsDeleteColumnVersion.add(new Pair<List<Tag>, Byte>(delTags,
-              deleteCellVisTagsFormat));
+          visiblityTagsDeleteColumnVersion.add(new Pair<>(delTags, deleteCellVisTagsFormat));
           hasVisTag = true;
         }
         break;
@@ -184,7 +177,7 @@ public class VisibilityScanDeleteTracker extends ScanDeleteTracker {
               // visibilityTagsDeleteFamily is ArrayList
               Triple<List<Tag>, Byte, Long> triple = visibilityTagsDeleteFamily.get(i);
               if (timestamp <= triple.getThird()) {
-                List<Tag> putVisTags = new ArrayList<Tag>();
+                List<Tag> putVisTags = new ArrayList<>();
                 Byte putCellVisTagsFormat = VisibilityUtils.extractVisibilityTags(cell, putVisTags);
                 boolean matchFound = VisibilityLabelServiceManager.getInstance()
                     .getVisibilityLabelService().matchVisibility(putVisTags, putCellVisTagsFormat,
@@ -220,7 +213,7 @@ public class VisibilityScanDeleteTracker extends ScanDeleteTracker {
               // visibilityTagsDeleteFamilyVersion is ArrayList
               Triple<List<Tag>, Byte, Long> triple = visibilityTagsDeleteFamilyVersion.get(i);
               if (timestamp == triple.getThird()) {
-                List<Tag> putVisTags = new ArrayList<Tag>();
+                List<Tag> putVisTags = new ArrayList<>();
                 Byte putCellVisTagsFormat = VisibilityUtils.extractVisibilityTags(cell, putVisTags);
                 boolean matchFound = VisibilityLabelServiceManager.getInstance()
                     .getVisibilityLabelService().matchVisibility(putVisTags, putCellVisTagsFormat,
@@ -250,7 +243,7 @@ public class VisibilityScanDeleteTracker extends ScanDeleteTracker {
             if (visibilityTagsDeleteColumns != null) {
               if (!visibilityTagsDeleteColumns.isEmpty()) {
                 for (Pair<List<Tag>, Byte> tags : visibilityTagsDeleteColumns) {
-                  List<Tag> putVisTags = new ArrayList<Tag>();
+                  List<Tag> putVisTags = new ArrayList<>();
                   Byte putCellVisTagsFormat =
                       VisibilityUtils.extractVisibilityTags(cell, putVisTags);
                   boolean matchFound = VisibilityLabelServiceManager.getInstance()
@@ -279,7 +272,7 @@ public class VisibilityScanDeleteTracker extends ScanDeleteTracker {
             if (visiblityTagsDeleteColumnVersion != null) {
               if (!visiblityTagsDeleteColumnVersion.isEmpty()) {
                 for (Pair<List<Tag>, Byte> tags : visiblityTagsDeleteColumnVersion) {
-                  List<Tag> putVisTags = new ArrayList<Tag>();
+                  List<Tag> putVisTags = new ArrayList<>();
                   Byte putCellVisTagsFormat =
                       VisibilityUtils.extractVisibilityTags(cell, putVisTags);
                   boolean matchFound = VisibilityLabelServiceManager.getInstance()
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/security/visibility/VisibilityUtils.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/security/visibility/VisibilityUtils.java
index 1db506d..4441c08 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/security/visibility/VisibilityUtils.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/security/visibility/VisibilityUtils.java
@@ -175,7 +175,7 @@ public class VisibilityUtils {
     String slgClassesCommaSeparated = conf.get(VISIBILITY_LABEL_GENERATOR_CLASS);
     // We have only System level SLGs now. The order of execution will be same as the order in the
     // comma separated config value
-    List<ScanLabelGenerator> slgs = new ArrayList<ScanLabelGenerator>();
+    List<ScanLabelGenerator> slgs = new ArrayList<>();
     if (StringUtils.isNotEmpty(slgClassesCommaSeparated)) {
       String[] slgClasses = slgClassesCommaSeparated.split(COMMA);
       for (String slgClass : slgClasses) {
@@ -266,7 +266,7 @@ public class VisibilityUtils {
 
   public static Filter createVisibilityLabelFilter(Region region, Authorizations authorizations)
       throws IOException {
-    Map<ByteRange, Integer> cfVsMaxVersions = new HashMap<ByteRange, Integer>();
+    Map<ByteRange, Integer> cfVsMaxVersions = new HashMap<>();
     for (HColumnDescriptor hcd : region.getTableDesc().getFamilies()) {
       cfVsMaxVersions.put(new SimpleMutableByteRange(hcd.getName()), hcd.getMaxVersions());
     }
@@ -302,10 +302,10 @@ public class VisibilityUtils {
       throw new IOException(e);
     }
     node = EXP_EXPANDER.expand(node);
-    List<Tag> tags = new ArrayList<Tag>();
+    List<Tag> tags = new ArrayList<>();
     ByteArrayOutputStream baos = new ByteArrayOutputStream();
     DataOutputStream dos = new DataOutputStream(baos);
-    List<Integer> labelOrdinals = new ArrayList<Integer>();
+    List<Integer> labelOrdinals = new ArrayList<>();
     // We will be adding this tag before the visibility tags and the presence of this
     // tag indicates we are supporting deletes with cell visibility
     if (withSerializationFormat) {
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/security/visibility/expression/NonLeafExpressionNode.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/security/visibility/expression/NonLeafExpressionNode.java
index 4399ecc..9903b9b 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/security/visibility/expression/NonLeafExpressionNode.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/security/visibility/expression/NonLeafExpressionNode.java
@@ -26,7 +26,7 @@ import org.apache.hadoop.hbase.classification.InterfaceAudience;
 @InterfaceAudience.Private
 public class NonLeafExpressionNode implements ExpressionNode {
   private Operator op;
-  private List<ExpressionNode> childExps = new ArrayList<ExpressionNode>(2);
+  private List<ExpressionNode> childExps = new ArrayList<>(2);
 
   public NonLeafExpressionNode() {
 
@@ -46,7 +46,7 @@ public class NonLeafExpressionNode implements ExpressionNode {
 
   public NonLeafExpressionNode(Operator op, ExpressionNode... exps) {
     this.op = op;
-    List<ExpressionNode> expLst = new ArrayList<ExpressionNode>();
+    List<ExpressionNode> expLst = new ArrayList<>();
     Collections.addAll(expLst, exps);
     this.childExps = expLst;
   }
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/snapshot/ExportSnapshot.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/snapshot/ExportSnapshot.java
index a5507fc..efae7e4 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/snapshot/ExportSnapshot.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/snapshot/ExportSnapshot.java
@@ -564,7 +564,7 @@ public class ExportSnapshot extends AbstractHBaseTool implements Tool {
       final FileSystem fs, final Path snapshotDir) throws IOException {
     SnapshotDescription snapshotDesc = SnapshotDescriptionUtils.readSnapshotInfo(fs, snapshotDir);
 
-    final List<Pair<SnapshotFileInfo, Long>> files = new ArrayList<Pair<SnapshotFileInfo, Long>>();
+    final List<Pair<SnapshotFileInfo, Long>> files = new ArrayList<>();
     final TableName table = TableName.valueOf(snapshotDesc.getTable());
 
     // Get snapshot files
@@ -591,7 +591,7 @@ public class ExportSnapshot extends AbstractHBaseTool implements Tool {
             } else {
               size = HFileLink.buildFromHFileLinkPattern(conf, path).getFileStatus(fs).getLen();
             }
-            files.add(new Pair<SnapshotFileInfo, Long>(fileInfo, size));
+            files.add(new Pair<>(fileInfo, size));
           }
         }
     });
@@ -618,8 +618,7 @@ public class ExportSnapshot extends AbstractHBaseTool implements Tool {
     });
 
     // create balanced groups
-    List<List<Pair<SnapshotFileInfo, Long>>> fileGroups =
-      new LinkedList<List<Pair<SnapshotFileInfo, Long>>>();
+    List<List<Pair<SnapshotFileInfo, Long>>> fileGroups = new LinkedList<>();
     long[] sizeGroups = new long[ngroups];
     int hi = files.size() - 1;
     int lo = 0;
@@ -630,7 +629,7 @@ public class ExportSnapshot extends AbstractHBaseTool implements Tool {
 
     while (hi >= lo) {
       if (g == fileGroups.size()) {
-        group = new LinkedList<Pair<SnapshotFileInfo, Long>>();
+        group = new LinkedList<>();
         fileGroups.add(group);
       } else {
         group = fileGroups.get(g);
@@ -703,7 +702,7 @@ public class ExportSnapshot extends AbstractHBaseTool implements Tool {
       public ExportSnapshotInputSplit(final List<Pair<SnapshotFileInfo, Long>> snapshotFiles) {
         this.files = new ArrayList(snapshotFiles.size());
         for (Pair<SnapshotFileInfo, Long> fileInfo: snapshotFiles) {
-          this.files.add(new Pair<BytesWritable, Long>(
+          this.files.add(new Pair<>(
             new BytesWritable(fileInfo.getFirst().toByteArray()), fileInfo.getSecond()));
           this.length += fileInfo.getSecond();
         }
@@ -726,13 +725,13 @@ public class ExportSnapshot extends AbstractHBaseTool implements Tool {
       @Override
       public void readFields(DataInput in) throws IOException {
         int count = in.readInt();
-        files = new ArrayList<Pair<BytesWritable, Long>>(count);
+        files = new ArrayList<>(count);
         length = 0;
         for (int i = 0; i < count; ++i) {
           BytesWritable fileInfo = new BytesWritable();
           fileInfo.readFields(in);
           long size = in.readLong();
-          files.add(new Pair<BytesWritable, Long>(fileInfo, size));
+          files.add(new Pair<>(fileInfo, size));
           length += size;
         }
       }
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/snapshot/RestoreSnapshotHelper.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/snapshot/RestoreSnapshotHelper.java
index 98afe8b..63839c4 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/snapshot/RestoreSnapshotHelper.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/snapshot/RestoreSnapshotHelper.java
@@ -108,11 +108,9 @@ import org.apache.hadoop.io.IOUtils;
 public class RestoreSnapshotHelper {
   private static final Log LOG = LogFactory.getLog(RestoreSnapshotHelper.class);
 
-  private final Map<byte[], byte[]> regionsMap =
-        new TreeMap<byte[], byte[]>(Bytes.BYTES_COMPARATOR);
+  private final Map<byte[], byte[]> regionsMap = new TreeMap<>(Bytes.BYTES_COMPARATOR);
 
-  private final Map<String, Pair<String, String> > parentsMap =
-      new HashMap<String, Pair<String, String> >();
+  private final Map<String, Pair<String, String> > parentsMap = new HashMap<>();
 
   private final ForeignExceptionDispatcher monitor;
   private final MonitoredTask status;
@@ -187,7 +185,7 @@ public class RestoreSnapshotHelper {
 
     // Take a copy of the manifest.keySet() since we are going to modify
     // this instance, by removing the regions already present in the restore dir.
-    Set<String> regionNames = new HashSet<String>(regionManifests.keySet());
+    Set<String> regionNames = new HashSet<>(regionManifests.keySet());
 
     HRegionInfo mobRegion = MobUtils.getMobRegionInfo(snapshotManifest.getTableDescriptor()
         .getTableName());
@@ -213,7 +211,7 @@ public class RestoreSnapshotHelper {
       status.setStatus("Restoring table regions...");
       if (regionNames.contains(mobRegion.getEncodedName())) {
         // restore the mob region in case
-        List<HRegionInfo> mobRegions = new ArrayList<HRegionInfo>(1);
+        List<HRegionInfo> mobRegions = new ArrayList<>(1);
         mobRegions.add(mobRegion);
         restoreHdfsMobRegions(exec, regionManifests, mobRegions);
         regionNames.remove(mobRegion.getEncodedName());
@@ -230,7 +228,7 @@ public class RestoreSnapshotHelper {
 
     // Regions to Add: present in the snapshot but not in the current table
     if (regionNames.size() > 0) {
-      List<HRegionInfo> regionsToAdd = new ArrayList<HRegionInfo>(regionNames.size());
+      List<HRegionInfo> regionsToAdd = new ArrayList<>(regionNames.size());
 
       monitor.rethrowException();
       // add the mob region
@@ -344,14 +342,14 @@ public class RestoreSnapshotHelper {
 
     void addRegionToRemove(final HRegionInfo hri) {
       if (regionsToRemove == null) {
-        regionsToRemove = new LinkedList<HRegionInfo>();
+        regionsToRemove = new LinkedList<>();
       }
       regionsToRemove.add(hri);
     }
 
     void addRegionToRestore(final HRegionInfo hri) {
       if (regionsToRestore == null) {
-        regionsToRestore = new LinkedList<HRegionInfo>();
+        regionsToRestore = new LinkedList<>();
       }
       regionsToRestore.add(hri);
     }
@@ -361,7 +359,7 @@ public class RestoreSnapshotHelper {
       if (regionInfos == null || parentsMap.isEmpty()) return;
 
       // Extract region names and offlined regions
-      Map<String, HRegionInfo> regionsByName = new HashMap<String, HRegionInfo>(regionInfos.size());
+      Map<String, HRegionInfo> regionsByName = new HashMap<>(regionInfos.size());
       List<HRegionInfo> parentRegions = new LinkedList<>();
       for (HRegionInfo regionInfo: regionInfos) {
         if (regionInfo.isSplitParent()) {
@@ -441,10 +439,10 @@ public class RestoreSnapshotHelper {
   private Map<String, List<SnapshotRegionManifest.StoreFile>> getRegionHFileReferences(
       final SnapshotRegionManifest manifest) {
     Map<String, List<SnapshotRegionManifest.StoreFile>> familyMap =
-      new HashMap<String, List<SnapshotRegionManifest.StoreFile>>(manifest.getFamilyFilesCount());
+      new HashMap<>(manifest.getFamilyFilesCount());
     for (SnapshotRegionManifest.FamilyFiles familyFiles: manifest.getFamilyFilesList()) {
       familyMap.put(familyFiles.getFamilyName().toStringUtf8(),
-        new ArrayList<SnapshotRegionManifest.StoreFile>(familyFiles.getStoreFilesList()));
+        new ArrayList<>(familyFiles.getStoreFilesList()));
     }
     return familyMap;
   }
@@ -489,8 +487,7 @@ public class RestoreSnapshotHelper {
       List<SnapshotRegionManifest.StoreFile> snapshotFamilyFiles =
           snapshotFiles.remove(familyDir.getName());
       if (snapshotFamilyFiles != null) {
-        List<SnapshotRegionManifest.StoreFile> hfilesToAdd =
-            new ArrayList<SnapshotRegionManifest.StoreFile>();
+        List<SnapshotRegionManifest.StoreFile> hfilesToAdd = new ArrayList<>();
         for (SnapshotRegionManifest.StoreFile storeFile: snapshotFamilyFiles) {
           if (familyFiles.contains(storeFile.getName())) {
             // HFile already present
@@ -546,7 +543,7 @@ public class RestoreSnapshotHelper {
     FileStatus[] hfiles = FSUtils.listStatus(fs, familyDir);
     if (hfiles == null) return Collections.emptySet();
 
-    Set<String> familyFiles = new HashSet<String>(hfiles.length);
+    Set<String> familyFiles = new HashSet<>(hfiles.length);
     for (int i = 0; i < hfiles.length; ++i) {
       String hfileName = hfiles[i].getPath().getName();
       familyFiles.add(hfileName);
@@ -564,8 +561,7 @@ public class RestoreSnapshotHelper {
       final List<HRegionInfo> regions) throws IOException {
     if (regions == null || regions.isEmpty()) return null;
 
-    final Map<String, HRegionInfo> snapshotRegions =
-      new HashMap<String, HRegionInfo>(regions.size());
+    final Map<String, HRegionInfo> snapshotRegions = new HashMap<>(regions.size());
 
     // clone region info (change embedded tableName with the new one)
     HRegionInfo[] clonedRegionsInfo = new HRegionInfo[regions.size()];
@@ -742,7 +738,7 @@ public class RestoreSnapshotHelper {
     synchronized (parentsMap) {
       Pair<String, String> daughters = parentsMap.get(clonedRegionName);
       if (daughters == null) {
-        daughters = new Pair<String, String>(regionName, null);
+        daughters = new Pair<>(regionName, null);
         parentsMap.put(clonedRegionName, daughters);
       } else if (!regionName.equals(daughters.getFirst())) {
         daughters.setSecond(regionName);
@@ -778,7 +774,7 @@ public class RestoreSnapshotHelper {
     FileStatus[] regionDirs = FSUtils.listStatus(fs, tableDir, new FSUtils.RegionDirFilter(fs));
     if (regionDirs == null) return null;
 
-    List<HRegionInfo> regions = new ArrayList<HRegionInfo>(regionDirs.length);
+    List<HRegionInfo> regions = new ArrayList<>(regionDirs.length);
     for (int i = 0; i < regionDirs.length; ++i) {
       HRegionInfo hri = HRegionFileSystem.loadRegionInfoFileContent(fs, regionDirs[i].getPath());
       regions.add(hri);
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/snapshot/SnapshotInfo.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/snapshot/SnapshotInfo.java
index 85d3af3..6dbd3f0 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/snapshot/SnapshotInfo.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/snapshot/SnapshotInfo.java
@@ -598,8 +598,7 @@ public final class SnapshotInfo extends AbstractHBaseTool {
     Path snapshotDir = SnapshotDescriptionUtils.getSnapshotsDir(rootDir);
     FileStatus[] snapshots = fs.listStatus(snapshotDir,
         new SnapshotDescriptionUtils.CompletedSnaphotDirectoriesFilter(fs));
-    List<SnapshotDescription> snapshotLists =
-      new ArrayList<SnapshotDescription>(snapshots.length);
+    List<SnapshotDescription> snapshotLists = new ArrayList<>(snapshots.length);
     for (FileStatus snapshotDirStat: snapshots) {
       HBaseProtos.SnapshotDescription snapshotDesc =
           SnapshotDescriptionUtils.readSnapshotInfo(fs, snapshotDirStat.getPath());
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/snapshot/SnapshotManifest.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/snapshot/SnapshotManifest.java
index 47e3073..4e838ad 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/snapshot/SnapshotManifest.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/snapshot/SnapshotManifest.java
@@ -220,7 +220,7 @@ public final class SnapshotManifest {
       Object familyData = visitor.familyOpen(regionData, store.getFamily().getName());
       monitor.rethrowException();
 
-      List<StoreFile> storeFiles = new ArrayList<StoreFile>(store.getStorefiles());
+      List<StoreFile> storeFiles = new ArrayList<>(store.getStorefiles());
       if (LOG.isDebugEnabled()) {
         LOG.debug("Adding snapshot references for " + storeFiles  + " hfiles");
       }
@@ -305,7 +305,7 @@ public final class SnapshotManifest {
     FileStatus[] stats = FSUtils.listStatus(fs, storeDir);
     if (stats == null) return null;
 
-    ArrayList<StoreFileInfo> storeFiles = new ArrayList<StoreFileInfo>(stats.length);
+    ArrayList<StoreFileInfo> storeFiles = new ArrayList<>(stats.length);
     for (int i = 0; i < stats.length; ++i) {
       storeFiles.add(new StoreFileInfo(conf, fs, stats[i]));
     }
@@ -374,8 +374,7 @@ public final class SnapshotManifest {
             tpool.shutdown();
           }
           if (v1Regions != null && v2Regions != null) {
-            regionManifests =
-              new ArrayList<SnapshotRegionManifest>(v1Regions.size() + v2Regions.size());
+            regionManifests = new ArrayList<>(v1Regions.size() + v2Regions.size());
             regionManifests.addAll(v1Regions);
             regionManifests.addAll(v2Regions);
           } else if (v1Regions != null) {
@@ -427,8 +426,7 @@ public final class SnapshotManifest {
   public Map<String, SnapshotRegionManifest> getRegionManifestsMap() {
     if (regionManifests == null || regionManifests.isEmpty()) return null;
 
-    HashMap<String, SnapshotRegionManifest> regionsMap =
-        new HashMap<String, SnapshotRegionManifest>(regionManifests.size());
+    HashMap<String, SnapshotRegionManifest> regionsMap = new HashMap<>(regionManifests.size());
     for (SnapshotRegionManifest manifest: regionManifests) {
       String regionName = getRegionNameFromManifest(manifest);
       regionsMap.put(regionName, manifest);
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/snapshot/SnapshotManifestV1.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/snapshot/SnapshotManifestV1.java
index cceeebc..46893f9 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/snapshot/SnapshotManifestV1.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/snapshot/SnapshotManifestV1.java
@@ -121,7 +121,7 @@ public final class SnapshotManifestV1 {
     }
 
     final ExecutorCompletionService<SnapshotRegionManifest> completionService =
-      new ExecutorCompletionService<SnapshotRegionManifest>(executor);
+      new ExecutorCompletionService<>(executor);
     for (final FileStatus region: regions) {
       completionService.submit(new Callable<SnapshotRegionManifest>() {
         @Override
@@ -132,8 +132,7 @@ public final class SnapshotManifestV1 {
       });
     }
 
-    ArrayList<SnapshotRegionManifest> regionsManifest =
-        new ArrayList<SnapshotRegionManifest>(regions.length);
+    ArrayList<SnapshotRegionManifest> regionsManifest = new ArrayList<>(regions.length);
     try {
       for (int i = 0; i < regions.length; ++i) {
         regionsManifest.add(completionService.take().get());
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/snapshot/SnapshotManifestV2.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/snapshot/SnapshotManifestV2.java
index a1341fb..567f42d 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/snapshot/SnapshotManifestV2.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/snapshot/SnapshotManifestV2.java
@@ -139,7 +139,7 @@ public final class SnapshotManifestV2 {
     if (manifestFiles == null || manifestFiles.length == 0) return null;
 
     final ExecutorCompletionService<SnapshotRegionManifest> completionService =
-      new ExecutorCompletionService<SnapshotRegionManifest>(executor);
+      new ExecutorCompletionService<>(executor);
     for (final FileStatus st: manifestFiles) {
       completionService.submit(new Callable<SnapshotRegionManifest>() {
         @Override
@@ -157,8 +157,7 @@ public final class SnapshotManifestV2 {
       });
     }
 
-    ArrayList<SnapshotRegionManifest> regionsManifest =
-        new ArrayList<SnapshotRegionManifest>(manifestFiles.length);
+    ArrayList<SnapshotRegionManifest> regionsManifest = new ArrayList<>(manifestFiles.length);
     try {
       for (int i = 0; i < manifestFiles.length; ++i) {
         regionsManifest.add(completionService.take().get());
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/snapshot/SnapshotReferenceUtil.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/snapshot/SnapshotReferenceUtil.java
index 8cd438e..7a2bfe6 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/snapshot/SnapshotReferenceUtil.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/snapshot/SnapshotReferenceUtil.java
@@ -210,8 +210,7 @@ public final class SnapshotReferenceUtil {
       return;
     }
 
-    final ExecutorCompletionService<Void> completionService =
-      new ExecutorCompletionService<Void>(exec);
+    final ExecutorCompletionService<Void> completionService = new ExecutorCompletionService<>(exec);
 
     for (final SnapshotRegionManifest regionManifest : regionManifests) {
       completionService.submit(new Callable<Void>() {
@@ -345,7 +344,7 @@ public final class SnapshotReferenceUtil {
   private static Set<String> getHFileNames(final Configuration conf, final FileSystem fs,
       final Path snapshotDir, final SnapshotDescription snapshotDesc)
       throws IOException {
-    final Set<String> names = new HashSet<String>();
+    final Set<String> names = new HashSet<>();
     visitTableStoreFiles(conf, fs, snapshotDir, snapshotDesc, new StoreFileVisitor() {
       @Override
       public void storeFile(final HRegionInfo regionInfo, final String family,
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/tool/Canary.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/tool/Canary.java
index 73160bc..ee93cdb 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/tool/Canary.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/tool/Canary.java
@@ -140,8 +140,8 @@ public final class Canary implements Tool {
     private AtomicLong readFailureCount = new AtomicLong(0),
         writeFailureCount = new AtomicLong(0);
 
-    private Map<String, String> readFailures = new ConcurrentHashMap<String, String>();
-    private Map<String, String> writeFailures = new ConcurrentHashMap<String, String>();
+    private Map<String, String> readFailures = new ConcurrentHashMap<>();
+    private Map<String, String> writeFailures = new ConcurrentHashMap<>();
 
     @Override
     public long getReadFailureCount() {
@@ -949,7 +949,7 @@ public final class Canary implements Tool {
     public void run() {
       if (this.initAdmin()) {
         try {
-          List<Future<Void>> taskFutures = new LinkedList<Future<Void>>();
+          List<Future<Void>> taskFutures = new LinkedList<>();
           if (this.targets != null && this.targets.length > 0) {
             String[] tables = generateMonitorTables(this.targets);
             this.initialized = true;
@@ -996,7 +996,7 @@ public final class Canary implements Tool {
       if (this.useRegExp) {
         Pattern pattern = null;
         HTableDescriptor[] tds = null;
-        Set<String> tmpTables = new TreeSet<String>();
+        Set<String> tmpTables = new TreeSet<>();
         try {
           if (LOG.isDebugEnabled()) {
             LOG.debug(String.format("reading list of tables"));
@@ -1040,7 +1040,7 @@ public final class Canary implements Tool {
       if (LOG.isDebugEnabled()) {
         LOG.debug(String.format("reading list of tables"));
       }
-      List<Future<Void>> taskFutures = new LinkedList<Future<Void>>();
+      List<Future<Void>> taskFutures = new LinkedList<>();
       for (HTableDescriptor table : admin.listTables()) {
         if (admin.isTableEnabled(table.getTableName())
             && (!table.getTableName().equals(writeTableName))) {
@@ -1078,7 +1078,7 @@ public final class Canary implements Tool {
         admin.deleteTable(writeTableName);
         createWriteTable(numberOfServers);
       }
-      HashSet<ServerName> serverSet = new HashSet<ServerName>();
+      HashSet<ServerName> serverSet = new HashSet<>();
       for (Pair<HRegionInfo, ServerName> pair : pairs) {
         serverSet.add(pair.getSecond());
       }
@@ -1165,7 +1165,7 @@ public final class Canary implements Tool {
     } else {
       LOG.warn(String.format("Table %s is not enabled", tableName));
     }
-    return new LinkedList<Future<Void>>();
+    return new LinkedList<>();
   }
 
   /*
@@ -1183,7 +1183,7 @@ public final class Canary implements Tool {
     try {
       table = admin.getConnection().getTable(tableDesc.getTableName());
     } catch (TableNotFoundException e) {
-      return new ArrayList<Future<Void>>();
+      return new ArrayList<>();
     }
     finally {
       if (table !=null) {
@@ -1191,7 +1191,7 @@ public final class Canary implements Tool {
       }
     }
 
-    List<RegionTask> tasks = new ArrayList<RegionTask>();
+    List<RegionTask> tasks = new ArrayList<>();
     RegionLocator regionLocator = null;
     try {
       regionLocator = admin.getConnection().getRegionLocator(tableDesc.getTableName());
@@ -1290,7 +1290,7 @@ public final class Canary implements Tool {
     }
 
     private boolean checkNoTableNames() {
-      List<String> foundTableNames = new ArrayList<String>();
+      List<String> foundTableNames = new ArrayList<>();
       TableName[] tableNames = null;
 
       if (LOG.isDebugEnabled()) {
@@ -1323,8 +1323,8 @@ public final class Canary implements Tool {
     }
 
     private void monitorRegionServers(Map<String, List<HRegionInfo>> rsAndRMap) {
-      List<RegionServerTask> tasks = new ArrayList<RegionServerTask>();
-      Map<String, AtomicLong> successMap = new HashMap<String, AtomicLong>();
+      List<RegionServerTask> tasks = new ArrayList<>();
+      Map<String, AtomicLong> successMap = new HashMap<>();
       Random rand = new Random();
       for (Map.Entry<String, List<HRegionInfo>> entry : rsAndRMap.entrySet()) {
         String serverName = entry.getKey();
@@ -1379,7 +1379,7 @@ public final class Canary implements Tool {
     }
 
     private Map<String, List<HRegionInfo>> getAllRegionServerByName() {
-      Map<String, List<HRegionInfo>> rsAndRMap = new HashMap<String, List<HRegionInfo>>();
+      Map<String, List<HRegionInfo>> rsAndRMap = new HashMap<>();
       Table table = null;
       RegionLocator regionLocator = null;
       try {
@@ -1400,7 +1400,7 @@ public final class Canary implements Tool {
             if (rsAndRMap.containsKey(rsName)) {
               regions = rsAndRMap.get(rsName);
             } else {
-              regions = new ArrayList<HRegionInfo>();
+              regions = new ArrayList<>();
               rsAndRMap.put(rsName, regions);
             }
             regions.add(r);
@@ -1438,7 +1438,7 @@ public final class Canary implements Tool {
       Map<String, List<HRegionInfo>> filteredRsAndRMap = null;
 
       if (this.targets != null && this.targets.length > 0) {
-        filteredRsAndRMap = new HashMap<String, List<HRegionInfo>>();
+        filteredRsAndRMap = new HashMap<>();
         Pattern pattern = null;
         Matcher matcher = null;
         boolean regExpFound = false;
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/util/BoundedPriorityBlockingQueue.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/util/BoundedPriorityBlockingQueue.java
index 4a93151..354382c 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/util/BoundedPriorityBlockingQueue.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/util/BoundedPriorityBlockingQueue.java
@@ -152,7 +152,7 @@ public class BoundedPriorityBlockingQueue<E> extends AbstractQueue<E> implements
    */
   public BoundedPriorityBlockingQueue(int capacity,
       Comparator<? super E> comparator) {
-    this.queue = new PriorityQueue<E>(capacity, comparator);
+    this.queue = new PriorityQueue<>(capacity, comparator);
   }
 
   public boolean offer(E e) {
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/util/CollectionBackedScanner.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/util/CollectionBackedScanner.java
index 3f05969..9e36290 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/util/CollectionBackedScanner.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/util/CollectionBackedScanner.java
@@ -66,7 +66,7 @@ public class CollectionBackedScanner extends NonReversedNonLazyKeyValueScanner {
       Cell... array) {
     this.comparator = comparator;
 
-    List<Cell> tmp = new ArrayList<Cell>(array.length);
+    List<Cell> tmp = new ArrayList<>(array.length);
     Collections.addAll(tmp, array);
     Collections.sort(tmp, comparator);
     data = tmp;
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/util/ConnectionCache.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/util/ConnectionCache.java
index 0659a0d..87e867f 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/util/ConnectionCache.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/util/ConnectionCache.java
@@ -50,9 +50,8 @@ import org.apache.commons.logging.LogFactory;
 public class ConnectionCache {
   private static final Log LOG = LogFactory.getLog(ConnectionCache.class);
 
-  private final Map<String, ConnectionInfo>
-   connections = new ConcurrentHashMap<String, ConnectionInfo>();
-  private final KeyLocker<String> locker = new KeyLocker<String>();
+  private final Map<String, ConnectionInfo> connections = new ConcurrentHashMap<>();
+  private final KeyLocker<String> locker = new KeyLocker<>();
   private final String realUserName;
   private final UserGroupInformation realUser;
   private final UserProvider userProvider;
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/util/EncryptionTest.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/util/EncryptionTest.java
index f45ecff..6692ee8 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/util/EncryptionTest.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/util/EncryptionTest.java
@@ -40,10 +40,9 @@ import org.apache.hadoop.hbase.security.EncryptionUtil;
 public class EncryptionTest {
   private static final Log LOG = LogFactory.getLog(EncryptionTest.class);
 
-  static final Map<String, Boolean> keyProviderResults = new ConcurrentHashMap<String, Boolean>();
-  static final Map<String, Boolean> cipherProviderResults =
-    new ConcurrentHashMap<String, Boolean>();
-  static final Map<String, Boolean> cipherResults = new ConcurrentHashMap<String, Boolean>();
+  static final Map<String, Boolean> keyProviderResults = new ConcurrentHashMap<>();
+  static final Map<String, Boolean> cipherProviderResults = new ConcurrentHashMap<>();
+  static final Map<String, Boolean> cipherResults = new ConcurrentHashMap<>();
 
   private EncryptionTest() {
   }
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/util/FSHDFSUtils.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/util/FSHDFSUtils.java
index 0d880d0..de49d38 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/util/FSHDFSUtils.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/util/FSHDFSUtils.java
@@ -58,7 +58,7 @@ public class FSHDFSUtils extends FSUtils {
    */
   private static Set<InetSocketAddress> getNNAddresses(DistributedFileSystem fs,
                                                       Configuration conf) {
-    Set<InetSocketAddress> addresses = new HashSet<InetSocketAddress>();
+    Set<InetSocketAddress> addresses = new HashSet<>();
     String serviceName = fs.getCanonicalServiceName();
 
     if (serviceName.startsWith("ha-hdfs")) {
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/util/FSRegionScanner.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/util/FSRegionScanner.java
index b0af52b..0bc8783 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/util/FSRegionScanner.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/util/FSRegionScanner.java
@@ -72,7 +72,7 @@ class FSRegionScanner implements Runnable {
   public void run() {
     try {
       // empty the map for each region
-      Map<String, AtomicInteger> blockCountMap = new HashMap<String, AtomicInteger>();
+      Map<String, AtomicInteger> blockCountMap = new HashMap<>();
 
       //get table name
       String tableName = regionPath.getParent().getName();
@@ -145,7 +145,7 @@ class FSRegionScanner implements Runnable {
       }
 
       if (regionDegreeLocalityMapping != null && totalBlkCount > 0) {
-        Map<String, Float> hostLocalityMap = new HashMap<String, Float>();
+        Map<String, Float> hostLocalityMap = new HashMap<>();
         for (Map.Entry<String, AtomicInteger> entry : blockCountMap.entrySet()) {
           String host = entry.getKey();
           if (host.endsWith(".")) {
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/util/FSTableDescriptors.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/util/FSTableDescriptors.java
index a100a15..c2ca3eb 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/util/FSTableDescriptors.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/util/FSTableDescriptors.java
@@ -89,8 +89,7 @@ public class FSTableDescriptors implements TableDescriptors {
   // This cache does not age out the old stuff.  Thinking is that the amount
   // of data we keep up in here is so small, no need to do occasional purge.
   // TODO.
-  private final Map<TableName, HTableDescriptor> cache =
-    new ConcurrentHashMap<TableName, HTableDescriptor>();
+  private final Map<TableName, HTableDescriptor> cache = new ConcurrentHashMap<>();
 
   /**
    * Table descriptor for <code>hbase:meta</code> catalog table
@@ -271,7 +270,7 @@ public class FSTableDescriptors implements TableDescriptors {
   @Override
   public Map<String, HTableDescriptor> getAllDescriptors()
   throws IOException {
-    Map<String, HTableDescriptor> tds = new TreeMap<String, HTableDescriptor>();
+    Map<String, HTableDescriptor> tds = new TreeMap<>();
 
     if (fsvisited && usecache) {
       for (Map.Entry<TableName, HTableDescriptor> entry: this.cache.entrySet()) {
@@ -307,7 +306,7 @@ public class FSTableDescriptors implements TableDescriptors {
    */
   @Override
   public Map<String, HTableDescriptor> getAll() throws IOException {
-    Map<String, HTableDescriptor> htds = new TreeMap<String, HTableDescriptor>();
+    Map<String, HTableDescriptor> htds = new TreeMap<>();
     Map<String, HTableDescriptor> allDescriptors = getAllDescriptors();
     for (Map.Entry<String, HTableDescriptor> entry : allDescriptors
         .entrySet()) {
@@ -323,7 +322,7 @@ public class FSTableDescriptors implements TableDescriptors {
   @Override
   public Map<String, HTableDescriptor> getByNamespace(String name)
   throws IOException {
-    Map<String, HTableDescriptor> htds = new TreeMap<String, HTableDescriptor>();
+    Map<String, HTableDescriptor> htds = new TreeMap<>();
     List<Path> tableDirs =
         FSUtils.getLocalTableDirs(fs, FSUtils.getNamespaceDir(rootdir, name));
     for (Path d: tableDirs) {
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/util/FSUtils.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/util/FSUtils.java
index 84b3436..c78ba06 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/util/FSUtils.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/util/FSUtils.java
@@ -1240,7 +1240,7 @@ public abstract class FSUtils {
   public static Map<String, Integer> getTableFragmentation(
     final FileSystem fs, final Path hbaseRootDir)
   throws IOException {
-    Map<String, Integer> frags = new HashMap<String, Integer>();
+    Map<String, Integer> frags = new HashMap<>();
     int cfCountTotal = 0;
     int cfFragTotal = 0;
     PathFilter regionFilter = new RegionDirFilter(fs);
@@ -1434,7 +1434,7 @@ public abstract class FSUtils {
 
   public static List<Path> getTableDirs(final FileSystem fs, final Path rootdir)
       throws IOException {
-    List<Path> tableDirs = new LinkedList<Path>();
+    List<Path> tableDirs = new LinkedList<>();
 
     for(FileStatus status :
         fs.globStatus(new Path(rootdir,
@@ -1455,7 +1455,7 @@ public abstract class FSUtils {
       throws IOException {
     // presumes any directory under hbase.rootdir is a table
     FileStatus[] dirs = fs.listStatus(rootdir, new UserTableDirFilter(fs));
-    List<Path> tabledirs = new ArrayList<Path>(dirs.length);
+    List<Path> tabledirs = new ArrayList<>(dirs.length);
     for (FileStatus dir: dirs) {
       tabledirs.add(dir.getPath());
     }
@@ -1511,9 +1511,9 @@ public abstract class FSUtils {
     // assumes we are in a table dir.
     List<FileStatus> rds = listStatusWithStatusFilter(fs, tableDir, new RegionDirFilter(fs));
     if (rds == null) {
-      return new ArrayList<Path>();
+      return new ArrayList<>();
     }
-    List<Path> regionDirs = new ArrayList<Path>(rds.size());
+    List<Path> regionDirs = new ArrayList<>(rds.size());
     for (FileStatus rdfs: rds) {
       Path rdPath = rdfs.getPath();
       regionDirs.add(rdPath);
@@ -1563,7 +1563,7 @@ public abstract class FSUtils {
   public static List<Path> getFamilyDirs(final FileSystem fs, final Path regionDir) throws IOException {
     // assumes we are in a region dir.
     FileStatus[] fds = fs.listStatus(regionDir, new FamilyDirFilter(fs));
-    List<Path> familyDirs = new ArrayList<Path>(fds.length);
+    List<Path> familyDirs = new ArrayList<>(fds.length);
     for (FileStatus fdfs: fds) {
       Path fdPath = fdfs.getPath();
       familyDirs.add(fdPath);
@@ -1574,9 +1574,9 @@ public abstract class FSUtils {
   public static List<Path> getReferenceFilePaths(final FileSystem fs, final Path familyDir) throws IOException {
     List<FileStatus> fds = listStatusWithStatusFilter(fs, familyDir, new ReferenceFileFilter(fs));
     if (fds == null) {
-      return new ArrayList<Path>();
+      return new ArrayList<>();
     }
-    List<Path> referenceFiles = new ArrayList<Path>(fds.size());
+    List<Path> referenceFiles = new ArrayList<>(fds.size());
     for (FileStatus fdfs: fds) {
       Path fdPath = fdfs.getPath();
       referenceFiles.add(fdPath);
@@ -1709,14 +1709,14 @@ public abstract class FSUtils {
       ExecutorService executor, final ErrorReporter errors) throws IOException, InterruptedException {
 
     final Map<String, Path> finalResultMap =
-        resultMap == null ? new ConcurrentHashMap<String, Path>(128, 0.75f, 32) : resultMap;
+        resultMap == null ? new ConcurrentHashMap<>(128, 0.75f, 32) : resultMap;
 
     // only include the directory paths to tables
     Path tableDir = FSUtils.getTableDir(hbaseRootDir, tableName);
     // Inside a table, there are compaction.dir directories to skip.  Otherwise, all else
     // should be regions.
     final FamilyDirFilter familyFilter = new FamilyDirFilter(fs);
-    final Vector<Exception> exceptions = new Vector<Exception>();
+    final Vector<Exception> exceptions = new Vector<>();
 
     try {
       List<FileStatus> regionDirs = FSUtils.listStatusWithStatusFilter(fs, tableDir, new RegionDirFilter(fs));
@@ -1724,7 +1724,7 @@ public abstract class FSUtils {
         return finalResultMap;
       }
 
-      final List<Future<?>> futures = new ArrayList<Future<?>>(regionDirs.size());
+      final List<Future<?>> futures = new ArrayList<>(regionDirs.size());
 
       for (FileStatus regionDir : regionDirs) {
         if (null != errors) {
@@ -1740,7 +1740,7 @@ public abstract class FSUtils {
           @Override
           public void run() {
             try {
-              HashMap<String,Path> regionStoreFileMap = new HashMap<String, Path>();
+              HashMap<String,Path> regionStoreFileMap = new HashMap<>();
               List<FileStatus> familyDirs = FSUtils.listStatusWithStatusFilter(fs, dd, familyFilter);
               if (familyDirs == null) {
                 if (!fs.exists(dd)) {
@@ -1785,7 +1785,7 @@ public abstract class FSUtils {
           Future<?> future = executor.submit(getRegionStoreFileMapCall);
           futures.add(future);
         } else {
-          FutureTask<?> future = new FutureTask<Object>(getRegionStoreFileMapCall, null);
+          FutureTask<?> future = new FutureTask<>(getRegionStoreFileMapCall, null);
           future.run();
           futures.add(future);
         }
@@ -1871,7 +1871,7 @@ public abstract class FSUtils {
     final FileSystem fs, final Path hbaseRootDir, PathFilter sfFilter,
     ExecutorService executor, ErrorReporter errors)
   throws IOException, InterruptedException {
-    ConcurrentHashMap<String, Path> map = new ConcurrentHashMap<String, Path>(1024, 0.75f, 32);
+    ConcurrentHashMap<String, Path> map = new ConcurrentHashMap<>(1024, 0.75f, 32);
 
     // if this method looks similar to 'getTableFragmentation' that is because
     // it was borrowed from it.
@@ -1907,7 +1907,7 @@ public abstract class FSUtils {
   public static List<FileStatus> filterFileStatuses(Iterator<FileStatus> input,
       FileStatusFilter filter) {
     if (input == null) return null;
-    ArrayList<FileStatus> results = new ArrayList<FileStatus>();
+    ArrayList<FileStatus> results = new ArrayList<>();
     while (input.hasNext()) {
       FileStatus f = input.next();
       if (filter.accept(f)) {
@@ -2167,8 +2167,7 @@ public abstract class FSUtils {
   public static Map<String, Map<String, Float>> getRegionDegreeLocalityMappingFromFS(
       final Configuration conf, final String desiredTable, int threadPoolSize)
       throws IOException {
-    Map<String, Map<String, Float>> regionDegreeLocalityMapping =
-        new ConcurrentHashMap<String, Map<String, Float>>();
+    Map<String, Map<String, Float>> regionDegreeLocalityMapping = new ConcurrentHashMap<>();
     getRegionLocalityMappingFromFS(conf, desiredTable, threadPoolSize, null,
         regionDegreeLocalityMapping);
     return regionDegreeLocalityMapping;
@@ -2253,7 +2252,7 @@ public abstract class FSUtils {
     // run in multiple threads
     ThreadPoolExecutor tpe = new ThreadPoolExecutor(threadPoolSize,
         threadPoolSize, 60, TimeUnit.SECONDS,
-        new ArrayBlockingQueue<Runnable>(statusList.length));
+        new ArrayBlockingQueue<>(statusList.length));
     try {
       // ignore all file status items that are not of interest
       for (FileStatus regionStatus : statusList) {
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/util/HBaseFsck.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/util/HBaseFsck.java
index 4d44187..7b3b25b 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/util/HBaseFsck.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/util/HBaseFsck.java
@@ -256,7 +256,7 @@ public class HBaseFsck extends Configured implements Closeable {
 
   // limit checking/fixes to listed tables, if empty attempt to check/fix all
   // hbase:meta are always checked
-  private Set<TableName> tablesIncluded = new HashSet<TableName>();
+  private Set<TableName> tablesIncluded = new HashSet<>();
   private int maxMerge = DEFAULT_MAX_MERGE; // maximum number of overlapping regions to merge
   // maximum number of overlapping regions to sideline
   private int maxOverlapsToSideline = DEFAULT_OVERLAPS_TO_SIDELINE;
@@ -280,9 +280,9 @@ public class HBaseFsck extends Configured implements Closeable {
    * name to HbckInfo structure.  The information contained in HbckInfo is used
    * to detect and correct consistency (hdfs/meta/deployment) problems.
    */
-  private TreeMap<String, HbckInfo> regionInfoMap = new TreeMap<String, HbckInfo>();
+  private TreeMap<String, HbckInfo> regionInfoMap = new TreeMap<>();
   // Empty regioninfo qualifiers in hbase:meta
-  private Set<Result> emptyRegionInfoQualifiers = new HashSet<Result>();
+  private Set<Result> emptyRegionInfoQualifiers = new HashSet<>();
 
   /**
    * This map from Tablename -> TableInfo contains the structures necessary to
@@ -294,22 +294,19 @@ public class HBaseFsck extends Configured implements Closeable {
    * unless checkMetaOnly is specified, in which case, it contains only
    * the meta table
    */
-  private SortedMap<TableName, TableInfo> tablesInfo =
-      new ConcurrentSkipListMap<TableName, TableInfo>();
+  private SortedMap<TableName, TableInfo> tablesInfo = new ConcurrentSkipListMap<>();
 
   /**
    * When initially looking at HDFS, we attempt to find any orphaned data.
    */
   private List<HbckInfo> orphanHdfsDirs = Collections.synchronizedList(new ArrayList<HbckInfo>());
 
-  private Map<TableName, Set<String>> orphanTableDirs =
-      new HashMap<TableName, Set<String>>();
-  private Map<TableName, TableState> tableStates =
-      new HashMap<TableName, TableState>();
+  private Map<TableName, Set<String>> orphanTableDirs = new HashMap<>();
+  private Map<TableName, TableState> tableStates = new HashMap<>();
   private final RetryCounterFactory lockFileRetryCounterFactory;
   private final RetryCounterFactory createZNodeRetryCounterFactory;
 
-  private Map<TableName, Set<String>> skippedRegions = new HashMap<TableName, Set<String>>();
+  private Map<TableName, Set<String>> skippedRegions = new HashMap<>();
 
   private ZooKeeperWatcher zkw = null;
   private String hbckEphemeralNodePath = null;
@@ -431,7 +428,7 @@ public class HBaseFsck extends Configured implements Closeable {
     RetryCounter retryCounter = lockFileRetryCounterFactory.create();
     FileLockCallable callable = new FileLockCallable(retryCounter);
     ExecutorService executor = Executors.newFixedThreadPool(1);
-    FutureTask<FSDataOutputStream> futureTask = new FutureTask<FSDataOutputStream>(callable);
+    FutureTask<FSDataOutputStream> futureTask = new FutureTask<>(callable);
     executor.execute(futureTask);
     final int timeoutInSeconds = getConf().getInt(
       "hbase.hbck.lockfile.maxwaittime", DEFAULT_WAIT_FOR_LOCK_TIMEOUT);
@@ -977,7 +974,7 @@ public class HBaseFsck extends Configured implements Closeable {
         // expand the range to include the range of all hfiles
         if (orphanRegionRange == null) {
           // first range
-          orphanRegionRange = new Pair<byte[], byte[]>(start, end);
+          orphanRegionRange = new Pair<>(start, end);
         } else {
           // TODO add test
 
@@ -1267,7 +1264,7 @@ public class HBaseFsck extends Configured implements Closeable {
     Collection<HbckInfo> hbckInfos = regionInfoMap.values();
 
     // Parallelized read of .regioninfo files.
-    List<WorkItemHdfsRegionInfo> hbis = new ArrayList<WorkItemHdfsRegionInfo>(hbckInfos.size());
+    List<WorkItemHdfsRegionInfo> hbis = new ArrayList<>(hbckInfos.size());
     List<Future<Void>> hbiFutures;
 
     for (HbckInfo hbi : hbckInfos) {
@@ -1323,7 +1320,7 @@ public class HBaseFsck extends Configured implements Closeable {
             //should only report once for each table
             errors.reportError(ERROR_CODE.NO_TABLEINFO_FILE,
                 "Unable to read .tableinfo from " + hbaseRoot + "/" + tableName);
-            Set<String> columns = new HashSet<String>();
+            Set<String> columns = new HashSet<>();
             orphanTableDirs.put(tableName, getColumnFamilyList(columns, hbi));
           }
         }
@@ -1402,7 +1399,7 @@ public class HBaseFsck extends Configured implements Closeable {
   public void fixOrphanTables() throws IOException {
     if (shouldFixTableOrphans() && !orphanTableDirs.isEmpty()) {
 
-      List<TableName> tmpList = new ArrayList<TableName>(orphanTableDirs.keySet().size());
+      List<TableName> tmpList = new ArrayList<>(orphanTableDirs.keySet().size());
       tmpList.addAll(orphanTableDirs.keySet());
       HTableDescriptor[] htds = getHTableDescriptors(tmpList);
       Iterator<Entry<TableName, Set<String>>> iter =
@@ -1485,7 +1482,7 @@ public class HBaseFsck extends Configured implements Closeable {
    */
   private ArrayList<Put> generatePuts(
       SortedMap<TableName, TableInfo> tablesInfo) throws IOException {
-    ArrayList<Put> puts = new ArrayList<Put>();
+    ArrayList<Put> puts = new ArrayList<>();
     boolean hasProblems = false;
     for (Entry<TableName, TableInfo> e : tablesInfo.entrySet()) {
       TableName name = e.getKey();
@@ -1936,7 +1933,7 @@ public class HBaseFsck extends Configured implements Closeable {
   void processRegionServers(Collection<ServerName> regionServerList)
     throws IOException, InterruptedException {
 
-    List<WorkItemRegion> workItems = new ArrayList<WorkItemRegion>(regionServerList.size());
+    List<WorkItemRegion> workItems = new ArrayList<>(regionServerList.size());
     List<Future<Void>> workFutures;
 
     // loop to contact each region server in parallel
@@ -1966,8 +1963,7 @@ public class HBaseFsck extends Configured implements Closeable {
     // Divide the checks in two phases. One for default/primary replicas and another
     // for the non-primary ones. Keeps code cleaner this way.
 
-    List<CheckRegionConsistencyWorkItem> workItems =
-        new ArrayList<CheckRegionConsistencyWorkItem>(regionInfoMap.size());
+    List<CheckRegionConsistencyWorkItem> workItems = new ArrayList<>(regionInfoMap.size());
     for (java.util.Map.Entry<String, HbckInfo> e: regionInfoMap.entrySet()) {
       if (e.getValue().getReplicaId() == HRegionInfo.DEFAULT_REPLICA_ID) {
         workItems.add(new CheckRegionConsistencyWorkItem(e.getKey(), e.getValue()));
@@ -1979,8 +1975,7 @@ public class HBaseFsck extends Configured implements Closeable {
     setCheckHdfs(false); //replicas don't have any hdfs data
     // Run a pass over the replicas and fix any assignment issues that exist on the currently
     // deployed/undeployed replicas.
-    List<CheckRegionConsistencyWorkItem> replicaWorkItems =
-        new ArrayList<CheckRegionConsistencyWorkItem>(regionInfoMap.size());
+    List<CheckRegionConsistencyWorkItem> replicaWorkItems = new ArrayList<>(regionInfoMap.size());
     for (java.util.Map.Entry<String, HbckInfo> e: regionInfoMap.entrySet()) {
       if (e.getValue().getReplicaId() != HRegionInfo.DEFAULT_REPLICA_ID) {
         replicaWorkItems.add(new CheckRegionConsistencyWorkItem(e.getKey(), e.getValue()));
@@ -2065,7 +2060,7 @@ public class HBaseFsck extends Configured implements Closeable {
   private void addSkippedRegion(final HbckInfo hbi) {
     Set<String> skippedRegionNames = skippedRegions.get(hbi.getTableName());
     if (skippedRegionNames == null) {
-      skippedRegionNames = new HashSet<String>();
+      skippedRegionNames = new HashSet<>();
     }
     skippedRegionNames.add(hbi.getRegionNameAsString());
     skippedRegions.put(hbi.getTableName(), skippedRegionNames);
@@ -2570,7 +2565,7 @@ public class HBaseFsck extends Configured implements Closeable {
    * @throws IOException
    */
   SortedMap<TableName, TableInfo> checkIntegrity() throws IOException {
-    tablesInfo = new TreeMap<TableName,TableInfo> ();
+    tablesInfo = new TreeMap<>();
     LOG.debug("There are " + regionInfoMap.size() + " region info entries");
     for (HbckInfo hbi : regionInfoMap.values()) {
       // Check only valid, working regions
@@ -2753,16 +2748,16 @@ public class HBaseFsck extends Configured implements Closeable {
     TreeSet <ServerName> deployedOn;
 
     // backwards regions
-    final List<HbckInfo> backwards = new ArrayList<HbckInfo>();
+    final List<HbckInfo> backwards = new ArrayList<>();
 
     // sidelined big overlapped regions
-    final Map<Path, HbckInfo> sidelinedRegions = new HashMap<Path, HbckInfo>();
+    final Map<Path, HbckInfo> sidelinedRegions = new HashMap<>();
 
     // region split calculator
-    final RegionSplitCalculator<HbckInfo> sc = new RegionSplitCalculator<HbckInfo>(cmp);
+    final RegionSplitCalculator<HbckInfo> sc = new RegionSplitCalculator<>(cmp);
 
     // Histogram of different HTableDescriptors found.  Ideally there is only one!
-    final Set<HTableDescriptor> htds = new HashSet<HTableDescriptor>();
+    final Set<HTableDescriptor> htds = new HashSet<>();
 
     // key = start split, values = set of splits in problem group
     final Multimap<byte[], HbckInfo> overlapGroups =
@@ -2773,7 +2768,7 @@ public class HBaseFsck extends Configured implements Closeable {
 
     TableInfo(TableName name) {
       this.tableName = name;
-      deployedOn = new TreeSet <ServerName>();
+      deployedOn = new TreeSet <>();
     }
 
     /**
@@ -2829,7 +2824,7 @@ public class HBaseFsck extends Configured implements Closeable {
     public synchronized ImmutableList<HRegionInfo> getRegionsFromMeta() {
       // lazy loaded, synchronized to ensure a single load
       if (regionsFromMeta == null) {
-        List<HRegionInfo> regions = new ArrayList<HRegionInfo>();
+        List<HRegionInfo> regions = new ArrayList<>();
         for (HbckInfo h : HBaseFsck.this.regionInfoMap.values()) {
           if (tableName.equals(h.getTableName())) {
             if (h.metaEntry != null) {
@@ -3031,7 +3026,7 @@ public class HBaseFsck extends Configured implements Closeable {
         Pair<byte[], byte[]> range = null;
         for (HbckInfo hi : overlap) {
           if (range == null) {
-            range = new Pair<byte[], byte[]>(hi.getStartKey(), hi.getEndKey());
+            range = new Pair<>(hi.getStartKey(), hi.getEndKey());
           } else {
             if (RegionSplitCalculator.BYTES_COMPARATOR
                 .compare(hi.getStartKey(), range.getFirst()) < 0) {
@@ -3200,7 +3195,7 @@ public class HBaseFsck extends Configured implements Closeable {
           overlapGroups.putAll(problemKey, ranges);
 
           // record errors
-          ArrayList<HbckInfo> subRange = new ArrayList<HbckInfo>(ranges);
+          ArrayList<HbckInfo> subRange = new ArrayList<>(ranges);
           //  this dumb and n^2 but this shouldn't happen often
           for (HbckInfo r1 : ranges) {
             if (r1.getReplicaId() != HRegionInfo.DEFAULT_REPLICA_ID) continue;
@@ -3275,7 +3270,7 @@ public class HBaseFsck extends Configured implements Closeable {
         throws IOException {
       // we parallelize overlap handler for the case we have lots of groups to fix.  We can
       // safely assume each group is independent.
-      List<WorkItemOverlapMerge> merges = new ArrayList<WorkItemOverlapMerge>(overlapGroups.size());
+      List<WorkItemOverlapMerge> merges = new ArrayList<>(overlapGroups.size());
       List<Future<Void>> rets;
       for (Collection<HbckInfo> overlap : overlapGroups.asMap().values()) {
         //
@@ -3364,7 +3359,7 @@ public class HBaseFsck extends Configured implements Closeable {
    * @throws IOException if an error is encountered
    */
   HTableDescriptor[] getTables(AtomicInteger numSkipped) {
-    List<TableName> tableNames = new ArrayList<TableName>();
+    List<TableName> tableNames = new ArrayList<>();
     long now = EnvironmentEdgeManager.currentTime();
 
     for (HbckInfo hbi : regionInfoMap.values()) {
@@ -3429,7 +3424,7 @@ public class HBaseFsck extends Configured implements Closeable {
     * @throws InterruptedException
     */
   boolean checkMetaRegion() throws IOException, KeeperException, InterruptedException {
-    Map<Integer, HbckInfo> metaRegions = new HashMap<Integer, HbckInfo>();
+    Map<Integer, HbckInfo> metaRegions = new HashMap<>();
     for (HbckInfo value : regionInfoMap.values()) {
       if (value.metaEntry != null && value.metaEntry.isMetaRegion()) {
         metaRegions.put(value.getReplicaId(), value);
@@ -3442,7 +3437,7 @@ public class HBaseFsck extends Configured implements Closeable {
     // Check the deployed servers. It should be exactly one server for each replica.
     for (int i = 0; i < metaReplication; i++) {
       HbckInfo metaHbckInfo = metaRegions.remove(i);
-      List<ServerName> servers = new ArrayList<ServerName>();
+      List<ServerName> servers = new ArrayList<>();
       if (metaHbckInfo != null) {
         servers = metaHbckInfo.deployedOn;
       }
@@ -3979,10 +3974,10 @@ public class HBaseFsck extends Configured implements Closeable {
     // How frequently calls to progress() will create output
     private static final int progressThreshold = 100;
 
-    Set<TableInfo> errorTables = new HashSet<TableInfo>();
+    Set<TableInfo> errorTables = new HashSet<>();
 
     // for use by unit tests to verify which errors were discovered
-    private ArrayList<ERROR_CODE> errorList = new ArrayList<ERROR_CODE>();
+    private ArrayList<ERROR_CODE> errorList = new ArrayList<>();
 
     @Override
     public void clear() {
@@ -4183,11 +4178,11 @@ public class HBaseFsck extends Configured implements Closeable {
 
     @Override
     public synchronized Void call() throws InterruptedException, ExecutionException {
-      final Vector<Exception> exceptions = new Vector<Exception>();
+      final Vector<Exception> exceptions = new Vector<>();
 
       try {
         final FileStatus[] regionDirs = fs.listStatus(tableDir.getPath());
-        final List<Future<?>> futures = new ArrayList<Future<?>>(regionDirs.length);
+        final List<Future<?>> futures = new ArrayList<>(regionDirs.length);
 
         for (final FileStatus regionDir : regionDirs) {
           errors.progress();
@@ -4554,7 +4549,7 @@ public class HBaseFsck extends Configured implements Closeable {
   }
 
   Set<TableName> getIncludedTables() {
-    return new HashSet<TableName>(tablesIncluded);
+    return new HashSet<>(tablesIncluded);
   }
 
   /**
@@ -4865,7 +4860,7 @@ public class HBaseFsck extends Configured implements Closeable {
         HFileCorruptionChecker hfcc = createHFileCorruptionChecker(sidelineCorruptHFiles);
         setHFileCorruptionChecker(hfcc); // so we can get result
         Collection<TableName> tables = getIncludedTables();
-        Collection<Path> tableDirs = new ArrayList<Path>();
+        Collection<Path> tableDirs = new ArrayList<>();
         Path rootdir = FSUtils.getRootDir(getConf());
         if (tables.size() > 0) {
           for (TableName t : tables) {
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/util/IdLock.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/util/IdLock.java
index 7f283e6..e5dbae2 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/util/IdLock.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/util/IdLock.java
@@ -58,8 +58,7 @@ public class IdLock {
     }
   }
 
-  private ConcurrentMap<Long, Entry> map =
-      new ConcurrentHashMap<Long, Entry>();
+  private ConcurrentMap<Long, Entry> map = new ConcurrentHashMap<>();
 
   /**
    * Blocks until the lock corresponding to the given id is acquired.
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/util/IdReadWriteLock.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/util/IdReadWriteLock.java
index 98ce80d..caf3265 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/util/IdReadWriteLock.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/util/IdReadWriteLock.java
@@ -46,8 +46,7 @@ public class IdReadWriteLock {
   private static final int NB_CONCURRENT_LOCKS = 1000;
   // The pool to get entry from, entries are mapped by weak reference to make it able to be
   // garbage-collected asap
-  private final WeakObjectPool<Long, ReentrantReadWriteLock> lockPool =
-      new WeakObjectPool<Long, ReentrantReadWriteLock>(
+  private final WeakObjectPool<Long, ReentrantReadWriteLock> lockPool = new WeakObjectPool<>(
           new WeakObjectPool.ObjectFactory<Long, ReentrantReadWriteLock>() {
             @Override
             public ReentrantReadWriteLock createObject(Long id) {
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/util/JvmVersion.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/util/JvmVersion.java
index b0bca00..9f4b271 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/util/JvmVersion.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/util/JvmVersion.java
@@ -32,7 +32,7 @@ import org.apache.hadoop.hbase.classification.InterfaceStability;
 @InterfaceAudience.Private
 @InterfaceStability.Stable
 public abstract class JvmVersion {
-  private static Set<String> BAD_JVM_VERSIONS = new HashSet<String>();
+  private static Set<String> BAD_JVM_VERSIONS = new HashSet<>();
   static {
     BAD_JVM_VERSIONS.add("1.6.0_18");
   }
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/util/ModifyRegionUtils.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/util/ModifyRegionUtils.java
index f11d38b..d7749c2 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/util/ModifyRegionUtils.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/util/ModifyRegionUtils.java
@@ -128,9 +128,8 @@ public abstract class ModifyRegionUtils {
       final RegionFillTask task) throws IOException {
     if (newRegions == null) return null;
     int regionNumber = newRegions.length;
-    CompletionService<HRegionInfo> completionService =
-      new ExecutorCompletionService<HRegionInfo>(exec);
-    List<HRegionInfo> regionInfos = new ArrayList<HRegionInfo>();
+    CompletionService<HRegionInfo> completionService = new ExecutorCompletionService<>(exec);
+    List<HRegionInfo> regionInfos = new ArrayList<>();
     for (final HRegionInfo newRegion : newRegions) {
       completionService.submit(new Callable<HRegionInfo>() {
         @Override
@@ -193,8 +192,7 @@ public abstract class ModifyRegionUtils {
    */
   public static void editRegions(final ThreadPoolExecutor exec,
       final Collection<HRegionInfo> regions, final RegionEditTask task) throws IOException {
-    final ExecutorCompletionService<Void> completionService =
-      new ExecutorCompletionService<Void>(exec);
+    final ExecutorCompletionService<Void> completionService = new ExecutorCompletionService<>(exec);
     for (final HRegionInfo hri: regions) {
       completionService.submit(new Callable<Void>() {
         @Override
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/util/MunkresAssignment.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/util/MunkresAssignment.java
index 8cb880d..4721781 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/util/MunkresAssignment.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/util/MunkresAssignment.java
@@ -111,7 +111,7 @@ public class MunkresAssignment {
     mask = new byte[rows][cols];
     rowsCovered = new boolean[rows];
     colsCovered = new boolean[cols];
-    path = new LinkedList<Pair<Integer, Integer>>();
+    path = new LinkedList<>();
 
     leastInRow = new float[rows];
     leastInRowIndex = new int[rows];
@@ -330,8 +330,7 @@ public class MunkresAssignment {
           // starting from the uncovered primed zero (there is only one). Since
           // we have already found it, save it as the first node in the path.
           path.clear();
-          path.offerLast(new Pair<Integer, Integer>(zero.getFirst(),
-              zero.getSecond()));
+          path.offerLast(new Pair<>(zero.getFirst(), zero.getSecond()));
           return true;
         }
       }
@@ -439,7 +438,7 @@ public class MunkresAssignment {
   private Pair<Integer, Integer> findUncoveredZero() {
     for (int r = 0; r < rows; r++) {
       if (leastInRow[r] == 0) {
-        return new Pair<Integer, Integer>(r, leastInRowIndex[r]);
+        return new Pair<>(r, leastInRowIndex[r]);
       }
     }
     return null;
@@ -476,7 +475,7 @@ public class MunkresAssignment {
   private Pair<Integer, Integer> starInRow(int r) {
     for (int c = 0; c < cols; c++) {
       if (mask[r][c] == STAR) {
-        return new Pair<Integer, Integer>(r, c);
+        return new Pair<>(r, c);
       }
     }
     return null;
@@ -491,7 +490,7 @@ public class MunkresAssignment {
   private Pair<Integer, Integer> starInCol(int c) {
     for (int r = 0; r < rows; r++) {
       if (mask[r][c] == STAR) {
-        return new Pair<Integer, Integer>(r, c);
+        return new Pair<>(r, c);
       }
     }
     return null;
@@ -506,7 +505,7 @@ public class MunkresAssignment {
   private Pair<Integer, Integer> primeInRow(int r) {
     for (int c = 0; c < cols; c++) {
       if (mask[r][c] == PRIME) {
-        return new Pair<Integer, Integer>(r, c);
+        return new Pair<>(r, c);
       }
     }
     return null;
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/util/RegionMover.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/util/RegionMover.java
index 01ee201..ce018da 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/util/RegionMover.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/util/RegionMover.java
@@ -397,7 +397,7 @@ public class RegionMover extends AbstractHBaseTool {
     LOG.info("Moving " + regionsToMove.size() + " regions to " + server + " using "
         + this.maxthreads + " threads.Ack mode:" + this.ack);
     ExecutorService moveRegionsPool = Executors.newFixedThreadPool(this.maxthreads);
-    List<Future<Boolean>> taskList = new ArrayList<Future<Boolean>>();
+    List<Future<Boolean>> taskList = new ArrayList<>();
     int counter = 0;
     while (counter < regionsToMove.size()) {
       HRegionInfo region = regionsToMove.get(counter);
@@ -461,7 +461,7 @@ public class RegionMover extends AbstractHBaseTool {
       justification="FB is wrong; its size is read")
   private void unloadRegions(Admin admin, String server, ArrayList<String> regionServers,
       boolean ack, List<HRegionInfo> movedRegions) throws Exception {
-    List<HRegionInfo> regionsToMove = new ArrayList<HRegionInfo>();// FindBugs: DLS_DEAD_LOCAL_STORE
+    List<HRegionInfo> regionsToMove = new ArrayList<>();// FindBugs: DLS_DEAD_LOCAL_STORE
     regionsToMove = getRegions(this.conf, server);
     if (regionsToMove.isEmpty()) {
       LOG.info("No Regions to move....Quitting now");
@@ -481,7 +481,7 @@ public class RegionMover extends AbstractHBaseTool {
           + regionServers.size() + " servers using " + this.maxthreads + " threads .Ack Mode:"
           + ack);
       ExecutorService moveRegionsPool = Executors.newFixedThreadPool(this.maxthreads);
-      List<Future<Boolean>> taskList = new ArrayList<Future<Boolean>>();
+      List<Future<Boolean>> taskList = new ArrayList<>();
       int serverIndex = 0;
       while (counter < regionsToMove.size()) {
         if (ack) {
@@ -636,7 +636,7 @@ public class RegionMover extends AbstractHBaseTool {
   }
 
   private List<HRegionInfo> readRegionsFromFile(String filename) throws IOException {
-    List<HRegionInfo> regions = new ArrayList<HRegionInfo>();
+    List<HRegionInfo> regions = new ArrayList<>();
     File f = new File(filename);
     if (!f.exists()) {
       return regions;
@@ -758,7 +758,7 @@ public class RegionMover extends AbstractHBaseTool {
    * @return List of servers from the exclude file in format 'hostname:port'.
    */
   private ArrayList<String> readExcludes(String excludeFile) throws IOException {
-    ArrayList<String> excludeServers = new ArrayList<String>();
+    ArrayList<String> excludeServers = new ArrayList<>();
     if (excludeFile == null) {
       return excludeServers;
     } else {
@@ -821,9 +821,8 @@ public class RegionMover extends AbstractHBaseTool {
    * @throws IOException
    */
   private ArrayList<String> getServers(Admin admin) throws IOException {
-    ArrayList<ServerName> serverInfo =
-        new ArrayList<ServerName>(admin.getClusterStatus().getServers());
-    ArrayList<String> regionServers = new ArrayList<String>(serverInfo.size());
+    ArrayList<ServerName> serverInfo = new ArrayList<>(admin.getClusterStatus().getServers());
+    ArrayList<String> regionServers = new ArrayList<>(serverInfo.size());
     for (ServerName server : serverInfo) {
       regionServers.add(server.getServerName());
     }
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/util/RegionSizeCalculator.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/util/RegionSizeCalculator.java
index c616a25..8249630 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/util/RegionSizeCalculator.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/util/RegionSizeCalculator.java
@@ -53,7 +53,7 @@ public class RegionSizeCalculator {
   /**
    * Maps each region to its size in bytes.
    * */
-  private final Map<byte[], Long> sizeMap = new TreeMap<byte[], Long>(Bytes.BYTES_COMPARATOR);
+  private final Map<byte[], Long> sizeMap = new TreeMap<>(Bytes.BYTES_COMPARATOR);
 
   static final String ENABLE_REGIONSIZECALCULATOR = "hbase.regionsizecalculator.enable";
   private static final long MEGABYTE = 1024L * 1024L;
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/util/RegionSplitCalculator.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/util/RegionSplitCalculator.java
index eeef1ae..e07966e 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/util/RegionSplitCalculator.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/util/RegionSplitCalculator.java
@@ -62,7 +62,7 @@ public class RegionSplitCalculator<R extends KeyRange> {
    * Invariant: once populated this has 0 entries if empty or at most n+1 values
    * where n == number of added ranges.
    */
-  private final TreeSet<byte[]> splits = new TreeSet<byte[]>(BYTES_COMPARATOR);
+  private final TreeSet<byte[]> splits = new TreeSet<>(BYTES_COMPARATOR);
 
   /**
    * This is a map from start key to regions with the same start key.
@@ -177,11 +177,11 @@ public class RegionSplitCalculator<R extends KeyRange> {
    */
   public static <R extends KeyRange> List<R>
       findBigRanges(Collection<R> bigOverlap, int count) {
-    List<R> bigRanges = new ArrayList<R>();
+    List<R> bigRanges = new ArrayList<>();
 
     // The key is the count of overlaps,
     // The value is a list of ranges that have that many overlaps
-    TreeMap<Integer, List<R>> overlapRangeMap = new TreeMap<Integer, List<R>>();
+    TreeMap<Integer, List<R>> overlapRangeMap = new TreeMap<>();
     for (R r: bigOverlap) {
       // Calculates the # of overlaps for each region
       // and populates rangeOverlapMap
@@ -206,7 +206,7 @@ public class RegionSplitCalculator<R extends KeyRange> {
         Integer key = Integer.valueOf(overlappedRegions);
         List<R> ranges = overlapRangeMap.get(key);
         if (ranges == null) {
-          ranges = new ArrayList<R>();
+          ranges = new ArrayList<>();
           overlapRangeMap.put(key, ranges);
         }
         ranges.add(r);
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/util/RegionSplitter.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/util/RegionSplitter.java
index ce1b441..87ff010 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/util/RegionSplitter.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/util/RegionSplitter.java
@@ -768,7 +768,7 @@ public class RegionSplitter {
     Path hbDir = FSUtils.getRootDir(conf);
     Path tableDir = FSUtils.getTableDir(hbDir, tableName);
     Path splitFile = new Path(tableDir, "_balancedSplit");
-    return new Pair<Path, Path>(tableDir, splitFile);
+    return new Pair<>(tableDir, splitFile);
   }
 
   static LinkedList<Pair<byte[], byte[]>> getSplits(final Connection connection,
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/util/ServerCommandLine.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/util/ServerCommandLine.java
index e6b746c..9cc6d5a 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/util/ServerCommandLine.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/util/ServerCommandLine.java
@@ -94,7 +94,7 @@ public abstract class ServerCommandLine extends Configured implements Tool {
   public static void logProcessInfo(Configuration conf) {
     // log environment variables unless asked not to
     if (conf == null || !conf.getBoolean("hbase.envvars.logging.disabled", false)) {
-      Set<String> skipWords = new HashSet<String>(DEFAULT_SKIP_WORDS);
+      Set<String> skipWords = new HashSet<>(DEFAULT_SKIP_WORDS);
       if (conf != null) {
         String[] confSkipWords = conf.getStrings("hbase.envvars.logging.skipwords");
         if (confSkipWords != null) {
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/util/SortedCopyOnWriteSet.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/util/SortedCopyOnWriteSet.java
index 62163bf..05e0f49 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/util/SortedCopyOnWriteSet.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/util/SortedCopyOnWriteSet.java
@@ -49,15 +49,15 @@ public class SortedCopyOnWriteSet<E> implements SortedSet<E> {
   private volatile SortedSet<E> internalSet;
 
   public SortedCopyOnWriteSet() {
-    this.internalSet = new TreeSet<E>();
+    this.internalSet = new TreeSet<>();
   }
 
   public SortedCopyOnWriteSet(Collection<? extends E> c) {
-    this.internalSet = new TreeSet<E>(c);
+    this.internalSet = new TreeSet<>(c);
   }
 
   public SortedCopyOnWriteSet(Comparator<? super E> comparator) {
-    this.internalSet = new TreeSet<E>(comparator);
+    this.internalSet = new TreeSet<>(comparator);
   }
 
   @Override
@@ -92,7 +92,7 @@ public class SortedCopyOnWriteSet<E> implements SortedSet<E> {
 
   @Override
   public synchronized boolean add(E e) {
-    SortedSet<E> newSet = new TreeSet<E>(internalSet);
+    SortedSet<E> newSet = new TreeSet<>(internalSet);
     boolean added = newSet.add(e);
     internalSet = newSet;
     return added;
@@ -100,7 +100,7 @@ public class SortedCopyOnWriteSet<E> implements SortedSet<E> {
 
   @Override
   public synchronized boolean remove(Object o) {
-    SortedSet<E> newSet = new TreeSet<E>(internalSet);
+    SortedSet<E> newSet = new TreeSet<>(internalSet);
     boolean removed = newSet.remove(o);
     internalSet = newSet;
     return removed;
@@ -113,7 +113,7 @@ public class SortedCopyOnWriteSet<E> implements SortedSet<E> {
 
   @Override
   public synchronized boolean addAll(Collection<? extends E> c) {
-    SortedSet<E> newSet = new TreeSet<E>(internalSet);
+    SortedSet<E> newSet = new TreeSet<>(internalSet);
     boolean changed = newSet.addAll(c);
     internalSet = newSet;
     return changed;
@@ -121,7 +121,7 @@ public class SortedCopyOnWriteSet<E> implements SortedSet<E> {
 
   @Override
   public synchronized boolean retainAll(Collection<?> c) {
-    SortedSet<E> newSet = new TreeSet<E>(internalSet);
+    SortedSet<E> newSet = new TreeSet<>(internalSet);
     boolean changed = newSet.retainAll(c);
     internalSet = newSet;
     return changed;
@@ -129,7 +129,7 @@ public class SortedCopyOnWriteSet<E> implements SortedSet<E> {
 
   @Override
   public synchronized boolean removeAll(Collection<?> c) {
-    SortedSet<E> newSet = new TreeSet<E>(internalSet);
+    SortedSet<E> newSet = new TreeSet<>(internalSet);
     boolean changed = newSet.removeAll(c);
     internalSet = newSet;
     return changed;
@@ -139,9 +139,9 @@ public class SortedCopyOnWriteSet<E> implements SortedSet<E> {
   public synchronized void clear() {
     Comparator<? super E> comparator = internalSet.comparator();
     if (comparator != null) {
-      internalSet = new TreeSet<E>(comparator);
+      internalSet = new TreeSet<>(comparator);
     } else {
-      internalSet = new TreeSet<E>();
+      internalSet = new TreeSet<>();
     }
   }
 
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/util/SortedList.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/util/SortedList.java
index 39f1f41..3f5576e 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/util/SortedList.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/util/SortedList.java
@@ -118,7 +118,7 @@ public class SortedList<E> implements List<E>, RandomAccess {
 
   @Override
   public synchronized boolean add(E e) {
-    ArrayList<E> newList = new ArrayList<E>(list);
+    ArrayList<E> newList = new ArrayList<>(list);
     boolean changed = newList.add(e);
     if (changed) {
       Collections.sort(newList, comparator);
@@ -129,7 +129,7 @@ public class SortedList<E> implements List<E>, RandomAccess {
 
   @Override
   public synchronized boolean remove(Object o) {
-    ArrayList<E> newList = new ArrayList<E>(list);
+    ArrayList<E> newList = new ArrayList<>(list);
     // Removals in ArrayList won't break sorting
     boolean changed = newList.remove(o);
     list = Collections.unmodifiableList(newList);
@@ -143,7 +143,7 @@ public class SortedList<E> implements List<E>, RandomAccess {
 
   @Override
   public synchronized boolean addAll(Collection<? extends E> c) {
-    ArrayList<E> newList = new ArrayList<E>(list);
+    ArrayList<E> newList = new ArrayList<>(list);
     boolean changed = newList.addAll(c);
     if (changed) {
       Collections.sort(newList, comparator);
@@ -154,7 +154,7 @@ public class SortedList<E> implements List<E>, RandomAccess {
 
   @Override
   public synchronized boolean addAll(int index, Collection<? extends E> c) {
-    ArrayList<E> newList = new ArrayList<E>(list);
+    ArrayList<E> newList = new ArrayList<>(list);
     boolean changed = newList.addAll(index, c);
     if (changed) {
       Collections.sort(newList, comparator);
@@ -165,7 +165,7 @@ public class SortedList<E> implements List<E>, RandomAccess {
 
   @Override
   public synchronized boolean removeAll(Collection<?> c) {
-    ArrayList<E> newList = new ArrayList<E>(list);
+    ArrayList<E> newList = new ArrayList<>(list);
     // Removals in ArrayList won't break sorting
     boolean changed = newList.removeAll(c);
     list = Collections.unmodifiableList(newList);
@@ -174,7 +174,7 @@ public class SortedList<E> implements List<E>, RandomAccess {
 
   @Override
   public synchronized boolean retainAll(Collection<?> c) {
-    ArrayList<E> newList = new ArrayList<E>(list);
+    ArrayList<E> newList = new ArrayList<>(list);
     // Removals in ArrayList won't break sorting
     boolean changed = newList.retainAll(c);
     list = Collections.unmodifiableList(newList);
@@ -193,7 +193,7 @@ public class SortedList<E> implements List<E>, RandomAccess {
 
   @Override
   public synchronized E set(int index, E element) {
-    ArrayList<E> newList = new ArrayList<E>(list);
+    ArrayList<E> newList = new ArrayList<>(list);
     E result = newList.set(index, element);
     Collections.sort(list, comparator);
     list = Collections.unmodifiableList(newList);
@@ -202,7 +202,7 @@ public class SortedList<E> implements List<E>, RandomAccess {
 
   @Override
   public synchronized void add(int index, E element) {
-    ArrayList<E> newList = new ArrayList<E>(list);
+    ArrayList<E> newList = new ArrayList<>(list);
     newList.add(index, element);
     Collections.sort(list, comparator);
     list = Collections.unmodifiableList(newList);
@@ -210,7 +210,7 @@ public class SortedList<E> implements List<E>, RandomAccess {
 
   @Override
   public synchronized E remove(int index) {
-    ArrayList<E> newList = new ArrayList<E>(list);
+    ArrayList<E> newList = new ArrayList<>(list);
     // Removals in ArrayList won't break sorting
     E result = newList.remove(index);
     list = Collections.unmodifiableList(newList);
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/util/hbck/HFileCorruptionChecker.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/util/hbck/HFileCorruptionChecker.java
index 820da7a..82200bd 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/util/hbck/HFileCorruptionChecker.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/util/hbck/HFileCorruptionChecker.java
@@ -66,14 +66,14 @@ public class HFileCorruptionChecker {
   final FileSystem fs;
   final CacheConfig cacheConf;
   final ExecutorService executor;
-  final Set<Path> corrupted = new ConcurrentSkipListSet<Path>();
-  final Set<Path> failures = new ConcurrentSkipListSet<Path>();
-  final Set<Path> quarantined = new ConcurrentSkipListSet<Path>();
-  final Set<Path> missing = new ConcurrentSkipListSet<Path>();
-  final Set<Path> corruptedMobFiles = new ConcurrentSkipListSet<Path>();
-  final Set<Path> failureMobFiles = new ConcurrentSkipListSet<Path>();
-  final Set<Path> missedMobFiles = new ConcurrentSkipListSet<Path>();
-  final Set<Path> quarantinedMobFiles = new ConcurrentSkipListSet<Path>();
+  final Set<Path> corrupted = new ConcurrentSkipListSet<>();
+  final Set<Path> failures = new ConcurrentSkipListSet<>();
+  final Set<Path> quarantined = new ConcurrentSkipListSet<>();
+  final Set<Path> missing = new ConcurrentSkipListSet<>();
+  final Set<Path> corruptedMobFiles = new ConcurrentSkipListSet<>();
+  final Set<Path> failureMobFiles = new ConcurrentSkipListSet<>();
+  final Set<Path> missedMobFiles = new ConcurrentSkipListSet<>();
+  final Set<Path> quarantinedMobFiles = new ConcurrentSkipListSet<>();
   final boolean inQuarantineMode;
   final AtomicInteger hfilesChecked = new AtomicInteger();
   final AtomicInteger mobFilesChecked = new AtomicInteger();
@@ -343,7 +343,7 @@ public class HFileCorruptionChecker {
     }
 
     // Parallelize check at the region dir level
-    List<RegionDirChecker> rdcs = new ArrayList<RegionDirChecker>(rds.size() + 1);
+    List<RegionDirChecker> rdcs = new ArrayList<>(rds.size() + 1);
     List<Future<Void>> rdFutures;
 
     for (FileStatus rdFs : rds) {
@@ -451,14 +451,14 @@ public class HFileCorruptionChecker {
    * @return the set of check failure file paths after checkTables is called.
    */
   public Collection<Path> getFailures() {
-    return new HashSet<Path>(failures);
+    return new HashSet<>(failures);
   }
 
   /**
    * @return the set of corrupted file paths after checkTables is called.
    */
   public Collection<Path> getCorrupted() {
-    return new HashSet<Path>(corrupted);
+    return new HashSet<>(corrupted);
   }
 
   /**
@@ -472,7 +472,7 @@ public class HFileCorruptionChecker {
    * @return the set of successfully quarantined paths after checkTables is called.
    */
   public Collection<Path> getQuarantined() {
-    return new HashSet<Path>(quarantined);
+    return new HashSet<>(quarantined);
   }
 
   /**
@@ -480,21 +480,21 @@ public class HFileCorruptionChecker {
    *  compaction or flushes.
    */
   public Collection<Path> getMissing() {
-    return new HashSet<Path>(missing);
+    return new HashSet<>(missing);
   }
 
   /**
    * @return the set of check failure mob file paths after checkTables is called.
    */
   public Collection<Path> getFailureMobFiles() {
-    return new HashSet<Path>(failureMobFiles);
+    return new HashSet<>(failureMobFiles);
   }
 
   /**
    * @return the set of corrupted mob file paths after checkTables is called.
    */
   public Collection<Path> getCorruptedMobFiles() {
-    return new HashSet<Path>(corruptedMobFiles);
+    return new HashSet<>(corruptedMobFiles);
   }
 
   /**
@@ -508,7 +508,7 @@ public class HFileCorruptionChecker {
    * @return the set of successfully quarantined paths after checkTables is called.
    */
   public Collection<Path> getQuarantinedMobFiles() {
-    return new HashSet<Path>(quarantinedMobFiles);
+    return new HashSet<>(quarantinedMobFiles);
   }
 
   /**
@@ -516,7 +516,7 @@ public class HFileCorruptionChecker {
    *  deletion/moves from compaction.
    */
   public Collection<Path> getMissedMobFiles() {
-    return new HashSet<Path>(missedMobFiles);
+    return new HashSet<>(missedMobFiles);
   }
 
   /**
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/wal/AbstractFSWALProvider.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/wal/AbstractFSWALProvider.java
index bdd319d..9dd85d8 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/wal/AbstractFSWALProvider.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/wal/AbstractFSWALProvider.java
@@ -115,7 +115,7 @@ public abstract class AbstractFSWALProvider<T extends AbstractFSWAL<?>> implemen
     if (wal == null) {
       return Collections.emptyList();
     }
-    List<WAL> wals = new ArrayList<WAL>(1);
+    List<WAL> wals = new ArrayList<>(1);
     wals.add(wal);
     return wals;
   }
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/wal/BoundedGroupingStrategy.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/wal/BoundedGroupingStrategy.java
index 5b32347..81b1c00 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/wal/BoundedGroupingStrategy.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/wal/BoundedGroupingStrategy.java
@@ -38,8 +38,7 @@ public class BoundedGroupingStrategy implements RegionGroupingStrategy{
   static final String NUM_REGION_GROUPS = "hbase.wal.regiongrouping.numgroups";
   static final int DEFAULT_NUM_REGION_GROUPS = 2;
 
-  private ConcurrentHashMap<String, String> groupNameCache =
-      new ConcurrentHashMap<String, String>();
+  private ConcurrentHashMap<String, String> groupNameCache = new ConcurrentHashMap<>();
   private AtomicInteger counter = new AtomicInteger(0);
   private String[] groupNames;
 
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/wal/DisabledWALProvider.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/wal/DisabledWALProvider.java
index 5bee923..b442f07 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/wal/DisabledWALProvider.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/wal/DisabledWALProvider.java
@@ -68,7 +68,7 @@ class DisabledWALProvider implements WALProvider {
 
   @Override
   public List<WAL> getWALs() throws IOException {
-    List<WAL> wals = new ArrayList<WAL>(1);
+    List<WAL> wals = new ArrayList<>(1);
     wals.add(disabled);
     return wals;
   }
@@ -89,8 +89,7 @@ class DisabledWALProvider implements WALProvider {
   }
 
   private static class DisabledWAL implements WAL {
-    protected final List<WALActionsListener> listeners =
-        new CopyOnWriteArrayList<WALActionsListener>();
+    protected final List<WALActionsListener> listeners = new CopyOnWriteArrayList<>();
     protected final Path path;
     protected final WALCoprocessorHost coprocessorHost;
     protected final AtomicBoolean closed = new AtomicBoolean(false);
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/wal/RegionGroupingProvider.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/wal/RegionGroupingProvider.java
index 25e70d7..dee36e8 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/wal/RegionGroupingProvider.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/wal/RegionGroupingProvider.java
@@ -171,7 +171,7 @@ public class RegionGroupingProvider implements WALProvider {
 
   @Override
   public List<WAL> getWALs() throws IOException {
-    List<WAL> wals = new ArrayList<WAL>();
+    List<WAL> wals = new ArrayList<>();
     for (WALProvider provider : cached.values()) {
       wals.addAll(provider.getWALs());
     }
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/wal/WALFactory.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/wal/WALFactory.java
index abdc20c..114715f 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/wal/WALFactory.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/wal/WALFactory.java
@@ -93,7 +93,7 @@ public class WALFactory {
   // The meta updates are written to a different wal. If this
   // regionserver holds meta regions, then this ref will be non-null.
   // lazily intialized; most RegionServers don't deal with META
-  final AtomicReference<WALProvider> metaProvider = new AtomicReference<WALProvider>();
+  final AtomicReference<WALProvider> metaProvider = new AtomicReference<>();
 
   /**
    * Configuration-specified WAL Reader used when a custom reader is requested
@@ -368,7 +368,7 @@ public class WALFactory {
   // untangle the reliance on state in the filesystem. They rely on singleton
   // WALFactory that just provides Reader / Writers.
   // For now, first Configuration object wins. Practically this just impacts the reader/writer class
-  private static final AtomicReference<WALFactory> singleton = new AtomicReference<WALFactory>();
+  private static final AtomicReference<WALFactory> singleton = new AtomicReference<>();
   private static final String SINGLETON_ID = WALFactory.class.getName();
   
   // public only for FSHLog
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/wal/WALKey.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/wal/WALKey.java
index 276ab36..9a8003a 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/wal/WALKey.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/wal/WALKey.java
@@ -192,19 +192,19 @@ public class WALKey implements SequenceId, Comparable<WALKey> {
 
   public WALKey() {
     init(null, null, 0L, HConstants.LATEST_TIMESTAMP,
-        new ArrayList<UUID>(), HConstants.NO_NONCE, HConstants.NO_NONCE, null, null);
+        new ArrayList<>(), HConstants.NO_NONCE, HConstants.NO_NONCE, null, null);
   }
 
   public WALKey(final NavigableMap<byte[], Integer> replicationScope) {
     init(null, null, 0L, HConstants.LATEST_TIMESTAMP,
-        new ArrayList<UUID>(), HConstants.NO_NONCE, HConstants.NO_NONCE, null, replicationScope);
+        new ArrayList<>(), HConstants.NO_NONCE, HConstants.NO_NONCE, null, replicationScope);
   }
 
   @VisibleForTesting
   public WALKey(final byte[] encodedRegionName, final TableName tablename,
                 long logSeqNum,
       final long now, UUID clusterId) {
-    List<UUID> clusterIds = new ArrayList<UUID>(1);
+    List<UUID> clusterIds = new ArrayList<>(1);
     clusterIds.add(clusterId);
     init(encodedRegionName, tablename, logSeqNum, now, clusterIds,
         HConstants.NO_NONCE, HConstants.NO_NONCE, null, null);
@@ -543,7 +543,7 @@ public class WALKey implements SequenceId, Comparable<WALKey> {
    * @return a Map containing data from this key
    */
   public Map<String, Object> toStringMap() {
-    Map<String, Object> stringMap = new HashMap<String, Object>();
+    Map<String, Object> stringMap = new HashMap<>();
     stringMap.put("table", tablename);
     stringMap.put("region", Bytes.toStringBinary(encodedRegionName));
     stringMap.put("sequence", getSequenceId());
@@ -684,7 +684,7 @@ public class WALKey implements SequenceId, Comparable<WALKey> {
     }
     this.replicationScope = null;
     if (walKey.getScopesCount() > 0) {
-      this.replicationScope = new TreeMap<byte[], Integer>(Bytes.BYTES_COMPARATOR);
+      this.replicationScope = new TreeMap<>(Bytes.BYTES_COMPARATOR);
       for (FamilyScope scope : walKey.getScopesList()) {
         byte[] family = (compressionContext == null) ? scope.getFamily().toByteArray() :
           uncompressor.uncompress(scope.getFamily(), compressionContext.familyDict);
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/wal/WALPrettyPrinter.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/wal/WALPrettyPrinter.java
index a6fd85f..37473e9 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/wal/WALPrettyPrinter.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/wal/WALPrettyPrinter.java
@@ -283,10 +283,10 @@ public class WALPrettyPrinter {
         if (region != null && !((String) txn.get("region")).equals(region))
           continue;
         // initialize list into which we will store atomic actions
-        List<Map> actions = new ArrayList<Map>();
+        List<Map> actions = new ArrayList<>();
         for (Cell cell : edit.getCells()) {
           // add atomic operation to txn
-          Map<String, Object> op = new HashMap<String, Object>(toStringMap(cell));
+          Map<String, Object> op = new HashMap<>(toStringMap(cell));
           if (outputValues) op.put("value", Bytes.toStringBinary(CellUtil.cloneValue(cell)));
           // check row output filter
           if (row == null || ((String) op.get("row")).equals(row)) {
@@ -328,7 +328,7 @@ public class WALPrettyPrinter {
   }
 
   private static Map<String, Object> toStringMap(Cell cell) {
-    Map<String, Object> stringMap = new HashMap<String, Object>();
+    Map<String, Object> stringMap = new HashMap<>();
     stringMap.put("row",
         Bytes.toStringBinary(cell.getRowArray(), cell.getRowOffset(), cell.getRowLength()));
     stringMap.put("family", Bytes.toStringBinary(cell.getFamilyArray(), cell.getFamilyOffset(),
@@ -339,7 +339,7 @@ public class WALPrettyPrinter {
     stringMap.put("timestamp", cell.getTimestamp());
     stringMap.put("vlen", cell.getValueLength());
     if (cell.getTagsLength() > 0) {
-      List<String> tagsString = new ArrayList<String>();
+      List<String> tagsString = new ArrayList<>();
       Iterator<Tag> tagsIterator = CellUtil.tagsIterator(cell);
       while (tagsIterator.hasNext()) {
         Tag tag = tagsIterator.next();
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/wal/WALSplitter.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/wal/WALSplitter.java
index 2cf2c6b..d87c71b 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/wal/WALSplitter.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/wal/WALSplitter.java
@@ -158,11 +158,10 @@ public class WALSplitter {
   protected boolean distributedLogReplay;
 
   // Map encodedRegionName -> lastFlushedSequenceId
-  protected Map<String, Long> lastFlushedSequenceIds = new ConcurrentHashMap<String, Long>();
+  protected Map<String, Long> lastFlushedSequenceIds = new ConcurrentHashMap<>();
 
   // Map encodedRegionName -> maxSeqIdInStores
-  protected Map<String, Map<byte[], Long>> regionMaxSeqIdInStores =
-      new ConcurrentHashMap<String, Map<byte[], Long>>();
+  protected Map<String, Map<byte[], Long>> regionMaxSeqIdInStores = new ConcurrentHashMap<>();
 
   // Failed region server that the wal file being split belongs to
   protected String failedServerName = "";
@@ -245,7 +244,7 @@ public class WALSplitter {
       FileSystem fs, Configuration conf, final WALFactory factory) throws IOException {
     final FileStatus[] logfiles = SplitLogManager.getFileList(conf,
         Collections.singletonList(logDir), null);
-    List<Path> splits = new ArrayList<Path>();
+    List<Path> splits = new ArrayList<>();
     if (logfiles != null && logfiles.length > 0) {
       for (FileStatus logfile: logfiles) {
         WALSplitter s = new WALSplitter(factory, conf, rootDir, fs, null, null,
@@ -331,7 +330,7 @@ public class WALSplitter {
             }
           } else if (sequenceIdChecker != null) {
             RegionStoreSequenceIds ids = sequenceIdChecker.getLastSequenceId(region);
-            Map<byte[], Long> maxSeqIdInStores = new TreeMap<byte[], Long>(Bytes.BYTES_COMPARATOR);
+            Map<byte[], Long> maxSeqIdInStores = new TreeMap<>(Bytes.BYTES_COMPARATOR);
             for (StoreSequenceId storeSeqId : ids.getStoreSequenceIdList()) {
               maxSeqIdInStores.put(storeSeqId.getFamilyName().toByteArray(),
                 storeSeqId.getSequenceId());
@@ -447,8 +446,8 @@ public class WALSplitter {
 
   private static void finishSplitLogFile(Path rootdir, Path oldLogDir,
       Path logPath, Configuration conf) throws IOException {
-    List<Path> processedLogs = new ArrayList<Path>();
-    List<Path> corruptedLogs = new ArrayList<Path>();
+    List<Path> processedLogs = new ArrayList<>();
+    List<Path> corruptedLogs = new ArrayList<>();
     FileSystem fs;
     fs = rootdir.getFileSystem(conf);
     if (ZKSplitLog.isCorrupted(rootdir, logPath.getName(), fs)) {
@@ -614,7 +613,7 @@ public class WALSplitter {
    */
   public static NavigableSet<Path> getSplitEditFilesSorted(final FileSystem fs,
       final Path regiondir) throws IOException {
-    NavigableSet<Path> filesSorted = new TreeSet<Path>();
+    NavigableSet<Path> filesSorted = new TreeSet<>();
     Path editsdir = getRegionDirRecoveredEditsDir(regiondir);
     if (!fs.exists(editsdir))
       return filesSorted;
@@ -872,7 +871,7 @@ public class WALSplitter {
   public static class PipelineController {
     // If an exception is thrown by one of the other threads, it will be
     // stored here.
-    AtomicReference<Throwable> thrown = new AtomicReference<Throwable>();
+    AtomicReference<Throwable> thrown = new AtomicReference<>();
 
     // Wait/notify for when data has been produced by the writer thread,
     // consumed by the reader thread, or an exception occurred
@@ -906,13 +905,12 @@ public class WALSplitter {
   public static class EntryBuffers {
     PipelineController controller;
 
-    Map<byte[], RegionEntryBuffer> buffers =
-      new TreeMap<byte[], RegionEntryBuffer>(Bytes.BYTES_COMPARATOR);
+    Map<byte[], RegionEntryBuffer> buffers = new TreeMap<>(Bytes.BYTES_COMPARATOR);
 
     /* Track which regions are currently in the middle of writing. We don't allow
        an IO thread to pick up bytes from a region if we're already writing
        data for that region in a different IO thread. */
-    Set<byte[]> currentlyWriting = new TreeSet<byte[]>(Bytes.BYTES_COMPARATOR);
+    Set<byte[]> currentlyWriting = new TreeSet<>(Bytes.BYTES_COMPARATOR);
 
     long totalBuffered = 0;
     long maxHeapUsage;
@@ -1027,7 +1025,7 @@ public class WALSplitter {
     RegionEntryBuffer(TableName tableName, byte[] region) {
       this.tableName = tableName;
       this.encodedRegionName = region;
-      this.entryBuffer = new LinkedList<Entry>();
+      this.entryBuffer = new LinkedList<>();
     }
 
     long appendEntry(Entry entry) {
@@ -1148,7 +1146,7 @@ public class WALSplitter {
 
     /* Set of regions which we've decided should not output edits */
     protected final Set<byte[]> blacklistedRegions = Collections
-        .synchronizedSet(new TreeSet<byte[]>(Bytes.BYTES_COMPARATOR));
+        .synchronizedSet(new TreeSet<>(Bytes.BYTES_COMPARATOR));
 
     protected boolean closeAndCleanCompleted = false;
 
@@ -1360,7 +1358,7 @@ public class WALSplitter {
     private List<Path> close() throws IOException {
       Preconditions.checkState(!closeAndCleanCompleted);
 
-      final List<Path> paths = new ArrayList<Path>();
+      final List<Path> paths = new ArrayList<>();
       final List<IOException> thrown = Lists.newArrayList();
       ThreadPoolExecutor closeThreadPool = Threads.getBoundedCachedThreadPool(numThreads, 30L,
         TimeUnit.SECONDS, new ThreadFactory() {
@@ -1372,8 +1370,7 @@ public class WALSplitter {
             return t;
           }
         });
-      CompletionService<Void> completionService =
-        new ExecutorCompletionService<Void>(closeThreadPool);
+      CompletionService<Void> completionService = new ExecutorCompletionService<>(closeThreadPool);
       for (final Map.Entry<byte[], SinkWriter> writersEntry : writers.entrySet()) {
         if (LOG.isTraceEnabled()) {
           LOG.trace("Submitting close of " + ((WriterAndPath)writersEntry.getValue()).p);
@@ -1558,7 +1555,7 @@ public class WALSplitter {
       }
       // Create the array list for the cells that aren't filtered.
       // We make the assumption that most cells will be kept.
-      ArrayList<Cell> keptCells = new ArrayList<Cell>(logEntry.getEdit().getCells().size());
+      ArrayList<Cell> keptCells = new ArrayList<>(logEntry.getEdit().getCells().size());
       for (Cell cell : logEntry.getEdit().getCells()) {
         if (CellUtil.matchingFamily(cell, WALEdit.METAFAMILY)) {
           keptCells.add(cell);
@@ -1639,7 +1636,7 @@ public class WALSplitter {
      */
     @Override
     public Map<byte[], Long> getOutputCounts() {
-      TreeMap<byte[], Long> ret = new TreeMap<byte[], Long>(Bytes.BYTES_COMPARATOR);
+      TreeMap<byte[], Long> ret = new TreeMap<>(Bytes.BYTES_COMPARATOR);
       synchronized (writers) {
         for (Map.Entry<byte[], SinkWriter> entry : writers.entrySet()) {
           ret.put(entry.getKey(), entry.getValue().editsWritten);
@@ -1705,8 +1702,7 @@ public class WALSplitter {
     private final Set<String> recoveredRegions = Collections.synchronizedSet(new HashSet<String>());
     private final Map<String, RegionServerWriter> writers = new ConcurrentHashMap<>();
     // online encoded region name -> region location map
-    private final Map<String, HRegionLocation> onlineRegions =
-        new ConcurrentHashMap<String, HRegionLocation>();
+    private final Map<String, HRegionLocation> onlineRegions = new ConcurrentHashMap<>();
 
     private final Map<TableName, ClusterConnection> tableNameToHConnectionMap = Collections
         .synchronizedMap(new TreeMap<TableName, ClusterConnection>());
@@ -1859,7 +1855,7 @@ public class WALSplitter {
                 + encodeRegionNameStr);
             lastFlushedSequenceIds.put(encodeRegionNameStr, Long.MAX_VALUE);
             if (nonExistentTables == null) {
-              nonExistentTables = new TreeSet<TableName>();
+              nonExistentTables = new TreeSet<>();
             }
             nonExistentTables.add(table);
             this.skippedEdits.incrementAndGet();
@@ -1906,7 +1902,7 @@ public class WALSplitter {
                 Collections.synchronizedList(new ArrayList<Pair<HRegionLocation, Entry>>());
             serverToBufferQueueMap.put(locKey, queue);
           }
-          queue.add(new Pair<HRegionLocation, Entry>(loc, entry));
+          queue.add(new Pair<>(loc, entry));
         }
         // store regions we have recovered so far
         addToRecoveredRegions(loc.getRegionInfo().getEncodedName());
@@ -1957,7 +1953,7 @@ public class WALSplitter {
               loc.getRegionInfo().getEncodedName());
         if (ids != null) {
           lastFlushedSequenceId = ids.getLastFlushedSequenceId();
-          Map<byte[], Long> storeIds = new TreeMap<byte[], Long>(Bytes.BYTES_COMPARATOR);
+          Map<byte[], Long> storeIds = new TreeMap<>(Bytes.BYTES_COMPARATOR);
           List<StoreSequenceId> maxSeqIdInStores = ids.getStoreSequenceIdList();
           for (StoreSequenceId id : maxSeqIdInStores) {
             storeIds.put(id.getFamilyName().toByteArray(), id.getSequenceId());
@@ -2102,7 +2098,7 @@ public class WALSplitter {
         if (hasEditsInDisablingOrDisabledTables) {
           splits = logRecoveredEditsOutputSink.finishWritingAndClose();
         } else {
-          splits = new ArrayList<Path>();
+          splits = new ArrayList<>();
         }
         // returns an empty array in order to keep interface same as old way
         return splits;
@@ -2316,13 +2312,13 @@ public class WALSplitter {
 
     if (entry == null) {
       // return an empty array
-      return new ArrayList<MutationReplay>();
+      return new ArrayList<>();
     }
 
     long replaySeqId = (entry.getKey().hasOrigSequenceNumber()) ?
       entry.getKey().getOrigSequenceNumber() : entry.getKey().getLogSequenceNumber();
     int count = entry.getAssociatedCellCount();
-    List<MutationReplay> mutations = new ArrayList<MutationReplay>();
+    List<MutationReplay> mutations = new ArrayList<>();
     Cell previousCell = null;
     Mutation m = null;
     WALKey key = null;
@@ -2369,7 +2365,7 @@ public class WALSplitter {
     if (logEntry != null) {
       org.apache.hadoop.hbase.shaded.protobuf.generated.WALProtos.WALKey walKeyProto =
           entry.getKey();
-      List<UUID> clusterIds = new ArrayList<UUID>(walKeyProto.getClusterIdsCount());
+      List<UUID> clusterIds = new ArrayList<>(walKeyProto.getClusterIdsCount());
       for (HBaseProtos.UUID uuid : entry.getKey().getClusterIdsList()) {
         clusterIds.add(new UUID(uuid.getMostSigBits(), uuid.getLeastSigBits()));
       }
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/zookeeper/DrainingServerTracker.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/zookeeper/DrainingServerTracker.java
index e6d3b7f..32e0862 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/zookeeper/DrainingServerTracker.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/zookeeper/DrainingServerTracker.java
@@ -50,7 +50,7 @@ public class DrainingServerTracker extends ZooKeeperListener {
   private static final Log LOG = LogFactory.getLog(DrainingServerTracker.class);
 
   private ServerManager serverManager;
-  private final NavigableSet<ServerName> drainingServers = new TreeSet<ServerName>();
+  private final NavigableSet<ServerName> drainingServers = new TreeSet<>();
   private Abortable abortable;
 
   public DrainingServerTracker(ZooKeeperWatcher watcher,
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/zookeeper/MiniZooKeeperCluster.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/zookeeper/MiniZooKeeperCluster.java
index ff73073..7dea269 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/zookeeper/MiniZooKeeperCluster.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/zookeeper/MiniZooKeeperCluster.java
@@ -80,9 +80,9 @@ public class MiniZooKeeperCluster {
     this.started = false;
     this.configuration = configuration;
     activeZKServerIndex = -1;
-    zooKeeperServers = new ArrayList<ZooKeeperServer>();
-    clientPortList = new ArrayList<Integer>();
-    standaloneServerFactoryList = new ArrayList<NIOServerCnxnFactory>();
+    zooKeeperServers = new ArrayList<>();
+    clientPortList = new ArrayList<>();
+    standaloneServerFactoryList = new ArrayList<>();
     connectionTimeout = configuration.getInt(HConstants.ZK_SESSION_TIMEOUT + ".localHBaseCluster",
       DEFAULT_CONNECTION_TIMEOUT);
   }
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/zookeeper/RegionServerTracker.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/zookeeper/RegionServerTracker.java
index 19d2d00..69cd233 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/zookeeper/RegionServerTracker.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/zookeeper/RegionServerTracker.java
@@ -48,8 +48,7 @@ import org.apache.zookeeper.KeeperException;
 @InterfaceAudience.Private
 public class RegionServerTracker extends ZooKeeperListener {
   private static final Log LOG = LogFactory.getLog(RegionServerTracker.class);
-  private NavigableMap<ServerName, RegionServerInfo> regionServers = 
-      new TreeMap<ServerName, RegionServerInfo>();
+  private NavigableMap<ServerName, RegionServerInfo> regionServers = new TreeMap<>();
   private ServerManager serverManager;
   private Server server;
 
@@ -154,7 +153,7 @@ public class RegionServerTracker extends ZooKeeperListener {
    */
   public List<ServerName> getOnlineServers() {
     synchronized (this.regionServers) {
-      return new ArrayList<ServerName>(this.regionServers.keySet());
+      return new ArrayList<>(this.regionServers.keySet());
     }
   }
 }
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/zookeeper/ZKServerTool.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/zookeeper/ZKServerTool.java
index 455cfd2..b96924d 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/zookeeper/ZKServerTool.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/zookeeper/ZKServerTool.java
@@ -36,7 +36,7 @@ import java.util.List;
 @InterfaceAudience.LimitedPrivate(HBaseInterfaceAudience.TOOLS)
 public class ZKServerTool {
   public static ServerName[] readZKNodes(Configuration conf) {
-    List<ServerName> hosts = new LinkedList<ServerName>();
+    List<ServerName> hosts = new LinkedList<>();
     String quorum = conf.get(HConstants.ZOOKEEPER_QUORUM, HConstants.LOCALHOST);
 
     String[] values = quorum.split(",");
diff --git a/hbase-server/src/main/resources/hbase-webapps/master/table.jsp b/hbase-server/src/main/resources/hbase-webapps/master/table.jsp
index 897dad7..0f8a289 100644
--- a/hbase-server/src/main/resources/hbase-webapps/master/table.jsp
+++ b/hbase-server/src/main/resources/hbase-webapps/master/table.jsp
@@ -372,11 +372,11 @@ if ( fqtn != null ) {
   long totalStoreFileCount = 0;
   long totalMemSize = 0;
   String urlRegionServer = null;
-  Map<ServerName, Integer> regDistribution = new TreeMap<ServerName, Integer>();
-  Map<ServerName, Integer> primaryRegDistribution = new TreeMap<ServerName, Integer>();
+  Map<ServerName, Integer> regDistribution = new TreeMap<>();
+  Map<ServerName, Integer> primaryRegDistribution = new TreeMap<>();
   List<HRegionLocation> regions = r.getAllRegionLocations();
-  Map<HRegionInfo, RegionLoad> regionsToLoad = new LinkedHashMap<HRegionInfo, RegionLoad>();
-  Map<HRegionInfo, ServerName> regionsToServer = new LinkedHashMap<HRegionInfo, ServerName>();
+  Map<HRegionInfo, RegionLoad> regionsToLoad = new LinkedHashMap<>();
+  Map<HRegionInfo, ServerName> regionsToServer = new LinkedHashMap<>();
   for (HRegionLocation hriEntry : regions) {
     HRegionInfo regionInfo = hriEntry.getRegionInfo();
     ServerName addr = hriEntry.getServerName();
@@ -448,7 +448,7 @@ ShowDetailName&Start/End Key<input type="checkbox" id="showWhole" style="margin-
 </tr>
 
 <%
-  List<Map.Entry<HRegionInfo, RegionLoad>> entryList = new ArrayList<Map.Entry<HRegionInfo, RegionLoad>>(regionsToLoad.entrySet());
+  List<Map.Entry<HRegionInfo, RegionLoad>> entryList = new ArrayList<>(regionsToLoad.entrySet());
   if(sortKey != null) {
     if (sortKey.equals("readrequest")) {
       Collections.sort(entryList,
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/HBaseTestingUtility.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/HBaseTestingUtility.java
index 724761a..47170b1 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/HBaseTestingUtility.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/HBaseTestingUtility.java
@@ -2287,7 +2287,7 @@ public class HBaseTestingUtility extends HBaseCommonTestingUtility {
 
   public int countRows(final InternalScanner scanner) throws IOException {
     int scannedCount = 0;
-    List<Cell> results = new ArrayList<Cell>();
+    List<Cell> results = new ArrayList<>();
     boolean hasMore = true;
     while (hasMore) {
       hasMore = scanner.next(results);
@@ -2367,7 +2367,7 @@ public class HBaseTestingUtility extends HBaseCommonTestingUtility {
   throws IOException {
     Table meta = getConnection().getTable(TableName.META_TABLE_NAME);
     Arrays.sort(startKeys, Bytes.BYTES_COMPARATOR);
-    List<HRegionInfo> newRegions = new ArrayList<HRegionInfo>(startKeys.length);
+    List<HRegionInfo> newRegions = new ArrayList<>(startKeys.length);
     MetaTableAccessor
         .updateTableState(getConnection(), htd.getTableName(), TableState.State.ENABLED);
     // add custom ones
@@ -2426,7 +2426,7 @@ public class HBaseTestingUtility extends HBaseCommonTestingUtility {
   public List<byte[]> getMetaTableRows() throws IOException {
     // TODO: Redo using MetaTableAccessor class
     Table t = getConnection().getTable(TableName.META_TABLE_NAME);
-    List<byte[]> rows = new ArrayList<byte[]>();
+    List<byte[]> rows = new ArrayList<>();
     ResultScanner s = t.getScanner(new Scan());
     for (Result result : s) {
       LOG.info("getMetaTableRows: row -> " +
@@ -2446,7 +2446,7 @@ public class HBaseTestingUtility extends HBaseCommonTestingUtility {
   public List<byte[]> getMetaTableRows(TableName tableName) throws IOException {
     // TODO: Redo using MetaTableAccessor.
     Table t = getConnection().getTable(TableName.META_TABLE_NAME);
-    List<byte[]> rows = new ArrayList<byte[]>();
+    List<byte[]> rows = new ArrayList<>();
     ResultScanner s = t.getScanner(new Scan());
     for (Result result : s) {
       HRegionInfo info = MetaTableAccessor.getHRegionInfo(result);
@@ -3219,7 +3219,7 @@ public class HBaseTestingUtility extends HBaseCommonTestingUtility {
 
   public static NavigableSet<String> getAllOnlineRegions(MiniHBaseCluster cluster)
       throws IOException {
-    NavigableSet<String> online = new TreeSet<String>();
+    NavigableSet<String> online = new TreeSet<>();
     for (RegionServerThread rst : cluster.getLiveRegionServerThreads()) {
       try {
         for (HRegionInfo region :
@@ -3391,7 +3391,7 @@ public class HBaseTestingUtility extends HBaseCommonTestingUtility {
         // readpoint 0.
         0);
 
-    List<Cell> result = new ArrayList<Cell>();
+    List<Cell> result = new ArrayList<>();
     scanner.next(result);
     if (!result.isEmpty()) {
       // verify that we are on the row we want:
@@ -3601,7 +3601,7 @@ public class HBaseTestingUtility extends HBaseCommonTestingUtility {
     private static final int MAX_RANDOM_PORT = 0xfffe;
 
     /** A set of ports that have been claimed using {@link #randomFreePort()}. */
-    private final Set<Integer> takenRandomPorts = new HashSet<Integer>();
+    private final Set<Integer> takenRandomPorts = new HashSet<>();
 
     private final Random random;
     private final AvailablePortChecker portChecker;
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/HTestConst.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/HTestConst.java
index e5334bf..268f79c 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/HTestConst.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/HTestConst.java
@@ -41,7 +41,7 @@ public class HTestConst {
   public static final byte[] DEFAULT_CF_BYTES = Bytes.toBytes(DEFAULT_CF_STR);
 
   public static final Set<String> DEFAULT_CF_STR_SET =
-      Collections.unmodifiableSet(new HashSet<String>(
+      Collections.unmodifiableSet(new HashSet<>(
           Arrays.asList(new String[] { DEFAULT_CF_STR })));
 
   public static final String DEFAULT_ROW_STR = "MyTestRow";
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/MetaMockingUtil.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/MetaMockingUtil.java
index 42e2811..9a1515b 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/MetaMockingUtil.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/MetaMockingUtil.java
@@ -68,7 +68,7 @@ public class MetaMockingUtil {
    */
   public static Result getMetaTableRowResult(HRegionInfo region, final ServerName sn,
       HRegionInfo splita, HRegionInfo splitb) throws IOException {
-    List<Cell> kvs = new ArrayList<Cell>();
+    List<Cell> kvs = new ArrayList<>();
     if (region != null) {
       kvs.add(new KeyValue(
         region.getRegionName(),
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/MiniHBaseCluster.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/MiniHBaseCluster.java
index a8887d4..55529c6 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/MiniHBaseCluster.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/MiniHBaseCluster.java
@@ -672,7 +672,7 @@ public class MiniHBaseCluster extends HBaseCluster {
   }
 
   public List<HRegion> getRegions(TableName tableName) {
-    List<HRegion> ret = new ArrayList<HRegion>();
+    List<HRegion> ret = new ArrayList<>();
     for (JVMClusterUtil.RegionServerThread rst : getRegionServerThreads()) {
       HRegionServer hrs = rst.getRegionServer();
       for (Region region : hrs.getOnlineRegionsLocalContext()) {
@@ -770,7 +770,7 @@ public class MiniHBaseCluster extends HBaseCluster {
   }
 
   public List<HRegion> findRegionsForTable(TableName tableName) {
-    ArrayList<HRegion> ret = new ArrayList<HRegion>();
+    ArrayList<HRegion> ret = new ArrayList<>();
     for (JVMClusterUtil.RegionServerThread rst : getRegionServerThreads()) {
       HRegionServer hrs = rst.getRegionServer();
       for (Region region : hrs.getOnlineRegions(tableName)) {
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/MockRegionServerServices.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/MockRegionServerServices.java
index 5e2a70f..7740e66 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/MockRegionServerServices.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/MockRegionServerServices.java
@@ -63,9 +63,9 @@ import com.google.protobuf.Service;
  */
 public class MockRegionServerServices implements RegionServerServices {
   protected static final Log LOG = LogFactory.getLog(MockRegionServerServices.class);
-  private final Map<String, Region> regions = new HashMap<String, Region>();
+  private final Map<String, Region> regions = new HashMap<>();
   private final ConcurrentSkipListMap<byte[], Boolean> rit =
-    new ConcurrentSkipListMap<byte[], Boolean>(Bytes.BYTES_COMPARATOR);
+    new ConcurrentSkipListMap<>(Bytes.BYTES_COMPARATOR);
   private HFileSystem hfs = null;
   private final Configuration conf;
   private ZooKeeperWatcher zkw = null;
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/MultithreadedTestUtil.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/MultithreadedTestUtil.java
index 7e251e7..cf07b42 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/MultithreadedTestUtil.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/MultithreadedTestUtil.java
@@ -39,7 +39,7 @@ public abstract class MultithreadedTestUtil {
     private Throwable err = null;
     private boolean stopped = false;
     private int threadDoneCount = 0;
-    private Set<TestThread> testThreads = new HashSet<TestThread>();
+    private Set<TestThread> testThreads = new HashSet<>();
 
     public TestContext(Configuration configuration) {
       this.conf = configuration;
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/PerformanceEvaluation.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/PerformanceEvaluation.java
index b7e4a71..f8345b1 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/PerformanceEvaluation.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/PerformanceEvaluation.java
@@ -146,7 +146,7 @@ public class PerformanceEvaluation extends Configured implements Tool {
   private static final BigDecimal BYTES_PER_MB = BigDecimal.valueOf(1024 * 1024);
   private static final TestOptions DEFAULT_OPTS = new TestOptions();
 
-  private static Map<String, CmdDescriptor> COMMANDS = new TreeMap<String, CmdDescriptor>();
+  private static Map<String, CmdDescriptor> COMMANDS = new TreeMap<>();
   private static final Path PERF_EVAL_DIR = new Path("performance_evaluation");
 
   static {
@@ -536,7 +536,7 @@ public class PerformanceEvaluation extends Configured implements Tool {
     Path inputFile = new Path(inputDir, JOB_INPUT_FILENAME);
     PrintStream out = new PrintStream(fs.create(inputFile));
     // Make input random.
-    Map<Integer, String> m = new TreeMap<Integer, String>();
+    Map<Integer, String> m = new TreeMap<>();
     Hash h = MurmurHash.getInstance();
     int perClientRows = (opts.totalRows / opts.numClientThreads);
     try {
@@ -1311,7 +1311,7 @@ public class PerformanceEvaluation extends Configured implements Tool {
     protected Pair<byte[], byte[]> generateStartAndStopRows(int maxRange) {
       int start = this.rand.nextInt(Integer.MAX_VALUE) % opts.totalRows;
       int stop = start + maxRange;
-      return new Pair<byte[],byte[]>(format(start), format(stop));
+      return new Pair<>(format(start), format(stop));
     }
 
     @Override
@@ -1375,7 +1375,7 @@ public class PerformanceEvaluation extends Configured implements Tool {
       consistency = options.replicas == DEFAULT_OPTS.replicas ? null : Consistency.TIMELINE;
       if (opts.multiGet > 0) {
         LOG.info("MultiGet enabled. Sending GETs in batches of " + opts.multiGet + ".");
-        this.gets = new ArrayList<Get>(opts.multiGet);
+        this.gets = new ArrayList<>(opts.multiGet);
       }
     }
 
@@ -2207,7 +2207,7 @@ public class PerformanceEvaluation extends Configured implements Tool {
     }
 
     try {
-      LinkedList<String> argv = new LinkedList<String>();
+      LinkedList<String> argv = new LinkedList<>();
       argv.addAll(Arrays.asList(args));
       TestOptions opts = parseOpts(argv);
 
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/PerformanceEvaluationCommons.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/PerformanceEvaluationCommons.java
index 3809a13..e2350e8 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/PerformanceEvaluationCommons.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/PerformanceEvaluationCommons.java
@@ -67,7 +67,7 @@ public class PerformanceEvaluationCommons {
   public static void concurrentReads(final Runnable r) {
     final int count = 1;
     long now = System.currentTimeMillis();
-    List<Thread> threads = new ArrayList<Thread>(count);
+    List<Thread> threads = new ArrayList<>(count);
     for (int i = 0; i < count; i++) {
       threads.add(new Thread(r, "concurrentRead-" + i));
     }
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/TestCheckTestClasses.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/TestCheckTestClasses.java
index 06b98f7..23ca57f 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/TestCheckTestClasses.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/TestCheckTestClasses.java
@@ -39,7 +39,7 @@ public class TestCheckTestClasses {
    */
   @Test
   public void checkClasses() throws Exception {
-    List<Class<?>> badClasses = new java.util.ArrayList<Class<?>>();
+    List<Class<?>> badClasses = new java.util.ArrayList<>();
     ClassTestFinder classFinder = new ClassTestFinder();
     for (Class<?> c : classFinder.findClasses(false)) {
       if (ClassTestFinder.getCategoryAnnotations(c).length == 0) {
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/TestGlobalMemStoreSize.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/TestGlobalMemStoreSize.java
index 5cc7ed9..8d19c1b 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/TestGlobalMemStoreSize.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/TestGlobalMemStoreSize.java
@@ -158,7 +158,7 @@ public class TestGlobalMemStoreSize {
   }
 
   private List<HRegionServer> getOnlineRegionServers() {
-    List<HRegionServer> list = new ArrayList<HRegionServer>();
+    List<HRegionServer> list = new ArrayList<>();
     for (JVMClusterUtil.RegionServerThread rst :
           cluster.getRegionServerThreads()) {
       if (rst.getRegionServer().isOnline()) {
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/TestHDFSBlocksDistribution.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/TestHDFSBlocksDistribution.java
index 2329fc2..06cfdcf 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/TestHDFSBlocksDistribution.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/TestHDFSBlocksDistribution.java
@@ -50,7 +50,7 @@ public class TestHDFSBlocksDistribution {
 
   public class MockHDFSBlocksDistribution extends HDFSBlocksDistribution {
     public Map<String,HostAndWeight> getHostAndWeights() {
-      HashMap<String, HostAndWeight> map = new HashMap<String, HostAndWeight>();
+      HashMap<String, HostAndWeight> map = new HashMap<>();
       map.put("test", new HostAndWeight(null, 100));
       return map;
     }
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/TestMetaTableAccessorNoCluster.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/TestMetaTableAccessorNoCluster.java
index 870ebb3..9915f99 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/TestMetaTableAccessorNoCluster.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/TestMetaTableAccessorNoCluster.java
@@ -87,7 +87,7 @@ public class TestMetaTableAccessorNoCluster {
   public void testGetHRegionInfo() throws IOException {
     assertNull(MetaTableAccessor.getHRegionInfo(new Result()));
 
-    List<Cell> kvs = new ArrayList<Cell>();
+    List<Cell> kvs = new ArrayList<>();
     Result r = Result.create(kvs);
     assertNull(MetaTableAccessor.getHRegionInfo(r));
 
@@ -141,7 +141,7 @@ public class TestMetaTableAccessorNoCluster {
       // show.  We will know if they happened or not because we will ask
       // mockito at the end of this test to verify that scan was indeed
       // called the wanted number of times.
-      List<Cell> kvs = new ArrayList<Cell>();
+      List<Cell> kvs = new ArrayList<>();
       final byte [] rowToVerify = Bytes.toBytes("rowToVerify");
       kvs.add(new KeyValue(rowToVerify,
         HConstants.CATALOG_FAMILY, HConstants.REGIONINFO_QUALIFIER,
@@ -152,7 +152,7 @@ public class TestMetaTableAccessorNoCluster {
       kvs.add(new KeyValue(rowToVerify,
         HConstants.CATALOG_FAMILY, HConstants.STARTCODE_QUALIFIER,
         Bytes.toBytes(sn.getStartcode())));
-      final List<CellScannable> cellScannables = new ArrayList<CellScannable>(1);
+      final List<CellScannable> cellScannables = new ArrayList<>(1);
       cellScannables.add(Result.create(kvs));
       final ScanResponse.Builder builder = ScanResponse.newBuilder();
       for (CellScannable result : cellScannables) {
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/TestPartialResultsFromClientSide.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/TestPartialResultsFromClientSide.java
index 61e3467..b53de3b 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/TestPartialResultsFromClientSide.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/TestPartialResultsFromClientSide.java
@@ -273,7 +273,7 @@ public class TestPartialResultsFromClientSide {
     int iterationCount = 0;
 
     while (oneShotResult != null && oneShotResult.rawCells() != null) {
-      List<Cell> aggregatePartialCells = new ArrayList<Cell>();
+      List<Cell> aggregatePartialCells = new ArrayList<>();
       do {
         partialResult = partialScanner.next();
         assertTrue("Partial Result is null. iteration: " + iterationCount, partialResult != null);
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/TestRegionRebalancing.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/TestRegionRebalancing.java
index 03c5524..283d79d 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/TestRegionRebalancing.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/TestRegionRebalancing.java
@@ -231,7 +231,7 @@ public class TestRegionRebalancing {
   }
 
   private List<HRegionServer> getOnlineRegionServers() {
-    List<HRegionServer> list = new ArrayList<HRegionServer>();
+    List<HRegionServer> list = new ArrayList<>();
     for (JVMClusterUtil.RegionServerThread rst :
         UTIL.getHBaseCluster().getRegionServerThreads()) {
       if (rst.getRegionServer().isOnline()) {
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/TestServerSideScanMetricsFromClientSide.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/TestServerSideScanMetricsFromClientSide.java
index b516cbb..7eaa742 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/TestServerSideScanMetricsFromClientSide.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/TestServerSideScanMetricsFromClientSide.java
@@ -280,7 +280,7 @@ public class TestServerSideScanMetricsFromClientSide {
     filter = new SingleColumnValueFilter(FAMILIES[0], QUALIFIERS[0], CompareOp.NOT_EQUAL, VALUE);
     testRowsFilteredMetric(baseScan, filter, ROWS.length);
 
-    List<Filter> filters = new ArrayList<Filter>();
+    List<Filter> filters = new ArrayList<>();
     filters.add(new RowFilter(CompareOp.EQUAL, new BinaryComparator(ROWS[0])));
     filters.add(new RowFilter(CompareOp.EQUAL, new BinaryComparator(ROWS[3])));
     int numberOfMatchingRowFilters = filters.size();
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/TestStochasticBalancerJmxMetrics.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/TestStochasticBalancerJmxMetrics.java
index d4f7cdd..c4abd89 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/TestStochasticBalancerJmxMetrics.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/TestStochasticBalancerJmxMetrics.java
@@ -222,7 +222,7 @@ public class TestStochasticBalancerJmxMetrics extends BalancerTestBase {
       target = new ObjectName("Hadoop", pairs);
       MBeanInfo beanInfo = mb.getMBeanInfo(target);
 
-      Set<String> existingAttrs = new HashSet<String>();
+      Set<String> existingAttrs = new HashSet<>();
       for (MBeanAttributeInfo attrInfo : beanInfo.getAttributes()) {
         existingAttrs.add(attrInfo.getName());
       }
@@ -255,7 +255,7 @@ public class TestStochasticBalancerJmxMetrics extends BalancerTestBase {
    * Given the tables and functions, return metrics names that should exist in JMX
    */
   private Set<String> getExpectedJmxMetrics(String[] tableNames, String[] functionNames) {
-    Set<String> ret = new HashSet<String>();
+    Set<String> ret = new HashSet<>();
 
     for (String tableName : tableNames) {
       ret.add(StochasticLoadBalancer.composeAttributeName(tableName, "Overall"));
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/backup/TestHFileArchiving.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/backup/TestHFileArchiving.java
index 284251f..1acb842 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/backup/TestHFileArchiving.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/backup/TestHFileArchiving.java
@@ -193,7 +193,7 @@ public class TestHFileArchiving {
     FileStatus[] regionFiles = FSUtils.listStatus(fs, regionDir, null);
     Assert.assertNotNull("No files in the region directory", regionFiles);
     if (LOG.isDebugEnabled()) {
-      List<Path> files = new ArrayList<Path>();
+      List<Path> files = new ArrayList<>();
       for (FileStatus file : regionFiles) {
         files.add(file.getPath());
       }
@@ -269,7 +269,7 @@ public class TestHFileArchiving {
   private void assertArchiveFiles(FileSystem fs, List<String> storeFiles, long timeout) throws IOException {
     long end = System.currentTimeMillis() + timeout;
     Path archiveDir = HFileArchiveUtil.getArchivePath(UTIL.getConfiguration());
-    List<String> archivedFiles = new ArrayList<String>();
+    List<String> archivedFiles = new ArrayList<>();
 
     // We have to ensure that the DeleteTableHandler is finished. HBaseAdmin.deleteXXX() can return before all files
     // are archived. We should fix HBASE-5487 and fix synchronous operations from admin.
@@ -434,7 +434,7 @@ public class TestHFileArchiving {
         return true;
       }
     });
-    return recurseOnFiles(fs, files, new ArrayList<String>());
+    return recurseOnFiles(fs, files, new ArrayList<>());
   }
 
   /** Recursively lookup all the file names under the file[] array **/
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/backup/example/TestZooKeeperTableArchiveClient.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/backup/example/TestZooKeeperTableArchiveClient.java
index 64139ee..fc56ebd 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/backup/example/TestZooKeeperTableArchiveClient.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/backup/example/TestZooKeeperTableArchiveClient.java
@@ -80,7 +80,7 @@ public class TestZooKeeperTableArchiveClient {
   private static final byte[] TEST_FAM = Bytes.toBytes("fam");
   private static final byte[] TABLE_NAME = Bytes.toBytes(STRING_TABLE_NAME);
   private static ZKTableArchiveClient archivingClient;
-  private final List<Path> toCleanup = new ArrayList<Path>();
+  private final List<Path> toCleanup = new ArrayList<>();
   private static ClusterConnection CONNECTION;
   private static RegionServerServices rss;
 
@@ -178,7 +178,7 @@ public class TestZooKeeperTableArchiveClient {
     // create the region
     HColumnDescriptor hcd = new HColumnDescriptor(TEST_FAM);
     HRegion region = UTIL.createTestRegion(STRING_TABLE_NAME, hcd);
-    List<Region> regions = new ArrayList<Region>();
+    List<Region> regions = new ArrayList<>();
     regions.add(region);
     when(rss.getOnlineRegions()).thenReturn(regions);
     final CompactedHFilesDischarger compactionCleaner =
@@ -231,7 +231,7 @@ public class TestZooKeeperTableArchiveClient {
     // create the region
     HColumnDescriptor hcd = new HColumnDescriptor(TEST_FAM);
     HRegion region = UTIL.createTestRegion(STRING_TABLE_NAME, hcd);
-    List<Region> regions = new ArrayList<Region>();
+    List<Region> regions = new ArrayList<>();
     regions.add(region);
     when(rss.getOnlineRegions()).thenReturn(regions);
     final CompactedHFilesDischarger compactionCleaner =
@@ -241,7 +241,7 @@ public class TestZooKeeperTableArchiveClient {
     // create the another table that we don't archive
     hcd = new HColumnDescriptor(TEST_FAM);
     HRegion otherRegion = UTIL.createTestRegion(otherTable, hcd);
-    regions = new ArrayList<Region>();
+    regions = new ArrayList<>();
     regions.add(otherRegion);
     when(rss.getOnlineRegions()).thenReturn(regions);
     final CompactedHFilesDischarger compactionCleaner1 = new CompactedHFilesDischarger(100, stop,
@@ -388,7 +388,7 @@ public class TestZooKeeperTableArchiveClient {
       return null;
     }
 
-    List<Path> allFiles = new ArrayList<Path>();
+    List<Path> allFiles = new ArrayList<>();
     for (FileStatus file : files) {
       if (file.isDirectory()) {
         List<Path> subFiles = getAllFiles(fs, file.getPath());
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestAdmin1.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestAdmin1.java
index f694210..7b69db4 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestAdmin1.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestAdmin1.java
@@ -1002,7 +1002,7 @@ public class TestAdmin1 {
       // insert rows into column families. The number of rows that have values
       // in a specific column family is decided by rowCounts[familyIndex]
       for (int index = 0; index < familyNames.length; index++) {
-        ArrayList<Put> puts = new ArrayList<Put>(rowCounts[index]);
+        ArrayList<Put> puts = new ArrayList<>(rowCounts[index]);
         for (int i = 0; i < rowCounts[index]; i++) {
           byte[] k = Bytes.toBytes(i);
           Put put = new Put(k);
@@ -1143,7 +1143,7 @@ public class TestAdmin1 {
     } while (oldRegions.size() != 9); //3 regions * 3 replicas
     // write some data to the table
     Table ht = TEST_UTIL.getConnection().getTable(tableName);
-    List<Put> puts = new ArrayList<Put>();
+    List<Put> puts = new ArrayList<>();
     byte[] qualifier = "c".getBytes();
     Put put = new Put(new byte[]{(byte)'1'});
     put.addColumn(cf, qualifier, "100".getBytes());
@@ -1295,7 +1295,7 @@ public class TestAdmin1 {
     byte[] q1 = Bytes.toBytes("q1");
     byte[] v1 = Bytes.toBytes("v1");
     p.addColumn(Bytes.toBytes(fn), q1, v1);
-    List<Put> puts = new ArrayList<Put>(2);
+    List<Put> puts = new ArrayList<>(2);
     puts.add(p);
     p = new Put(Bytes.toBytes("rep1_rk"));
     p.addColumn(Bytes.toBytes(fn1), q1, v1);
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestAdmin2.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestAdmin2.java
index eb15d91..0014401 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestAdmin2.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestAdmin2.java
@@ -776,7 +776,7 @@ public class TestAdmin2 {
 
     // Drain all region servers.
     Collection<ServerName> clusterServers = admin.getClusterStatus().getServers();
-    drainingServers = new ArrayList<ServerName>();
+    drainingServers = new ArrayList<>();
     for (ServerName server : clusterServers) {
       drainingServers.add(server);
     }
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestAvoidCellReferencesIntoShippedBlocks.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestAvoidCellReferencesIntoShippedBlocks.java
index ab09c5e..e7d7f0a 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestAvoidCellReferencesIntoShippedBlocks.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestAvoidCellReferencesIntoShippedBlocks.java
@@ -231,7 +231,7 @@ public class TestAvoidCellReferencesIntoShippedBlocks {
           } catch (InterruptedException e) {
           }
         }
-        List<BlockCacheKey> cacheList = new ArrayList<BlockCacheKey>();
+        List<BlockCacheKey> cacheList = new ArrayList<>();
         Iterator<CachedBlock> iterator = cache.iterator();
         // evict all the blocks
         while (iterator.hasNext()) {
@@ -379,7 +379,7 @@ public class TestAvoidCellReferencesIntoShippedBlocks {
       Thread evictorThread = new Thread() {
         @Override
         public void run() {
-          List<BlockCacheKey> cacheList = new ArrayList<BlockCacheKey>();
+          List<BlockCacheKey> cacheList = new ArrayList<>();
           Iterator<CachedBlock> iterator = cache.iterator();
           // evict all the blocks
           while (iterator.hasNext()) {
@@ -416,7 +416,7 @@ public class TestAvoidCellReferencesIntoShippedBlocks {
             }
             assertEquals("Count the rows", count, 2);
             iterator = cache.iterator();
-            List<BlockCacheKey> newCacheList = new ArrayList<BlockCacheKey>();
+            List<BlockCacheKey> newCacheList = new ArrayList<>();
             while (iterator.hasNext()) {
               CachedBlock next = iterator.next();
               BlockCacheKey cacheKey = new BlockCacheKey(next.getFilename(), next.getOffset());
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestBlockEvictionFromClient.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestBlockEvictionFromClient.java
index f453662..023095f 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestBlockEvictionFromClient.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestBlockEvictionFromClient.java
@@ -1329,7 +1329,7 @@ public class TestBlockEvictionFromClient {
 
   private static class MultiGetThread extends Thread {
     private final Table table;
-    private final List<Get> gets = new ArrayList<Get>();
+    private final List<Get> gets = new ArrayList<>();
     public MultiGetThread(Table table) {
       this.table = table;
     }
@@ -1565,7 +1565,7 @@ public class TestBlockEvictionFromClient {
     static final AtomicInteger countOfGets = new AtomicInteger(0);
     static final AtomicBoolean waitForGets = new AtomicBoolean(false);
     static final AtomicBoolean throwException = new AtomicBoolean(false);
-    private static final AtomicReference<CountDownLatch> cdl = new AtomicReference<CountDownLatch>(
+    private static final AtomicReference<CountDownLatch> cdl = new AtomicReference<>(
         new CountDownLatch(0));
 
     @Override
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestClientOperationInterrupt.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestClientOperationInterrupt.java
index c1cb0a6..62ceca3 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestClientOperationInterrupt.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestClientOperationInterrupt.java
@@ -96,7 +96,7 @@ public class TestClientOperationInterrupt {
     final AtomicInteger badEx = new AtomicInteger(0);
     final AtomicInteger noInt = new AtomicInteger(0);
     final AtomicInteger done = new AtomicInteger(0);
-    List<Thread> threads = new ArrayList<Thread>();
+    List<Thread> threads = new ArrayList<>();
 
     final int nbThread = 100;
 
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestConnectionUtils.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestConnectionUtils.java
index 69729f0..c3e4a28 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestConnectionUtils.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestConnectionUtils.java
@@ -42,7 +42,7 @@ public class TestConnectionUtils {
       retries[i] = ConnectionUtils.getPauseTime(baseTime, 0);
     }
 
-    Set<Long> retyTimeSet = new TreeSet<Long>();
+    Set<Long> retyTimeSet = new TreeSet<>();
     for (long l : retries) {
       /*make sure that there is some jitter but only 1%*/
       assertTrue(l >= baseTime);
@@ -62,7 +62,7 @@ public class TestConnectionUtils {
     long minTimeExpected = (long) (basePause * 0.75f);
     int testTries = 100;
 
-    Set<Long> timeSet = new TreeSet<Long>();
+    Set<Long> timeSet = new TreeSet<>();
     for (int i = 0; i < testTries; i++) {
       long withJitter = ConnectionUtils.addJitter(basePause, 0.5f);
       assertTrue(withJitter >= minTimeExpected);
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestFastFail.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestFastFail.java
index 2aed0ff..465bdfb 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestFastFail.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestFastFail.java
@@ -163,7 +163,7 @@ public class TestFastFail {
     final AtomicInteger numBlockedWorkers = new AtomicInteger(0);
     final AtomicInteger numPreemptiveFastFailExceptions = new AtomicInteger(0);
 
-    List<Future<Boolean>> futures = new ArrayList<Future<Boolean>>();
+    List<Future<Boolean>> futures = new ArrayList<>();
     for (int i = 0; i < nThreads; i++) {
       futures.add(service.submit(new Callable<Boolean>() {
         /**
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestFromClientSide.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestFromClientSide.java
index aab586d..b1a0d3c 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestFromClientSide.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestFromClientSide.java
@@ -190,7 +190,7 @@ public class TestFromClientSide {
   @Test
   public void testDuplicateAppend() throws Exception {
     HTableDescriptor hdt = TEST_UTIL.createTableDescriptor(name.getMethodName());
-    Map<String, String> kvs = new HashMap<String, String>();
+    Map<String, String> kvs = new HashMap<>();
     kvs.put(HConnectionTestingUtility.SleepAtFirstRpcCall.SLEEP_TIME_CONF_KEY, "2000");
     hdt.addCoprocessor(HConnectionTestingUtility.SleepAtFirstRpcCall.class.getName(), null, 1, kvs);
     TEST_UTIL.createTable(hdt, new byte[][] { ROW }).close();
@@ -2299,7 +2299,7 @@ public class TestFromClientSide {
       result = ht.get(get);
       assertTrue(result.size() == 1);
     }
-    ArrayList<Delete> deletes = new ArrayList<Delete>();
+    ArrayList<Delete> deletes = new ArrayList<>();
     for (int i = 0; i < 10; i++) {
       byte [] bytes = Bytes.toBytes(i);
       delete = new Delete(bytes);
@@ -4707,7 +4707,7 @@ public class TestFromClientSide {
 
     final Object waitLock = new Object();
     ExecutorService executorService = Executors.newFixedThreadPool(numVersions);
-    final AtomicReference<AssertionError> error = new AtomicReference<AssertionError>(null);
+    final AtomicReference<AssertionError> error = new AtomicReference<>(null);
     for (int versions = numVersions; versions < numVersions * 2; versions++) {
       final int versionsCopy = versions;
       executorService.submit(new Callable<Void>() {
@@ -5315,7 +5315,7 @@ public class TestFromClientSide {
 
   private List<HRegionLocation> getRegionsInRange(TableName tableName, byte[] startKey,
       byte[] endKey) throws IOException {
-    List<HRegionLocation> regionsInRange = new ArrayList<HRegionLocation>();
+    List<HRegionLocation> regionsInRange = new ArrayList<>();
     byte[] currentKey = startKey;
     final boolean endKeyIsEndOfTable = Bytes.equals(endKey, HConstants.EMPTY_END_ROW);
     try (RegionLocator r = TEST_UTIL.getConnection().getRegionLocator(tableName);) {
@@ -6237,7 +6237,7 @@ public class TestFromClientSide {
     HRegionLocator locator =
         (HRegionLocator) admin.getConnection().getRegionLocator(htd.getTableName());
     for (int regionReplication = 1; regionReplication < 4; regionReplication++) {
-      List<RegionLocations> regionLocations = new ArrayList<RegionLocations>();
+      List<RegionLocations> regionLocations = new ArrayList<>();
 
       // mock region locations coming from meta with multiple replicas
       for (HRegionInfo region : regions) {
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestFromClientSide3.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestFromClientSide3.java
index 3680822..7f44a2a 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestFromClientSide3.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestFromClientSide3.java
@@ -338,7 +338,7 @@ public class TestFromClientSide3 {
     put.addColumn(FAMILY, QUALIFIER, VALUE);
     table.put(put);
 
-    List<Get> gets = new ArrayList<Get>();
+    List<Get> gets = new ArrayList<>();
     gets.add(new Get(ROW));
     gets.add(null);
     gets.add(new Get(ANOTHERROW));
@@ -432,7 +432,7 @@ public class TestFromClientSide3 {
     put.addColumn(FAMILY, QUALIFIER, VALUE);
     table.put (put);
 
-    List<Get> gets = new ArrayList<Get>();
+    List<Get> gets = new ArrayList<>();
     gets.add(new Get(ANOTHERROW));
     gets.add(new Get(Bytes.add(ROW, new byte[] { 0x00 })));
     gets.add(new Get(ROW));
@@ -450,7 +450,7 @@ public class TestFromClientSide3 {
     put.addColumn(FAMILY, QUALIFIER, VALUE);
     table.put(put);
 
-    gets = new ArrayList<Get>();
+    gets = new ArrayList<>();
     gets.add(new Get(new byte[] { 0x00 }));
     gets.add(new Get(new byte[] { 0x00, 0x00 }));
     results = table.existsAll(gets);
@@ -462,7 +462,7 @@ public class TestFromClientSide3 {
     put.addColumn(FAMILY, QUALIFIER, VALUE);
     table.put(put);
 
-    gets = new ArrayList<Get>();
+    gets = new ArrayList<>();
     gets.add(new Get(new byte[] { (byte) 0xff }));
     gets.add(new Get(new byte[] { (byte) 0xff, (byte) 0xff }));
     gets.add(new Get(new byte[] { (byte) 0xff, (byte) 0xff, (byte) 0xff }));
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestHBaseAdminNoCluster.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestHBaseAdminNoCluster.java
index 8d93a0a..1eb83d9 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestHBaseAdminNoCluster.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestHBaseAdminNoCluster.java
@@ -170,7 +170,7 @@ public class TestHBaseAdminNoCluster {
     testMasterOperationIsRetried(new MethodCaller() {
       @Override
       public void call(Admin admin) throws Exception {
-        admin.getTableDescriptorsByTableName(new ArrayList<TableName>());
+        admin.getTableDescriptorsByTableName(new ArrayList<>());
       }
       @Override
       public void verify(MasterKeepAliveConnection masterAdmin, int count) throws Exception {
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestHCM.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestHCM.java
index 8475828..70be7fa 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestHCM.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestHCM.java
@@ -244,7 +244,7 @@ public class TestHCM {
   public void testClusterConnection() throws IOException {
     ThreadPoolExecutor otherPool = new ThreadPoolExecutor(1, 1,
         5, TimeUnit.SECONDS,
-        new SynchronousQueue<Runnable>(),
+        new SynchronousQueue<>(),
         Threads.newDaemonThreadFactory("test-hcm"));
 
     Connection con1 = ConnectionFactory.createConnection(TEST_UTIL.getConfiguration());
@@ -748,7 +748,7 @@ public class TestHCM {
     // 4 steps: ready=0; doGets=1; mustStop=2; stopped=3
     final AtomicInteger step = new AtomicInteger(0);
 
-    final AtomicReference<Throwable> failed = new AtomicReference<Throwable>(null);
+    final AtomicReference<Throwable> failed = new AtomicReference<>(null);
     Thread t = new Thread("testConnectionCloseThread") {
       @Override
       public void run() {
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestHTableMultiplexer.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestHTableMultiplexer.java
index 26764d3..5c47de0 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestHTableMultiplexer.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestHTableMultiplexer.java
@@ -131,7 +131,7 @@ public class TestHTableMultiplexer {
       }
 
       // MultiPut case
-      List<Put> multiput = new ArrayList<Put>();
+      List<Put> multiput = new ArrayList<>();
       for (int i = 0; i < NUM_REGIONS; i++) {
         byte [] row = endRows[i];
         if (row == null || row.length <= 0) continue;
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestIllegalTableDescriptor.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestIllegalTableDescriptor.java
index 9a2aa3d..999760d 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestIllegalTableDescriptor.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestIllegalTableDescriptor.java
@@ -200,7 +200,7 @@ public class TestIllegalTableDescriptor {
   }
 
   private static class ListAppender extends AppenderSkeleton {
-    private final List<String> messages = new ArrayList<String>();
+    private final List<String> messages = new ArrayList<>();
 
     @Override
     protected void append(LoggingEvent event) {
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestIncrementsFromClientSide.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestIncrementsFromClientSide.java
index 3d7e1dc..4406812 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestIncrementsFromClientSide.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestIncrementsFromClientSide.java
@@ -94,7 +94,7 @@ public class TestIncrementsFromClientSide {
   @Test
   public void testDuplicateIncrement() throws Exception {
     HTableDescriptor hdt = TEST_UTIL.createTableDescriptor(TableName.valueOf(name.getMethodName()));
-    Map<String, String> kvs = new HashMap<String, String>();
+    Map<String, String> kvs = new HashMap<>();
     kvs.put(HConnectionTestingUtility.SleepAtFirstRpcCall.SLEEP_TIME_CONF_KEY, "2000");
     hdt.addCoprocessor(HConnectionTestingUtility.SleepAtFirstRpcCall.class.getName(), null, 1, kvs);
     TEST_UTIL.createTable(hdt, new byte[][] { ROW }).close();
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestIntraRowPagination.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestIntraRowPagination.java
index 1f6dc98..43a2e77 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestIntraRowPagination.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestIntraRowPagination.java
@@ -68,7 +68,7 @@ public class TestIntraRowPagination {
       Result result;
       boolean toLog = true;
 
-      List<Cell> kvListExp = new ArrayList<Cell>();
+      List<Cell> kvListExp = new ArrayList<>();
 
       int storeOffset = 1;
       int storeLimit = 3;
@@ -91,8 +91,8 @@ public class TestIntraRowPagination {
       scan.setRowOffsetPerColumnFamily(storeOffset);
       scan.setMaxResultsPerColumnFamily(storeLimit);
       RegionScanner scanner = region.getScanner(scan);
-      List<Cell> kvListScan = new ArrayList<Cell>();
-      List<Cell> results = new ArrayList<Cell>();
+      List<Cell> kvListScan = new ArrayList<>();
+      List<Cell> results = new ArrayList<>();
       while (scanner.next(results) || !results.isEmpty()) {
         kvListScan.addAll(results);
         results.clear();
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestMultiParallel.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestMultiParallel.java
index 1209d25..a3bcc76 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestMultiParallel.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestMultiParallel.java
@@ -110,7 +110,7 @@ public class TestMultiParallel {
     // not a multiple of the number of regions
     int numKeys = (int) ((float) starterKeys.length * 10.33F);
 
-    List<byte[]> keys = new ArrayList<byte[]>();
+    List<byte[]> keys = new ArrayList<>();
     for (int i = 0; i < numKeys; i++) {
       int kIdx = i % starterKeys.length;
       byte[] k = starterKeys[kIdx];
@@ -155,7 +155,7 @@ public class TestMultiParallel {
         try (Table t = connection.getTable(TEST_TABLE, executor)) {
           List<Put> puts = constructPutRequests(); // creates a Put for every region
           t.batch(puts, null);
-          HashSet<ServerName> regionservers = new HashSet<ServerName>();
+          HashSet<ServerName> regionservers = new HashSet<>();
           try (RegionLocator locator = connection.getRegionLocator(TEST_TABLE)) {
             for (Row r : puts) {
               HRegionLocation location = locator.getRegionLocation(r.getRow());
@@ -180,7 +180,7 @@ public class TestMultiParallel {
     table.batch(puts, null);
 
     // create a list of gets and run it
-    List<Row> gets = new ArrayList<Row>();
+    List<Row> gets = new ArrayList<>();
     for (byte[] k : KEYS) {
       Get get = new Get(k);
       get.addColumn(BYTES_FAMILY, QUALIFIER);
@@ -190,7 +190,7 @@ public class TestMultiParallel {
     table.batch(gets, multiRes);
 
     // Same gets using individual call API
-    List<Result> singleRes = new ArrayList<Result>();
+    List<Result> singleRes = new ArrayList<>();
     for (Row get : gets) {
       singleRes.add(table.get((Get) get));
     }
@@ -214,7 +214,7 @@ public class TestMultiParallel {
     LOG.info("test=testBadFam");
     Table table = UTIL.getConnection().getTable(TEST_TABLE);
 
-    List<Row> actions = new ArrayList<Row>();
+    List<Row> actions = new ArrayList<>();
     Put p = new Put(Bytes.toBytes("row1"));
     p.addColumn(Bytes.toBytes("bad_family"), Bytes.toBytes("qual"), Bytes.toBytes("value"));
     actions.add(p);
@@ -368,7 +368,7 @@ public class TestMultiParallel {
     validateSizeAndEmpty(results, KEYS.length);
 
     // Deletes
-    List<Row> deletes = new ArrayList<Row>();
+    List<Row> deletes = new ArrayList<>();
     for (int i = 0; i < KEYS.length; i++) {
       Delete delete = new Delete(KEYS[i]);
       delete.addFamily(BYTES_FAMILY);
@@ -399,7 +399,7 @@ public class TestMultiParallel {
     validateSizeAndEmpty(results, KEYS.length);
 
     // Deletes
-    ArrayList<Delete> deletes = new ArrayList<Delete>();
+    ArrayList<Delete> deletes = new ArrayList<>();
     for (int i = 0; i < KEYS.length; i++) {
       Delete delete = new Delete(KEYS[i]);
       delete.addFamily(BYTES_FAMILY);
@@ -422,7 +422,7 @@ public class TestMultiParallel {
     LOG.info("test=testBatchWithManyColsInOneRowGetAndPut");
     Table table = UTIL.getConnection().getTable(TEST_TABLE);
 
-    List<Row> puts = new ArrayList<Row>();
+    List<Row> puts = new ArrayList<>();
     for (int i = 0; i < 100; i++) {
       Put put = new Put(ONE_ROW);
       byte[] qual = Bytes.toBytes("column" + i);
@@ -436,7 +436,7 @@ public class TestMultiParallel {
     validateSizeAndEmpty(results, 100);
 
     // get the data back and validate that it is correct
-    List<Row> gets = new ArrayList<Row>();
+    List<Row> gets = new ArrayList<>();
     for (int i = 0; i < 100; i++) {
       Get get = new Get(ONE_ROW);
       byte[] qual = Bytes.toBytes("column" + i);
@@ -478,7 +478,7 @@ public class TestMultiParallel {
     Append a = new Append(ONE_ROW);
     a.add(BYTES_FAMILY, QUAL1, Bytes.toBytes("def"));
     a.add(BYTES_FAMILY, QUAL4, Bytes.toBytes("xyz"));
-    List<Row> actions = new ArrayList<Row>();
+    List<Row> actions = new ArrayList<>();
     actions.add(inc);
     actions.add(a);
 
@@ -604,7 +604,7 @@ public class TestMultiParallel {
 
     // Batch: get, get, put(new col), delete, get, get of put, get of deleted,
     // put
-    List<Row> actions = new ArrayList<Row>();
+    List<Row> actions = new ArrayList<>();
 
     byte[] qual2 = Bytes.toBytes("qual2");
     byte[] val2 = Bytes.toBytes("putvalue2");
@@ -693,7 +693,7 @@ public class TestMultiParallel {
   private void validateLoadedData(Table table) throws IOException {
     // get the data back and validate that it is correct
     LOG.info("Validating data on " + table);
-    List<Get> gets = new ArrayList<Get>();
+    List<Get> gets = new ArrayList<>();
     for (byte[] k : KEYS) {
       Get get = new Get(k);
       get.addColumn(BYTES_FAMILY, QUALIFIER);
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestReplicaWithCluster.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestReplicaWithCluster.java
index 22e88da..becb2eb 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestReplicaWithCluster.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestReplicaWithCluster.java
@@ -377,12 +377,12 @@ public class TestReplicaWithCluster {
     final int numRows = 10;
     final byte[] qual = Bytes.toBytes("qual");
     final byte[] val  = Bytes.toBytes("val");
-    final List<Pair<byte[], String>> famPaths = new ArrayList<Pair<byte[], String>>();
+    final List<Pair<byte[], String>> famPaths = new ArrayList<>();
     for (HColumnDescriptor col : hdt.getColumnFamilies()) {
       Path hfile = new Path(dir, col.getNameAsString());
       TestHRegionServerBulkLoad.createHFile(HTU.getTestFileSystem(), hfile, col.getName(),
         qual, val, numRows);
-      famPaths.add(new Pair<byte[], String>(col.getName(), hfile.toString()));
+      famPaths.add(new Pair<>(col.getName(), hfile.toString()));
     }
 
     // bulk load HFiles
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestReplicasClient.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestReplicasClient.java
index 7cbb5ad..7b4442b 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestReplicasClient.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestReplicasClient.java
@@ -103,7 +103,7 @@ public class TestReplicasClient {
     static final AtomicBoolean slowDownNext = new AtomicBoolean(false);
     static final AtomicInteger countOfNext = new AtomicInteger(0);
     private static final AtomicReference<CountDownLatch> cdl =
-        new AtomicReference<CountDownLatch>(new CountDownLatch(0));
+        new AtomicReference<>(new CountDownLatch(0));
     Random r = new Random();
     public SlowMeCopro() {
     }
@@ -530,7 +530,7 @@ public class TestReplicasClient {
   public void testCancelOfMultiGet() throws Exception {
     openRegion(hriSecondary);
     try {
-      List<Put> puts = new ArrayList<Put>(2);
+      List<Put> puts = new ArrayList<>(2);
       byte[] b1 = Bytes.toBytes("testCancelOfMultiGet" + 0);
       Put p = new Put(b1);
       p.addColumn(f, b1, b1);
@@ -552,7 +552,7 @@ public class TestReplicasClient {
       // Make primary slowdown
       SlowMeCopro.getCdl().set(new CountDownLatch(1));
 
-      List<Get> gets = new ArrayList<Get>();
+      List<Get> gets = new ArrayList<>();
       Get g = new Get(b1);
       g.setCheckExistenceOnly(true);
       g.setConsistency(Consistency.TIMELINE);
@@ -762,7 +762,7 @@ public class TestReplicasClient {
     Iterator<Result> iter = scanner.iterator();
 
     // Maps of row keys that we have seen so far
-    HashMap<String, Boolean> map = new HashMap<String, Boolean>();
+    HashMap<String, Boolean> map = new HashMap<>();
 
     // Tracked metrics
     int rowCount = 0;
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestRestoreSnapshotFromClient.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestRestoreSnapshotFromClient.java
index ec6e020..3190fb9 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestRestoreSnapshotFromClient.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestRestoreSnapshotFromClient.java
@@ -304,7 +304,7 @@ public class TestRestoreSnapshotFromClient {
 
   private Set<String> getFamiliesFromFS(final TableName tableName) throws IOException {
     MasterFileSystem mfs = TEST_UTIL.getMiniHBaseCluster().getMaster().getMasterFileSystem();
-    Set<String> families = new HashSet<String>();
+    Set<String> families = new HashSet<>();
     Path tableDir = FSUtils.getTableDir(mfs.getRootDir(), tableName);
     for (Path regionDir: FSUtils.getRegionDirs(mfs.getFileSystem(), tableDir)) {
       for (Path familyDir: FSUtils.getFamilyDirs(mfs.getFileSystem(), regionDir)) {
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestScannersFromClientSide.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestScannersFromClientSide.java
index 42fecfb..6f40093 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestScannersFromClientSide.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestScannersFromClientSide.java
@@ -151,7 +151,7 @@ public class TestScannersFromClientSide {
     scanner = ht.getScanner(scan);
 
     // c4:4, c5:5, c6:6, c7:7
-    kvListExp = new ArrayList<Cell>();
+    kvListExp = new ArrayList<>();
     kvListExp.add(new KeyValue(ROW, FAMILY, QUALIFIERS[4], 4, VALUE));
     kvListExp.add(new KeyValue(ROW, FAMILY, QUALIFIERS[5], 5, VALUE));
     kvListExp.add(new KeyValue(ROW, FAMILY, QUALIFIERS[6], 6, VALUE));
@@ -166,14 +166,14 @@ public class TestScannersFromClientSide {
     scanner = ht.getScanner(scan);
 
     // First batch: c4:4, c5:5
-    kvListExp = new ArrayList<Cell>();
+    kvListExp = new ArrayList<>();
     kvListExp.add(new KeyValue(ROW, FAMILY, QUALIFIERS[4], 4, VALUE));
     kvListExp.add(new KeyValue(ROW, FAMILY, QUALIFIERS[5], 5, VALUE));
     result = scanner.next();
     verifyResult(result, kvListExp, toLog, "Testing first batch of scan");
 
     // Second batch: c6:6, c7:7
-    kvListExp = new ArrayList<Cell>();
+    kvListExp = new ArrayList<>();
     kvListExp.add(new KeyValue(ROW, FAMILY, QUALIFIERS[6], 6, VALUE));
     kvListExp.add(new KeyValue(ROW, FAMILY, QUALIFIERS[7], 7, VALUE));
     result = scanner.next();
@@ -204,7 +204,7 @@ public class TestScannersFromClientSide {
     byte[] cellValue = Bytes.createMaxByteArray(cellSize);
 
     Put put;
-    List<Put> puts = new ArrayList<Put>();
+    List<Put> puts = new ArrayList<>();
     for (int row = 0; row < ROWS.length; row++) {
       put = new Put(ROWS[row]);
       for (int qual = 0; qual < QUALIFIERS.length; qual++) {
@@ -244,7 +244,7 @@ public class TestScannersFromClientSide {
     Table ht = TEST_UTIL.createTable(tableName, FAMILY);
 
     Put put;
-    List<Put> puts = new ArrayList<Put>();
+    List<Put> puts = new ArrayList<>();
     for (int row = 0; row < ROWS.length; row++) {
       put = new Put(ROWS[row]);
       for (int qual = 0; qual < QUALIFIERS.length; qual++) {
@@ -327,7 +327,7 @@ public class TestScannersFromClientSide {
     boolean toLog = true;
     List<Cell> kvListExp;
 
-    kvListExp = new ArrayList<Cell>();
+    kvListExp = new ArrayList<>();
     // Insert one CF for row[0]
     put = new Put(ROW);
     for (int i=0; i < 10; i++) {
@@ -344,7 +344,7 @@ public class TestScannersFromClientSide {
     get = new Get(ROW);
     get.setMaxResultsPerColumnFamily(2);
     result = ht.get(get);
-    kvListExp = new ArrayList<Cell>();
+    kvListExp = new ArrayList<>();
     kvListExp.add(new KeyValue(ROW, FAMILIES[0], QUALIFIERS[0], 1, VALUE));
     kvListExp.add(new KeyValue(ROW, FAMILIES[0], QUALIFIERS[1], 1, VALUE));
     verifyResult(result, kvListExp, toLog, "Testing basic setMaxResults");
@@ -355,7 +355,7 @@ public class TestScannersFromClientSide {
     get.setFilter(new ColumnRangeFilter(QUALIFIERS[2], true, QUALIFIERS[5],
                                         true));
     result = ht.get(get);
-    kvListExp = new ArrayList<Cell>();
+    kvListExp = new ArrayList<>();
     kvListExp.add(new KeyValue(ROW, FAMILIES[0], QUALIFIERS[2], 1, VALUE));
     kvListExp.add(new KeyValue(ROW, FAMILIES[0], QUALIFIERS[3], 1, VALUE));
     kvListExp.add(new KeyValue(ROW, FAMILIES[0], QUALIFIERS[4], 1, VALUE));
@@ -383,7 +383,7 @@ public class TestScannersFromClientSide {
     get.addFamily(FAMILIES[1]);
     get.addFamily(FAMILIES[2]);
     result = ht.get(get);
-    kvListExp = new ArrayList<Cell>();
+    kvListExp = new ArrayList<>();
     //Exp: CF1:q0, ..., q9, CF2: q0, q1, q10, q11, ..., q19
     for (int i=0; i < 10; i++) {
       kvListExp.add(new KeyValue(ROW, FAMILIES[1], QUALIFIERS[i], 1, VALUE));
@@ -401,7 +401,7 @@ public class TestScannersFromClientSide {
     get.setMaxResultsPerColumnFamily(3);
     get.setFilter(new ColumnRangeFilter(QUALIFIERS[2], true, null, true));
     result = ht.get(get);
-    kvListExp = new ArrayList<Cell>();
+    kvListExp = new ArrayList<>();
     for (int i=2; i < 5; i++) {
       kvListExp.add(new KeyValue(ROW, FAMILIES[0], QUALIFIERS[i], 1, VALUE));
     }
@@ -417,7 +417,7 @@ public class TestScannersFromClientSide {
     get.setMaxResultsPerColumnFamily(7);
     get.setFilter(new ColumnPrefixFilter(QUALIFIERS[1]));
     result = ht.get(get);
-    kvListExp = new ArrayList<Cell>();
+    kvListExp = new ArrayList<>();
     kvListExp.add(new KeyValue(ROW, FAMILIES[0], QUALIFIERS[1], 1, VALUE));
     kvListExp.add(new KeyValue(ROW, FAMILIES[1], QUALIFIERS[1], 1, VALUE));
     kvListExp.add(new KeyValue(ROW, FAMILIES[2], QUALIFIERS[1], 1, VALUE));
@@ -448,7 +448,7 @@ public class TestScannersFromClientSide {
     boolean toLog = true;
     List<Cell> kvListExp, kvListScan;
 
-    kvListExp = new ArrayList<Cell>();
+    kvListExp = new ArrayList<>();
 
     for (int r=0; r < ROWS.length; r++) {
       put = new Put(ROWS[r]);
@@ -467,7 +467,7 @@ public class TestScannersFromClientSide {
     scan = new Scan();
     scan.setMaxResultsPerColumnFamily(4);
     ResultScanner scanner = ht.getScanner(scan);
-    kvListScan = new ArrayList<Cell>();
+    kvListScan = new ArrayList<>();
     while ((result = scanner.next()) != null) {
       for (Cell kv : result.listCells()) {
         kvListScan.add(kv);
@@ -498,7 +498,7 @@ public class TestScannersFromClientSide {
     List<Cell> kvListExp;
 
     // Insert one CF for row
-    kvListExp = new ArrayList<Cell>();
+    kvListExp = new ArrayList<>();
     put = new Put(ROW);
     for (int i=0; i < 10; i++) {
       KeyValue kv = new KeyValue(ROW, FAMILIES[0], QUALIFIERS[i], 1, VALUE);
@@ -519,7 +519,7 @@ public class TestScannersFromClientSide {
     get = new Get(ROW);
     get.setRowOffsetPerColumnFamily(20);
     result = ht.get(get);
-    kvListExp = new ArrayList<Cell>();
+    kvListExp = new ArrayList<>();
     verifyResult(result, kvListExp, toLog, "Testing offset > #kvs");
 
     //offset + maxResultPerCF
@@ -527,7 +527,7 @@ public class TestScannersFromClientSide {
     get.setRowOffsetPerColumnFamily(4);
     get.setMaxResultsPerColumnFamily(5);
     result = ht.get(get);
-    kvListExp = new ArrayList<Cell>();
+    kvListExp = new ArrayList<>();
     for (int i=4; i < 9; i++) {
       kvListExp.add(new KeyValue(ROW, FAMILIES[0], QUALIFIERS[i], 1, VALUE));
     }
@@ -540,7 +540,7 @@ public class TestScannersFromClientSide {
     get.setFilter(new ColumnRangeFilter(QUALIFIERS[2], true, QUALIFIERS[5],
                                         true));
     result = ht.get(get);
-    kvListExp = new ArrayList<Cell>();
+    kvListExp = new ArrayList<>();
     kvListExp.add(new KeyValue(ROW, FAMILIES[0], QUALIFIERS[3], 1, VALUE));
     kvListExp.add(new KeyValue(ROW, FAMILIES[0], QUALIFIERS[4], 1, VALUE));
     kvListExp.add(new KeyValue(ROW, FAMILIES[0], QUALIFIERS[5], 1, VALUE));
@@ -563,7 +563,7 @@ public class TestScannersFromClientSide {
     get.addFamily(FAMILIES[1]);
     get.addFamily(FAMILIES[2]);
     result = ht.get(get);
-    kvListExp = new ArrayList<Cell>();
+    kvListExp = new ArrayList<>();
     //Exp: CF1:q4, q5, CF2: q4, q5
     kvListExp.add(new KeyValue(ROW, FAMILIES[1], QUALIFIERS[4], 1, VALUE));
     kvListExp.add(new KeyValue(ROW, FAMILIES[1], QUALIFIERS[5], 1, VALUE));
@@ -644,7 +644,7 @@ public class TestScannersFromClientSide {
     }
 
     // c0:0, c1:1
-    kvListExp = new ArrayList<Cell>();
+    kvListExp = new ArrayList<>();
     kvListExp.add(new KeyValue(ROW, FAMILY, QUALIFIERS[0], 0, VALUE));
     kvListExp.add(new KeyValue(ROW, FAMILY, QUALIFIERS[1], 1, VALUE));
     result = scanner.next();
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestSnapshotFromClient.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestSnapshotFromClient.java
index 3d8ee55..f2f3b26 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestSnapshotFromClient.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestSnapshotFromClient.java
@@ -337,7 +337,7 @@ public class TestSnapshotFromClient {
       LOG.debug(table2Snapshot1 + " completed.");
 
       List<SnapshotDescription> listTableSnapshots = admin.listTableSnapshots("test.*", ".*");
-      List<String> listTableSnapshotNames = new ArrayList<String>();
+      List<String> listTableSnapshotNames = new ArrayList<>();
       assertEquals(3, listTableSnapshots.size());
       for (SnapshotDescription s : listTableSnapshots) {
         listTableSnapshotNames.add(s.getName());
@@ -379,7 +379,7 @@ public class TestSnapshotFromClient {
 
       List<SnapshotDescription> listTableSnapshots =
           admin.listTableSnapshots("test.*", "Table1.*");
-      List<String> listTableSnapshotNames = new ArrayList<String>();
+      List<String> listTableSnapshotNames = new ArrayList<>();
       assertEquals(2, listTableSnapshots.size());
       for (SnapshotDescription s : listTableSnapshots) {
         listTableSnapshotNames.add(s.getName());
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestSnapshotMetadata.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestSnapshotMetadata.java
index 55086b5..99c4340 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestSnapshotMetadata.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestSnapshotMetadata.java
@@ -193,7 +193,7 @@ public class TestSnapshotMetadata {
     final byte[] snapshotName = Bytes.toBytes(snapshotNameAsString);
 
     // restore the snapshot into a cloned table and examine the output
-    List<byte[]> familiesList = new ArrayList<byte[]>();
+    List<byte[]> familiesList = new ArrayList<>();
     Collections.addAll(familiesList, families);
 
     // Create a snapshot in which all families are empty
@@ -262,8 +262,8 @@ public class TestSnapshotMetadata {
     // populate it with data
     final byte[] familyForUpdate = BLOCKSIZE_FAM;
 
-    List<byte[]> familiesWithDataList = new ArrayList<byte[]>();
-    List<byte[]> emptyFamiliesList = new ArrayList<byte[]>();
+    List<byte[]> familiesWithDataList = new ArrayList<>();
+    List<byte[]> emptyFamiliesList = new ArrayList<>();
     if (addData) {
       Table original = UTIL.getConnection().getTable(originalTableName);
       UTIL.loadTable(original, familyForUpdate); // family arbitrarily chosen
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestTimestampsFilter.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestTimestampsFilter.java
index 5c487d7..0a1fafe 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestTimestampsFilter.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestTimestampsFilter.java
@@ -145,7 +145,7 @@ public class TestTimestampsFilter {
 
     // Request an empty list of versions using the Timestamps filter;
     // Should return none.
-    kvs = getNVersions(ht, FAMILY, 2, 2, new ArrayList<Long>());
+    kvs = getNVersions(ht, FAMILY, 2, 2, new ArrayList<>());
     assertEquals(0, kvs == null? 0: kvs.length);
 
     //
@@ -192,7 +192,7 @@ public class TestTimestampsFilter {
     p.addColumn(FAMILY, Bytes.toBytes("column4"), (long) 3, Bytes.toBytes("value4-3"));
     ht.put(p);
 
-    ArrayList<Long> timestamps = new ArrayList<Long>();
+    ArrayList<Long> timestamps = new ArrayList<>();
     timestamps.add(new Long(3));
     TimestampsFilter filter = new TimestampsFilter(timestamps);
 
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/client/replication/TestReplicationAdmin.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/client/replication/TestReplicationAdmin.java
index 481f311..f092a48 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/client/replication/TestReplicationAdmin.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/client/replication/TestReplicationAdmin.java
@@ -255,7 +255,7 @@ public class TestReplicationAdmin {
 
     // append table column family: f1 of t3 to replication
     tableCFs.clear();
-    tableCFs.put(tableName3, new ArrayList<String>());
+    tableCFs.put(tableName3, new ArrayList<>());
     tableCFs.get(tableName3).add("f1");
     admin.appendPeerTableCFs(ID_ONE, tableCFs);
     result = ReplicationSerDeHelper.parseTableCFsFromConfig(admin.getPeerTableCFs(ID_ONE));
@@ -269,7 +269,7 @@ public class TestReplicationAdmin {
     assertEquals("f1", result.get(tableName3).get(0));
 
     tableCFs.clear();
-    tableCFs.put(tableName4, new ArrayList<String>());
+    tableCFs.put(tableName4, new ArrayList<>());
     tableCFs.get(tableName4).add("f1");
     tableCFs.get(tableName4).add("f2");
     admin.appendPeerTableCFs(ID_ONE, tableCFs);
@@ -289,10 +289,10 @@ public class TestReplicationAdmin {
 
     // append "table5" => [], then append "table5" => ["f1"]
     tableCFs.clear();
-    tableCFs.put(tableName5, new ArrayList<String>());
+    tableCFs.put(tableName5, new ArrayList<>());
     admin.appendPeerTableCFs(ID_ONE, tableCFs);
     tableCFs.clear();
-    tableCFs.put(tableName5, new ArrayList<String>());
+    tableCFs.put(tableName5, new ArrayList<>());
     tableCFs.get(tableName5).add("f1");
     admin.appendPeerTableCFs(ID_ONE, tableCFs);
     result = ReplicationSerDeHelper.parseTableCFsFromConfig(admin.getPeerTableCFs(ID_ONE));
@@ -303,11 +303,11 @@ public class TestReplicationAdmin {
 
     // append "table6" => ["f1"], then append "table6" => []
     tableCFs.clear();
-    tableCFs.put(tableName6, new ArrayList<String>());
+    tableCFs.put(tableName6, new ArrayList<>());
     tableCFs.get(tableName6).add("f1");
     admin.appendPeerTableCFs(ID_ONE, tableCFs);
     tableCFs.clear();
-    tableCFs.put(tableName6, new ArrayList<String>());
+    tableCFs.put(tableName6, new ArrayList<>());
     admin.appendPeerTableCFs(ID_ONE, tableCFs);
     result = ReplicationSerDeHelper.parseTableCFsFromConfig(admin.getPeerTableCFs(ID_ONE));
     assertEquals(6, result.size());
@@ -339,7 +339,7 @@ public class TestReplicationAdmin {
 
     tableCFs.clear();
     tableCFs.put(tableName1, null);
-    tableCFs.put(tableName2, new ArrayList<String>());
+    tableCFs.put(tableName2, new ArrayList<>());
     tableCFs.get(tableName2).add("cf1");
     admin.setPeerTableCFs(ID_ONE, tableCFs);
     try {
@@ -360,7 +360,7 @@ public class TestReplicationAdmin {
 
     try {
       tableCFs.clear();
-      tableCFs.put(tableName1, new ArrayList<String>());
+      tableCFs.put(tableName1, new ArrayList<>());
       tableCFs.get(tableName1).add("f1");
       admin.removePeerTableCFs(ID_ONE, tableCFs);
       assertTrue(false);
@@ -382,13 +382,13 @@ public class TestReplicationAdmin {
     } catch (ReplicationException e) {
     }
     tableCFs.clear();
-    tableCFs.put(tableName2, new ArrayList<String>());
+    tableCFs.put(tableName2, new ArrayList<>());
     tableCFs.get(tableName2).add("cf1");
     admin.removePeerTableCFs(ID_ONE, tableCFs);
     assertNull(admin.getPeerTableCFs(ID_ONE));
 
     tableCFs.clear();
-    tableCFs.put(tableName4, new ArrayList<String>());
+    tableCFs.put(tableName4, new ArrayList<>());
     admin.setPeerTableCFs(ID_ONE, tableCFs);
     admin.removePeerTableCFs(ID_ONE, tableCFs);
     assertNull(admin.getPeerTableCFs(ID_ONE));
@@ -407,7 +407,7 @@ public class TestReplicationAdmin {
     admin.peerAdded(ID_ONE);
 
     rpc = admin.getPeerConfig(ID_ONE);
-    Set<String> namespaces = new HashSet<String>();
+    Set<String> namespaces = new HashSet<>();
     namespaces.add(ns1);
     namespaces.add(ns2);
     rpc.setNamespaces(namespaces);
@@ -448,7 +448,7 @@ public class TestReplicationAdmin {
     admin.updatePeerConfig(ID_ONE, rpc);
     rpc = admin.getPeerConfig(ID_ONE);
     Map<TableName, List<String>> tableCfs = new HashMap<>();
-    tableCfs.put(tableName1, new ArrayList<String>());
+    tableCfs.put(tableName1, new ArrayList<>());
     rpc.setTableCFsMap(tableCfs);
     try {
       admin.updatePeerConfig(ID_ONE, rpc);
@@ -460,7 +460,7 @@ public class TestReplicationAdmin {
 
     rpc = admin.getPeerConfig(ID_ONE);
     tableCfs.clear();
-    tableCfs.put(tableName2, new ArrayList<String>());
+    tableCfs.put(tableName2, new ArrayList<>());
     rpc.setTableCFsMap(tableCfs);
     admin.updatePeerConfig(ID_ONE, rpc);
     rpc = admin.getPeerConfig(ID_ONE);
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/client/replication/TestReplicationAdminWithClusters.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/client/replication/TestReplicationAdminWithClusters.java
index 24889ad..b44ecbf 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/client/replication/TestReplicationAdminWithClusters.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/client/replication/TestReplicationAdminWithClusters.java
@@ -183,8 +183,7 @@ public class TestReplicationAdminWithClusters extends TestReplicationBase {
     }
     assertFalse("Table should not exists in the peer cluster", admin2.isTableAvailable(TestReplicationBase.tableName));
 
-    Map<TableName, ? extends Collection<String>> tableCfs =
-        new HashMap<TableName, Collection<String>>();
+    Map<TableName, ? extends Collection<String>> tableCfs = new HashMap<>();
     tableCfs.put(tableName, null);
     try {
       adminExt.setPeerTableCFs(peerId, tableCfs);
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/constraint/TestConstraints.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/constraint/TestConstraints.java
index acc3fca..12a229d 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/constraint/TestConstraints.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/constraint/TestConstraints.java
@@ -75,11 +75,9 @@ public class TestConstraints {
   @Test
   public void testReadWriteWithConf() throws Throwable {
     HTableDescriptor desc = new HTableDescriptor(TableName.valueOf(name.getMethodName()));
-    Constraints.add(
-        desc,
-        new Pair<Class<? extends Constraint>, Configuration>(
-            CheckConfigurationConstraint.class, CheckConfigurationConstraint
-                .getConfiguration()));
+    Constraints.add(desc,
+      new Pair<>(CheckConfigurationConstraint.class,
+        CheckConfigurationConstraint.getConfiguration()));
 
     List<? extends Constraint> c = Constraints.getConstraints(desc, this
         .getClass().getClassLoader());
@@ -88,7 +86,7 @@ public class TestConstraints {
     assertEquals(CheckConfigurationConstraint.class, c.get(0).getClass());
 
     // check to make sure that we overwrite configurations
-    Constraints.add(desc, new Pair<Class<? extends Constraint>, Configuration>(
+    Constraints.add(desc, new Pair<>(
         CheckConfigurationConstraint.class, new Configuration(false)));
 
     try {
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/coprocessor/TestCoprocessorInterface.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/coprocessor/TestCoprocessorInterface.java
index 21d9861..422c54b 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/coprocessor/TestCoprocessorInterface.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/coprocessor/TestCoprocessorInterface.java
@@ -349,7 +349,7 @@ public class TestCoprocessorInterface {
     RegionScanner scanner = region.getCoprocessorHost().postScannerOpen(s, region.getScanner(s));
     assertTrue(scanner instanceof CustomScanner);
     // this would throw an exception before HBASE-4197
-    scanner.next(new ArrayList<Cell>());
+    scanner.next(new ArrayList<>());
 
     HBaseTestingUtility.closeRegionAndWAL(region);
     Coprocessor c = region.getCoprocessorHost().
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/coprocessor/TestHTableWrapper.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/coprocessor/TestHTableWrapper.java
index 04d8c8c..9f20ba2 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/coprocessor/TestHTableWrapper.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/coprocessor/TestHTableWrapper.java
@@ -221,7 +221,7 @@ public class TestHTableWrapper {
 
     // multiple deletes:
     Delete[] deletes = new Delete[] { new Delete(ROW_D), new Delete(ROW_E) };
-    hTableInterface.delete(new ArrayList<Delete>(Arrays.asList(deletes)));
+    hTableInterface.delete(new ArrayList<>(Arrays.asList(deletes)));
     checkRowsValues(new byte[][] { ROW_D, ROW_E }, new byte[][] { null, null });
   }
 
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/coprocessor/TestOpenTableInCoprocessor.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/coprocessor/TestOpenTableInCoprocessor.java
index b75fc79..1ed0008 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/coprocessor/TestOpenTableInCoprocessor.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/coprocessor/TestOpenTableInCoprocessor.java
@@ -92,7 +92,7 @@ public class TestOpenTableInCoprocessor {
       long keepAliveTime = 60;
       ThreadPoolExecutor pool =
           new ThreadPoolExecutor(1, maxThreads, keepAliveTime, TimeUnit.SECONDS,
-              new SynchronousQueue<Runnable>(), Threads.newDaemonThreadFactory("hbase-table"));
+              new SynchronousQueue<>(), Threads.newDaemonThreadFactory("hbase-table"));
       pool.allowCoreThreadTimeOut(true);
       return pool;
     }
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/coprocessor/TestRegionObserverBypass.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/coprocessor/TestRegionObserverBypass.java
index fb87ff6..63d7544 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/coprocessor/TestRegionObserverBypass.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/coprocessor/TestRegionObserverBypass.java
@@ -112,7 +112,7 @@ public class TestRegionObserverBypass {
     EnvironmentEdgeManagerTestHelper.injectEdge(new IncrementingEnvironmentEdge());
 
     Table t = util.getConnection().getTable(tableName);
-    List<Put> puts = new ArrayList<Put>();
+    List<Put> puts = new ArrayList<>();
     Put p = new Put(row1);
     p.addColumn(dummy, dummy, dummy);
     puts.add(p);
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/coprocessor/TestRegionObserverInterface.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/coprocessor/TestRegionObserverInterface.java
index 2e0db44..7b4cc40 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/coprocessor/TestRegionObserverInterface.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/coprocessor/TestRegionObserverInterface.java
@@ -424,7 +424,7 @@ public class TestRegionObserverInterface {
 
         @Override
         public boolean next(List<Cell> results, ScannerContext scannerContext) throws IOException {
-          List<Cell> internalResults = new ArrayList<Cell>();
+          List<Cell> internalResults = new ArrayList<>();
           boolean hasMore;
           do {
             hasMore = scanner.next(internalResults, scannerContext);
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/coprocessor/TestWALObserver.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/coprocessor/TestWALObserver.java
index 2ab91c1..5fb5421 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/coprocessor/TestWALObserver.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/coprocessor/TestWALObserver.java
@@ -181,8 +181,7 @@ public class TestWALObserver {
     HRegionInfo hri = createBasic3FamilyHRegionInfo(Bytes.toString(TEST_TABLE));
     final HTableDescriptor htd = createBasic3FamilyHTD(Bytes
         .toString(TEST_TABLE));
-    NavigableMap<byte[], Integer> scopes = new TreeMap<byte[], Integer>(
-        Bytes.BYTES_COMPARATOR);
+    NavigableMap<byte[], Integer> scopes = new TreeMap<>(Bytes.BYTES_COMPARATOR);
     for(byte[] fam : htd.getFamiliesKeys()) {
       scopes.put(fam, 0);
     }
@@ -272,8 +271,7 @@ public class TestWALObserver {
     final HRegionInfo hri = createBasic3FamilyHRegionInfo(Bytes.toString(TEST_TABLE));
     final HTableDescriptor htd = createBasic3FamilyHTD(Bytes.toString(TEST_TABLE));
     final MultiVersionConcurrencyControl mvcc = new MultiVersionConcurrencyControl();
-    NavigableMap<byte[], Integer> scopes = new TreeMap<byte[], Integer>(
-        Bytes.BYTES_COMPARATOR);
+    NavigableMap<byte[], Integer> scopes = new TreeMap<>(Bytes.BYTES_COMPARATOR);
     for(byte[] fam : htd.getFamiliesKeys()) {
       scopes.put(fam, 0);
     }
@@ -328,8 +326,7 @@ public class TestWALObserver {
     WALEdit edit = new WALEdit();
     long now = EnvironmentEdgeManager.currentTime();
     final int countPerFamily = 1000;
-    NavigableMap<byte[], Integer> scopes = new TreeMap<byte[], Integer>(
-        Bytes.BYTES_COMPARATOR);
+    NavigableMap<byte[], Integer> scopes = new TreeMap<>(Bytes.BYTES_COMPARATOR);
     for (HColumnDescriptor hcd : htd.getFamilies()) {
       scopes.put(hcd.getName(), 0);
     }
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/favored/TestFavoredNodeAssignmentHelper.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/favored/TestFavoredNodeAssignmentHelper.java
index a195ec7..d624c93 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/favored/TestFavoredNodeAssignmentHelper.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/favored/TestFavoredNodeAssignmentHelper.java
@@ -56,9 +56,8 @@ import com.google.common.collect.Sets;
 @Category({MasterTests.class, SmallTests.class})
 public class TestFavoredNodeAssignmentHelper {
 
-  private static List<ServerName> servers = new ArrayList<ServerName>();
-  private static Map<String, List<ServerName>> rackToServers = new HashMap<String,
-      List<ServerName>>();
+  private static List<ServerName> servers = new ArrayList<>();
+  private static Map<String, List<ServerName>> rackToServers = new HashMap<>();
   private static RackManager rackManager = Mockito.mock(RackManager.class);
 
   // Some tests have randomness, so we run them multiple times
@@ -76,7 +75,7 @@ public class TestFavoredNodeAssignmentHelper {
       if (i < 10) {
         Mockito.when(rackManager.getRack(server)).thenReturn("rack1");
         if (rackToServers.get("rack1") == null) {
-          List<ServerName> servers = new ArrayList<ServerName>();
+          List<ServerName> servers = new ArrayList<>();
           rackToServers.put("rack1", servers);
         }
         rackToServers.get("rack1").add(server);
@@ -84,7 +83,7 @@ public class TestFavoredNodeAssignmentHelper {
       if (i >= 10 && i < 20) {
         Mockito.when(rackManager.getRack(server)).thenReturn("rack2");
         if (rackToServers.get("rack2") == null) {
-          List<ServerName> servers = new ArrayList<ServerName>();
+          List<ServerName> servers = new ArrayList<>();
           rackToServers.put("rack2", servers);
         }
         rackToServers.get("rack2").add(server);
@@ -92,7 +91,7 @@ public class TestFavoredNodeAssignmentHelper {
       if (i >= 20 && i < 30) {
         Mockito.when(rackManager.getRack(server)).thenReturn("rack3");
         if (rackToServers.get("rack3") == null) {
-          List<ServerName> servers = new ArrayList<ServerName>();
+          List<ServerName> servers = new ArrayList<>();
           rackToServers.put("rack3", servers);
         }
         rackToServers.get("rack3").add(server);
@@ -105,7 +104,7 @@ public class TestFavoredNodeAssignmentHelper {
   // work with from any given rack
   // Return a rondom 'count' number of servers from 'rack'
   private static List<ServerName> getServersFromRack(Map<String, Integer> rackToServerCount) {
-    List<ServerName> chosenServers = new ArrayList<ServerName>();
+    List<ServerName> chosenServers = new ArrayList<>();
     for (Map.Entry<String, Integer> entry : rackToServerCount.entrySet()) {
       List<ServerName> servers = rackToServers.get(entry.getKey());
       for (int i = 0; i < entry.getValue(); i++) {
@@ -119,7 +118,7 @@ public class TestFavoredNodeAssignmentHelper {
   public void testSmallCluster() {
     // Test the case where we cannot assign favored nodes (because the number
     // of nodes in the cluster is too less)
-    Map<String,Integer> rackToServerCount = new HashMap<String,Integer>();
+    Map<String,Integer> rackToServerCount = new HashMap<>();
     rackToServerCount.put("rack1", 2);
     List<ServerName> servers = getServersFromRack(rackToServerCount);
     FavoredNodeAssignmentHelper helper = new FavoredNodeAssignmentHelper(servers,
@@ -158,7 +157,7 @@ public class TestFavoredNodeAssignmentHelper {
   public void testSecondaryAndTertiaryPlacementWithSingleRack() {
     // Test the case where there is a single rack and we need to choose
     // Primary/Secondary/Tertiary from a single rack.
-    Map<String,Integer> rackToServerCount = new HashMap<String,Integer>();
+    Map<String,Integer> rackToServerCount = new HashMap<>();
     rackToServerCount.put("rack1", 10);
     // have lots of regions to test with
     Triple<Map<HRegionInfo, ServerName>, FavoredNodeAssignmentHelper, List<HRegionInfo>>
@@ -184,7 +183,7 @@ public class TestFavoredNodeAssignmentHelper {
   public void testSecondaryAndTertiaryPlacementWithSingleServer() {
     // Test the case where we have a single node in the cluster. In this case
     // the primary can be assigned but the secondary/tertiary would be null
-    Map<String,Integer> rackToServerCount = new HashMap<String,Integer>();
+    Map<String,Integer> rackToServerCount = new HashMap<>();
     rackToServerCount.put("rack1", 1);
     Triple<Map<HRegionInfo, ServerName>, FavoredNodeAssignmentHelper, List<HRegionInfo>>
       primaryRSMapAndHelper = secondaryAndTertiaryRSPlacementHelper(1, rackToServerCount);
@@ -202,7 +201,7 @@ public class TestFavoredNodeAssignmentHelper {
   public void testSecondaryAndTertiaryPlacementWithMultipleRacks() {
     // Test the case where we have multiple racks and the region servers
     // belong to multiple racks
-    Map<String,Integer> rackToServerCount = new HashMap<String,Integer>();
+    Map<String,Integer> rackToServerCount = new HashMap<>();
     rackToServerCount.put("rack1", 10);
     rackToServerCount.put("rack2", 10);
 
@@ -233,7 +232,7 @@ public class TestFavoredNodeAssignmentHelper {
   public void testSecondaryAndTertiaryPlacementWithLessThanTwoServersInRacks() {
     // Test the case where we have two racks but with less than two servers in each
     // We will not have enough machines to select secondary/tertiary
-    Map<String,Integer> rackToServerCount = new HashMap<String,Integer>();
+    Map<String,Integer> rackToServerCount = new HashMap<>();
     rackToServerCount.put("rack1", 1);
     rackToServerCount.put("rack2", 1);
     Triple<Map<HRegionInfo, ServerName>, FavoredNodeAssignmentHelper, List<HRegionInfo>>
@@ -257,7 +256,7 @@ public class TestFavoredNodeAssignmentHelper {
     // racks than what the primary is on. But if the other rack doesn't have
     // enough nodes to have both secondary/tertiary RSs, the tertiary is placed
     // on the same rack as the primary server is on
-    Map<String,Integer> rackToServerCount = new HashMap<String,Integer>();
+    Map<String,Integer> rackToServerCount = new HashMap<>();
     rackToServerCount.put("rack1", 2);
     rackToServerCount.put("rack2", 1);
     Triple<Map<HRegionInfo, ServerName>, FavoredNodeAssignmentHelper, List<HRegionInfo>>
@@ -290,7 +289,7 @@ public class TestFavoredNodeAssignmentHelper {
         new HashMap<ServerName, List<HRegionInfo>>();
     helper.initialize();
     // create regions
-    List<HRegionInfo> regions = new ArrayList<HRegionInfo>(regionCount);
+    List<HRegionInfo> regions = new ArrayList<>(regionCount);
     for (int i = 0; i < regionCount; i++) {
       HRegionInfo region = new HRegionInfo(TableName.valueOf(name.getMethodName()),
           Bytes.toBytes(i), Bytes.toBytes(i + 1));
@@ -298,13 +297,12 @@ public class TestFavoredNodeAssignmentHelper {
     }
     // place the regions
     helper.placePrimaryRSAsRoundRobin(assignmentMap, primaryRSMap, regions);
-    return new Triple<Map<HRegionInfo, ServerName>, FavoredNodeAssignmentHelper, List<HRegionInfo>>
-                   (primaryRSMap, helper, regions);
+    return new Triple<>(primaryRSMap, helper, regions);
   }
 
   private void primaryRSPlacement(int regionCount, Map<HRegionInfo, ServerName> primaryRSMap,
       int firstRackSize, int secondRackSize, int thirdRackSize) {
-    Map<String,Integer> rackToServerCount = new HashMap<String,Integer>();
+    Map<String,Integer> rackToServerCount = new HashMap<>();
     rackToServerCount.put("rack1", firstRackSize);
     rackToServerCount.put("rack2", secondRackSize);
     rackToServerCount.put("rack3", thirdRackSize);
@@ -315,11 +313,10 @@ public class TestFavoredNodeAssignmentHelper {
 
     assertTrue(helper.canPlaceFavoredNodes());
 
-    Map<ServerName, List<HRegionInfo>> assignmentMap =
-        new HashMap<ServerName, List<HRegionInfo>>();
-    if (primaryRSMap == null) primaryRSMap = new HashMap<HRegionInfo, ServerName>();
+    Map<ServerName, List<HRegionInfo>> assignmentMap = new HashMap<>();
+    if (primaryRSMap == null) primaryRSMap = new HashMap<>();
     // create some regions
-    List<HRegionInfo> regions = new ArrayList<HRegionInfo>(regionCount);
+    List<HRegionInfo> regions = new ArrayList<>(regionCount);
     for (int i = 0; i < regionCount; i++) {
       HRegionInfo region = new HRegionInfo(TableName.valueOf("foobar"),
           Bytes.toBytes(i), Bytes.toBytes(i + 1));
@@ -354,11 +351,11 @@ public class TestFavoredNodeAssignmentHelper {
     //Verify the ordering was as expected by inserting the racks and regions
     //in sorted maps. The keys being the racksize and numregions; values are
     //the relative positions of the racksizes and numregions respectively
-    SortedMap<Integer, Integer> rackMap = new TreeMap<Integer, Integer>();
+    SortedMap<Integer, Integer> rackMap = new TreeMap<>();
     rackMap.put(firstRackSize, 1);
     rackMap.put(secondRackSize, 2);
     rackMap.put(thirdRackSize, 3);
-    SortedMap<Integer, Integer> regionMap = new TreeMap<Integer, Integer>();
+    SortedMap<Integer, Integer> regionMap = new TreeMap<>();
     regionMap.put(regionsOnRack1, 1);
     regionMap.put(regionsOnRack2, 2);
     regionMap.put(regionsOnRack3, 3);
@@ -390,7 +387,7 @@ public class TestFavoredNodeAssignmentHelper {
     helper.initialize();
     assertTrue(helper.canPlaceFavoredNodes());
 
-    List<HRegionInfo> regions = new ArrayList<HRegionInfo>(20);
+    List<HRegionInfo> regions = new ArrayList<>(20);
     for (int i = 0; i < 20; i++) {
       HRegionInfo region = new HRegionInfo(TableName.valueOf(name.getMethodName()),
           Bytes.toBytes(i), Bytes.toBytes(i + 1));
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/filter/TestColumnPrefixFilter.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/filter/TestColumnPrefixFilter.java
index 1d24140..9b71d45 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/filter/TestColumnPrefixFilter.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/filter/TestColumnPrefixFilter.java
@@ -70,13 +70,12 @@ public class TestColumnPrefixFilter {
       List<String> columns = generateRandomWords(10000, "column");
       long maxTimestamp = 2;
 
-      List<Cell> kvList = new ArrayList<Cell>();
+      List<Cell> kvList = new ArrayList<>();
 
-      Map<String, List<Cell>> prefixMap = new HashMap<String,
-          List<Cell>>();
+      Map<String, List<Cell>> prefixMap = new HashMap<>();
 
-      prefixMap.put("p", new ArrayList<Cell>());
-      prefixMap.put("s", new ArrayList<Cell>());
+      prefixMap.put("p", new ArrayList<>());
+      prefixMap.put("s", new ArrayList<>());
 
       String valueString = "ValueString";
 
@@ -108,7 +107,7 @@ public class TestColumnPrefixFilter {
         scan.setFilter(filter);
 
         InternalScanner scanner = region.getScanner(scan);
-        List<Cell> results = new ArrayList<Cell>();
+        List<Cell> results = new ArrayList<>();
         while (scanner.next(results))
           ;
         assertEquals(prefixMap.get(s).size(), results.size());
@@ -133,13 +132,12 @@ public class TestColumnPrefixFilter {
       List<String> columns = generateRandomWords(10000, "column");
       long maxTimestamp = 2;
 
-      List<Cell> kvList = new ArrayList<Cell>();
+      List<Cell> kvList = new ArrayList<>();
 
-      Map<String, List<Cell>> prefixMap = new HashMap<String,
-          List<Cell>>();
+      Map<String, List<Cell>> prefixMap = new HashMap<>();
 
-      prefixMap.put("p", new ArrayList<Cell>());
-      prefixMap.put("s", new ArrayList<Cell>());
+      prefixMap.put("p", new ArrayList<>());
+      prefixMap.put("s", new ArrayList<>());
 
       String valueString = "ValueString";
 
@@ -174,7 +172,7 @@ public class TestColumnPrefixFilter {
         scan.setFilter(filterList);
 
         InternalScanner scanner = region.getScanner(scan);
-        List<Cell> results = new ArrayList<Cell>();
+        List<Cell> results = new ArrayList<>();
         while (scanner.next(results))
           ;
         assertEquals(prefixMap.get(s).size(), results.size());
@@ -187,7 +185,7 @@ public class TestColumnPrefixFilter {
   }
 
   List<String> generateRandomWords(int numberOfWords, String suffix) {
-    Set<String> wordSet = new HashSet<String>();
+    Set<String> wordSet = new HashSet<>();
     for (int i = 0; i < numberOfWords; i++) {
       int lengthOfWords = (int) (Math.random()*2) + 1;
       char[] wordChar = new char[lengthOfWords];
@@ -202,7 +200,7 @@ public class TestColumnPrefixFilter {
       }
       wordSet.add(word);
     }
-    List<String> wordList = new ArrayList<String>(wordSet);
+    List<String> wordList = new ArrayList<>(wordSet);
     return wordList;
   }
 
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/filter/TestColumnRangeFilter.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/filter/TestColumnRangeFilter.java
index 04377b0..f03a4f0 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/filter/TestColumnRangeFilter.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/filter/TestColumnRangeFilter.java
@@ -171,18 +171,18 @@ public class TestColumnRangeFilter {
     long maxTimestamp = 2;
     List<String> columns = generateRandomWords(20000, 8);
 
-    List<KeyValue> kvList = new ArrayList<KeyValue>();
+    List<KeyValue> kvList = new ArrayList<>();
 
-    Map<StringRange, List<KeyValue>> rangeMap = new HashMap<StringRange, List<KeyValue>>();
+    Map<StringRange, List<KeyValue>> rangeMap = new HashMap<>();
 
     rangeMap.put(new StringRange(null, true, "b", false),
-        new ArrayList<KeyValue>());
+        new ArrayList<>());
     rangeMap.put(new StringRange("p", true, "q", false),
-        new ArrayList<KeyValue>());
+        new ArrayList<>());
     rangeMap.put(new StringRange("r", false, "s", true),
-        new ArrayList<KeyValue>());
+        new ArrayList<>());
     rangeMap.put(new StringRange("z", false, null, false),
-        new ArrayList<KeyValue>());
+        new ArrayList<>());
     String valueString = "ValueString";
 
     for (String row : rows) {
@@ -216,7 +216,7 @@ public class TestColumnRangeFilter {
           s.isEndInclusive());
       scan.setFilter(filter);
       ResultScanner scanner = ht.getScanner(scan);
-      List<Cell> results = new ArrayList<Cell>();
+      List<Cell> results = new ArrayList<>();
       LOG.info("scan column range: " + s.toString());
       long timeBeforeScan = System.currentTimeMillis();
 
@@ -245,7 +245,7 @@ public class TestColumnRangeFilter {
   }
 
   List<String> generateRandomWords(int numberOfWords, int maxLengthOfWords) {
-    Set<String> wordSet = new HashSet<String>();
+    Set<String> wordSet = new HashSet<>();
     for (int i = 0; i < numberOfWords; i++) {
       int lengthOfWords = (int) (Math.random() * maxLengthOfWords) + 1;
       char[] wordChar = new char[lengthOfWords];
@@ -255,7 +255,7 @@ public class TestColumnRangeFilter {
       String word = new String(wordChar);
       wordSet.add(word);
     }
-    List<String> wordList = new ArrayList<String>(wordSet);
+    List<String> wordList = new ArrayList<>(wordSet);
     return wordList;
   }
 
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/filter/TestDependentColumnFilter.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/filter/TestDependentColumnFilter.java
index 1b00ae8..704441a 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/filter/TestDependentColumnFilter.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/filter/TestDependentColumnFilter.java
@@ -125,7 +125,7 @@ public class TestDependentColumnFilter {
   }
 
   private List<KeyValue> makeTestVals() {
-    List<KeyValue> testVals = new ArrayList<KeyValue>();
+    List<KeyValue> testVals = new ArrayList<>();
     testVals.add(new KeyValue(ROWS[0], FAMILIES[0], QUALIFIER, STAMPS[0], BAD_VALS[0]));
     testVals.add(new KeyValue(ROWS[0], FAMILIES[0], QUALIFIER, STAMPS[1], BAD_VALS[1]));
     testVals.add(new KeyValue(ROWS[0], FAMILIES[1], QUALIFIER, STAMPS[1], BAD_VALS[2]));
@@ -147,7 +147,7 @@ public class TestDependentColumnFilter {
   private void verifyScan(Scan s, long expectedRows, long expectedCells)
   throws IOException {
     InternalScanner scanner = this.region.getScanner(s);
-    List<Cell> results = new ArrayList<Cell>();
+    List<Cell> results = new ArrayList<>();
     int i = 0;
     int cells = 0;
     for (boolean done = true; done; i++) {
@@ -231,7 +231,7 @@ public class TestDependentColumnFilter {
   @Test
   public void testFilterDropping() throws Exception {
     Filter filter = new DependentColumnFilter(FAMILIES[0], QUALIFIER);
-    List<Cell> accepted = new ArrayList<Cell>();
+    List<Cell> accepted = new ArrayList<>();
     for(Cell val : testVals) {
       if(filter.filterKeyValue(val) == ReturnCode.INCLUDE) {
         accepted.add(val);
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/filter/TestFilter.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/filter/TestFilter.java
index e4af75f..a403c24 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/filter/TestFilter.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/filter/TestFilter.java
@@ -275,7 +275,7 @@ public class TestFilter {
 
     // reseek to row three.
     scanner.reseek(ROWS_THREE[1]);
-    List<Cell> results = new ArrayList<Cell>();
+    List<Cell> results = new ArrayList<>();
 
     // the results should belong to ROWS_THREE[1]
     scanner.next(results);
@@ -285,7 +285,7 @@ public class TestFilter {
     }
     // again try to reseek to a value before ROWS_THREE[1]
     scanner.reseek(ROWS_ONE[1]);
-    results = new ArrayList<Cell>();
+    results = new ArrayList<>();
     // This time no seek would have been done to ROWS_ONE[1]
     scanner.next(results);
     for (Cell keyValue : results) {
@@ -511,7 +511,7 @@ public class TestFilter {
     InternalScanner scanner = this.region.getScanner(s);
     int scannerCounter = 0;
     while (true) {
-      boolean isMoreResults = scanner.next(new ArrayList<Cell>());
+      boolean isMoreResults = scanner.next(new ArrayList<>());
       scannerCounter++;
 
       if (scannerCounter >= pageSize) {
@@ -540,7 +540,7 @@ public class TestFilter {
 
     InternalScanner scanner = this.region.getScanner(s);
     while (true) {
-      ArrayList<Cell> values = new ArrayList<Cell>();
+      ArrayList<Cell> values = new ArrayList<>();
       boolean isMoreResults = scanner.next(values);
       if (!isMoreResults
           || !Bytes.toString(CellUtil.cloneRow(values.get(0))).startsWith(prefix)) {
@@ -575,7 +575,7 @@ public class TestFilter {
     InternalScanner scanner = this.region.getScanner(s);
     int scannerCounter = 0;
     while (true) {
-      boolean isMoreResults = scanner.next(new ArrayList<Cell>());
+      boolean isMoreResults = scanner.next(new ArrayList<>());
       scannerCounter++;
 
       if (scannerCounter >= pageSize) {
@@ -629,7 +629,7 @@ public class TestFilter {
     s.setFilter(filter);
 
     InternalScanner scanner = this.region.getScanner(s);
-    ArrayList<Cell> values = new ArrayList<Cell>();
+    ArrayList<Cell> values = new ArrayList<>();
     scanner.next(values);
     assertTrue("All rows should be filtered out", values.isEmpty());
   }
@@ -652,7 +652,7 @@ public class TestFilter {
 
     InternalScanner scanner = this.region.getScanner(s);
     while (true) {
-      ArrayList<Cell> values = new ArrayList<Cell>();
+      ArrayList<Cell> values = new ArrayList<>();
       boolean isMoreResults = scanner.next(values);
       if (!isMoreResults || !Bytes.toString(CellUtil.cloneRow(values.get(0))).startsWith(prefix)) {
         assertTrue("The WhileMatchFilter should now filter all remaining", filter.filterAllRemaining());
@@ -681,7 +681,7 @@ public class TestFilter {
 
     InternalScanner scanner = this.region.getScanner(s);
     while (true) {
-      ArrayList<Cell> values = new ArrayList<Cell>();
+      ArrayList<Cell> values = new ArrayList<>();
       boolean isMoreResults = scanner.next(values);
       assertTrue("The WhileMatchFilter should now filter all remaining", filter.filterAllRemaining());
       if (!isMoreResults) {
@@ -1370,7 +1370,7 @@ public class TestFilter {
     // Test getting a single row, single key using Row, Qualifier, and Value
     // regular expression and substring filters
     // Use must pass all
-    List<Filter> filters = new ArrayList<Filter>();
+    List<Filter> filters = new ArrayList<>();
     filters.add(new RowFilter(CompareOp.EQUAL, new RegexStringComparator(".+-2")));
     filters.add(new QualifierFilter(CompareOp.EQUAL, new RegexStringComparator(".+-2")));
     filters.add(new ValueFilter(CompareOp.EQUAL, new SubstringComparator("One")));
@@ -1520,7 +1520,7 @@ public class TestFilter {
     Scan s1 = new Scan();
     s1.setFilter(filterList);
     InternalScanner scanner = testRegion.getScanner(s1);
-    List<Cell> results = new ArrayList<Cell>();
+    List<Cell> results = new ArrayList<>();
     int resultCount = 0;
     while (scanner.next(results)) {
       resultCount++;
@@ -1553,7 +1553,7 @@ public class TestFilter {
     // Now let's grab rows that have Q_ONE[0](VALUES[0]) and Q_ONE[2](VALUES[1])
     // Since group two rows don't have these qualifiers, they will pass
     // so limiting scan to group one
-    List<Filter> filters = new ArrayList<Filter>();
+    List<Filter> filters = new ArrayList<>();
     filters.add(new SingleColumnValueFilter(FAMILIES[0], QUALIFIERS_ONE[0],
         CompareOp.EQUAL, VALUES[0]));
     filters.add(new SingleColumnValueFilter(FAMILIES[0], QUALIFIERS_ONE[2],
@@ -1573,7 +1573,7 @@ public class TestFilter {
 
     // In order to get expected behavior without limiting to group one
     // need to wrap SCVFs in SkipFilters
-    filters = new ArrayList<Filter>();
+    filters = new ArrayList<>();
     filters.add(new SkipFilter(new SingleColumnValueFilter(FAMILIES[0], QUALIFIERS_ONE[0],
         CompareOp.EQUAL, VALUES[0])));
     filters.add(new SkipFilter(new SingleColumnValueFilter(FAMILIES[0], QUALIFIERS_ONE[2],
@@ -1661,7 +1661,7 @@ public class TestFilter {
   private void verifyScan(Scan s, long expectedRows, long expectedKeys)
   throws IOException {
     InternalScanner scanner = this.region.getScanner(s);
-    List<Cell> results = new ArrayList<Cell>();
+    List<Cell> results = new ArrayList<>();
     int i = 0;
     for (boolean done = true; done; i++) {
       done = scanner.next(results);
@@ -1683,7 +1683,7 @@ public class TestFilter {
       long expectedKeys)
   throws IOException {
     InternalScanner scanner = this.region.getScanner(s);
-    List<Cell> results = new ArrayList<Cell>();
+    List<Cell> results = new ArrayList<>();
     int i = 0;
     for (boolean done = true; done; i++) {
       done = scanner.next(results);
@@ -1704,7 +1704,7 @@ public class TestFilter {
   private void verifyScanFull(Scan s, KeyValue [] kvs)
   throws IOException {
     InternalScanner scanner = this.region.getScanner(s);
-    List<Cell> results = new ArrayList<Cell>();
+    List<Cell> results = new ArrayList<>();
     int row = 0;
     int idx = 0;
     for (boolean done = true; done; row++) {
@@ -1735,7 +1735,7 @@ public class TestFilter {
   private void verifyScanFullNoValues(Scan s, KeyValue [] kvs, boolean useLen)
   throws IOException {
     InternalScanner scanner = this.region.getScanner(s);
-    List<Cell> results = new ArrayList<Cell>();
+    List<Cell> results = new ArrayList<>();
     int row = 0;
     int idx = 0;
     for (boolean more = true; more; row++) {
@@ -2073,7 +2073,7 @@ public class TestFilter {
     Scan s1 = new Scan();
     s1.setFilter(rowFilter);
     InternalScanner scanner = testRegion.getScanner(s1);
-    List<Cell> results = new ArrayList<Cell>();
+    List<Cell> results = new ArrayList<>();
     int i = 5;
     for (boolean done = true; done; i++) {
       done = scanner.next(results);
@@ -2092,7 +2092,7 @@ public class TestFilter {
     s1 = new Scan();
     s1.setFilter(subFilterList);
     scanner = testRegion.getScanner(s1);
-    results = new ArrayList<Cell>();
+    results = new ArrayList<>();
     for (i=0; i<=4; i+=2) {
       scanner.next(results);
       assertTrue(CellUtil.matchingRow(results.get(0), Bytes.toBytes("row" + i)));
@@ -2108,7 +2108,7 @@ public class TestFilter {
     s1 = new Scan();
     s1.setFilter(filterList);
     scanner = testRegion.getScanner(s1);
-    results = new ArrayList<Cell>();
+    results = new ArrayList<>();
     for (i=0; i<=4; i+=2) {
       scanner.next(results);
       assertTrue(CellUtil.matchingRow(results.get(0), Bytes.toBytes("row" + i)));
@@ -2129,7 +2129,7 @@ public class TestFilter {
     s1 = new Scan();
     s1.setFilter(filterList);
     scanner = testRegion.getScanner(s1);
-    results = new ArrayList<Cell>();
+    results = new ArrayList<>();
     for (i=0; i<=4; i+=2) {
       scanner.next(results);
       assertTrue(CellUtil.matchingRow(results.get(0), Bytes.toBytes("row" + i)));
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/filter/TestFilterList.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/filter/TestFilterList.java
index f80317b..ad71fcc 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/filter/TestFilterList.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/filter/TestFilterList.java
@@ -125,7 +125,7 @@ public class TestFilterList {
   }
 
   private Filter getFilterMPONE() {
-    List<Filter> filters = new ArrayList<Filter>();
+    List<Filter> filters = new ArrayList<>();
     filters.add(new PageFilter(MAX_PAGES));
     filters.add(new WhileMatchFilter(new PrefixFilter(Bytes.toBytes("yyy"))));
     Filter filterMPONE =
@@ -191,7 +191,7 @@ public class TestFilterList {
   }
 
   private Filter getMPALLFilter() {
-    List<Filter> filters = new ArrayList<Filter>();
+    List<Filter> filters = new ArrayList<>();
     filters.add(new PageFilter(MAX_PAGES));
     filters.add(new WhileMatchFilter(new PrefixFilter(Bytes.toBytes("yyy"))));
     Filter filterMPALL =
@@ -239,7 +239,7 @@ public class TestFilterList {
   }
 
   public Filter getOrderingFilter() {
-    List<Filter> filters = new ArrayList<Filter>();
+    List<Filter> filters = new ArrayList<>();
     filters.add(new PrefixFilter(Bytes.toBytes("yyy")));
     filters.add(new PageFilter(MAX_PAGES));
     Filter filterMPONE =
@@ -370,7 +370,7 @@ public class TestFilterList {
    */
   @Test
   public void testSerialization() throws Exception {
-    List<Filter> filters = new ArrayList<Filter>();
+    List<Filter> filters = new ArrayList<>();
     filters.add(new PageFilter(MAX_PAGES));
     filters.add(new WhileMatchFilter(new PrefixFilter(Bytes.toBytes("yyy"))));
     Filter filterMPALL =
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/filter/TestFilterListOrOperatorWithBlkCnt.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/filter/TestFilterListOrOperatorWithBlkCnt.java
index bbde09d..39abc95 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/filter/TestFilterListOrOperatorWithBlkCnt.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/filter/TestFilterListOrOperatorWithBlkCnt.java
@@ -101,13 +101,13 @@ public class TestFilterListOrOperatorWithBlkCnt {
     scan.setMaxVersions();
     long blocksStart = getBlkAccessCount();
 
-    List<RowRange> ranges1 = new ArrayList<RowRange>();
+    List<RowRange> ranges1 = new ArrayList<>();
     ranges1.add(new RowRange(Bytes.toBytes(10), true, Bytes.toBytes(15), false));
     ranges1.add(new RowRange(Bytes.toBytes(9980), true, Bytes.toBytes(9985), false));
 
     MultiRowRangeFilter filter1 = new MultiRowRangeFilter(ranges1);
 
-    List<RowRange> ranges2 = new ArrayList<RowRange>();
+    List<RowRange> ranges2 = new ArrayList<>();
     ranges2.add(new RowRange(Bytes.toBytes(15), true, Bytes.toBytes(20), false));
     ranges2.add(new RowRange(Bytes.toBytes(9985), true, Bytes.toBytes(9990), false));
 
@@ -156,7 +156,7 @@ public class TestFilterListOrOperatorWithBlkCnt {
       scan.setStopRow(stopRow);
     }
     ResultScanner scanner = ht.getScanner(scan);
-    List<Cell> kvList = new ArrayList<Cell>();
+    List<Cell> kvList = new ArrayList<>();
     Result r;
     while ((r = scanner.next()) != null) {
       for (Cell kv : r.listCells()) {
@@ -168,7 +168,7 @@ public class TestFilterListOrOperatorWithBlkCnt {
 
   private int getResultsSize(Table ht, Scan scan) throws IOException {
     ResultScanner scanner = ht.getScanner(scan);
-    List<Cell> results = new ArrayList<Cell>();
+    List<Cell> results = new ArrayList<>();
     Result r;
     while ((r = scanner.next()) != null) {
       for (Cell kv : r.listCells()) {
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/filter/TestFilterSerialization.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/filter/TestFilterSerialization.java
index 7c9651d..37e0d2d 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/filter/TestFilterSerialization.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/filter/TestFilterSerialization.java
@@ -105,12 +105,12 @@ public class TestFilterSerialization {
   @Test
   public void testFilterList() throws Exception {
     // empty filter list
-    FilterList filterList = new FilterList(new LinkedList<Filter>());
+    FilterList filterList = new FilterList(new LinkedList<>());
     assertTrue(filterList.areSerializedFieldsEqual(
       ProtobufUtil.toFilter(ProtobufUtil.toFilter(filterList))));
 
     // non-empty filter list
-    LinkedList<Filter> list = new LinkedList<Filter>();
+    LinkedList<Filter> list = new LinkedList<>();
     list.add(new ColumnCountGetFilter(1));
     list.add(new RowFilter(CompareFilter.CompareOp.EQUAL,
       new SubstringComparator("testFilterList")));
@@ -131,7 +131,7 @@ public class TestFilterSerialization {
   @Test
   public void testFirstKeyValueMatchingQualifiersFilter() throws Exception {
     // empty qualifiers set
-    TreeSet<byte []> set = new TreeSet<byte []>(Bytes.BYTES_COMPARATOR);
+    TreeSet<byte []> set = new TreeSet<>(Bytes.BYTES_COMPARATOR);
     FirstKeyValueMatchingQualifiersFilter firstKeyValueMatchingQualifiersFilter =
       new FirstKeyValueMatchingQualifiersFilter(set);
     assertTrue(firstKeyValueMatchingQualifiersFilter.areSerializedFieldsEqual(
@@ -155,9 +155,9 @@ public class TestFilterSerialization {
 
   @Test
   public void testFuzzyRowFilter() throws Exception {
-    LinkedList<Pair<byte[], byte[]>> fuzzyList = new LinkedList<Pair<byte[], byte[]>>();
-    fuzzyList.add(new Pair<byte[], byte[]>(Bytes.toBytes("999"),new byte[] {0, 0, 1}));
-    fuzzyList.add(new Pair<byte[], byte[]>(Bytes.toBytes("abcd"),new byte[] {1, 0, 1, 1}));
+    LinkedList<Pair<byte[], byte[]>> fuzzyList = new LinkedList<>();
+    fuzzyList.add(new Pair<>(Bytes.toBytes("999"),new byte[] {0, 0, 1}));
+    fuzzyList.add(new Pair<>(Bytes.toBytes("abcd"),new byte[] {1, 0, 1, 1}));
     FuzzyRowFilter fuzzyRowFilter = new FuzzyRowFilter(fuzzyList);
     assertTrue(fuzzyRowFilter.areSerializedFieldsEqual(
       ProtobufUtil.toFilter(ProtobufUtil.toFilter(fuzzyRowFilter))));
@@ -294,12 +294,12 @@ public class TestFilterSerialization {
   @Test
   public void testTimestampsFilter() throws Exception {
     // Empty timestamp list
-    TimestampsFilter timestampsFilter = new TimestampsFilter(new LinkedList<Long>());
+    TimestampsFilter timestampsFilter = new TimestampsFilter(new LinkedList<>());
     assertTrue(timestampsFilter.areSerializedFieldsEqual(
       ProtobufUtil.toFilter(ProtobufUtil.toFilter(timestampsFilter))));
 
     // Non-empty timestamp list
-    LinkedList<Long> list = new LinkedList<Long>();
+    LinkedList<Long> list = new LinkedList<>();
     list.add(new Long(System.currentTimeMillis()));
     list.add(new Long(System.currentTimeMillis()));
     timestampsFilter = new TimestampsFilter(list);
@@ -326,7 +326,7 @@ public class TestFilterSerialization {
 
   @Test
   public void testMultiRowRangeFilter() throws Exception {
-    List<RowRange> ranges = new ArrayList<RowRange>();
+    List<RowRange> ranges = new ArrayList<>();
     ranges.add(new RowRange(Bytes.toBytes(30), true, Bytes.toBytes(40), false));
     ranges.add(new RowRange(Bytes.toBytes(10), true, Bytes.toBytes(20), false));
     ranges.add(new RowRange(Bytes.toBytes(60), true, Bytes.toBytes(70), false));
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/filter/TestFilterWithScanLimits.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/filter/TestFilterWithScanLimits.java
index 0d2940c..4b2842c 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/filter/TestFilterWithScanLimits.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/filter/TestFilterWithScanLimits.java
@@ -96,7 +96,7 @@ public class TestFilterWithScanLimits extends FilterTestingCluster {
     try {
       createTable(tableName, columnFamily);
       Table table = openTable(tableName);
-      List<Put> puts = new ArrayList<Put>();
+      List<Put> puts = new ArrayList<>();
 
       // row1 => <f1:c1, 1_c1>, <f1:c2, 1_c2>, <f1:c3, 1_c3>, <f1:c4,1_c4>,
       // <f1:c5, 1_c5>
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/filter/TestFilterWrapper.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/filter/TestFilterWrapper.java
index 59873be..c4c3e36 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/filter/TestFilterWrapper.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/filter/TestFilterWrapper.java
@@ -75,7 +75,7 @@ public class TestFilterWrapper {
     int row_number = 0;
     try {
       Scan scan = new Scan();
-      List<Filter> fs = new ArrayList<Filter>();
+      List<Filter> fs = new ArrayList<>();
 
       DependentColumnFilter f1 = new DependentColumnFilter(Bytes.toBytes("f1"),
           Bytes.toBytes("c5"), true, CompareFilter.CompareOp.EQUAL,
@@ -115,7 +115,7 @@ public class TestFilterWrapper {
     try {
       Table table = connection.getTable(name);
       assertTrue("Fail to create the table", admin.tableExists(name));
-      List<Put> puts = new ArrayList<Put>();
+      List<Put> puts = new ArrayList<>();
 
       // row1 => <f1:c1, 1_c1, ts=1>, <f1:c2, 1_c2, ts=2>, <f1:c3, 1_c3,ts=3>,
       // <f1:c4,1_c4, ts=4>, <f1:c5, 1_c5, ts=5>
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/filter/TestFirstKeyValueMatchingQualifiersFilter.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/filter/TestFirstKeyValueMatchingQualifiersFilter.java
index 0d045f7..dbda361 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/filter/TestFirstKeyValueMatchingQualifiersFilter.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/filter/TestFirstKeyValueMatchingQualifiersFilter.java
@@ -45,7 +45,7 @@ public class TestFirstKeyValueMatchingQualifiersFilter extends TestCase {
    * @throws Exception
    */
   public void testFirstKeyMatchingQualifierFilter() throws Exception {
-    Set<byte[]> quals = new TreeSet<byte[]>(Bytes.BYTES_COMPARATOR);
+    Set<byte[]> quals = new TreeSet<>(Bytes.BYTES_COMPARATOR);
     quals.add(COLUMN_QUALIFIER_1);
     quals.add(COLUMN_QUALIFIER_2);
     Filter filter = new FirstKeyValueMatchingQualifiersFilter(quals);
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/filter/TestFuzzyRowAndColumnRangeFilter.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/filter/TestFuzzyRowAndColumnRangeFilter.java
index 53d87d3..989a93b 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/filter/TestFuzzyRowAndColumnRangeFilter.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/filter/TestFuzzyRowAndColumnRangeFilter.java
@@ -149,7 +149,7 @@ public class TestFuzzyRowAndColumnRangeFilter {
 
     byte[] mask = new byte[] {0 , 0, 1, 1, 1, 1, 0, 0, 0, 0};
 
-    Pair<byte[], byte[]> pair = new Pair<byte[], byte[]>(fuzzyKey, mask);
+    Pair<byte[], byte[]> pair = new Pair<>(fuzzyKey, mask);
     FuzzyRowFilter fuzzyRowFilter = new FuzzyRowFilter(Lists.newArrayList(pair));
     ColumnRangeFilter columnRangeFilter = new ColumnRangeFilter(Bytes.toBytes(cqStart), true
             , Bytes.toBytes(4), true);
@@ -167,7 +167,7 @@ public class TestFuzzyRowAndColumnRangeFilter {
     scan.setFilter(filterList);
 
     ResultScanner scanner = hTable.getScanner(scan);
-    List<Cell> results = new ArrayList<Cell>();
+    List<Cell> results = new ArrayList<>();
     Result result;
     long timeBeforeScan = System.currentTimeMillis();
     while ((result = scanner.next()) != null) {
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/filter/TestFuzzyRowFilterEndToEnd.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/filter/TestFuzzyRowFilterEndToEnd.java
index 21aac70..3c11efe 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/filter/TestFuzzyRowFilterEndToEnd.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/filter/TestFuzzyRowFilterEndToEnd.java
@@ -137,10 +137,10 @@ public class TestFuzzyRowFilterEndToEnd {
 
     TEST_UTIL.flush();
 
-    List<Pair<byte[], byte[]>> data = new ArrayList<Pair<byte[], byte[]>>();
+    List<Pair<byte[], byte[]>> data = new ArrayList<>();
     byte[] fuzzyKey = Bytes.toBytesBinary("\\x9B\\x00\\x044e");
     byte[] mask = new byte[] { 0, 0, 0, 0, 0 };
-    data.add(new Pair<byte[], byte[]>(fuzzyKey, mask));
+    data.add(new Pair<>(fuzzyKey, mask));
     FuzzyRowFilter filter = new FuzzyRowFilter(data);
 
     Scan scan = new Scan();
@@ -187,10 +187,10 @@ public class TestFuzzyRowFilterEndToEnd {
 
     TEST_UTIL.flush();
 
-    List<Pair<byte[], byte[]>> data =  new ArrayList<Pair<byte[], byte[]>>();
+    List<Pair<byte[], byte[]>> data =  new ArrayList<>();
     byte[] fuzzyKey = Bytes.toBytesBinary("\\x00\\x00\\x044");
     byte[] mask = new byte[] { 1,0,0,0};
-    data.add(new Pair<byte[], byte[]>(fuzzyKey, mask));
+    data.add(new Pair<>(fuzzyKey, mask));
     FuzzyRowFilter filter = new FuzzyRowFilter(data);
     
     Scan scan = new Scan();
@@ -254,7 +254,7 @@ public class TestFuzzyRowFilterEndToEnd {
 
     byte[] mask = new byte[] { 0, 0, 1, 1, 1, 1, 0, 0, 0, 0 };
 
-    List<Pair<byte[], byte[]>> list = new ArrayList<Pair<byte[], byte[]>>();
+    List<Pair<byte[], byte[]>> list = new ArrayList<>();
     for (int i = 0; i < totalFuzzyKeys; i++) {
       byte[] fuzzyKey = new byte[10];
       ByteBuffer buf = ByteBuffer.wrap(fuzzyKey);
@@ -265,7 +265,7 @@ public class TestFuzzyRowFilterEndToEnd {
       }
       buf.putInt(i);
 
-      Pair<byte[], byte[]> pair = new Pair<byte[], byte[]>(fuzzyKey, mask);
+      Pair<byte[], byte[]> pair = new Pair<>(fuzzyKey, mask);
       list.add(pair);
     }
 
@@ -286,7 +286,7 @@ public class TestFuzzyRowFilterEndToEnd {
 
     byte[] mask = new byte[] { 0, 0, 1, 1, 1, 1, 0, 0, 0, 0 };
 
-    List<Pair<byte[], byte[]>> list = new ArrayList<Pair<byte[], byte[]>>();
+    List<Pair<byte[], byte[]>> list = new ArrayList<>();
 
     for (int i = 0; i < totalFuzzyKeys; i++) {
       byte[] fuzzyKey = new byte[10];
@@ -298,7 +298,7 @@ public class TestFuzzyRowFilterEndToEnd {
       }
       buf.putInt(i * 2);
 
-      Pair<byte[], byte[]> pair = new Pair<byte[], byte[]>(fuzzyKey, mask);
+      Pair<byte[], byte[]> pair = new Pair<>(fuzzyKey, mask);
       list.add(pair);
     }
 
@@ -325,7 +325,7 @@ public class TestFuzzyRowFilterEndToEnd {
     HRegion first = regions.get(0);
     first.getScanner(scan);
     RegionScanner scanner = first.getScanner(scan);
-    List<Cell> results = new ArrayList<Cell>();
+    List<Cell> results = new ArrayList<>();
     // Result result;
     long timeBeforeScan = System.currentTimeMillis();
     int found = 0;
@@ -408,8 +408,8 @@ public class TestFuzzyRowFilterEndToEnd {
 
     byte[] mask2 = new byte[] { 0, 0, 0, 0, 0, 0, 1, 1, 1, 1 };
 
-    Pair<byte[], byte[]> pair1 = new Pair<byte[], byte[]>(fuzzyKey1, mask1);
-    Pair<byte[], byte[]> pair2 = new Pair<byte[], byte[]>(fuzzyKey2, mask2);
+    Pair<byte[], byte[]> pair1 = new Pair<>(fuzzyKey1, mask1);
+    Pair<byte[], byte[]> pair2 = new Pair<>(fuzzyKey2, mask2);
 
     FuzzyRowFilter fuzzyRowFilter1 = new FuzzyRowFilter(Lists.newArrayList(pair1));
     FuzzyRowFilter fuzzyRowFilter2 = new FuzzyRowFilter(Lists.newArrayList(pair2));
@@ -426,7 +426,7 @@ public class TestFuzzyRowFilterEndToEnd {
     scan.setFilter(filterList);
 
     ResultScanner scanner = hTable.getScanner(scan);
-    List<Cell> results = new ArrayList<Cell>();
+    List<Cell> results = new ArrayList<>();
     Result result;
     long timeBeforeScan = System.currentTimeMillis();
     while ((result = scanner.next()) != null) {
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/filter/TestInvocationRecordFilter.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/filter/TestInvocationRecordFilter.java
index 8291e52..159769e 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/filter/TestInvocationRecordFilter.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/filter/TestInvocationRecordFilter.java
@@ -84,8 +84,8 @@ public class TestInvocationRecordFilter {
 
   @Test
   public void testFilterInvocation() throws Exception {
-    List<Integer> selectQualifiers = new ArrayList<Integer>();
-    List<Integer> expectedQualifiers = new ArrayList<Integer>();
+    List<Integer> selectQualifiers = new ArrayList<>();
+    List<Integer> expectedQualifiers = new ArrayList<>();
 
     selectQualifiers.add(-1);
     verifyInvocationResults(selectQualifiers.toArray(new Integer[selectQualifiers.size()]),
@@ -127,7 +127,7 @@ public class TestInvocationRecordFilter {
 
     get.setFilter(new InvocationRecordFilter());
 
-    List<KeyValue> expectedValues = new ArrayList<KeyValue>();
+    List<KeyValue> expectedValues = new ArrayList<>();
     for (int i = 0; i < expectedQualifiers.length; i++) {
       expectedValues.add(new KeyValue(ROW_BYTES, FAMILY_NAME_BYTES, Bytes
           .toBytes(QUALIFIER_PREFIX + expectedQualifiers[i]),
@@ -136,8 +136,8 @@ public class TestInvocationRecordFilter {
     }
 
     Scan scan = new Scan(get);
-    List<Cell> actualValues = new ArrayList<Cell>();
-    List<Cell> temp = new ArrayList<Cell>();
+    List<Cell> actualValues = new ArrayList<>();
+    List<Cell> temp = new ArrayList<>();
     InternalScanner scanner = this.region.getScanner(scan);
     while (scanner.next(temp)) {
       actualValues.addAll(temp);
@@ -161,7 +161,7 @@ public class TestInvocationRecordFilter {
    */
   private static class InvocationRecordFilter extends FilterBase {
 
-    private List<Cell> visitedKeyValues = new ArrayList<Cell>();
+    private List<Cell> visitedKeyValues = new ArrayList<>();
 
     public void reset() {
       visitedKeyValues.clear();
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/filter/TestMultiRowRangeFilter.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/filter/TestMultiRowRangeFilter.java
index 271edaf..0b1c368 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/filter/TestMultiRowRangeFilter.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/filter/TestMultiRowRangeFilter.java
@@ -120,48 +120,48 @@ public class TestMultiRowRangeFilter {
 
   @Test
   public void testMergeAndSortWithEmptyStartRow() throws IOException {
-    List<RowRange> ranges = new ArrayList<RowRange>();
+    List<RowRange> ranges = new ArrayList<>();
     ranges.add(new RowRange(Bytes.toBytes(""), true, Bytes.toBytes(20), false));
     ranges.add(new RowRange(Bytes.toBytes(15), true, Bytes.toBytes(40), false));
     List<RowRange> actualRanges = MultiRowRangeFilter.sortAndMerge(ranges);
-    List<RowRange> expectedRanges = new ArrayList<RowRange>();
+    List<RowRange> expectedRanges = new ArrayList<>();
     expectedRanges.add(new RowRange(Bytes.toBytes(""), true, Bytes.toBytes(40), false));
     assertRangesEqual(expectedRanges, actualRanges);
   }
 
   @Test
   public void testMergeAndSortWithEmptyStopRow() throws IOException {
-    List<RowRange> ranges = new ArrayList<RowRange>();
+    List<RowRange> ranges = new ArrayList<>();
     ranges.add(new RowRange(Bytes.toBytes(10), true, Bytes.toBytes(20), false));
     ranges.add(new RowRange(Bytes.toBytes(15), true, Bytes.toBytes(""), false));
     ranges.add(new RowRange(Bytes.toBytes(30), true, Bytes.toBytes(70), false));
     List<RowRange> actualRanges = MultiRowRangeFilter.sortAndMerge(ranges);
-    List<RowRange> expectedRanges = new ArrayList<RowRange>();
+    List<RowRange> expectedRanges = new ArrayList<>();
     expectedRanges.add(new RowRange(Bytes.toBytes(10), true, Bytes.toBytes(""), false));
     assertRangesEqual(expectedRanges, actualRanges);
   }
 
   @Test
   public void testMergeAndSortWithEmptyStartRowAndStopRow() throws IOException {
-    List<RowRange> ranges = new ArrayList<RowRange>();
+    List<RowRange> ranges = new ArrayList<>();
     ranges.add(new RowRange(Bytes.toBytes(10), true, Bytes.toBytes(20), false));
     ranges.add(new RowRange(Bytes.toBytes(""), true, Bytes.toBytes(""), false));
     ranges.add(new RowRange(Bytes.toBytes(30), true, Bytes.toBytes(70), false));
     List<RowRange> actualRanges = MultiRowRangeFilter.sortAndMerge(ranges);
-    List<RowRange> expectedRanges = new ArrayList<RowRange>();
+    List<RowRange> expectedRanges = new ArrayList<>();
     expectedRanges.add(new RowRange(Bytes.toBytes(""), true, Bytes.toBytes(""), false));
     assertRangesEqual(expectedRanges, actualRanges);
   }
 
   @Test(expected=IllegalArgumentException.class)
   public void testMultiRowRangeWithoutRange() throws IOException {
-    List<RowRange> ranges = new ArrayList<RowRange>();
+    List<RowRange> ranges = new ArrayList<>();
     new MultiRowRangeFilter(ranges);
   }
 
   @Test(expected=IllegalArgumentException.class)
   public void testMultiRowRangeWithInvalidRange() throws IOException {
-    List<RowRange> ranges = new ArrayList<RowRange>();
+    List<RowRange> ranges = new ArrayList<>();
     ranges.add(new RowRange(Bytes.toBytes(10), true, Bytes.toBytes(20), false));
     // the start row larger than the stop row
     ranges.add(new RowRange(Bytes.toBytes(80), true, Bytes.toBytes(20), false));
@@ -171,12 +171,12 @@ public class TestMultiRowRangeFilter {
 
   @Test
   public void testMergeAndSortWithoutOverlap() throws IOException {
-    List<RowRange> ranges = new ArrayList<RowRange>();
+    List<RowRange> ranges = new ArrayList<>();
     ranges.add(new RowRange(Bytes.toBytes(10), true, Bytes.toBytes(20), false));
     ranges.add(new RowRange(Bytes.toBytes(30), true, Bytes.toBytes(40), false));
     ranges.add(new RowRange(Bytes.toBytes(60), true, Bytes.toBytes(70), false));
     List<RowRange> actualRanges = MultiRowRangeFilter.sortAndMerge(ranges);
-    List<RowRange> expectedRanges = new ArrayList<RowRange>();
+    List<RowRange> expectedRanges = new ArrayList<>();
     expectedRanges.add(new RowRange(Bytes.toBytes(10), true, Bytes.toBytes(20), false));
     expectedRanges.add(new RowRange(Bytes.toBytes(30), true, Bytes.toBytes(40), false));
     expectedRanges.add(new RowRange(Bytes.toBytes(60), true, Bytes.toBytes(70), false));
@@ -185,7 +185,7 @@ public class TestMultiRowRangeFilter {
 
   @Test
   public void testMergeAndSortWithOverlap() throws IOException {
-    List<RowRange> ranges = new ArrayList<RowRange>();
+    List<RowRange> ranges = new ArrayList<>();
     ranges.add(new RowRange(Bytes.toBytes(10), true, Bytes.toBytes(20), false));
     ranges.add(new RowRange(Bytes.toBytes(15), true, Bytes.toBytes(40), false));
     ranges.add(new RowRange(Bytes.toBytes(20), true, Bytes.toBytes(30), false));
@@ -194,7 +194,7 @@ public class TestMultiRowRangeFilter {
     ranges.add(new RowRange(Bytes.toBytes(90), true, Bytes.toBytes(100), false));
     ranges.add(new RowRange(Bytes.toBytes(95), true, Bytes.toBytes(100), false));
     List<RowRange> actualRanges = MultiRowRangeFilter.sortAndMerge(ranges);
-    List<RowRange> expectedRanges = new ArrayList<RowRange>();
+    List<RowRange> expectedRanges = new ArrayList<>();
     expectedRanges.add(new RowRange(Bytes.toBytes(10), true, Bytes.toBytes(70), false));
     expectedRanges.add(new RowRange(Bytes.toBytes(90), true, Bytes.toBytes(100), false));
     assertRangesEqual(expectedRanges, actualRanges);
@@ -202,22 +202,22 @@ public class TestMultiRowRangeFilter {
 
   @Test
   public void testMergeAndSortWithStartRowInclusive() throws IOException {
-    List<RowRange> ranges = new ArrayList<RowRange>();
+    List<RowRange> ranges = new ArrayList<>();
     ranges.add(new RowRange(Bytes.toBytes(10), true, Bytes.toBytes(20), false));
     ranges.add(new RowRange(Bytes.toBytes(20), true, Bytes.toBytes(""), false));
     List<RowRange> actualRanges = MultiRowRangeFilter.sortAndMerge(ranges);
-    List<RowRange> expectedRanges = new ArrayList<RowRange>();
+    List<RowRange> expectedRanges = new ArrayList<>();
     expectedRanges.add(new RowRange(Bytes.toBytes(10), true, Bytes.toBytes(""), false));
     assertRangesEqual(expectedRanges, actualRanges);
   }
 
   @Test
   public void testMergeAndSortWithRowExclusive() throws IOException {
-    List<RowRange> ranges = new ArrayList<RowRange>();
+    List<RowRange> ranges = new ArrayList<>();
     ranges.add(new RowRange(Bytes.toBytes(10), true, Bytes.toBytes(20), false));
     ranges.add(new RowRange(Bytes.toBytes(20), false, Bytes.toBytes(""), false));
     List<RowRange> actualRanges = MultiRowRangeFilter.sortAndMerge(ranges);
-    List<RowRange> expectedRanges = new ArrayList<RowRange>();
+    List<RowRange> expectedRanges = new ArrayList<>();
     expectedRanges.add(new RowRange(Bytes.toBytes(10), true, Bytes.toBytes(20), false));
     expectedRanges.add(new RowRange(Bytes.toBytes(20), false, Bytes.toBytes(""), false));
     assertRangesEqual(expectedRanges, actualRanges);
@@ -225,11 +225,11 @@ public class TestMultiRowRangeFilter {
 
   @Test
   public void testMergeAndSortWithRowInclusive() throws IOException {
-    List<RowRange> ranges = new ArrayList<RowRange>();
+    List<RowRange> ranges = new ArrayList<>();
     ranges.add(new RowRange(Bytes.toBytes(10), true, Bytes.toBytes(20), true));
     ranges.add(new RowRange(Bytes.toBytes(20), false, Bytes.toBytes(""), false));
     List<RowRange> actualRanges = MultiRowRangeFilter.sortAndMerge(ranges);
-    List<RowRange> expectedRanges = new ArrayList<RowRange>();
+    List<RowRange> expectedRanges = new ArrayList<>();
     expectedRanges.add(new RowRange(Bytes.toBytes(10), true, Bytes.toBytes(""), false));
     assertRangesEqual(expectedRanges, actualRanges);
   }
@@ -255,7 +255,7 @@ public class TestMultiRowRangeFilter {
     Scan scan = new Scan();
     scan.setMaxVersions();
 
-    List<RowRange> ranges = new ArrayList<RowRange>();
+    List<RowRange> ranges = new ArrayList<>();
     ranges.add(new RowRange(Bytes.toBytes(10), true, Bytes.toBytes(20), false));
     ranges.add(new RowRange(Bytes.toBytes(15), true, Bytes.toBytes(40), false));
     ranges.add(new RowRange(Bytes.toBytes(65), true, Bytes.toBytes(75), false));
@@ -283,7 +283,7 @@ public class TestMultiRowRangeFilter {
     Scan scan = new Scan();
     scan.setMaxVersions();
 
-    List<RowRange> ranges = new ArrayList<RowRange>();
+    List<RowRange> ranges = new ArrayList<>();
     ranges.add(new RowRange(Bytes.toBytes(30), true, Bytes.toBytes(40), false));
     ranges.add(new RowRange(Bytes.toBytes(10), true, Bytes.toBytes(20), false));
     ranges.add(new RowRange(Bytes.toBytes(60), true, Bytes.toBytes(70), false));
@@ -309,7 +309,7 @@ public class TestMultiRowRangeFilter {
     Scan scan = new Scan();
     scan.setMaxVersions();
 
-    List<RowRange> ranges = new ArrayList<RowRange>();
+    List<RowRange> ranges = new ArrayList<>();
     ranges.add(new RowRange(Bytes.toBytes(""), true, Bytes.toBytes(10), false));
     ranges.add(new RowRange(Bytes.toBytes(30), true, Bytes.toBytes(40), false));
 
@@ -331,7 +331,7 @@ public class TestMultiRowRangeFilter {
     Scan scan = new Scan();
     scan.setMaxVersions();
 
-    List<RowRange> ranges = new ArrayList<RowRange>();
+    List<RowRange> ranges = new ArrayList<>();
     ranges.add(new RowRange(Bytes.toBytes(10), true, Bytes.toBytes(""), false));
     ranges.add(new RowRange(Bytes.toBytes(30), true, Bytes.toBytes(40), false));
 
@@ -353,7 +353,7 @@ public class TestMultiRowRangeFilter {
     Scan scan = new Scan();
     scan.setMaxVersions();
 
-    List<RowRange> ranges = new ArrayList<RowRange>();
+    List<RowRange> ranges = new ArrayList<>();
     ranges.add(new RowRange(Bytes.toBytes(10), true, Bytes.toBytes(20), false));
     ranges.add(new RowRange(Bytes.toBytes(20), true, Bytes.toBytes(40), false));
     ranges.add(new RowRange(Bytes.toBytes(65), true, Bytes.toBytes(75), false));
@@ -384,7 +384,7 @@ public class TestMultiRowRangeFilter {
     Scan scan = new Scan();
     scan.setMaxVersions();
 
-    List<RowRange> ranges = new ArrayList<RowRange>();
+    List<RowRange> ranges = new ArrayList<>();
     ranges.add(new RowRange(Bytes.toBytes(10), true, Bytes.toBytes(20), false));
     ranges.add(new RowRange(Bytes.toBytes(20), false, Bytes.toBytes(40), false));
     ranges.add(new RowRange(Bytes.toBytes(65), true, Bytes.toBytes(75), false));
@@ -410,14 +410,14 @@ public class TestMultiRowRangeFilter {
     Scan scan = new Scan();
     scan.setMaxVersions();
 
-    List<RowRange> ranges1 = new ArrayList<RowRange>();
+    List<RowRange> ranges1 = new ArrayList<>();
     ranges1.add(new RowRange(Bytes.toBytes(10), true, Bytes.toBytes(20), false));
     ranges1.add(new RowRange(Bytes.toBytes(30), true, Bytes.toBytes(40), false));
     ranges1.add(new RowRange(Bytes.toBytes(60), true, Bytes.toBytes(70), false));
 
     MultiRowRangeFilter filter1 = new MultiRowRangeFilter(ranges1);
 
-    List<RowRange> ranges2 = new ArrayList<RowRange>();
+    List<RowRange> ranges2 = new ArrayList<>();
     ranges2.add(new RowRange(Bytes.toBytes(20), true, Bytes.toBytes(40), false));
     ranges2.add(new RowRange(Bytes.toBytes(80), true, Bytes.toBytes(90), false));
 
@@ -445,14 +445,14 @@ public class TestMultiRowRangeFilter {
     Scan scan = new Scan();
     scan.setMaxVersions();
 
-    List<RowRange> ranges1 = new ArrayList<RowRange>();
+    List<RowRange> ranges1 = new ArrayList<>();
     ranges1.add(new RowRange(Bytes.toBytes(30), true, Bytes.toBytes(40), false));
     ranges1.add(new RowRange(Bytes.toBytes(10), true, Bytes.toBytes(20), false));
     ranges1.add(new RowRange(Bytes.toBytes(60), true, Bytes.toBytes(70), false));
 
     MultiRowRangeFilter filter1 = new MultiRowRangeFilter(ranges1);
 
-    List<RowRange> ranges2 = new ArrayList<RowRange>();
+    List<RowRange> ranges2 = new ArrayList<>();
     ranges2.add(new RowRange(Bytes.toBytes(20), true, Bytes.toBytes(40), false));
     ranges2.add(new RowRange(Bytes.toBytes(80), true, Bytes.toBytes(90), false));
 
@@ -523,7 +523,7 @@ public class TestMultiRowRangeFilter {
       scan.setStopRow(stopRow);
     }
     ResultScanner scanner = ht.getScanner(scan);
-    List<Cell> kvList = new ArrayList<Cell>();
+    List<Cell> kvList = new ArrayList<>();
     Result r;
     while ((r = scanner.next()) != null) {
       for (Cell kv : r.listCells()) {
@@ -536,7 +536,7 @@ public class TestMultiRowRangeFilter {
 
   private int getResultsSize(Table ht, Scan scan) throws IOException {
     ResultScanner scanner = ht.getScanner(scan);
-    List<Cell> results = new ArrayList<Cell>();
+    List<Cell> results = new ArrayList<>();
     Result r;
     while ((r = scanner.next()) != null) {
       for (Cell kv : r.listCells()) {
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/filter/TestMultipleColumnPrefixFilter.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/filter/TestMultipleColumnPrefixFilter.java
index d8df298..d30cb37 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/filter/TestMultipleColumnPrefixFilter.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/filter/TestMultipleColumnPrefixFilter.java
@@ -73,14 +73,13 @@ public class TestMultipleColumnPrefixFilter {
     List<String> columns = generateRandomWords(10000, "column");
     long maxTimestamp = 2;
 
-    List<Cell> kvList = new ArrayList<Cell>();
+    List<Cell> kvList = new ArrayList<>();
 
-    Map<String, List<Cell>> prefixMap = new HashMap<String,
-        List<Cell>>();
+    Map<String, List<Cell>> prefixMap = new HashMap<>();
 
-    prefixMap.put("p", new ArrayList<Cell>());
-    prefixMap.put("q", new ArrayList<Cell>());
-    prefixMap.put("s", new ArrayList<Cell>());
+    prefixMap.put("p", new ArrayList<>());
+    prefixMap.put("q", new ArrayList<>());
+    prefixMap.put("s", new ArrayList<>());
 
     String valueString = "ValueString";
 
@@ -112,7 +111,7 @@ public class TestMultipleColumnPrefixFilter {
     
     filter = new MultipleColumnPrefixFilter(filter_prefix);
     scan.setFilter(filter);
-    List<Cell> results = new ArrayList<Cell>();  
+    List<Cell> results = new ArrayList<>();
     InternalScanner scanner = region.getScanner(scan);
     while (scanner.next(results))
       ;
@@ -140,14 +139,13 @@ public class TestMultipleColumnPrefixFilter {
     List<String> columns = generateRandomWords(10000, "column");
     long maxTimestamp = 3;
 
-    List<Cell> kvList = new ArrayList<Cell>();
+    List<Cell> kvList = new ArrayList<>();
 
-    Map<String, List<Cell>> prefixMap = new HashMap<String,
-        List<Cell>>();
+    Map<String, List<Cell>> prefixMap = new HashMap<>();
 
-    prefixMap.put("p", new ArrayList<Cell>());
-    prefixMap.put("q", new ArrayList<Cell>());
-    prefixMap.put("s", new ArrayList<Cell>());
+    prefixMap.put("p", new ArrayList<>());
+    prefixMap.put("q", new ArrayList<>());
+    prefixMap.put("s", new ArrayList<>());
 
     String valueString = "ValueString";
 
@@ -185,7 +183,7 @@ public class TestMultipleColumnPrefixFilter {
     
     filter = new MultipleColumnPrefixFilter(filter_prefix);
     scan.setFilter(filter);
-    List<Cell> results = new ArrayList<Cell>();  
+    List<Cell> results = new ArrayList<>();
     InternalScanner scanner = region.getScanner(scan);
     while (scanner.next(results))
       ;
@@ -230,7 +228,7 @@ public class TestMultipleColumnPrefixFilter {
  
     multiplePrefixFilter = new MultipleColumnPrefixFilter(filter_prefix);
     scan1.setFilter(multiplePrefixFilter);
-    List<Cell> results1 = new ArrayList<Cell>();  
+    List<Cell> results1 = new ArrayList<>();
     InternalScanner scanner1 = region.getScanner(scan1);
     while (scanner1.next(results1))
       ;
@@ -241,7 +239,7 @@ public class TestMultipleColumnPrefixFilter {
     singlePrefixFilter = new ColumnPrefixFilter(Bytes.toBytes("p"));
  
     scan2.setFilter(singlePrefixFilter);
-    List<Cell> results2 = new ArrayList<Cell>();  
+    List<Cell> results2 = new ArrayList<>();
     InternalScanner scanner2 = region.getScanner(scan1);
     while (scanner2.next(results2))
       ;
@@ -252,7 +250,7 @@ public class TestMultipleColumnPrefixFilter {
   }
   
   List<String> generateRandomWords(int numberOfWords, String suffix) {
-    Set<String> wordSet = new HashSet<String>();
+    Set<String> wordSet = new HashSet<>();
     for (int i = 0; i < numberOfWords; i++) {
       int lengthOfWords = (int) (Math.random()*2) + 1;
       char[] wordChar = new char[lengthOfWords];
@@ -267,7 +265,7 @@ public class TestMultipleColumnPrefixFilter {
       }
       wordSet.add(word);
     }
-    List<String> wordList = new ArrayList<String>(wordSet);
+    List<String> wordList = new ArrayList<>(wordSet);
     return wordList;
   }
 
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/filter/TestSingleColumnValueExcludeFilter.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/filter/TestSingleColumnValueExcludeFilter.java
index e23a394..0ef3ea7 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/filter/TestSingleColumnValueExcludeFilter.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/filter/TestSingleColumnValueExcludeFilter.java
@@ -58,7 +58,7 @@ public class TestSingleColumnValueExcludeFilter {
         CompareOp.EQUAL, VAL_1);
 
     // A 'match' situation
-    List<Cell> kvs = new ArrayList<Cell>();
+    List<Cell> kvs = new ArrayList<>();
     KeyValue kv = new KeyValue(ROW, COLUMN_FAMILY, COLUMN_QUALIFIER_2, VAL_1);
 
     kvs.add (new KeyValue(ROW, COLUMN_FAMILY, COLUMN_QUALIFIER_2, VAL_1));
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/http/TestGlobalFilter.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/http/TestGlobalFilter.java
index 0165b3d..acfe929 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/http/TestGlobalFilter.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/http/TestGlobalFilter.java
@@ -45,7 +45,7 @@ import org.junit.experimental.categories.Category;
 @Category({MiscTests.class, SmallTests.class})
 public class TestGlobalFilter extends HttpServerFunctionalTest {
   private static final Log LOG = LogFactory.getLog(HttpServer.class);
-  static final Set<String> RECORDS = new TreeSet<String>(); 
+  static final Set<String> RECORDS = new TreeSet<>();
 
   /** A very simple filter that records accessed uri's */
   static public class RecordingFilter implements Filter {
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/http/TestHttpServer.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/http/TestHttpServer.java
index 3b9e852..31b5b8d 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/http/TestHttpServer.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/http/TestHttpServer.java
@@ -84,7 +84,7 @@ public class TestHttpServer extends HttpServerFunctionalTest {
                       ) throws ServletException, IOException {
       PrintWriter out = response.getWriter();
       Map<String, String[]> params = request.getParameterMap();
-      SortedSet<String> keys = new TreeSet<String>(params.keySet());
+      SortedSet<String> keys = new TreeSet<>(params.keySet());
       for(String key: keys) {
         out.print(key);
         out.print(':');
@@ -109,7 +109,7 @@ public class TestHttpServer extends HttpServerFunctionalTest {
                       HttpServletResponse response
                       ) throws ServletException, IOException {
       PrintWriter out = response.getWriter();
-      SortedSet<String> sortedKeys = new TreeSet<String>();
+      SortedSet<String> sortedKeys = new TreeSet<>();
       Enumeration<String> keys = request.getParameterNames();
       while(keys.hasMoreElements()) {
         sortedKeys.add(keys.nextElement());
@@ -335,7 +335,7 @@ public class TestHttpServer extends HttpServerFunctionalTest {
    * Custom user->group mapping service.
    */
   public static class MyGroupsProvider extends ShellBasedUnixGroupsMapping {
-    static Map<String, List<String>> mapping = new HashMap<String, List<String>>();
+    static Map<String, List<String>> mapping = new HashMap<>();
 
     static void clearMapping() {
       mapping.clear();
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/http/TestPathFilter.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/http/TestPathFilter.java
index 33618ad..3c2de53 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/http/TestPathFilter.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/http/TestPathFilter.java
@@ -45,7 +45,7 @@ import org.junit.experimental.categories.Category;
 @Category({MiscTests.class, SmallTests.class})
 public class TestPathFilter extends HttpServerFunctionalTest {
   private static final Log LOG = LogFactory.getLog(HttpServer.class);
-  static final Set<String> RECORDS = new TreeSet<String>(); 
+  static final Set<String> RECORDS = new TreeSet<>();
 
   /** A very simple filter that records accessed uri's */
   static public class RecordingFilter implements Filter {
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/http/resource/JerseyResource.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/http/resource/JerseyResource.java
index da9519e..bf0e609 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/http/resource/JerseyResource.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/http/resource/JerseyResource.java
@@ -55,7 +55,7 @@ public class JerseyResource {
       ) throws IOException {
     LOG.info("get: " + PATH + "=" + path + ", " + OP + "=" + op);
 
-    final Map<String, Object> m = new TreeMap<String, Object>();
+    final Map<String, Object> m = new TreeMap<>();
     m.put(PATH, path);
     m.put(OP, op);
     final String js = JSON.toString(m);
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/http/ssl/KeyStoreTestUtil.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/http/ssl/KeyStoreTestUtil.java
index 8668738..234bd7a 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/http/ssl/KeyStoreTestUtil.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/http/ssl/KeyStoreTestUtil.java
@@ -206,7 +206,7 @@ public class KeyStoreTestUtil {
     File sslClientConfFile = new File(sslConfDir + "/ssl-client.xml");
     File sslServerConfFile = new File(sslConfDir + "/ssl-server.xml");
 
-    Map<String, X509Certificate> certs = new HashMap<String, X509Certificate>();
+    Map<String, X509Certificate> certs = new HashMap<>();
 
     if (useClientCert) {
       KeyPair cKP = KeyStoreTestUtil.generateKeyPair("RSA");
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/io/TestFileLink.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/io/TestFileLink.java
index 0da685f..8ee7d3d 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/io/TestFileLink.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/io/TestFileLink.java
@@ -126,7 +126,7 @@ public class TestFileLink {
 
     writeSomeData(fs, originalPath, 256 << 20, (byte)2);
 
-    List<Path> files = new ArrayList<Path>();
+    List<Path> files = new ArrayList<>();
     files.add(originalPath);
     files.add(archivedPath);
 
@@ -194,7 +194,7 @@ public class TestFileLink {
     assertEquals("hdfs", fs.getUri().getScheme());
 
     try {
-      List<Path> files = new ArrayList<Path>();
+      List<Path> files = new ArrayList<>();
       for (int i = 0; i < 3; i++) {
         Path path = new Path(String.format("test-data-%d", i));
         writeSomeData(fs, path, 1 << 20, (byte)i);
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/io/TestHalfStoreFileReader.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/io/TestHalfStoreFileReader.java
index 0e5f08e..6a0921f 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/io/TestHalfStoreFileReader.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/io/TestHalfStoreFileReader.java
@@ -241,7 +241,7 @@ public class TestHalfStoreFileReader {
   }
 
   List<KeyValue> genSomeKeys() {
-    List<KeyValue> ret = new ArrayList<KeyValue>(SIZE);
+    List<KeyValue> ret = new ArrayList<>(SIZE);
     for (int i = 0; i < SIZE; i++) {
       KeyValue kv =
           new KeyValue(
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/io/encoding/TestBufferedDataBlockEncoder.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/io/encoding/TestBufferedDataBlockEncoder.java
index d31af31..7e14228 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/io/encoding/TestBufferedDataBlockEncoder.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/io/encoding/TestBufferedDataBlockEncoder.java
@@ -59,7 +59,7 @@ public class TestBufferedDataBlockEncoder {
   @Test
   public void testEnsureSpaceForKey() {
     BufferedDataBlockEncoder.SeekerState state = new BufferedDataBlockEncoder.SeekerState(
-        new ObjectIntPair<ByteBuffer>(), false);
+        new ObjectIntPair<>(), false);
     for (int i = 1; i <= 65536; ++i) {
       state.keyLength = i;
       state.ensureSpaceForKey();
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/io/encoding/TestChangingEncoding.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/io/encoding/TestChangingEncoding.java
index a2cd50c..97f74af 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/io/encoding/TestChangingEncoding.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/io/encoding/TestChangingEncoding.java
@@ -77,8 +77,7 @@ public class TestChangingEncoding {
       createEncodingsToIterate();
 
   private static final List<DataBlockEncoding> createEncodingsToIterate() {
-    List<DataBlockEncoding> encodings = new ArrayList<DataBlockEncoding>(
-        Arrays.asList(DataBlockEncoding.values()));
+    List<DataBlockEncoding> encodings = new ArrayList<>(Arrays.asList(DataBlockEncoding.values()));
     encodings.add(DataBlockEncoding.NONE);
     return Collections.unmodifiableList(encodings);
   }
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/io/encoding/TestDataBlockEncoders.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/io/encoding/TestDataBlockEncoders.java
index 66fee6a..dc15bf5 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/io/encoding/TestDataBlockEncoders.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/io/encoding/TestDataBlockEncoders.java
@@ -124,7 +124,7 @@ public class TestDataBlockEncoders {
    */
   @Test
   public void testEmptyKeyValues() throws IOException {
-    List<KeyValue> kvList = new ArrayList<KeyValue>();
+    List<KeyValue> kvList = new ArrayList<>();
     byte[] row = new byte[0];
     byte[] family = new byte[0];
     byte[] qualifier = new byte[0];
@@ -151,7 +151,7 @@ public class TestDataBlockEncoders {
    */
   @Test
   public void testNegativeTimestamps() throws IOException {
-    List<KeyValue> kvList = new ArrayList<KeyValue>();
+    List<KeyValue> kvList = new ArrayList<>();
     byte[] row = new byte[0];
     byte[] family = new byte[0];
     byte[] qualifier = new byte[0];
@@ -190,8 +190,7 @@ public class TestDataBlockEncoders {
     List<KeyValue> sampleKv = generator.generateTestKeyValues(NUMBER_OF_KV, includesTags);
 
     // create all seekers
-    List<DataBlockEncoder.EncodedSeeker> encodedSeekers = 
-        new ArrayList<DataBlockEncoder.EncodedSeeker>();
+    List<DataBlockEncoder.EncodedSeeker> encodedSeekers = new ArrayList<>();
     for (DataBlockEncoding encoding : DataBlockEncoding.values()) {
       LOG.info("Encoding: " + encoding);
       // Off heap block data support not added for PREFIX_TREE DBE yet.
@@ -403,7 +402,7 @@ public class TestDataBlockEncoders {
   
   @Test
   public void testZeroByte() throws IOException {
-    List<KeyValue> kvList = new ArrayList<KeyValue>();
+    List<KeyValue> kvList = new ArrayList<>();
     byte[] row = Bytes.toBytes("abcd");
     byte[] family = new byte[] { 'f' };
     byte[] qualifier0 = new byte[] { 'b' };
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/io/encoding/TestEncodedSeekers.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/io/encoding/TestEncodedSeekers.java
index 0869df6..1b5c630 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/io/encoding/TestEncodedSeekers.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/io/encoding/TestEncodedSeekers.java
@@ -81,7 +81,7 @@ public class TestEncodedSeekers {
 
   @Parameters
   public static Collection<Object[]> parameters() {
-    List<Object[]> paramList = new ArrayList<Object[]>();
+    List<Object[]> paramList = new ArrayList<>();
     for (DataBlockEncoding encoding : DataBlockEncoding.values()) {
       for (boolean includeTags : new boolean[] { false, true }) {
         for (boolean compressTags : new boolean[] { false, true }) {
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/io/encoding/TestPrefixTree.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/io/encoding/TestPrefixTree.java
index e31a73b..6b13899 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/io/encoding/TestPrefixTree.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/io/encoding/TestPrefixTree.java
@@ -114,7 +114,7 @@ public class TestPrefixTree {
     scan.setStopRow(Bytes.toBytes("a-b-A-1:"));
 
     RegionScanner scanner = region.getScanner(scan);
-    List<Cell> cells = new ArrayList<Cell>();
+    List<Cell> cells = new ArrayList<>();
     for (int i = 0; i < 3; i++) {
       assertEquals(i < 2, scanner.next(cells));
       CellScanner cellScanner = Result.create(cells).cellScanner();
@@ -184,7 +184,7 @@ public class TestPrefixTree {
     region.flush(true);
     Scan scan = new Scan(Bytes.toBytes("obj29995"));
     RegionScanner scanner = region.getScanner(scan);
-    List<Cell> cells = new ArrayList<Cell>();
+    List<Cell> cells = new ArrayList<>();
     assertFalse(scanner.next(cells));
     assertArrayEquals(Bytes.toBytes("obj3"), Result.create(cells).getRow());
   }
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/io/encoding/TestPrefixTreeEncoding.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/io/encoding/TestPrefixTreeEncoding.java
index fd9b90b..decd39d 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/io/encoding/TestPrefixTreeEncoding.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/io/encoding/TestPrefixTreeEncoding.java
@@ -73,14 +73,13 @@ public class TestPrefixTreeEncoding {
   private static final int NUM_COLS_PER_ROW = 20;
 
   private int numBatchesWritten = 0;
-  private ConcurrentSkipListSet<Cell> kvset = new ConcurrentSkipListSet<Cell>(
-      CellComparator.COMPARATOR);
+  private ConcurrentSkipListSet<Cell> kvset = new ConcurrentSkipListSet<>(CellComparator.COMPARATOR);
 
   private static boolean formatRowNum = false;
 
   @Parameters
   public static Collection<Object[]> parameters() {
-    List<Object[]> paramList = new ArrayList<Object[]>();
+    List<Object[]> paramList = new ArrayList<>();
     {
       paramList.add(new Object[] { false });
       paramList.add(new Object[] { true });
@@ -228,7 +227,7 @@ public class TestPrefixTreeEncoding {
 
   private void verifySeeking(EncodedSeeker encodeSeeker,
       ByteBuffer encodedData, int batchId) {
-    List<KeyValue> kvList = new ArrayList<KeyValue>();
+    List<KeyValue> kvList = new ArrayList<>();
     for (int i = 0; i < NUM_ROWS_PER_BATCH; ++i) {
       kvList.clear();
       encodeSeeker.setCurrentBuffer(new SingleByteBuff(encodedData));
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/io/encoding/TestSeekBeforeWithReverseScan.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/io/encoding/TestSeekBeforeWithReverseScan.java
index 2826694..3bf189d 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/io/encoding/TestSeekBeforeWithReverseScan.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/io/encoding/TestSeekBeforeWithReverseScan.java
@@ -92,7 +92,7 @@ public class TestSeekBeforeWithReverseScan {
     scan.setFilter(new FirstKeyOnlyFilter());
     scan.addFamily(cfName);
     RegionScanner scanner = region.getScanner(scan);
-    List<Cell> res = new ArrayList<Cell>();
+    List<Cell> res = new ArrayList<>();
     int count = 1;
     while (scanner.next(res)) {
       count++;
@@ -130,7 +130,7 @@ public class TestSeekBeforeWithReverseScan {
     scan.setFilter(new FirstKeyOnlyFilter());
     scan.addFamily(cfName);
     RegionScanner scanner = region.getScanner(scan);
-    List<Cell> res = new ArrayList<Cell>();
+    List<Cell> res = new ArrayList<>();
     int count = 1;
     while (scanner.next(res)) {
       count++;
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/io/encoding/TestSeekToBlockWithEncoders.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/io/encoding/TestSeekToBlockWithEncoders.java
index 46cc9f9..cc70dc1 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/io/encoding/TestSeekToBlockWithEncoders.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/io/encoding/TestSeekToBlockWithEncoders.java
@@ -63,7 +63,7 @@ public class TestSeekToBlockWithEncoders {
    */
   @Test
   public void testSeekToBlockWithNonMatchingSeekKey() throws IOException {
-    List<KeyValue> sampleKv = new ArrayList<KeyValue>();
+    List<KeyValue> sampleKv = new ArrayList<>();
     KeyValue kv1 = new KeyValue(Bytes.toBytes("aaa"), Bytes.toBytes("f1"), Bytes.toBytes("q1"),
         Bytes.toBytes("val"));
     sampleKv.add(kv1);
@@ -89,7 +89,7 @@ public class TestSeekToBlockWithEncoders {
    */
   @Test
   public void testSeekingToBlockWithBiggerNonLength1() throws IOException {
-    List<KeyValue> sampleKv = new ArrayList<KeyValue>();
+    List<KeyValue> sampleKv = new ArrayList<>();
     KeyValue kv1 = new KeyValue(Bytes.toBytes("aaa"), Bytes.toBytes("f1"), Bytes.toBytes("q1"),
         Bytes.toBytes("val"));
     sampleKv.add(kv1);
@@ -115,7 +115,7 @@ public class TestSeekToBlockWithEncoders {
    */
   @Test
   public void testSeekingToBlockToANotAvailableKey() throws IOException {
-    List<KeyValue> sampleKv = new ArrayList<KeyValue>();
+    List<KeyValue> sampleKv = new ArrayList<>();
     KeyValue kv1 = new KeyValue(Bytes.toBytes("aaa"), Bytes.toBytes("f1"), Bytes.toBytes("q1"),
         Bytes.toBytes("val"));
     sampleKv.add(kv1);
@@ -141,7 +141,7 @@ public class TestSeekToBlockWithEncoders {
    */
   @Test
   public void testSeekToBlockWithDecreasingCommonPrefix() throws IOException {
-    List<KeyValue> sampleKv = new ArrayList<KeyValue>();
+    List<KeyValue> sampleKv = new ArrayList<>();
     KeyValue kv1 = new KeyValue(Bytes.toBytes("row10aaa"), Bytes.toBytes("f1"),
         Bytes.toBytes("q1"), Bytes.toBytes("val"));
     sampleKv.add(kv1);
@@ -160,7 +160,7 @@ public class TestSeekToBlockWithEncoders {
 
   @Test
   public void testSeekToBlockWithDiffQualifer() throws IOException {
-    List<KeyValue> sampleKv = new ArrayList<KeyValue>();
+    List<KeyValue> sampleKv = new ArrayList<>();
     KeyValue kv1 = new KeyValue(Bytes.toBytes("aaa"), Bytes.toBytes("f1"), Bytes.toBytes("q1"),
         Bytes.toBytes("val"));
     sampleKv.add(kv1);
@@ -180,7 +180,7 @@ public class TestSeekToBlockWithEncoders {
 
   @Test
   public void testSeekToBlockWithDiffQualiferOnSameRow() throws IOException {
-    List<KeyValue> sampleKv = new ArrayList<KeyValue>();
+    List<KeyValue> sampleKv = new ArrayList<>();
     KeyValue kv1 = new KeyValue(Bytes.toBytes("aaa"), Bytes.toBytes("f1"), Bytes.toBytes("q1"),
         Bytes.toBytes("val"));
     sampleKv.add(kv1);
@@ -203,7 +203,7 @@ public class TestSeekToBlockWithEncoders {
 
   @Test
   public void testSeekToBlockWithDiffQualiferOnSameRow1() throws IOException {
-    List<KeyValue> sampleKv = new ArrayList<KeyValue>();
+    List<KeyValue> sampleKv = new ArrayList<>();
     KeyValue kv1 = new KeyValue(Bytes.toBytes("aaa"), Bytes.toBytes("f1"), Bytes.toBytes("q1"),
         Bytes.toBytes("val"));
     sampleKv.add(kv1);
@@ -226,7 +226,7 @@ public class TestSeekToBlockWithEncoders {
 
   @Test
   public void testSeekToBlockWithDiffQualiferOnSameRowButDescendingInSize() throws IOException {
-    List<KeyValue> sampleKv = new ArrayList<KeyValue>();
+    List<KeyValue> sampleKv = new ArrayList<>();
     KeyValue kv1 = new KeyValue(Bytes.toBytes("aaa"), Bytes.toBytes("f1"), Bytes.toBytes("qual1"),
         Bytes.toBytes("val"));
     sampleKv.add(kv1);
@@ -249,7 +249,7 @@ public class TestSeekToBlockWithEncoders {
 
   @Test
   public void testSeekToBlockWithDiffFamilyAndQualifer() throws IOException {
-    List<KeyValue> sampleKv = new ArrayList<KeyValue>();
+    List<KeyValue> sampleKv = new ArrayList<>();
     KeyValue kv1 = new KeyValue(Bytes.toBytes("aaa"), Bytes.toBytes("fam1"), Bytes.toBytes("q1"),
         Bytes.toBytes("val"));
     sampleKv.add(kv1);
@@ -270,7 +270,7 @@ public class TestSeekToBlockWithEncoders {
   private void seekToTheKey(KeyValue expected, List<KeyValue> kvs, Cell toSeek)
       throws IOException {
     // create all seekers
-    List<DataBlockEncoder.EncodedSeeker> encodedSeekers = new ArrayList<DataBlockEncoder.EncodedSeeker>();
+    List<DataBlockEncoder.EncodedSeeker> encodedSeekers = new ArrayList<>();
     for (DataBlockEncoding encoding : DataBlockEncoding.values()) {
       if (encoding.getEncoder() == null || encoding == DataBlockEncoding.PREFIX_TREE) {
         continue;
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/io/hfile/CacheTestUtils.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/io/hfile/CacheTestUtils.java
index bd3f4c7..8d94766 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/io/hfile/CacheTestUtils.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/io/hfile/CacheTestUtils.java
@@ -77,7 +77,7 @@ public class CacheTestUtils {
         conf);
 
     final AtomicInteger totalQueries = new AtomicInteger();
-    final ConcurrentLinkedQueue<HFileBlockPair> blocksToTest = new ConcurrentLinkedQueue<HFileBlockPair>();
+    final ConcurrentLinkedQueue<HFileBlockPair> blocksToTest = new ConcurrentLinkedQueue<>();
     final AtomicInteger hits = new AtomicInteger();
     final AtomicInteger miss = new AtomicInteger();
 
@@ -344,7 +344,7 @@ public class CacheTestUtils {
   public static HFileBlockPair[] generateHFileBlocks(int blockSize, int numBlocks) {
     HFileBlockPair[] returnedBlocks = new HFileBlockPair[numBlocks];
     Random rand = new Random();
-    HashSet<String> usedStrings = new HashSet<String>();
+    HashSet<String> usedStrings = new HashSet<>();
     for (int i = 0; i < numBlocks; i++) {
       ByteBuffer cachedBuffer = ByteBuffer.allocate(blockSize);
       rand.nextBytes(cachedBuffer.array());
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/io/hfile/RandomDistribution.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/io/hfile/RandomDistribution.java
index 49f57de..bbc612f 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/io/hfile/RandomDistribution.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/io/hfile/RandomDistribution.java
@@ -128,8 +128,8 @@ public class RandomDistribution {
         throw new IllegalArgumentException("Invalid arguments");
       }
       random = r;
-      k = new ArrayList<Integer>();
-      v = new ArrayList<Double>();
+      k = new ArrayList<>();
+      v = new ArrayList<>();
 
       double sum = 0;
       int last = -1;
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/io/hfile/TestCacheOnWrite.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/io/hfile/TestCacheOnWrite.java
index 8f9c4f7..3315b6f 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/io/hfile/TestCacheOnWrite.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/io/hfile/TestCacheOnWrite.java
@@ -154,7 +154,7 @@ public class TestCacheOnWrite {
 
   private static List<BlockCache> getBlockCaches() throws IOException {
     Configuration conf = TEST_UTIL.getConfiguration();
-    List<BlockCache> blockcaches = new ArrayList<BlockCache>();
+    List<BlockCache> blockcaches = new ArrayList<>();
     // default
     blockcaches.add(new CacheConfig(conf).getBlockCache());
 
@@ -176,7 +176,7 @@ public class TestCacheOnWrite {
 
   @Parameters
   public static Collection<Object[]> getParameters() throws IOException {
-    List<Object[]> params = new ArrayList<Object[]>();
+    List<Object[]> params = new ArrayList<>();
     for (BlockCache blockCache : getBlockCaches()) {
       for (CacheOnWriteType cowType : CacheOnWriteType.values()) {
         for (Compression.Algorithm compress : HBaseTestingUtility.COMPRESSION_ALGORITHMS) {
@@ -261,12 +261,11 @@ public class TestCacheOnWrite {
     assertTrue(testDescription, scanner.seekTo());
 
     long offset = 0;
-    EnumMap<BlockType, Integer> blockCountByType =
-        new EnumMap<BlockType, Integer>(BlockType.class);
+    EnumMap<BlockType, Integer> blockCountByType = new EnumMap<>(BlockType.class);
 
     DataBlockEncoding encodingInCache = NoOpDataBlockEncoder.INSTANCE.getDataBlockEncoding();
-    List<Long> cachedBlocksOffset = new ArrayList<Long>();
-    Map<Long, HFileBlock> cachedBlocks = new HashMap<Long, HFileBlock>();
+    List<Long> cachedBlocksOffset = new ArrayList<>();
+    Map<Long, HFileBlock> cachedBlocks = new HashMap<>();
     while (offset < reader.getTrailer().getLoadOnOpenDataOffset()) {
       // Flags: don't cache the block, use pread, this is not a compaction.
       // Also, pass null for expected block type to avoid checking it.
@@ -383,7 +382,7 @@ public class TestCacheOnWrite {
       KeyValue kv;
       if(useTags) {
         Tag t = new ArrayBackedTag((byte) 1, "visibility");
-        List<Tag> tagList = new ArrayList<Tag>();
+        List<Tag> tagList = new ArrayList<>();
         tagList.add(t);
         Tag[] tags = new Tag[1];
         tags[0] = t;
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/io/hfile/TestFixedFileTrailer.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/io/hfile/TestFixedFileTrailer.java
index 95063ce..6145eca 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/io/hfile/TestFixedFileTrailer.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/io/hfile/TestFixedFileTrailer.java
@@ -77,7 +77,7 @@ public class TestFixedFileTrailer {
 
   @Parameters
   public static Collection<Object[]> getParameters() {
-    List<Object[]> versionsToTest = new ArrayList<Object[]>();
+    List<Object[]> versionsToTest = new ArrayList<>();
     for (int v = HFile.MIN_FORMAT_VERSION; v <= HFile.MAX_FORMAT_VERSION; ++v)
       versionsToTest.add(new Integer[] { v } );
     return versionsToTest;
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/io/hfile/TestHFileBlock.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/io/hfile/TestHFileBlock.java
index c75232a..1c87af4 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/io/hfile/TestHFileBlock.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/io/hfile/TestHFileBlock.java
@@ -123,7 +123,7 @@ public class TestHFileBlock {
 
   static int writeTestKeyValues(HFileBlock.Writer hbw, int seed, boolean includesMemstoreTS,
       boolean useTag) throws IOException {
-    List<KeyValue> keyValues = new ArrayList<KeyValue>();
+    List<KeyValue> keyValues = new ArrayList<>();
     Random randomizer = new Random(42l + seed); // just any fixed number
 
     // generate keyValues
@@ -383,8 +383,8 @@ public class TestHFileBlock {
                               .build();
           HFileBlock.Writer hbw = new HFileBlock.Writer(dataBlockEncoder, meta);
           long totalSize = 0;
-          final List<Integer> encodedSizes = new ArrayList<Integer>();
-          final List<ByteBuffer> encodedBlocks = new ArrayList<ByteBuffer>();
+          final List<Integer> encodedSizes = new ArrayList<>();
+          final List<ByteBuffer> encodedBlocks = new ArrayList<>();
           for (int blockId = 0; blockId < numBlocks; ++blockId) {
             hbw.startWriting(BlockType.DATA);
             writeTestKeyValues(hbw, blockId, includesMemstoreTS, includesTag);
@@ -532,11 +532,10 @@ public class TestHFileBlock {
                    ", pread=" + pread +
                    ", cacheOnWrite=" + cacheOnWrite);
           Path path = new Path(TEST_UTIL.getDataTestDir(), "prev_offset");
-          List<Long> expectedOffsets = new ArrayList<Long>();
-          List<Long> expectedPrevOffsets = new ArrayList<Long>();
-          List<BlockType> expectedTypes = new ArrayList<BlockType>();
-          List<ByteBuffer> expectedContents = cacheOnWrite
-              ? new ArrayList<ByteBuffer>() : null;
+          List<Long> expectedOffsets = new ArrayList<>();
+          List<Long> expectedPrevOffsets = new ArrayList<>();
+          List<BlockType> expectedTypes = new ArrayList<>();
+          List<ByteBuffer> expectedContents = cacheOnWrite ? new ArrayList<>() : null;
           long totalSize = writeBlocks(rand, algo, path, expectedOffsets,
               expectedPrevOffsets, expectedTypes, expectedContents);
 
@@ -718,8 +717,8 @@ public class TestHFileBlock {
     for (Compression.Algorithm compressAlgo : COMPRESSION_ALGORITHMS) {
       Path path = new Path(TEST_UTIL.getDataTestDir(), "concurrent_reading");
       Random rand = defaultRandom();
-      List<Long> offsets = new ArrayList<Long>();
-      List<BlockType> types = new ArrayList<BlockType>();
+      List<Long> offsets = new ArrayList<>();
+      List<BlockType> types = new ArrayList<>();
       writeBlocks(rand, compressAlgo, path, offsets, null, types, null);
       FSDataInputStream is = fs.open(path);
       long fileSize = fs.getFileStatus(path).getLen();
@@ -732,8 +731,7 @@ public class TestHFileBlock {
       HFileBlock.FSReader hbr = new HFileBlock.FSReaderImpl(is, fileSize, meta);
 
       Executor exec = Executors.newFixedThreadPool(NUM_READER_THREADS);
-      ExecutorCompletionService<Boolean> ecs =
-          new ExecutorCompletionService<Boolean>(exec);
+      ExecutorCompletionService<Boolean> ecs = new ExecutorCompletionService<>(exec);
 
       for (int i = 0; i < NUM_READER_THREADS; ++i) {
         ecs.submit(new BlockReaderThread("reader_" + (char) ('A' + i), hbr,
@@ -768,7 +766,7 @@ public class TestHFileBlock {
                         .withBytesPerCheckSum(HFile.DEFAULT_BYTES_PER_CHECKSUM)
                         .build();
     HFileBlock.Writer hbw = new HFileBlock.Writer(null, meta);
-    Map<BlockType, Long> prevOffsetByType = new HashMap<BlockType, Long>();
+    Map<BlockType, Long> prevOffsetByType = new HashMap<>();
     long totalSize = 0;
     for (int i = 0; i < NUM_TEST_BLOCKS; ++i) {
       long pos = os.getPos();
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/io/hfile/TestHFileBlockIndex.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/io/hfile/TestHFileBlockIndex.java
index ce6ec82..28930db 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/io/hfile/TestHFileBlockIndex.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/io/hfile/TestHFileBlockIndex.java
@@ -95,7 +95,7 @@ public class TestHFileBlockIndex {
   private long rootIndexOffset;
   private int numRootEntries;
   private int numLevels;
-  private static final List<byte[]> keys = new ArrayList<byte[]>();
+  private static final List<byte[]> keys = new ArrayList<>();
   private final Compression.Algorithm compr;
   private byte[] firstKeyInFile;
   private Configuration conf;
@@ -604,7 +604,7 @@ public class TestHFileBlockIndex {
       blockCache.evictBlocksByHfileName(hfilePath.getName());
 
       conf.setInt(HFileBlockIndex.MAX_CHUNK_SIZE_KEY, indexBlockSize);
-      Set<String> keyStrSet = new HashSet<String>();
+      Set<String> keyStrSet = new HashSet<>();
       byte[][] keys = new byte[NUM_KV][];
       byte[][] values = new byte[NUM_KV][];
 
@@ -674,7 +674,7 @@ public class TestHFileBlockIndex {
       HFileBlock.BlockIterator iter = fsReader.blockRange(0,
           reader.getTrailer().getLoadOnOpenDataOffset());
       HFileBlock block;
-      List<byte[]> blockKeys = new ArrayList<byte[]>();
+      List<byte[]> blockKeys = new ArrayList<>();
       while ((block = iter.nextBlock()) != null) {
         if (block.getBlockType() != BlockType.LEAF_INDEX)
           return;
@@ -762,7 +762,7 @@ public class TestHFileBlockIndex {
     HFile.Writer hfw = new HFile.WriterFactory(conf, cacheConf)
             .withFileContext(context)
             .withPath(fs, hfPath).create();
-    List<byte[]> keys = new ArrayList<byte[]>();
+    List<byte[]> keys = new ArrayList<>();
 
     // This should result in leaf-level indices and a root level index
     for (int i=0; i < 100; i++) {
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/io/hfile/TestHFileDataBlockEncoder.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/io/hfile/TestHFileDataBlockEncoder.java
index 387514e..ac939d1 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/io/hfile/TestHFileDataBlockEncoder.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/io/hfile/TestHFileDataBlockEncoder.java
@@ -243,8 +243,7 @@ public class TestHFileDataBlockEncoder {
    */
   @Parameters
   public static Collection<Object[]> getAllConfigurations() {
-    List<Object[]> configurations =
-        new ArrayList<Object[]>();
+    List<Object[]> configurations = new ArrayList<>();
 
     for (DataBlockEncoding diskAlgo : DataBlockEncoding.values()) {
       for (boolean includesMemstoreTS : new boolean[] { false, true }) {
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/io/hfile/TestHFileInlineToRootChunkConversion.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/io/hfile/TestHFileInlineToRootChunkConversion.java
index af4f2b8..f1528c2 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/io/hfile/TestHFileInlineToRootChunkConversion.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/io/hfile/TestHFileInlineToRootChunkConversion.java
@@ -58,7 +58,7 @@ public class TestHFileInlineToRootChunkConversion {
     HFile.Writer hfw = new HFile.WriterFactory(conf, cacheConf)
             .withFileContext(context)
             .withPath(fs, hfPath).create();
-    List<byte[]> keys = new ArrayList<byte[]>();
+    List<byte[]> keys = new ArrayList<>();
     StringBuilder sb = new StringBuilder();
 
     for (int i = 0; i < 4; ++i) {
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/io/hfile/TestHFileWriterV3.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/io/hfile/TestHFileWriterV3.java
index 983ec2f..fe6b549 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/io/hfile/TestHFileWriterV3.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/io/hfile/TestHFileWriterV3.java
@@ -128,7 +128,7 @@ public class TestHFileWriterV3 {
             .create();
 
     Random rand = new Random(9713312); // Just a fixed seed.
-    List<KeyValue> keyValues = new ArrayList<KeyValue>(entryCount);
+    List<KeyValue> keyValues = new ArrayList<>(entryCount);
 
     for (int i = 0; i < entryCount; ++i) {
       byte[] keyBytes = RandomKeyValueUtil.randomOrderedKey(rand, i);
@@ -137,7 +137,7 @@ public class TestHFileWriterV3 {
       byte[] valueBytes = RandomKeyValueUtil.randomValue(rand);
       KeyValue keyValue = null;
       if (useTags) {
-        ArrayList<Tag> tags = new ArrayList<Tag>();
+        ArrayList<Tag> tags = new ArrayList<>();
         for (int j = 0; j < 1 + rand.nextInt(4); j++) {
           byte[] tagBytes = new byte[16];
           rand.nextBytes(tagBytes);
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/io/hfile/TestLazyDataBlockDecompression.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/io/hfile/TestLazyDataBlockDecompression.java
index cf3c6ed..9253ce1 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/io/hfile/TestLazyDataBlockDecompression.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/io/hfile/TestLazyDataBlockDecompression.java
@@ -122,7 +122,7 @@ public class TestLazyDataBlockDecompression {
     reader.loadFileInfo();
     long offset = trailer.getFirstDataBlockOffset(),
       max = trailer.getLastDataBlockOffset();
-    List<HFileBlock> blocks = new ArrayList<HFileBlock>(4);
+    List<HFileBlock> blocks = new ArrayList<>(4);
     HFileBlock block;
     while (offset <= max) {
       block = reader.readBlock(offset, -1, /* cacheBlock */ true, /* pread */ false,
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/io/hfile/TestReseekTo.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/io/hfile/TestReseekTo.java
index 90e398d..a9ecf7b 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/io/hfile/TestReseekTo.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/io/hfile/TestReseekTo.java
@@ -72,8 +72,8 @@ public class TestReseekTo {
 
     String valueString = "Value";
 
-    List<Integer> keyList = new ArrayList<Integer>();
-    List<String> valueList = new ArrayList<String>();
+    List<Integer> keyList = new ArrayList<>();
+    List<String> valueList = new ArrayList<>();
 
     for (int key = 0; key < numberOfKeys; key++) {
       String value = valueString + key;
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/io/hfile/TestScannerFromBucketCache.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/io/hfile/TestScannerFromBucketCache.java
index 94e7219..f1775d0 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/io/hfile/TestScannerFromBucketCache.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/io/hfile/TestScannerFromBucketCache.java
@@ -208,7 +208,7 @@ public class TestScannerFromBucketCache {
       Scan scan = new Scan(row1);
       scan.addFamily(fam1);
       scan.setMaxVersions(10);
-      actual = new ArrayList<Cell>();
+      actual = new ArrayList<>();
       InternalScanner scanner = region.getScanner(scan);
 
       boolean hasNext = scanner.next(actual);
@@ -314,7 +314,7 @@ public class TestScannerFromBucketCache {
     }
 
     // Expected
-    List<Cell> expected = new ArrayList<Cell>();
+    List<Cell> expected = new ArrayList<>();
     expected.add(kv13);
     expected.add(kv12);
     expected.add(kv23);
@@ -326,7 +326,7 @@ public class TestScannerFromBucketCache {
     Scan scan = new Scan(row1);
     scan.addFamily(fam1);
     scan.setMaxVersions(MAX_VERSIONS);
-    List<Cell> actual = new ArrayList<Cell>();
+    List<Cell> actual = new ArrayList<>();
     InternalScanner scanner = region.getScanner(scan);
 
     boolean hasNext = scanner.next(actual);
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/io/hfile/TestScannerSelectionUsingKeyRange.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/io/hfile/TestScannerSelectionUsingKeyRange.java
index 9c6bb38..c834fca 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/io/hfile/TestScannerSelectionUsingKeyRange.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/io/hfile/TestScannerSelectionUsingKeyRange.java
@@ -61,7 +61,7 @@ public class TestScannerSelectionUsingKeyRange {
   private static final int NUM_ROWS = 8;
   private static final int NUM_COLS_PER_ROW = 5;
   private static final int NUM_FILES = 2;
-  private static final Map<Object, Integer> TYPE_COUNT = new HashMap<Object, Integer>(3);
+  private static final Map<Object, Integer> TYPE_COUNT = new HashMap<>(3);
   static {
     TYPE_COUNT.put(BloomType.ROWCOL, 0);
     TYPE_COUNT.put(BloomType.ROW, 0);
@@ -73,7 +73,7 @@ public class TestScannerSelectionUsingKeyRange {
 
   @Parameters
   public static Collection<Object[]> parameters() {
-    List<Object[]> params = new ArrayList<Object[]>();
+    List<Object[]> params = new ArrayList<>();
     for (Object type : TYPE_COUNT.keySet()) {
       params.add(new Object[] { type, TYPE_COUNT.get(type) });
     }
@@ -120,7 +120,7 @@ public class TestScannerSelectionUsingKeyRange {
     LruBlockCache cache = (LruBlockCache) cacheConf.getBlockCache();
     cache.clearCache();
     InternalScanner scanner = region.getScanner(scan);
-    List<Cell> results = new ArrayList<Cell>();
+    List<Cell> results = new ArrayList<>();
     while (scanner.next(results)) {
     }
     scanner.close();
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/io/hfile/TestScannerSelectionUsingTTL.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/io/hfile/TestScannerSelectionUsingTTL.java
index 08b259d..4af48ce 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/io/hfile/TestScannerSelectionUsingTTL.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/io/hfile/TestScannerSelectionUsingTTL.java
@@ -79,7 +79,7 @@ public class TestScannerSelectionUsingTTL {
 
   @Parameters
   public static Collection<Object[]> parameters() {
-    List<Object[]> params = new ArrayList<Object[]>();
+    List<Object[]> params = new ArrayList<>();
     for (int numFreshFiles = 1; numFreshFiles <= 3; ++numFreshFiles) {
       for (boolean explicitCompaction : new boolean[] { false, true }) {
         params.add(new Object[] { numFreshFiles, explicitCompaction });
@@ -135,7 +135,7 @@ public class TestScannerSelectionUsingTTL {
     LruBlockCache cache = (LruBlockCache) cacheConf.getBlockCache();
     cache.clearCache();
     InternalScanner scanner = region.getScanner(scan);
-    List<Cell> results = new ArrayList<Cell>();
+    List<Cell> results = new ArrayList<>();
     final int expectedKVsPerRow = numFreshFiles * NUM_COLS_PER_ROW;
     int numReturnedRows = 0;
     LOG.info("Scanning the entire table");
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/io/hfile/TestSeekTo.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/io/hfile/TestSeekTo.java
index a8fe3f0..d654bce 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/io/hfile/TestSeekTo.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/io/hfile/TestSeekTo.java
@@ -65,7 +65,7 @@ public class TestSeekTo {
   private final DataBlockEncoding encoding;
   @Parameters
   public static Collection<Object[]> parameters() {
-    List<Object[]> paramList = new ArrayList<Object[]>();
+    List<Object[]> paramList = new ArrayList<>();
     for (DataBlockEncoding encoding : DataBlockEncoding.values()) {
       paramList.add(new Object[] { encoding });
     }
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/io/hfile/bucket/TestBucketCache.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/io/hfile/bucket/TestBucketCache.java
index 6fe352d..0f16bfa 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/io/hfile/bucket/TestBucketCache.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/io/hfile/bucket/TestBucketCache.java
@@ -143,10 +143,10 @@ public class TestBucketCache {
     final List<Integer> BLOCKSIZES = Arrays.asList(4 * 1024, 8 * 1024, 64 * 1024, 96 * 1024);
 
     boolean full = false;
-    ArrayList<Long> allocations = new ArrayList<Long>();
+    ArrayList<Long> allocations = new ArrayList<>();
     // Fill the allocated extents by choosing a random blocksize. Continues selecting blocks until
     // the cache is completely filled.
-    List<Integer> tmp = new ArrayList<Integer>(BLOCKSIZES);
+    List<Integer> tmp = new ArrayList<>(BLOCKSIZES);
     while (!full) {
       Integer blockSize = null;
       try {
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/io/hfile/bucket/TestBucketWriterThread.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/io/hfile/bucket/TestBucketWriterThread.java
index 4d3f550..cfba69a 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/io/hfile/bucket/TestBucketWriterThread.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/io/hfile/bucket/TestBucketWriterThread.java
@@ -169,7 +169,7 @@ public class TestBucketWriterThread {
   private static void doDrainOfOneEntry(final BucketCache bc, final BucketCache.WriterThread wt,
       final BlockingQueue<RAMQueueEntry> q)
   throws InterruptedException {
-    List<RAMQueueEntry> rqes = BucketCache.getRAMQueueEntries(q, new ArrayList<RAMQueueEntry>(1));
+    List<RAMQueueEntry> rqes = BucketCache.getRAMQueueEntries(q, new ArrayList<>(1));
     wt.doDrain(rqes);
     assertTrue(q.isEmpty());
     assertTrue(bc.ramCache.isEmpty());
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/ipc/TestSimpleRpcScheduler.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/ipc/TestSimpleRpcScheduler.java
index 3535d23..04ac519 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/ipc/TestSimpleRpcScheduler.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/ipc/TestSimpleRpcScheduler.java
@@ -216,7 +216,7 @@ public class TestSimpleRpcScheduler {/*
       when(priority.getDeadline(eq(largeHead), any(Message.class))).thenReturn(50L);
       when(priority.getDeadline(eq(hugeHead), any(Message.class))).thenReturn(100L);
 
-      final ArrayList<Integer> work = new ArrayList<Integer>();
+      final ArrayList<Integer> work = new ArrayList<>();
       doAnswerTaskExecution(smallCallTask, work, 10, 250);
       doAnswerTaskExecution(largeCallTask, work, 50, 250);
       doAnswerTaskExecution(hugeCallTask, work, 100, 250);
@@ -312,7 +312,7 @@ public class TestSimpleRpcScheduler {/*
       when(scanCall.getHeader()).thenReturn(scanHead);
       when(scanCall.getParam()).thenReturn(scanCall.param);
 
-      ArrayList<Integer> work = new ArrayList<Integer>();
+      ArrayList<Integer> work = new ArrayList<>();
       doAnswerTaskExecution(putCallTask, work, 1, 1000);
       doAnswerTaskExecution(getCallTask, work, 2, 1000);
       doAnswerTaskExecution(scanCallTask, work, 3, 1000);
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/mapred/TestTableMapReduceUtil.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/mapred/TestTableMapReduceUtil.java
index fd0db6a..22dda35 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/mapred/TestTableMapReduceUtil.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/mapred/TestTableMapReduceUtil.java
@@ -218,7 +218,7 @@ public class TestTableMapReduceUtil {
         OutputCollector<ImmutableBytesWritable, Put> output, Reporter reporter)
         throws IOException {
       String strKey = Bytes.toString(key.get());
-      List<Put> result = new ArrayList<Put>();
+      List<Put> result = new ArrayList<>();
       while (values.hasNext())
         result.add(values.next());
 
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/mapreduce/MultiTableInputFormatTestBase.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/mapreduce/MultiTableInputFormatTestBase.java
index ca727e4..47421f1 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/mapreduce/MultiTableInputFormatTestBase.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/mapreduce/MultiTableInputFormatTestBase.java
@@ -234,7 +234,7 @@ public abstract class MultiTableInputFormatTestBase {
     c.set(KEY_STARTROW, start != null ? start : "");
     c.set(KEY_LASTROW, last != null ? last : "");
 
-    List<Scan> scans = new ArrayList<Scan>();
+    List<Scan> scans = new ArrayList<>();
 
     for (String tableName : TABLES) {
       Scan scan = new Scan();
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/mapreduce/NMapInputFormat.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/mapreduce/NMapInputFormat.java
index 92888ed..efacca9 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/mapreduce/NMapInputFormat.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/mapreduce/NMapInputFormat.java
@@ -46,15 +46,14 @@ public class NMapInputFormat extends InputFormat<NullWritable, NullWritable> {
   public RecordReader<NullWritable, NullWritable> createRecordReader(
       InputSplit split,
       TaskAttemptContext tac) throws IOException, InterruptedException {
-    return new SingleRecordReader<NullWritable, NullWritable>(
-        NullWritable.get(), NullWritable.get());
+    return new SingleRecordReader<>(NullWritable.get(), NullWritable.get());
   }
 
   @Override
   public List<InputSplit> getSplits(JobContext context) throws IOException,
       InterruptedException {
     int count = getNumMapTasks(context.getConfiguration());
-    List<InputSplit> splits = new ArrayList<InputSplit>(count);
+    List<InputSplit> splits = new ArrayList<>(count);
     for (int i = 0; i < count; i++) {
       splits.add(new NullInputSplit());
     }
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/mapreduce/TestGroupingTableMapper.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/mapreduce/TestGroupingTableMapper.java
index fc7b102..b7fdb47 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/mapreduce/TestGroupingTableMapper.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/mapreduce/TestGroupingTableMapper.java
@@ -51,7 +51,7 @@ public class TestGroupingTableMapper {
     Mapper<ImmutableBytesWritable, Result, ImmutableBytesWritable, Result>.Context context =
         mock(Mapper.Context.class);
     context.write(any(ImmutableBytesWritable.class), any(Result.class));
-    List<Cell> keyValue = new ArrayList<Cell>();
+    List<Cell> keyValue = new ArrayList<>();
     byte[] row = {};
     keyValue.add(new KeyValue(row, Bytes.toBytes("family2"), Bytes.toBytes("clm"), Bytes
         .toBytes("value1")));
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/mapreduce/TestHFileOutputFormat2.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/mapreduce/TestHFileOutputFormat2.java
index 52b2901..3c1bed8 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/mapreduce/TestHFileOutputFormat2.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/mapreduce/TestHFileOutputFormat2.java
@@ -448,7 +448,7 @@ public class TestHFileOutputFormat2  {
       writer = hof.getRecordWriter(context);
       final byte [] b = Bytes.toBytes("b");
 
-      List< Tag > tags = new ArrayList<Tag>();
+      List< Tag > tags = new ArrayList<>();
       tags.add(new ArrayBackedTag(TagType.TTL_TAG_TYPE, Bytes.toBytes(978670)));
       KeyValue kv = new KeyValue(b, b, b, HConstants.LATEST_TIMESTAMP, b, tags);
       writer.write(new ImmutableBytesWritable(), kv);
@@ -735,8 +735,7 @@ public class TestHFileOutputFormat2  {
    */
   private Map<String, Compression.Algorithm>
       getMockColumnFamiliesForCompression (int numCfs) {
-    Map<String, Compression.Algorithm> familyToCompression
-      = new HashMap<String, Compression.Algorithm>();
+    Map<String, Compression.Algorithm> familyToCompression = new HashMap<>();
     // use column family names having special characters
     if (numCfs-- > 0) {
       familyToCompression.put("Family1!@#!@#&", Compression.Algorithm.LZO);
@@ -809,8 +808,7 @@ public class TestHFileOutputFormat2  {
    */
   private Map<String, BloomType>
   getMockColumnFamiliesForBloomType (int numCfs) {
-    Map<String, BloomType> familyToBloomType =
-        new HashMap<String, BloomType>();
+    Map<String, BloomType> familyToBloomType = new HashMap<>();
     // use column family names having special characters
     if (numCfs-- > 0) {
       familyToBloomType.put("Family1!@#!@#&", BloomType.ROW);
@@ -881,8 +879,7 @@ public class TestHFileOutputFormat2  {
    */
   private Map<String, Integer>
   getMockColumnFamiliesForBlockSize (int numCfs) {
-    Map<String, Integer> familyToBlockSize =
-        new HashMap<String, Integer>();
+    Map<String, Integer> familyToBlockSize = new HashMap<>();
     // use column family names having special characters
     if (numCfs-- > 0) {
       familyToBlockSize.put("Family1!@#!@#&", 1234);
@@ -956,8 +953,7 @@ public class TestHFileOutputFormat2  {
    */
   private Map<String, DataBlockEncoding>
       getMockColumnFamiliesForDataBlockEncoding (int numCfs) {
-    Map<String, DataBlockEncoding> familyToDataBlockEncoding =
-        new HashMap<String, DataBlockEncoding>();
+    Map<String, DataBlockEncoding> familyToDataBlockEncoding = new HashMap<>();
     // use column family names having special characters
     if (numCfs-- > 0) {
       familyToDataBlockEncoding.put("Family1!@#!@#&", DataBlockEncoding.DIFF);
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/mapreduce/TestHRegionPartitioner.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/mapreduce/TestHRegionPartitioner.java
index abb600d..2867f13 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/mapreduce/TestHRegionPartitioner.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/mapreduce/TestHRegionPartitioner.java
@@ -59,7 +59,7 @@ public class TestHRegionPartitioner {
     UTIL.createTable(TableName.valueOf(name.getMethodName()), families, 1,
     Bytes.toBytes("aa"), Bytes.toBytes("cc"), 3);
 
-    HRegionPartitioner<Long, Long> partitioner = new HRegionPartitioner<Long, Long>();
+    HRegionPartitioner<Long, Long> partitioner = new HRegionPartitioner<>();
     Configuration configuration = UTIL.getConfiguration();
     configuration.set(TableOutputFormat.OUTPUT_TABLE, name.getMethodName());
     partitioner.setConf(configuration);
@@ -68,4 +68,4 @@ public class TestHRegionPartitioner {
     assertEquals(1, partitioner.getPartition(writable, 10L, 3));
     assertEquals(0, partitioner.getPartition(writable, 10L, 1));
   }
-}
\ No newline at end of file
+}
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/mapreduce/TestHashTable.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/mapreduce/TestHashTable.java
index 75d40a1..a7642af 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/mapreduce/TestHashTable.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/mapreduce/TestHashTable.java
@@ -146,8 +146,7 @@ public class TestHashTable {
       .put(95, new ImmutableBytesWritable(Bytes.fromHex("f57c447e32a08f4bf1abb2892839ac56")))
       .build();
   
-    Map<Integer, ImmutableBytesWritable> actualHashes
-      = new HashMap<Integer, ImmutableBytesWritable>();
+    Map<Integer, ImmutableBytesWritable> actualHashes = new HashMap<>();
     Path dataDir = new Path(testDir, HashTable.HASH_DATA_DIR);
     for (int i = 0; i < numHashFiles; i++) {
       Path hashPath = new Path(dataDir, HashTable.TableHash.getDataFileName(i));
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/mapreduce/TestImportExport.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/mapreduce/TestImportExport.java
index 7de012e..1866a35 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/mapreduce/TestImportExport.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/mapreduce/TestImportExport.java
@@ -628,7 +628,7 @@ public class TestImportExport {
   public void testAddFilterAndArguments() throws IOException {
     Configuration configuration = new Configuration();
 
-    List<String> args = new ArrayList<String>();
+    List<String> args = new ArrayList<>();
     args.add("param1");
     args.add("param2");
 
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/mapreduce/TestImportTSVWithOperationAttributes.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/mapreduce/TestImportTSVWithOperationAttributes.java
index 26f8dea..6d9b05b 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/mapreduce/TestImportTSVWithOperationAttributes.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/mapreduce/TestImportTSVWithOperationAttributes.java
@@ -176,7 +176,7 @@ public class TestImportTSVWithOperationAttributes implements Configurable {
     }
 
     // run the import
-    List<String> argv = new ArrayList<String>(Arrays.asList(args));
+    List<String> argv = new ArrayList<>(Arrays.asList(args));
     argv.add(inputPath.toString());
     Tool tool = new ImportTsv();
     LOG.debug("Running ImportTsv with arguments: " + argv);
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/mapreduce/TestImportTSVWithTTLs.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/mapreduce/TestImportTSVWithTTLs.java
index 21cae54..4ab3d29 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/mapreduce/TestImportTSVWithTTLs.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/mapreduce/TestImportTSVWithTTLs.java
@@ -138,7 +138,7 @@ public class TestImportTSVWithTTLs implements Configurable {
     }
 
     // run the import
-    List<String> argv = new ArrayList<String>(Arrays.asList(args));
+    List<String> argv = new ArrayList<>(Arrays.asList(args));
     argv.add(inputPath.toString());
     Tool tool = new ImportTsv();
     LOG.debug("Running ImportTsv with arguments: " + argv);
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/mapreduce/TestImportTSVWithVisibilityLabels.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/mapreduce/TestImportTSVWithVisibilityLabels.java
index 50d6b18..b8d973b 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/mapreduce/TestImportTSVWithVisibilityLabels.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/mapreduce/TestImportTSVWithVisibilityLabels.java
@@ -357,7 +357,7 @@ public class TestImportTSVWithVisibilityLabels implements Configurable {
     }
 
     // run the import
-    List<String> argv = new ArrayList<String>(Arrays.asList(args));
+    List<String> argv = new ArrayList<>(Arrays.asList(args));
     argv.add(inputPath.toString());
     Tool tool = new ImportTsv();
     LOG.debug("Running ImportTsv with arguments: " + argv);
@@ -397,9 +397,9 @@ public class TestImportTSVWithVisibilityLabels implements Configurable {
 
     // validate number and content of output columns
     LOG.debug("Validating HFiles.");
-    Set<String> configFamilies = new HashSet<String>();
+    Set<String> configFamilies = new HashSet<>();
     configFamilies.add(family);
-    Set<String> foundFamilies = new HashSet<String>();
+    Set<String> foundFamilies = new HashSet<>();
     int actualKVCount = 0;
     for (FileStatus cfStatus : fs.listStatus(new Path(outputPath), new OutputFilesFilter())) {
       LOG.debug("The output path has files");
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/mapreduce/TestImportTsv.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/mapreduce/TestImportTsv.java
index fd51544..b7d5c6f 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/mapreduce/TestImportTsv.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/mapreduce/TestImportTsv.java
@@ -114,7 +114,7 @@ public class TestImportTsv implements Configurable {
   @Before
   public void setup() throws Exception {
     tn = TableName.valueOf("test-" + UUID.randomUUID());
-    args = new HashMap<String, String>();
+    args = new HashMap<>();
     // Prepare the arguments required for the test.
     args.put(ImportTsv.COLUMNS_CONF_KEY, "HBASE_ROW_KEY,FAM:A,FAM:B");
     args.put(ImportTsv.SEPARATOR_CONF_KEY, "\u001b");
@@ -515,9 +515,9 @@ public class TestImportTsv implements Configurable {
       int expectedKVCount) throws IOException {
     // validate number and content of output columns
     LOG.debug("Validating HFiles.");
-    Set<String> configFamilies = new HashSet<String>();
+    Set<String> configFamilies = new HashSet<>();
     configFamilies.add(family);
-    Set<String> foundFamilies = new HashSet<String>();
+    Set<String> foundFamilies = new HashSet<>();
     int actualKVCount = 0;
     for (FileStatus cfStatus : fs.listStatus(new Path(outputPath), new OutputFilesFilter())) {
       String[] elements = cfStatus.getPath().toString().split(Path.SEPARATOR);
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/mapreduce/TestImportTsvParser.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/mapreduce/TestImportTsvParser.java
index 81e0a70..f569446 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/mapreduce/TestImportTsvParser.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/mapreduce/TestImportTsvParser.java
@@ -52,7 +52,7 @@ public class TestImportTsvParser {
   }
 
   private void checkParsing(ParsedLine parsed, Iterable<String> expected) {
-    ArrayList<String> parsedCols = new ArrayList<String>();
+    ArrayList<String> parsedCols = new ArrayList<>();
     for (int i = 0; i < parsed.getColumnCount(); i++) {
       parsedCols.add(Bytes.toString(parsed.getLineBytes(), parsed.getColumnOffset(i),
           parsed.getColumnLength(i)));
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/mapreduce/TestLoadIncrementalHFiles.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/mapreduce/TestLoadIncrementalHFiles.java
index 7167c19..a6dacf7 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/mapreduce/TestLoadIncrementalHFiles.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/mapreduce/TestLoadIncrementalHFiles.java
@@ -323,7 +323,7 @@ public class TestLoadIncrementalHFiles {
       list = new ArrayList<>();
     }
     if (useMap) {
-      map = new TreeMap<byte[], List<Path>>(Bytes.BYTES_COMPARATOR);
+      map = new TreeMap<>(Bytes.BYTES_COMPARATOR);
       map.put(FAMILY, list);
     }
     Path last = null;
@@ -630,7 +630,7 @@ public class TestLoadIncrementalHFiles {
 
   @Test(timeout = 120000)
   public void testInferBoundaries() {
-    TreeMap<byte[], Integer> map = new TreeMap<byte[], Integer>(Bytes.BYTES_COMPARATOR);
+    TreeMap<byte[], Integer> map = new TreeMap<>(Bytes.BYTES_COMPARATOR);
 
     /* Toy example
      *     c---------i            o------p          s---------t     v------x
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/mapreduce/TestMultiHFileOutputFormat.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/mapreduce/TestMultiHFileOutputFormat.java
index 738ae5f..958ed83 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/mapreduce/TestMultiHFileOutputFormat.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/mapreduce/TestMultiHFileOutputFormat.java
@@ -169,7 +169,7 @@ public class TestMultiHFileOutputFormat {
             byte keyBytes[] = new byte[keyLength];
             byte valBytes[] = new byte[valLength];
 
-            ArrayList<ImmutableBytesWritable> tables = new ArrayList<ImmutableBytesWritable>();
+            ArrayList<ImmutableBytesWritable> tables = new ArrayList<>();
             for (int i = 0; i < TABLES.length; i++) {
                 tables.add(new ImmutableBytesWritable(TABLES[i]));
             }
@@ -204,7 +204,7 @@ public class TestMultiHFileOutputFormat {
         protected void reduce(ImmutableBytesWritable table, java.lang.Iterable<KeyValue> kvs,
             org.apache.hadoop.mapreduce.Reducer<ImmutableBytesWritable, KeyValue, ImmutableBytesWritable, KeyValue>.Context context)
             throws java.io.IOException, InterruptedException {
-            TreeSet<KeyValue> map = new TreeSet<KeyValue>(KeyValue.COMPARATOR);
+            TreeSet<KeyValue> map = new TreeSet<>(KeyValue.COMPARATOR);
             for (KeyValue kv : kvs) {
                 try {
                     map.add(kv.clone());
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/mapreduce/TestRowCounter.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/mapreduce/TestRowCounter.java
index cd83199..3b84e2d 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/mapreduce/TestRowCounter.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/mapreduce/TestRowCounter.java
@@ -319,7 +319,7 @@ public class TestRowCounter {
     final byte[] col1 = Bytes.toBytes(COL1);
     final byte[] col2 = Bytes.toBytes(COL2);
     final byte[] col3 = Bytes.toBytes(COMPOSITE_COLUMN);
-    ArrayList<Put> rowsUpdate = new ArrayList<Put>();
+    ArrayList<Put> rowsUpdate = new ArrayList<>();
     // write few rows with two columns
     int i = 0;
     for (; i < totalRows - rowsWithOneCol; i++) {
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/mapreduce/TestSimpleTotalOrderPartitioner.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/mapreduce/TestSimpleTotalOrderPartitioner.java
index 119df80..0f41f33 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/mapreduce/TestSimpleTotalOrderPartitioner.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/mapreduce/TestSimpleTotalOrderPartitioner.java
@@ -42,8 +42,7 @@ public class TestSimpleTotalOrderPartitioner {
   public void testSplit() throws Exception {
     String start = "a";
     String end = "{";
-    SimpleTotalOrderPartitioner<byte []> p =
-      new SimpleTotalOrderPartitioner<byte []>();
+    SimpleTotalOrderPartitioner<byte []> p = new SimpleTotalOrderPartitioner<>();
     
     this.conf.set(SimpleTotalOrderPartitioner.START, start);
     this.conf.set(SimpleTotalOrderPartitioner.END, end);
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/mapreduce/TestTableSplit.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/mapreduce/TestTableSplit.java
index f1cda3c..4382c9c 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/mapreduce/TestTableSplit.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/mapreduce/TestTableSplit.java
@@ -48,7 +48,7 @@ public class TestTableSplit {
         "row-end".getBytes(), "location");
     assertEquals (split1, split2);
     assertTrue   (split1.hashCode() == split2.hashCode());
-    HashSet<TableSplit> set = new HashSet<TableSplit>(2);
+    HashSet<TableSplit> set = new HashSet<>(2);
     set.add(split1);
     set.add(split2);
     assertTrue(set.size() == 1);
@@ -68,7 +68,7 @@ public class TestTableSplit {
 
     assertEquals (split1, split2);
     assertTrue   (split1.hashCode() == split2.hashCode());
-    HashSet<TableSplit> set = new HashSet<TableSplit>(2);
+    HashSet<TableSplit> set = new HashSet<>(2);
     set.add(split1);
     set.add(split2);
     assertTrue(set.size() == 1);
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/mapreduce/TestTimeRangeMapRed.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/mapreduce/TestTimeRangeMapRed.java
index 8b7cdd7..6796c94 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/mapreduce/TestTimeRangeMapRed.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/mapreduce/TestTimeRangeMapRed.java
@@ -68,8 +68,7 @@ public class TestTimeRangeMapRed {
   private Admin admin;
 
   private static final byte [] KEY = Bytes.toBytes("row1");
-  private static final NavigableMap<Long, Boolean> TIMESTAMP =
-    new TreeMap<Long, Boolean>();
+  private static final NavigableMap<Long, Boolean> TIMESTAMP = new TreeMap<>();
   static {
     TIMESTAMP.put((long)1245620000, false);
     TIMESTAMP.put((long)1245620005, true); // include
@@ -112,7 +111,7 @@ public class TestTimeRangeMapRed {
     public void map(ImmutableBytesWritable key, Result result,
         Context context)
     throws IOException {
-      List<Long> tsList = new ArrayList<Long>();
+      List<Long> tsList = new ArrayList<>();
       for (Cell kv : result.listCells()) {
         tsList.add(kv.getTimestamp());
       }
@@ -152,7 +151,7 @@ public class TestTimeRangeMapRed {
     col.setMaxVersions(Integer.MAX_VALUE);
     desc.addFamily(col);
     admin.createTable(desc);
-    List<Put> puts = new ArrayList<Put>();
+    List<Put> puts = new ArrayList<>();
     for (Map.Entry<Long, Boolean> entry : TIMESTAMP.entrySet()) {
       Put put = new Put(KEY);
       put.setDurability(Durability.SKIP_WAL);
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/mapreduce/TestWALPlayer.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/mapreduce/TestWALPlayer.java
index 7e142bc..427c5cc 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/mapreduce/TestWALPlayer.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/mapreduce/TestWALPlayer.java
@@ -172,7 +172,7 @@ public class TestWALPlayer {
     when(context.getConfiguration()).thenReturn(configuration);
 
     WALEdit value = mock(WALEdit.class);
-    ArrayList<Cell> values = new ArrayList<Cell>();
+    ArrayList<Cell> values = new ArrayList<>();
     KeyValue kv1 = new KeyValue(Bytes.toBytes("row"), Bytes.toBytes("family"), null);
 
     values.add(kv1);
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/mapreduce/TestWALRecordReader.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/mapreduce/TestWALRecordReader.java
index fa1b9f4..34725b4 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/mapreduce/TestWALRecordReader.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/mapreduce/TestWALRecordReader.java
@@ -81,8 +81,7 @@ public class TestWALRecordReader {
   private static HTableDescriptor htd;
   private static Path logDir;
   protected MultiVersionConcurrencyControl mvcc;
-  protected static NavigableMap<byte[], Integer> scopes = new TreeMap<byte[], Integer>(
-      Bytes.BYTES_COMPARATOR);
+  protected static NavigableMap<byte[], Integer> scopes = new TreeMap<>(Bytes.BYTES_COMPARATOR);
 
   private static String getName() {
     return "TestWALRecordReader";
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/master/MockRegionServer.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/master/MockRegionServer.java
index 467d4a5..a5fe952 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/master/MockRegionServer.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/master/MockRegionServer.java
@@ -143,14 +143,12 @@ ClientProtos.ClientService.BlockingInterface, RegionServerServices {
    * key, need to use TreeMap and provide a Comparator.  Use
    * {@link #setGetResult(byte[], byte[], Result)} filling this map.
    */
-  private final Map<byte [], Map<byte [], Result>> gets =
-    new TreeMap<byte [], Map<byte [], Result>>(Bytes.BYTES_COMPARATOR);
+  private final Map<byte [], Map<byte [], Result>> gets = new TreeMap<>(Bytes.BYTES_COMPARATOR);
 
   /**
    * Map of regions to results to return when scanning.
    */
-  private final Map<byte [], Result []> nexts =
-    new TreeMap<byte [], Result []>(Bytes.BYTES_COMPARATOR);
+  private final Map<byte [], Result []> nexts = new TreeMap<>(Bytes.BYTES_COMPARATOR);
 
   /**
    * Data structure that holds regionname and index used scanning.
@@ -177,8 +175,7 @@ ClientProtos.ClientService.BlockingInterface, RegionServerServices {
   /**
    * Outstanding scanners and their offset into <code>nexts</code>
    */
-  private final Map<Long, RegionNameAndIndex> scannersAndOffsets =
-    new HashMap<Long, RegionNameAndIndex>();
+  private final Map<Long, RegionNameAndIndex> scannersAndOffsets = new HashMap<>();
 
   /**
    * @param sn Name of this mock regionserver
@@ -203,7 +200,7 @@ ClientProtos.ClientService.BlockingInterface, RegionServerServices {
     if (value == null) {
       // If no value already, create one.  Needs to be treemap because we are
       // using byte array as key.   Not thread safe.
-      value = new TreeMap<byte [], Result>(Bytes.BYTES_COMPARATOR);
+      value = new TreeMap<>(Bytes.BYTES_COMPARATOR);
       this.gets.put(regionName, value);
     }
     value.put(row, r);
@@ -402,7 +399,7 @@ ClientProtos.ClientService.BlockingInterface, RegionServerServices {
         Result result = next(scannerId);
         if (result != null) {
           builder.addCellsPerResult(result.size());
-          List<CellScannable> results = new ArrayList<CellScannable>(1);
+          List<CellScannable> results = new ArrayList<>(1);
           results.add(result);
           ((HBaseRpcController) controller).setCellScanner(CellUtil
               .createCellScanner(results));
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/master/TestAssignmentListener.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/master/TestAssignmentListener.java
index 5100a2b..78b75d5 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/master/TestAssignmentListener.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/master/TestAssignmentListener.java
@@ -333,14 +333,14 @@ public class TestAssignmentListener {
 
     // We'll start with 2 servers in draining that existed before the
     // HMaster started.
-    ArrayList<ServerName> drainingServers = new ArrayList<ServerName>();
+    ArrayList<ServerName> drainingServers = new ArrayList<>();
     drainingServers.add(SERVERNAME_A);
     drainingServers.add(SERVERNAME_B);
 
     // We'll have 2 servers that come online AFTER the DrainingServerTracker
     // is started (just as we see when we failover to the Backup HMaster).
     // One of these will already be a draining server.
-    HashMap<ServerName, ServerLoad> onlineServers = new HashMap<ServerName, ServerLoad>();
+    HashMap<ServerName, ServerLoad> onlineServers = new HashMap<>();
     onlineServers.put(SERVERNAME_A, ServerLoad.EMPTY_SERVERLOAD);
     onlineServers.put(SERVERNAME_C, ServerLoad.EMPTY_SERVERLOAD);
 
@@ -370,7 +370,7 @@ public class TestAssignmentListener {
         new ArrayList<ServerName>());
 
     // checkAndRecordNewServer() is how servers are added to the ServerManager.
-    ArrayList<ServerName> onlineDrainingServers = new ArrayList<ServerName>();
+    ArrayList<ServerName> onlineDrainingServers = new ArrayList<>();
     for (ServerName sn : onlineServers.keySet()){
       // Here's the actual test.
       serverManager.checkAndRecordNewServer(sn, onlineServers.get(sn));
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/master/TestAssignmentManagerOnCluster.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/master/TestAssignmentManagerOnCluster.java
index 242b012..449e1e6 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/master/TestAssignmentManagerOnCluster.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/master/TestAssignmentManagerOnCluster.java
@@ -624,7 +624,7 @@ public class TestAssignmentManagerOnCluster {
       HMaster master = TEST_UTIL.getHBaseCluster().getMaster();
       AssignmentManager am = master.getAssignmentManager();
 
-      Map<HRegionInfo, ServerName> regions = new HashMap<HRegionInfo, ServerName>();
+      Map<HRegionInfo, ServerName> regions = new HashMap<>();
       ServerName dest = TEST_UTIL.getHBaseCluster().getRegionServer(0).getServerName();
       regions.put(hri, dest);
       // retainAssignment but balancer cannot find a plan
@@ -838,7 +838,7 @@ public class TestAssignmentManagerOnCluster {
       assertNotNull(destServerName);
       assertFalse("Region should be assigned on a new region server",
         oldServerName.equals(destServerName));
-      List<HRegionInfo> regions = new ArrayList<HRegionInfo>();
+      List<HRegionInfo> regions = new ArrayList<>();
       regions.add(hri);
       am.assign(destServerName, regions);
 
@@ -1214,8 +1214,8 @@ public class TestAssignmentManagerOnCluster {
     rss.start();
     // Create 10 threads and make each do 10 puts related to region state update
     Thread[] th = new Thread[10];
-    List<String> nameList = new ArrayList<String>();
-    List<TableName> tableNameList = new ArrayList<TableName>();
+    List<String> nameList = new ArrayList<>();
+    List<TableName> tableNameList = new ArrayList<>();
     for (int i = 0; i < th.length; i++) {
       th[i] = new Thread() {
         @Override
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/master/TestCatalogJanitor.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/master/TestCatalogJanitor.java
index 52b58f1..cc73d9d 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/master/TestCatalogJanitor.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/master/TestCatalogJanitor.java
@@ -340,8 +340,7 @@ public class TestCatalogJanitor {
 
     // First test that our Comparator works right up in CatalogJanitor.
     // Just fo kicks.
-    SortedMap<HRegionInfo, Result> regions =
-      new TreeMap<HRegionInfo, Result>(new CatalogJanitor.SplitParentFirstComparator());
+    SortedMap<HRegionInfo, Result> regions = new TreeMap<>(new CatalogJanitor.SplitParentFirstComparator());
     // Now make sure that this regions map sorts as we expect it to.
     regions.put(parent, createResult(parent, splita, splitb));
     regions.put(splitb, createResult(splitb, splitba, splitbb));
@@ -434,16 +433,14 @@ public class TestCatalogJanitor {
         new byte[0]);
     Thread.sleep(1001);
 
-    final Map<HRegionInfo, Result> splitParents =
-        new TreeMap<HRegionInfo, Result>(new SplitParentFirstComparator());
+    final Map<HRegionInfo, Result> splitParents = new TreeMap<>(new SplitParentFirstComparator());
     splitParents.put(parent, createResult(parent, splita, splitb));
     splita.setOffline(true); //simulate that splita goes offline when it is split
     splitParents.put(splita, createResult(splita, splitaa,splitab));
 
-    final Map<HRegionInfo, Result> mergedRegions = new TreeMap<HRegionInfo, Result>();
+    final Map<HRegionInfo, Result> mergedRegions = new TreeMap<>();
     CatalogJanitor janitor = spy(new CatalogJanitor(services));
-    doReturn(new Triple<Integer, Map<HRegionInfo, Result>, Map<HRegionInfo, Result>>(
-            10, mergedRegions, splitParents)).when(janitor)
+    doReturn(new Triple<>(10, mergedRegions, splitParents)).when(janitor)
         .getMergedRegionsAndSplitParents();
 
     //create ref from splita to parent
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/master/TestClusterStatusPublisher.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/master/TestClusterStatusPublisher.java
index 5d47ede..68cab5a 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/master/TestClusterStatusPublisher.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/master/TestClusterStatusPublisher.java
@@ -49,7 +49,7 @@ public class TestClusterStatusPublisher {
     ClusterStatusPublisher csp = new ClusterStatusPublisher() {
       @Override
       protected List<Pair<ServerName, Long>> getDeadServers(long since) {
-        return new ArrayList<Pair<ServerName, Long>>();
+        return new ArrayList<>();
       }
     };
 
@@ -61,10 +61,10 @@ public class TestClusterStatusPublisher {
     ClusterStatusPublisher csp = new ClusterStatusPublisher() {
       @Override
       protected List<Pair<ServerName, Long>> getDeadServers(long since) {
-        List<Pair<ServerName, Long>> res = new ArrayList<Pair<ServerName, Long>>();
+        List<Pair<ServerName, Long>> res = new ArrayList<>();
         switch ((int) EnvironmentEdgeManager.currentTime()) {
           case 2:
-            res.add(new Pair<ServerName, Long>(ServerName.valueOf("hn", 10, 10), 1L));
+            res.add(new Pair<>(ServerName.valueOf("hn", 10, 10), 1L));
             break;
           case 1000:
             break;
@@ -87,9 +87,9 @@ public class TestClusterStatusPublisher {
     ClusterStatusPublisher csp = new ClusterStatusPublisher() {
       @Override
       protected List<Pair<ServerName, Long>> getDeadServers(long since) {
-        List<Pair<ServerName, Long>> res = new ArrayList<Pair<ServerName, Long>>();
+        List<Pair<ServerName, Long>> res = new ArrayList<>();
         for (int i = 0; i < 25; i++) {
-          res.add(new Pair<ServerName, Long>(ServerName.valueOf("hn" + i, 10, 10), 20L));
+          res.add(new Pair<>(ServerName.valueOf("hn" + i, 10, 10), 20L));
         }
 
         return res;
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/master/TestDistributedLogSplitting.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/master/TestDistributedLogSplitting.java
index d0b8494..4c8728f 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/master/TestDistributedLogSplitting.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/master/TestDistributedLogSplitting.java
@@ -329,7 +329,7 @@ public class TestDistributedLogSplitting {
 
     private final PerClientRandomNonceGenerator delegate = PerClientRandomNonceGenerator.get();
     private boolean isDups = false;
-    private LinkedList<Long> nonces = new LinkedList<Long>();
+    private LinkedList<Long> nonces = new LinkedList<>();
 
     public void startDups() {
       isDups = true;
@@ -370,7 +370,7 @@ public class TestDistributedLogSplitting {
             (ClusterConnection)TEST_UTIL.getConnection(), ng);
 
     try {
-      List<Increment> reqs = new ArrayList<Increment>();
+      List<Increment> reqs = new ArrayList<>();
       for (RegionServerThread rst : cluster.getLiveRegionServerThreads()) {
         HRegionServer hrs = rst.getRegionServer();
         List<HRegionInfo> hris = ProtobufUtil.getOnlineRegions(hrs.getRSRpcServices());
@@ -693,7 +693,7 @@ public class TestDistributedLogSplitting {
     try {
       final SplitLogManager slm = master.getMasterWalManager().getSplitLogManager();
 
-      Set<HRegionInfo> regionSet = new HashSet<HRegionInfo>();
+      Set<HRegionInfo> regionSet = new HashSet<>();
       HRegionInfo region = null;
       HRegionServer hrs = null;
       ServerName firstFailedServer = null;
@@ -942,7 +942,7 @@ public class TestDistributedLogSplitting {
     try {
       final SplitLogManager slm = master.getMasterWalManager().getSplitLogManager();
 
-      Set<HRegionInfo> regionSet = new HashSet<HRegionInfo>();
+      Set<HRegionInfo> regionSet = new HashSet<>();
       HRegionInfo region = null;
       HRegionServer hrs = null;
       HRegionServer dstRS = null;
@@ -1214,10 +1214,10 @@ public class TestDistributedLogSplitting {
     List<HRegionInfo> regions = ProtobufUtil.getOnlineRegions(hrs.getRSRpcServices());
 
     LOG.info("#regions = " + regions.size());
-    Set<HRegionInfo> tmpRegions = new HashSet<HRegionInfo>();
+    Set<HRegionInfo> tmpRegions = new HashSet<>();
     tmpRegions.add(HRegionInfo.FIRST_META_REGIONINFO);
     master.getMasterWalManager().prepareLogReplay(hrs.getServerName(), tmpRegions);
-    Set<HRegionInfo> userRegionSet = new HashSet<HRegionInfo>();
+    Set<HRegionInfo> userRegionSet = new HashSet<>();
     userRegionSet.addAll(regions);
     master.getMasterWalManager().prepareLogReplay(hrs.getServerName(), userRegionSet);
     boolean isMetaRegionInRecovery = false;
@@ -1591,7 +1591,7 @@ public class TestDistributedLogSplitting {
     htd.addFamily(new HColumnDescriptor(family));
     byte[] value = new byte[edit_size];
 
-    List<HRegionInfo> hris = new ArrayList<HRegionInfo>();
+    List<HRegionInfo> hris = new ArrayList<>();
     for (HRegionInfo region : regions) {
       if (!region.getTable().getNameAsString().equalsIgnoreCase(tname)) {
         continue;
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/master/TestMasterNoCluster.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/master/TestMasterNoCluster.java
index 37e714e..fe0e7b1 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/master/TestMasterNoCluster.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/master/TestMasterNoCluster.java
@@ -286,7 +286,7 @@ public class TestMasterNoCluster {
         // Record a newer server in server manager at first
         getServerManager().recordNewServerWithLock(newServer, ServerLoad.EMPTY_SERVERLOAD);
 
-        List<ServerName> onlineServers = new ArrayList<ServerName>();
+        List<ServerName> onlineServers = new ArrayList<>();
         onlineServers.add(deadServer);
         onlineServers.add(newServer);
         // Mock the region server tracker to pull the dead server from zk
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/master/TestMasterOperationsForRegionReplicas.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/master/TestMasterOperationsForRegionReplicas.java
index bb8a995..6c737e9 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/master/TestMasterOperationsForRegionReplicas.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/master/TestMasterOperationsForRegionReplicas.java
@@ -219,7 +219,7 @@ public class TestMasterOperationsForRegionReplicas {
       //just check that the number of default replica regions in the meta table are the same
       //as the number of regions the table was created with, and the count of the
       //replicas is numReplica for each region
-      Map<HRegionInfo, Integer> defaultReplicas = new HashMap<HRegionInfo, Integer>();
+      Map<HRegionInfo, Integer> defaultReplicas = new HashMap<>();
       for (HRegionInfo hri : hris) {
         Integer i;
         HRegionInfo regionReplica0 = RegionReplicaUtil.getRegionInfoForDefaultReplica(hri);
@@ -227,7 +227,7 @@ public class TestMasterOperationsForRegionReplicas {
             (i = defaultReplicas.get(regionReplica0)) == null ? 1 : i + 1);
       }
       assert(defaultReplicas.size() == numRegions);
-      Collection<Integer> counts = new HashSet<Integer>(defaultReplicas.values());
+      Collection<Integer> counts = new HashSet<>(defaultReplicas.values());
       assert(counts.size() == 1 && counts.contains(new Integer(numReplica)));
     } finally {
       ADMIN.disableTable(tableName);
@@ -248,7 +248,7 @@ public class TestMasterOperationsForRegionReplicas {
       desc.addFamily(new HColumnDescriptor("family"));
       ADMIN.createTable(desc, Bytes.toBytes("A"), Bytes.toBytes("Z"), numRegions);
       TEST_UTIL.waitTableEnabled(tableName);
-      Set<byte[]> tableRows = new HashSet<byte[]>();
+      Set<byte[]> tableRows = new HashSet<>();
       List<HRegionInfo> hris = MetaTableAccessor.getTableRegions(ADMIN.getConnection(), tableName);
       for (HRegionInfo hri : hris) {
         tableRows.add(hri.getRegionName());
@@ -317,7 +317,7 @@ public class TestMasterOperationsForRegionReplicas {
         continue;
       }
       List<HRegionInfo> regions = entry.getValue();
-      Set<byte[]> setOfStartKeys = new HashSet<byte[]>();
+      Set<byte[]> setOfStartKeys = new HashSet<>();
       for (HRegionInfo region : regions) {
         byte[] startKey = region.getStartKey();
         if (region.getTable().equals(table)) {
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/master/TestMasterStatusServlet.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/master/TestMasterStatusServlet.java
index af54ffc..b59e6ff 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/master/TestMasterStatusServlet.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/master/TestMasterStatusServlet.java
@@ -90,7 +90,7 @@ public class TestMasterStatusServlet {
     // Fake AssignmentManager and RIT
     AssignmentManager am = Mockito.mock(AssignmentManager.class);
     RegionStates rs = Mockito.mock(RegionStates.class);
-    Set<RegionState> regionsInTransition = new HashSet<RegionState>();
+    Set<RegionState> regionsInTransition = new HashSet<>();
     regionsInTransition.add(new RegionState(FAKE_HRI, RegionState.State.CLOSING, 12345L, FAKE_HOST));
     Mockito.doReturn(rs).when(am).getRegionStates();
     Mockito.doReturn(regionsInTransition).when(rs).getRegionsInTransition();
@@ -145,7 +145,7 @@ public class TestMasterStatusServlet {
     List<ServerName> servers = Lists.newArrayList(
         ServerName.valueOf("rootserver,123,12345"),
         ServerName.valueOf("metaserver,123,12345"));
-    Set<ServerName> deadServers = new HashSet<ServerName>(
+    Set<ServerName> deadServers = new HashSet<>(
         Lists.newArrayList(
             ServerName.valueOf("badserver,123,12345"),
             ServerName.valueOf("uglyserver,123,12345"))
@@ -164,8 +164,7 @@ public class TestMasterStatusServlet {
     RegionStates rs = Mockito.mock(RegionStates.class);
 
     // Add 100 regions as in-transition
-    TreeSet<RegionState> regionsInTransition = new TreeSet<RegionState>(
-      RegionStates.REGION_STATE_COMPARATOR);
+    TreeSet<RegionState> regionsInTransition = new TreeSet<>(RegionStates.REGION_STATE_COMPARATOR);
     for (byte i = 0; i < 100; i++) {
       HRegionInfo hri = new HRegionInfo(FAKE_TABLE.getTableName(),
           new byte[]{i}, new byte[]{(byte) (i+1)});
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/master/TestMasterWalManager.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/master/TestMasterWalManager.java
index 7c7531f..782c400 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/master/TestMasterWalManager.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/master/TestMasterWalManager.java
@@ -90,7 +90,7 @@ public class TestMasterWalManager {
     inRecoveringRegionPath = ZKUtil.joinZNode(inRecoveringRegionPath,
       inRecoveryServerName.getServerName());
     ZKUtil.createWithParents(zkw, inRecoveringRegionPath);
-    Set<ServerName> servers = new HashSet<ServerName>();
+    Set<ServerName> servers = new HashSet<>();
     servers.add(previouselyFaildServerName);
     mwm.removeStaleRecoveringRegionsFromZK(servers);
 
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/master/TestRegionPlacement.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/master/TestRegionPlacement.java
index b2be237..67add2f 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/master/TestRegionPlacement.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/master/TestRegionPlacement.java
@@ -80,8 +80,7 @@ public class TestRegionPlacement {
   private static Position[] positions = Position.values();
   private int lastRegionOnPrimaryRSCount = 0;
   private int REGION_NUM = 10;
-  private Map<HRegionInfo, ServerName[]> favoredNodesAssignmentPlan =
-      new HashMap<HRegionInfo, ServerName[]>();
+  private Map<HRegionInfo, ServerName[]> favoredNodesAssignmentPlan = new HashMap<>();
 
   @BeforeClass
   public static void setupBeforeClass() throws Exception {
@@ -204,14 +203,12 @@ public class TestRegionPlacement {
     } while (ServerName.isSameHostnameAndPort(metaServer, serverToKill) || isNamespaceServer ||
         TEST_UTIL.getHBaseCluster().getRegionServer(killIndex).getNumberOfOnlineRegions() == 0);
     LOG.debug("Stopping RS " + serverToKill);
-    Map<HRegionInfo, Pair<ServerName, ServerName>> regionsToVerify =
-        new HashMap<HRegionInfo, Pair<ServerName, ServerName>>();
+    Map<HRegionInfo, Pair<ServerName, ServerName>> regionsToVerify = new HashMap<>();
     // mark the regions to track
     for (Map.Entry<HRegionInfo, ServerName[]> entry : favoredNodesAssignmentPlan.entrySet()) {
       ServerName s = entry.getValue()[0];
       if (ServerName.isSameHostnameAndPort(s, serverToKill)) {
-        regionsToVerify.put(entry.getKey(), new Pair<ServerName, ServerName>(
-            entry.getValue()[1], entry.getValue()[2]));
+        regionsToVerify.put(entry.getKey(), new Pair<>(entry.getValue()[1], entry.getValue()[2]));
         LOG.debug("Adding " + entry.getKey() + " with sedcondary/tertiary " +
             entry.getValue()[1] + " " + entry.getValue()[2]);
       }
@@ -308,7 +305,7 @@ public class TestRegionPlacement {
       plan.getAssignmentMap().entrySet()) {
 
       // copy the server list from the original plan
-      List<ServerName> shuffledServerList = new ArrayList<ServerName>();
+      List<ServerName> shuffledServerList = new ArrayList<>();
       shuffledServerList.addAll(entry.getValue());
 
       // start to shuffle
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/master/TestRegionPlacement2.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/master/TestRegionPlacement2.java
index 7c6f08b..f10c368 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/master/TestRegionPlacement2.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/master/TestRegionPlacement2.java
@@ -79,12 +79,12 @@ public class TestRegionPlacement2 {
     LoadBalancer balancer = LoadBalancerFactory.getLoadBalancer(TEST_UTIL.getConfiguration());
     balancer.setMasterServices(TEST_UTIL.getMiniHBaseCluster().getMaster());
     balancer.initialize();
-    List<ServerName> servers = new ArrayList<ServerName>();
+    List<ServerName> servers = new ArrayList<>();
     for (int i = 0; i < SLAVES; i++) {
       ServerName server = TEST_UTIL.getMiniHBaseCluster().getRegionServer(i).getServerName();
       servers.add(server);
     }
-    List<HRegionInfo> regions = new ArrayList<HRegionInfo>(1);
+    List<HRegionInfo> regions = new ArrayList<>(1);
     HRegionInfo region = new HRegionInfo(TableName.valueOf(name.getMethodName()));
     regions.add(region);
     Map<ServerName,List<HRegionInfo>> assignmentMap = balancer.roundRobinAssignment(regions,
@@ -140,12 +140,12 @@ public class TestRegionPlacement2 {
     LoadBalancer balancer = LoadBalancerFactory.getLoadBalancer(TEST_UTIL.getConfiguration());
     balancer.setMasterServices(TEST_UTIL.getMiniHBaseCluster().getMaster());
     balancer.initialize();
-    List<ServerName> servers = new ArrayList<ServerName>();
+    List<ServerName> servers = new ArrayList<>();
     for (int i = 0; i < SLAVES; i++) {
       ServerName server = TEST_UTIL.getMiniHBaseCluster().getRegionServer(i).getServerName();
       servers.add(server);
     }
-    List<HRegionInfo> regions = new ArrayList<HRegionInfo>(1);
+    List<HRegionInfo> regions = new ArrayList<>(1);
     HRegionInfo region = new HRegionInfo(TableName.valueOf(name.getMethodName()));
     regions.add(region);
     ServerName serverBefore = balancer.randomAssignment(region, servers);
@@ -183,7 +183,7 @@ public class TestRegionPlacement2 {
 
   private List<ServerName> removeMatchingServers(Collection<ServerName> serversWithoutStartCode,
       List<ServerName> servers) {
-    List<ServerName> serversToRemove = new ArrayList<ServerName>();
+    List<ServerName> serversToRemove = new ArrayList<>();
     for (ServerName s : serversWithoutStartCode) {
       serversToRemove.addAll(removeMatchingServers(s, servers));
     }
@@ -192,7 +192,7 @@ public class TestRegionPlacement2 {
 
   private List<ServerName> removeMatchingServers(ServerName serverWithoutStartCode,
       List<ServerName> servers) {
-    List<ServerName> serversToRemove = new ArrayList<ServerName>();
+    List<ServerName> serversToRemove = new ArrayList<>();
     for (ServerName s : servers) {
       if (ServerName.isSameHostnameAndPort(s, serverWithoutStartCode)) {
         serversToRemove.add(s);
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/master/TestRollingRestart.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/master/TestRollingRestart.java
index ac99b29..80c6f3a 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/master/TestRollingRestart.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/master/TestRollingRestart.java
@@ -259,8 +259,8 @@ public class  TestRollingRestart {
 
   private NavigableSet<String> getDoubleAssignedRegions(
       MiniHBaseCluster cluster) throws IOException {
-    NavigableSet<String> online = new TreeSet<String>();
-    NavigableSet<String> doubled = new TreeSet<String>();
+    NavigableSet<String> online = new TreeSet<>();
+    NavigableSet<String> doubled = new TreeSet<>();
     for (RegionServerThread rst : cluster.getLiveRegionServerThreads()) {
       for (HRegionInfo region : ProtobufUtil.getOnlineRegions(
           rst.getRegionServer().getRSRpcServices())) {
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/master/balancer/BalancerTestBase.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/master/balancer/BalancerTestBase.java
index 0f427ad..f93449c 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/master/balancer/BalancerTestBase.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/master/balancer/BalancerTestBase.java
@@ -259,7 +259,7 @@ public class BalancerTestBase {
     }
 
     public List<String> resolve(List<String> names) {
-      List<String> ret = new ArrayList<String>(names.size());
+      List<String> ret = new ArrayList<>(names.size());
       for (String name : names) {
         ret.add("rack");
       }
@@ -345,14 +345,14 @@ public class BalancerTestBase {
    * Checks whether region replicas are not hosted on the same host.
    */
   public void assertRegionReplicaPlacement(Map<ServerName, List<HRegionInfo>> serverMap, RackManager rackManager) {
-    TreeMap<String, Set<HRegionInfo>> regionsPerHost = new TreeMap<String, Set<HRegionInfo>>();
-    TreeMap<String, Set<HRegionInfo>> regionsPerRack = new TreeMap<String, Set<HRegionInfo>>();
+    TreeMap<String, Set<HRegionInfo>> regionsPerHost = new TreeMap<>();
+    TreeMap<String, Set<HRegionInfo>> regionsPerRack = new TreeMap<>();
 
     for (Entry<ServerName, List<HRegionInfo>> entry : serverMap.entrySet()) {
       String hostname = entry.getKey().getHostname();
       Set<HRegionInfo> infos = regionsPerHost.get(hostname);
       if (infos == null) {
-        infos = new HashSet<HRegionInfo>();
+        infos = new HashSet<>();
         regionsPerHost.put(hostname, infos);
       }
 
@@ -372,7 +372,7 @@ public class BalancerTestBase {
       String rack = rackManager.getRack(entry.getKey());
       Set<HRegionInfo> infos = regionsPerRack.get(rack);
       if (infos == null) {
-        infos = new HashSet<HRegionInfo>();
+        infos = new HashSet<>();
         regionsPerRack.put(rack, infos);
       }
 
@@ -399,7 +399,7 @@ public class BalancerTestBase {
   }
 
   protected List<ServerAndLoad> convertToList(final Map<ServerName, List<HRegionInfo>> servers) {
-    List<ServerAndLoad> list = new ArrayList<ServerAndLoad>(servers.size());
+    List<ServerAndLoad> list = new ArrayList<>(servers.size());
     for (Map.Entry<ServerName, List<HRegionInfo>> e : servers.entrySet()) {
       list.add(new ServerAndLoad(e.getKey(), e.getValue().size()));
     }
@@ -407,7 +407,7 @@ public class BalancerTestBase {
   }
 
   protected String printMock(List<ServerAndLoad> balancedCluster) {
-    SortedSet<ServerAndLoad> sorted = new TreeSet<ServerAndLoad>(balancedCluster);
+    SortedSet<ServerAndLoad> sorted = new TreeSet<>(balancedCluster);
     ServerAndLoad[] arr = sorted.toArray(new ServerAndLoad[sorted.size()]);
     StringBuilder sb = new StringBuilder(sorted.size() * 4 + 4);
     sb.append("{ ");
@@ -434,9 +434,9 @@ public class BalancerTestBase {
   protected List<ServerAndLoad> reconcile(List<ServerAndLoad> list,
                                           List<RegionPlan> plans,
                                           Map<ServerName, List<HRegionInfo>> servers) {
-    List<ServerAndLoad> result = new ArrayList<ServerAndLoad>(list.size());
+    List<ServerAndLoad> result = new ArrayList<>(list.size());
 
-    Map<ServerName, ServerAndLoad> map = new HashMap<ServerName, ServerAndLoad>(list.size());
+    Map<ServerName, ServerAndLoad> map = new HashMap<>(list.size());
     for (ServerAndLoad sl : list) {
       map.put(sl.getServerName(), sl);
     }
@@ -477,7 +477,7 @@ public class BalancerTestBase {
 
   protected TreeMap<ServerName, List<HRegionInfo>> mockClusterServers(int[] mockCluster, int numTables) {
     int numServers = mockCluster.length;
-    TreeMap<ServerName, List<HRegionInfo>> servers = new TreeMap<ServerName, List<HRegionInfo>>();
+    TreeMap<ServerName, List<HRegionInfo>> servers = new TreeMap<>();
     for (int i = 0; i < numServers; i++) {
       int numRegions = mockCluster[i];
       ServerAndLoad sal = randomServer(0);
@@ -489,7 +489,7 @@ public class BalancerTestBase {
 
   protected TreeMap<ServerName, List<HRegionInfo>> mockUniformClusterServers(int[] mockCluster) {
     int numServers = mockCluster.length;
-    TreeMap<ServerName, List<HRegionInfo>> servers = new TreeMap<ServerName, List<HRegionInfo>>();
+    TreeMap<ServerName, List<HRegionInfo>> servers = new TreeMap<>();
     for (int i = 0; i < numServers; i++) {
       int numRegions = mockCluster[i];
       ServerAndLoad sal = randomServer(0);
@@ -507,12 +507,12 @@ public class BalancerTestBase {
       for (HRegionInfo hri : regions){
         TreeMap<ServerName, List<HRegionInfo>> servers = result.get(hri.getTable());
         if (servers == null) {
-          servers = new TreeMap<ServerName, List<HRegionInfo>>();
+          servers = new TreeMap<>();
           result.put(hri.getTable(), servers);
         }
         List<HRegionInfo> hrilist = servers.get(sal);
         if (hrilist == null) {
-          hrilist = new ArrayList<HRegionInfo>();
+          hrilist = new ArrayList<>();
           servers.put(sal, hrilist);
         }
         hrilist.add(hri);
@@ -520,20 +520,20 @@ public class BalancerTestBase {
     }
     for(Map.Entry<TableName, TreeMap<ServerName, List<HRegionInfo>>> entry : result.entrySet()){
       for(ServerName srn : clusterServers.keySet()){
-        if (!entry.getValue().containsKey(srn)) entry.getValue().put(srn, new ArrayList<HRegionInfo>());
+        if (!entry.getValue().containsKey(srn)) entry.getValue().put(srn, new ArrayList<>());
       }
     }
     return result;
   }
 
-  private Queue<HRegionInfo> regionQueue = new LinkedList<HRegionInfo>();
+  private Queue<HRegionInfo> regionQueue = new LinkedList<>();
 
   protected List<HRegionInfo> randomRegions(int numRegions) {
     return randomRegions(numRegions, -1);
   }
 
   protected List<HRegionInfo> randomRegions(int numRegions, int numTables) {
-    List<HRegionInfo> regions = new ArrayList<HRegionInfo>(numRegions);
+    List<HRegionInfo> regions = new ArrayList<>(numRegions);
     byte[] start = new byte[16];
     byte[] end = new byte[16];
     rand.nextBytes(start);
@@ -554,7 +554,7 @@ public class BalancerTestBase {
   }
 
   protected List<HRegionInfo> uniformRegions(int numRegions) {
-    List<HRegionInfo> regions = new ArrayList<HRegionInfo>(numRegions);
+    List<HRegionInfo> regions = new ArrayList<>(numRegions);
     byte[] start = new byte[16];
     byte[] end = new byte[16];
     rand.nextBytes(start);
@@ -574,7 +574,7 @@ public class BalancerTestBase {
     regionQueue.addAll(regions);
   }
 
-  private Queue<ServerName> serverQueue = new LinkedList<ServerName>();
+  private Queue<ServerName> serverQueue = new LinkedList<>();
 
   protected ServerAndLoad randomServer(final int numRegionsPerServer) {
     if (!this.serverQueue.isEmpty()) {
@@ -589,7 +589,7 @@ public class BalancerTestBase {
   }
 
   protected List<ServerAndLoad> randomServers(int numServers, int numRegionsPerServer) {
-    List<ServerAndLoad> servers = new ArrayList<ServerAndLoad>(numServers);
+    List<ServerAndLoad> servers = new ArrayList<>(numServers);
     for (int i = 0; i < numServers; i++) {
       servers.add(randomServer(numRegionsPerServer));
     }
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/master/balancer/TestBaseLoadBalancer.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/master/balancer/TestBaseLoadBalancer.java
index 02032fd..751adc5 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/master/balancer/TestBaseLoadBalancer.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/master/balancer/TestBaseLoadBalancer.java
@@ -185,7 +185,7 @@ public class TestBaseLoadBalancer extends BalancerTestBase {
     // Test simple case where all same servers are there
     List<ServerAndLoad> servers = randomServers(10, 10);
     List<HRegionInfo> regions = randomRegions(100);
-    Map<HRegionInfo, ServerName> existing = new TreeMap<HRegionInfo, ServerName>();
+    Map<HRegionInfo, ServerName> existing = new TreeMap<>();
     for (int i = 0; i < regions.size(); i++) {
       ServerName sn = servers.get(i % servers.size()).getServerName();
       // The old server would have had same host and port, but different
@@ -200,7 +200,7 @@ public class TestBaseLoadBalancer extends BalancerTestBase {
     assertRetainedAssignment(existing, listOfServerNames, assignment);
 
     // Include two new servers that were not there before
-    List<ServerAndLoad> servers2 = new ArrayList<ServerAndLoad>(servers);
+    List<ServerAndLoad> servers2 = new ArrayList<>(servers);
     servers2.add(randomServer(10));
     servers2.add(randomServer(10));
     listOfServerNames = getListOfServerNames(servers2);
@@ -208,7 +208,7 @@ public class TestBaseLoadBalancer extends BalancerTestBase {
     assertRetainedAssignment(existing, listOfServerNames, assignment);
 
     // Remove two of the servers that were previously there
-    List<ServerAndLoad> servers3 = new ArrayList<ServerAndLoad>(servers);
+    List<ServerAndLoad> servers3 = new ArrayList<>(servers);
     servers3.remove(0);
     servers3.remove(0);
     listOfServerNames = getListOfServerNames(servers3);
@@ -266,9 +266,9 @@ public class TestBaseLoadBalancer extends BalancerTestBase {
     // replica from one node to a specific other node or rack lowers the
     // availability of the region or not
 
-    List<HRegionInfo> list0 = new ArrayList<HRegionInfo>();
-    List<HRegionInfo> list1 = new ArrayList<HRegionInfo>();
-    List<HRegionInfo> list2 = new ArrayList<HRegionInfo>();
+    List<HRegionInfo> list0 = new ArrayList<>();
+    List<HRegionInfo> list1 = new ArrayList<>();
+    List<HRegionInfo> list2 = new ArrayList<>();
     // create a region (region1)
     HRegionInfo hri1 = new HRegionInfo(
         TableName.valueOf(name.getMethodName()), "key1".getBytes(), "key2".getBytes(),
@@ -282,8 +282,7 @@ public class TestBaseLoadBalancer extends BalancerTestBase {
     list0.add(hri1); //only region1
     list1.add(hri2); //only replica_of_region1
     list2.add(hri3); //only region2
-    Map<ServerName, List<HRegionInfo>> clusterState =
-        new LinkedHashMap<ServerName, List<HRegionInfo>>();
+    Map<ServerName, List<HRegionInfo>> clusterState = new LinkedHashMap<>();
     clusterState.put(servers[0], list0); //servers[0] hosts region1
     clusterState.put(servers[1], list1); //servers[1] hosts replica_of_region1
     clusterState.put(servers[2], list2); //servers[2] hosts region2
@@ -318,7 +317,7 @@ public class TestBaseLoadBalancer extends BalancerTestBase {
     clusterState.put(servers[0], list0); //servers[0], rack1 hosts region1
     clusterState.put(servers[5], list1); //servers[5], rack2 hosts replica_of_region1 and replica_of_region2
     clusterState.put(servers[6], list2); //servers[6], rack2 hosts region2
-    clusterState.put(servers[10], new ArrayList<HRegionInfo>()); //servers[10], rack3 hosts no region
+    clusterState.put(servers[10], new ArrayList<>()); //servers[10], rack3 hosts no region
     // create a cluster with the above clusterState
     cluster = new Cluster(clusterState, null, null, rackManager);
     // check whether a move of region1 from servers[0],rack1 to servers[6],rack2 would
@@ -335,9 +334,9 @@ public class TestBaseLoadBalancer extends BalancerTestBase {
 
   @Test (timeout=180000)
   public void testRegionAvailabilityWithRegionMoves() throws Exception {
-    List<HRegionInfo> list0 = new ArrayList<HRegionInfo>();
-    List<HRegionInfo> list1 = new ArrayList<HRegionInfo>();
-    List<HRegionInfo> list2 = new ArrayList<HRegionInfo>();
+    List<HRegionInfo> list0 = new ArrayList<>();
+    List<HRegionInfo> list1 = new ArrayList<>();
+    List<HRegionInfo> list2 = new ArrayList<>();
     // create a region (region1)
     HRegionInfo hri1 = new HRegionInfo(
         TableName.valueOf(name.getMethodName()), "key1".getBytes(), "key2".getBytes(),
@@ -351,8 +350,7 @@ public class TestBaseLoadBalancer extends BalancerTestBase {
     list0.add(hri1); //only region1
     list1.add(hri2); //only replica_of_region1
     list2.add(hri3); //only region2
-    Map<ServerName, List<HRegionInfo>> clusterState =
-        new LinkedHashMap<ServerName, List<HRegionInfo>>();
+    Map<ServerName, List<HRegionInfo>> clusterState = new LinkedHashMap<>();
     clusterState.put(servers[0], list0); //servers[0] hosts region1
     clusterState.put(servers[1], list1); //servers[1] hosts replica_of_region1
     clusterState.put(servers[2], list2); //servers[2] hosts region2
@@ -374,7 +372,7 @@ public class TestBaseLoadBalancer extends BalancerTestBase {
 
     // start over again
     clusterState.clear();
-    List<HRegionInfo> list3 = new ArrayList<HRegionInfo>();
+    List<HRegionInfo> list3 = new ArrayList<>();
     HRegionInfo hri4 = RegionReplicaUtil.getRegionInfoForReplica(hri3, 1);
     list3.add(hri4);
     clusterState.put(servers[0], list0); //servers[0], rack1 hosts region1
@@ -394,7 +392,7 @@ public class TestBaseLoadBalancer extends BalancerTestBase {
   }
 
   private List<ServerName> getListOfServerNames(final List<ServerAndLoad> sals) {
-    List<ServerName> list = new ArrayList<ServerName>();
+    List<ServerName> list = new ArrayList<>();
     for (ServerAndLoad e : sals) {
       list.add(e.getServerName());
     }
@@ -417,8 +415,8 @@ public class TestBaseLoadBalancer extends BalancerTestBase {
   private void assertRetainedAssignment(Map<HRegionInfo, ServerName> existing,
       List<ServerName> servers, Map<ServerName, List<HRegionInfo>> assignment) {
     // Verify condition 1, every region assigned, and to online server
-    Set<ServerName> onlineServerSet = new TreeSet<ServerName>(servers);
-    Set<HRegionInfo> assignedRegions = new TreeSet<HRegionInfo>();
+    Set<ServerName> onlineServerSet = new TreeSet<>(servers);
+    Set<HRegionInfo> assignedRegions = new TreeSet<>();
     for (Map.Entry<ServerName, List<HRegionInfo>> a : assignment.entrySet()) {
       assertTrue("Region assigned to server that was not listed as online",
         onlineServerSet.contains(a.getKey()));
@@ -428,7 +426,7 @@ public class TestBaseLoadBalancer extends BalancerTestBase {
     assertEquals(existing.size(), assignedRegions.size());
 
     // Verify condition 2, if server had existing assignment, must have same
-    Set<String> onlineHostNames = new TreeSet<String>();
+    Set<String> onlineHostNames = new TreeSet<>();
     for (ServerName s : servers) {
       onlineHostNames.add(s.getHostname());
     }
@@ -453,12 +451,12 @@ public class TestBaseLoadBalancer extends BalancerTestBase {
     // sharing same host and port
     List<ServerName> servers = getListOfServerNames(randomServers(10, 10));
     List<HRegionInfo> regions = randomRegions(101);
-    Map<ServerName, List<HRegionInfo>> clusterState = new HashMap<ServerName, List<HRegionInfo>>();
+    Map<ServerName, List<HRegionInfo>> clusterState = new HashMap<>();
 
     assignRegions(regions, servers, clusterState);
 
     // construct another list of servers, but sharing same hosts and ports
-    List<ServerName> oldServers = new ArrayList<ServerName>(servers.size());
+    List<ServerName> oldServers = new ArrayList<>(servers.size());
     for (ServerName sn : servers) {
       // The old server would have had same host and port, but different start code!
       oldServers.add(ServerName.valueOf(sn.getHostname(), sn.getPort(), sn.getStartcode() - 10));
@@ -479,7 +477,7 @@ public class TestBaseLoadBalancer extends BalancerTestBase {
       ServerName sn = servers.get(i % servers.size());
       List<HRegionInfo> regionsOfServer = clusterState.get(sn);
       if (regionsOfServer == null) {
-        regionsOfServer = new ArrayList<HRegionInfo>(10);
+        regionsOfServer = new ArrayList<>(10);
         clusterState.put(sn, regionsOfServer);
       }
 
@@ -492,7 +490,7 @@ public class TestBaseLoadBalancer extends BalancerTestBase {
     // tests whether region locations are handled correctly in Cluster
     List<ServerName> servers = getListOfServerNames(randomServers(10, 10));
     List<HRegionInfo> regions = randomRegions(101);
-    Map<ServerName, List<HRegionInfo>> clusterState = new HashMap<ServerName, List<HRegionInfo>>();
+    Map<ServerName, List<HRegionInfo>> clusterState = new HashMap<>();
 
     assignRegions(regions, servers, clusterState);
 
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/master/balancer/TestDefaultLoadBalancer.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/master/balancer/TestDefaultLoadBalancer.java
index 962daf7..610ecf7 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/master/balancer/TestDefaultLoadBalancer.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/master/balancer/TestDefaultLoadBalancer.java
@@ -129,16 +129,15 @@ public class TestDefaultLoadBalancer extends BalancerTestBase {
    */
   @Test (timeout=60000)
   public void testBalanceClusterOverall() throws Exception {
-    Map<TableName, Map<ServerName, List<HRegionInfo>>> clusterLoad
-            = new TreeMap<TableName, Map<ServerName, List<HRegionInfo>>>();
+    Map<TableName, Map<ServerName, List<HRegionInfo>>> clusterLoad = new TreeMap<>();
     for (int[] mockCluster : clusterStateMocks) {
       Map<ServerName, List<HRegionInfo>> clusterServers = mockClusterServers(mockCluster, 50);
       List<ServerAndLoad> clusterList = convertToList(clusterServers);
       clusterLoad.put(TableName.valueOf(name.getMethodName()), clusterServers);
       HashMap<TableName, TreeMap<ServerName, List<HRegionInfo>>> result = mockClusterServersWithTables(clusterServers);
       loadBalancer.setClusterLoad(clusterLoad);
-      List<RegionPlan> clusterplans = new ArrayList<RegionPlan>();
-      List<Pair<TableName, Integer>> regionAmountList = new ArrayList<Pair<TableName, Integer>>();
+      List<RegionPlan> clusterplans = new ArrayList<>();
+      List<Pair<TableName, Integer>> regionAmountList = new ArrayList<>();
       for(TreeMap<ServerName, List<HRegionInfo>> servers : result.values()){
         List<ServerAndLoad> list = convertToList(servers);
         LOG.info("Mock Cluster : " + printMock(list) + " " + printStats(list));
@@ -168,8 +167,7 @@ public class TestDefaultLoadBalancer extends BalancerTestBase {
    */
   @Test (timeout=60000)
   public void testImpactOfBalanceClusterOverall() throws Exception {
-    Map<TableName, Map<ServerName, List<HRegionInfo>>> clusterLoad
-            = new TreeMap<TableName, Map<ServerName, List<HRegionInfo>>>();
+    Map<TableName, Map<ServerName, List<HRegionInfo>>> clusterLoad = new TreeMap<>();
     Map<ServerName, List<HRegionInfo>> clusterServers = mockUniformClusterServers(mockUniformCluster);
     List<ServerAndLoad> clusterList = convertToList(clusterServers);
     clusterLoad.put(TableName.valueOf(name.getMethodName()), clusterServers);
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/master/balancer/TestRegionLocationFinder.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/master/balancer/TestRegionLocationFinder.java
index f18d722..365059c 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/master/balancer/TestRegionLocationFinder.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/master/balancer/TestRegionLocationFinder.java
@@ -101,7 +101,7 @@ public class TestRegionLocationFinder {
 
   @Test
   public void testMapHostNameToServerName() throws Exception {
-    List<String> topHosts = new ArrayList<String>();
+    List<String> topHosts = new ArrayList<>();
     for (int i = 0; i < ServerNum; i++) {
       HRegionServer server = cluster.getRegionServer(i);
       String serverHost = server.getServerName().getHostname();
@@ -151,7 +151,7 @@ public class TestRegionLocationFinder {
       if (regions.size() <= 0) {
         continue;
       }
-      List<HRegionInfo> regionInfos = new ArrayList<HRegionInfo>(regions.size());
+      List<HRegionInfo> regionInfos = new ArrayList<>(regions.size());
       for (Region region : regions) {
         regionInfos.add(region.getRegionInfo());
       }
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/master/balancer/TestStochasticLoadBalancer.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/master/balancer/TestStochasticLoadBalancer.java
index 368f4fa..fee98c9 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/master/balancer/TestStochasticLoadBalancer.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/master/balancer/TestStochasticLoadBalancer.java
@@ -73,8 +73,7 @@ public class TestStochasticLoadBalancer extends BalancerTestBase {
       RegionLoad rl = mock(RegionLoad.class);
       when(rl.getStorefileSizeMB()).thenReturn(i);
 
-      Map<byte[], RegionLoad> regionLoadMap =
-          new TreeMap<byte[], RegionLoad>(Bytes.BYTES_COMPARATOR);
+      Map<byte[], RegionLoad> regionLoadMap = new TreeMap<>(Bytes.BYTES_COMPARATOR);
       regionLoadMap.put(Bytes.toBytes(REGION_KEY), rl);
       when(sl.getRegionsLoad()).thenReturn(regionLoadMap);
 
@@ -341,7 +340,7 @@ public class TestStochasticLoadBalancer extends BalancerTestBase {
 
     ServerName deadSn = ServerName.valueOf(sn.getHostname(), sn.getPort(), sn.getStartcode() - 100);
 
-    serverMap.put(deadSn, new ArrayList<HRegionInfo>(0));
+    serverMap.put(deadSn, new ArrayList<>(0));
 
     plans = loadBalancer.balanceCluster(serverMap);
     assertNull(plans);
@@ -443,7 +442,7 @@ public class TestStochasticLoadBalancer extends BalancerTestBase {
     List<HRegionInfo> regions = randomRegions(1);
     ServerName s1 = ServerName.valueOf("host1", 1000, 11111);
     ServerName s2 = ServerName.valueOf("host11", 1000, 11111);
-    Map<ServerName, List<HRegionInfo>> map = new HashMap<ServerName, List<HRegionInfo>>();
+    Map<ServerName, List<HRegionInfo>> map = new HashMap<>();
     map.put(s1, regions);
     regions.add(RegionReplicaUtil.getRegionInfoForReplica(regions.get(0), 1));
     // until the step above s1 holds two replicas of a region
@@ -454,7 +453,7 @@ public class TestStochasticLoadBalancer extends BalancerTestBase {
     // and both the replicas are on the same rack
     map.clear();
     regions = randomRegions(1);
-    List<HRegionInfo> regionsOnS2 = new ArrayList<HRegionInfo>(1);
+    List<HRegionInfo> regionsOnS2 = new ArrayList<>(1);
     regionsOnS2.add(RegionReplicaUtil.getRegionInfoForReplica(regions.get(0), 1));
     map.put(s1, regions);
     map.put(s2, regionsOnS2);
@@ -569,12 +568,12 @@ public class TestStochasticLoadBalancer extends BalancerTestBase {
     int numNodesPerHost = 4;
 
     // create a new map with 4 RS per host.
-    Map<ServerName, List<HRegionInfo>> newServerMap = new TreeMap<ServerName, List<HRegionInfo>>(serverMap);
+    Map<ServerName, List<HRegionInfo>> newServerMap = new TreeMap<>(serverMap);
     for (Map.Entry<ServerName, List<HRegionInfo>> entry : serverMap.entrySet()) {
       for (int i=1; i < numNodesPerHost; i++) {
         ServerName s1 = entry.getKey();
         ServerName s2 = ServerName.valueOf(s1.getHostname(), s1.getPort() + i, 1); // create an RS for the same host
-        newServerMap.put(s2, new ArrayList<HRegionInfo>());
+        newServerMap.put(s2, new ArrayList<>());
       }
     }
 
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/master/cleaner/TestLogsCleaner.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/master/cleaner/TestLogsCleaner.java
index b6b5492..3467f08 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/master/cleaner/TestLogsCleaner.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/master/cleaner/TestLogsCleaner.java
@@ -176,7 +176,7 @@ public class TestLogsCleaner {
     rqc.set(cleaner, rqcMock);
 
     // This should return eventually when cversion stabilizes
-    cleaner.getDeletableFiles(new LinkedList<FileStatus>());
+    cleaner.getDeletableFiles(new LinkedList<>());
   }
 
   /**
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/master/cleaner/TestReplicationHFileCleaner.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/master/cleaner/TestReplicationHFileCleaner.java
index 817cfb4..6df05c0 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/master/cleaner/TestReplicationHFileCleaner.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/master/cleaner/TestReplicationHFileCleaner.java
@@ -140,7 +140,7 @@ public class TestReplicationHFileCleaner {
       cleaner.isFileDeletable(fs.getFileStatus(file)));
 
     List<Pair<Path, Path>> files = new ArrayList<>(1);
-    files.add(new Pair<Path, Path>(null, file));
+    files.add(new Pair<>(null, file));
     // 4. Add the file to hfile-refs queue
     rq.addHFileRefs(peerId, files);
     // 5. Assert file should not be deletable
@@ -159,7 +159,7 @@ public class TestReplicationHFileCleaner {
     fs.createNewFile(deletablefile);
     assertTrue("Test file not created!", fs.exists(deletablefile));
 
-    List<FileStatus> files = new ArrayList<FileStatus>(2);
+    List<FileStatus> files = new ArrayList<>(2);
     FileStatus f = new FileStatus();
     f.setPath(deletablefile);
     files.add(f);
@@ -168,7 +168,7 @@ public class TestReplicationHFileCleaner {
     files.add(f);
 
     List<Pair<Path, Path>> hfiles = new ArrayList<>(1);
-    hfiles.add(new Pair<Path, Path>(null, notDeletablefile));
+    hfiles.add(new Pair<>(null, notDeletablefile));
     // 2. Add one file to hfile-refs queue
     rq.addHFileRefs(peerId, hfiles);
 
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/master/procedure/MasterProcedureTestingUtility.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/master/procedure/MasterProcedureTestingUtility.java
index 7e6691d..ce8b0c6 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/master/procedure/MasterProcedureTestingUtility.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/master/procedure/MasterProcedureTestingUtility.java
@@ -462,7 +462,7 @@ public class MasterProcedureTestingUtility {
 
     public void addProcId(long procId) {
       if (procsToAbort == null) {
-        procsToAbort = new TreeSet<Long>();
+        procsToAbort = new TreeSet<>();
       }
       procsToAbort.add(procId);
     }
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/master/procedure/TestMasterProcedureSchedulerConcurrency.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/master/procedure/TestMasterProcedureSchedulerConcurrency.java
index df431a5..6d88502 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/master/procedure/TestMasterProcedureSchedulerConcurrency.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/master/procedure/TestMasterProcedureSchedulerConcurrency.java
@@ -84,8 +84,8 @@ public class TestMasterProcedureSchedulerConcurrency {
     assertEquals(opsCount.get(), queue.size());
 
     final Thread[] threads = new Thread[NUM_TABLES * 2];
-    final HashSet<TableName> concurrentTables = new HashSet<TableName>();
-    final ArrayList<String> failures = new ArrayList<String>();
+    final HashSet<TableName> concurrentTables = new HashSet<>();
+    final ArrayList<String> failures = new ArrayList<>();
     final AtomicInteger concurrentCount = new AtomicInteger(0);
     for (int i = 0; i < threads.length; ++i) {
       threads[i] = new Thread() {
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/master/snapshot/TestSnapshotFileCache.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/master/snapshot/TestSnapshotFileCache.java
index 51aff6d..76d4585 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/master/snapshot/TestSnapshotFileCache.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/master/snapshot/TestSnapshotFileCache.java
@@ -199,7 +199,7 @@ public class TestSnapshotFileCache {
 
   class SnapshotFiles implements SnapshotFileCache.SnapshotFileInspector {
     public Collection<String> filesUnderSnapshot(final Path snapshotDir) throws IOException {
-      Collection<String> files =  new HashSet<String>();
+      Collection<String> files =  new HashSet<>();
       files.addAll(SnapshotReferenceUtil.getHFileNames(UTIL.getConfiguration(), fs, snapshotDir));
       return files;
     }
@@ -223,7 +223,7 @@ public class TestSnapshotFileCache {
   private void createAndTestSnapshot(final SnapshotFileCache cache,
       final SnapshotMock.SnapshotBuilder builder,
       final boolean tmp, final boolean removeOnExit) throws IOException {
-    List<Path> files = new ArrayList<Path>();
+    List<Path> files = new ArrayList<>();
     for (int i = 0; i < 3; ++i) {
       for (Path filePath: builder.addRegion()) {
         String fileName = filePath.getName();
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/master/snapshot/TestSnapshotHFileCleaner.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/master/snapshot/TestSnapshotHFileCleaner.java
index 30bea8c..fba250d 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/master/snapshot/TestSnapshotHFileCleaner.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/master/snapshot/TestSnapshotHFileCleaner.java
@@ -116,7 +116,7 @@ public class TestSnapshotHFileCleaner {
 
   class SnapshotFiles implements SnapshotFileCache.SnapshotFileInspector {
     public Collection<String> filesUnderSnapshot(final Path snapshotDir) throws IOException {
-      Collection<String> files =  new HashSet<String>();
+      Collection<String> files =  new HashSet<>();
       files.addAll(SnapshotReferenceUtil.getHFileNames(TEST_UTIL.getConfiguration(), fs, snapshotDir));
       return files;
     }
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/mob/compactions/TestMobCompactor.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/mob/compactions/TestMobCompactor.java
index b73b943..83936aa 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/mob/compactions/TestMobCompactor.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/mob/compactions/TestMobCompactor.java
@@ -624,7 +624,7 @@ public class TestMobCompactor {
     // the ref name is the new file
     Path mobFamilyPath =
       MobUtils.getMobFamilyPath(TEST_UTIL.getConfiguration(), tableName, hcd1.getNameAsString());
-    List<Path> paths = new ArrayList<Path>();
+    List<Path> paths = new ArrayList<>();
     if (fs.exists(mobFamilyPath)) {
       FileStatus[] files = fs.listStatus(mobFamilyPath);
       for (FileStatus file : files) {
@@ -1015,7 +1015,7 @@ public class TestMobCompactor {
   private static ExecutorService createThreadPool(Configuration conf) {
     int maxThreads = 10;
     long keepAliveTime = 60;
-    final SynchronousQueue<Runnable> queue = new SynchronousQueue<Runnable>();
+    final SynchronousQueue<Runnable> queue = new SynchronousQueue<>();
     ThreadPoolExecutor pool = new ThreadPoolExecutor(1, maxThreads,
         keepAliveTime, TimeUnit.SECONDS, queue,
         Threads.newDaemonThreadFactory("MobFileCompactionChore"),
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/mob/compactions/TestPartitionedMobCompactor.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/mob/compactions/TestPartitionedMobCompactor.java
index 3aaf0e4..290e6f4 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/mob/compactions/TestPartitionedMobCompactor.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/mob/compactions/TestPartitionedMobCompactor.java
@@ -715,7 +715,7 @@ public class TestPartitionedMobCompactor {
       @Override
       protected List<Path> performCompaction(PartitionedMobCompactionRequest request)
           throws IOException {
-        List<Path> delFilePaths = new ArrayList<Path>();
+        List<Path> delFilePaths = new ArrayList<>();
         for (CompactionDelPartition delPartition: request.getDelPartitions()) {
           for (Path p : delPartition.listDelFiles()) {
             delFilePaths.add(p);
@@ -848,7 +848,7 @@ public class TestPartitionedMobCompactor {
    * @return the cell size
    */
   private int countDelCellsInDelFiles(List<Path> paths) throws IOException {
-    List<StoreFile> sfs = new ArrayList<StoreFile>();
+    List<StoreFile> sfs = new ArrayList<>();
     int size = 0;
     for(Path path : paths) {
       StoreFile sf = new StoreFile(fs, path, conf, cacheConf, BloomType.NONE);
@@ -878,7 +878,7 @@ public class TestPartitionedMobCompactor {
   private static ExecutorService createThreadPool() {
     int maxThreads = 10;
     long keepAliveTime = 60;
-    final SynchronousQueue<Runnable> queue = new SynchronousQueue<Runnable>();
+    final SynchronousQueue<Runnable> queue = new SynchronousQueue<>();
     ThreadPoolExecutor pool = new ThreadPoolExecutor(1, maxThreads, keepAliveTime,
       TimeUnit.SECONDS, queue, Threads.newDaemonThreadFactory("MobFileCompactionChore"),
       new RejectedExecutionHandler() {
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/procedure/SimpleMasterProcedureManager.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/procedure/SimpleMasterProcedureManager.java
index 91279b6..e71318b 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/procedure/SimpleMasterProcedureManager.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/procedure/SimpleMasterProcedureManager.java
@@ -82,7 +82,7 @@ public class SimpleMasterProcedureManager extends MasterProcedureManager {
     ForeignExceptionDispatcher monitor = new ForeignExceptionDispatcher(desc.getInstance());
 
     List<ServerName> serverNames = master.getServerManager().getOnlineServersList();
-    List<String> servers = new ArrayList<String>();
+    List<String> servers = new ArrayList<>();
     for (ServerName sn : serverNames) {
       servers.add(sn.toString());
     }
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/procedure/SimpleRSProcedureManager.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/procedure/SimpleRSProcedureManager.java
index 7620bbb..58efa87 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/procedure/SimpleRSProcedureManager.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/procedure/SimpleRSProcedureManager.java
@@ -119,15 +119,15 @@ public class SimpleRSProcedureManager extends RegionServerProcedureManager {
     private final ExecutorCompletionService<Void> taskPool;
     private final ThreadPoolExecutor executor;
     private volatile boolean aborted;
-    private final List<Future<Void>> futures = new ArrayList<Future<Void>>();
+    private final List<Future<Void>> futures = new ArrayList<>();
     private final String name;
 
     public SimpleSubprocedurePool(String name, Configuration conf) {
       this.name = name;
       executor = new ThreadPoolExecutor(1, 1, 500, TimeUnit.SECONDS,
-          new LinkedBlockingQueue<Runnable>(),
+          new LinkedBlockingQueue<>(),
           new DaemonThreadFactory("rs(" + name + ")-procedure-pool"));
-      taskPool = new ExecutorCompletionService<Void>(executor);
+      taskPool = new ExecutorCompletionService<>(executor);
     }
 
     /**
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/procedure/TestProcedure.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/procedure/TestProcedure.java
index c424b6d..fa934d9 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/procedure/TestProcedure.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/procedure/TestProcedure.java
@@ -86,7 +86,7 @@ public class TestProcedure {
   @Test(timeout = 60000)
   public void testSingleMember() throws Exception {
     // The member
-    List<String> members =  new ArrayList<String>();
+    List<String> members =  new ArrayList<>();
     members.add("member");
     LatchedProcedure proc = new LatchedProcedure(coord, new ForeignExceptionDispatcher(), 100,
         Integer.MAX_VALUE, "op", null, members);
@@ -130,7 +130,7 @@ public class TestProcedure {
   @Test(timeout = 60000)
   public void testMultipleMember() throws Exception {
     // 2 members
-    List<String> members =  new ArrayList<String>();
+    List<String> members =  new ArrayList<>();
     members.add("member1");
     members.add("member2");
 
@@ -181,7 +181,7 @@ public class TestProcedure {
 
   @Test(timeout = 60000)
   public void testErrorPropagation() throws Exception {
-    List<String> members =  new ArrayList<String>();
+    List<String> members =  new ArrayList<>();
     members.add("member");
     Procedure proc = new Procedure(coord, new ForeignExceptionDispatcher(), 100,
         Integer.MAX_VALUE, "op", null, members);
@@ -206,7 +206,7 @@ public class TestProcedure {
 
   @Test(timeout = 60000)
   public void testBarrieredErrorPropagation() throws Exception {
-    List<String> members =  new ArrayList<String>();
+    List<String> members =  new ArrayList<>();
     members.add("member");
     LatchedProcedure proc = new LatchedProcedure(coord, new ForeignExceptionDispatcher(), 100,
         Integer.MAX_VALUE, "op", null, members);
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/procedure/TestProcedureManager.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/procedure/TestProcedureManager.java
index b52a8d6..2f0b5b9 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/procedure/TestProcedureManager.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/procedure/TestProcedureManager.java
@@ -64,7 +64,7 @@ public class TestProcedureManager {
     Admin admin = util.getAdmin();
 
     byte[] result = admin.execProcedureWithRet(SimpleMasterProcedureManager.SIMPLE_SIGNATURE,
-        "mytest", new HashMap<String, String>());
+        "mytest", new HashMap<>());
     assertArrayEquals("Incorrect return data from execProcedure",
       SimpleMasterProcedureManager.SIMPLE_DATA.getBytes(), result);
   }
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/procedure/TestZKProcedure.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/procedure/TestZKProcedure.java
index 211e9e6..9a77ce5 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/procedure/TestZKProcedure.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/procedure/TestZKProcedure.java
@@ -141,20 +141,19 @@ public class TestZKProcedure {
     // build and start members
     // NOTE: There is a single subprocedure builder for all members here.
     SubprocedureFactory subprocFactory = Mockito.mock(SubprocedureFactory.class);
-    List<Pair<ProcedureMember, ZKProcedureMemberRpcs>> procMembers = new ArrayList<Pair<ProcedureMember, ZKProcedureMemberRpcs>>(
-        members.length);
+    List<Pair<ProcedureMember, ZKProcedureMemberRpcs>> procMembers = new ArrayList<>(members.length);
     // start each member
     for (String member : members) {
       ZooKeeperWatcher watcher = newZooKeeperWatcher();
       ZKProcedureMemberRpcs comms = new ZKProcedureMemberRpcs(watcher, opDescription);
       ThreadPoolExecutor pool2 = ProcedureMember.defaultPool(member, 1, KEEP_ALIVE);
       ProcedureMember procMember = new ProcedureMember(comms, pool2, subprocFactory);
-      procMembers.add(new Pair<ProcedureMember, ZKProcedureMemberRpcs>(procMember, comms));
+      procMembers.add(new Pair<>(procMember, comms));
       comms.start(member, procMember);
     }
 
     // setup mock member subprocedures
-    final List<Subprocedure> subprocs = new ArrayList<Subprocedure>();
+    final List<Subprocedure> subprocs = new ArrayList<>();
     for (int i = 0; i < procMembers.size(); i++) {
       ForeignExceptionDispatcher cohortMonitor = new ForeignExceptionDispatcher();
       Subprocedure commit = Mockito
@@ -216,19 +215,18 @@ public class TestZKProcedure {
 
     // start a member for each node
     SubprocedureFactory subprocFactory = Mockito.mock(SubprocedureFactory.class);
-    List<Pair<ProcedureMember, ZKProcedureMemberRpcs>> members = new ArrayList<Pair<ProcedureMember, ZKProcedureMemberRpcs>>(
-        expected.size());
+    List<Pair<ProcedureMember, ZKProcedureMemberRpcs>> members = new ArrayList<>(expected.size());
     for (String member : expected) {
       ZooKeeperWatcher watcher = newZooKeeperWatcher();
       ZKProcedureMemberRpcs controller = new ZKProcedureMemberRpcs(watcher, opDescription);
       ThreadPoolExecutor pool2 = ProcedureMember.defaultPool(member, 1, KEEP_ALIVE);
       ProcedureMember mem = new ProcedureMember(controller, pool2, subprocFactory);
-      members.add(new Pair<ProcedureMember, ZKProcedureMemberRpcs>(mem, controller));
+      members.add(new Pair<>(mem, controller));
       controller.start(member, mem);
     }
 
     // setup mock subprocedures
-    final List<Subprocedure> cohortTasks = new ArrayList<Subprocedure>();
+    final List<Subprocedure> cohortTasks = new ArrayList<>();
     final int[] elem = new int[1];
     for (int i = 0; i < members.size(); i++) {
       ForeignExceptionDispatcher cohortMonitor = new ForeignExceptionDispatcher();
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/procedure/TestZKProcedureControllers.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/procedure/TestZKProcedureControllers.java
index 5b058b3..d864db2 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/procedure/TestZKProcedureControllers.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/procedure/TestZKProcedureControllers.java
@@ -180,7 +180,7 @@ public class TestZKProcedureControllers {
 
     CountDownLatch prepared = new CountDownLatch(expected.size());
     CountDownLatch committed = new CountDownLatch(expected.size());
-    ArrayList<byte[]> dataFromMembers = new ArrayList<byte[]>();
+    ArrayList<byte[]> dataFromMembers = new ArrayList<>();
 
     // mock out coordinator so we can keep track of zk progress
     ProcedureCoordinator coordinator = setupMockCoordinator(operationName,
@@ -256,7 +256,7 @@ public class TestZKProcedureControllers {
 
     final CountDownLatch prepared = new CountDownLatch(expected.size());
     final CountDownLatch committed = new CountDownLatch(expected.size());
-    ArrayList<byte[]> dataFromMembers = new ArrayList<byte[]>();
+    ArrayList<byte[]> dataFromMembers = new ArrayList<>();
 
     // mock out coordinator so we can keep track of zk progress
     ProcedureCoordinator coordinator = setupMockCoordinator(operationName,
@@ -403,14 +403,13 @@ public class TestZKProcedureControllers {
 
       // make a cohort controller for each expected node
 
-      List<ZKProcedureMemberRpcs> cohortControllers = new ArrayList<ZKProcedureMemberRpcs>();
+      List<ZKProcedureMemberRpcs> cohortControllers = new ArrayList<>();
       for (String nodeName : expected) {
         ZKProcedureMemberRpcs cc = new ZKProcedureMemberRpcs(watcher, operationName);
         cc.start(nodeName, member);
         cohortControllers.add(cc);
       }
-      return new Pair<ZKProcedureCoordinatorRpcs, List<ZKProcedureMemberRpcs>>(
-          controller, cohortControllers);
+      return new Pair<>(controller, cohortControllers);
     }
   };
 
@@ -427,7 +426,7 @@ public class TestZKProcedureControllers {
         ProcedureMember member, List<String> expected) throws Exception {
 
       // make a cohort controller for each expected node
-      List<ZKProcedureMemberRpcs> cohortControllers = new ArrayList<ZKProcedureMemberRpcs>();
+      List<ZKProcedureMemberRpcs> cohortControllers = new ArrayList<>();
       for (String nodeName : expected) {
         ZKProcedureMemberRpcs cc = new ZKProcedureMemberRpcs(watcher, operationName);
         cc.start(nodeName, member);
@@ -439,8 +438,7 @@ public class TestZKProcedureControllers {
           watcher, operationName, CONTROLLER_NODE_NAME);
       controller.start(coordinator);
 
-      return new Pair<ZKProcedureCoordinatorRpcs, List<ZKProcedureMemberRpcs>>(
-          controller, cohortControllers);
+      return new Pair<>(controller, cohortControllers);
     }
   };
 }
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/protobuf/TestReplicationProtobuf.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/protobuf/TestReplicationProtobuf.java
index 057a35d..1e3a0c2 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/protobuf/TestReplicationProtobuf.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/protobuf/TestReplicationProtobuf.java
@@ -41,19 +41,19 @@ public class TestReplicationProtobuf {
    */
   @Test
   public void testGetCellScanner() throws IOException {
-    List<Cell> a = new ArrayList<Cell>();
+    List<Cell> a = new ArrayList<>();
     KeyValue akv = new KeyValue(Bytes.toBytes("a"), -1L);
     a.add(akv);
     // Add a few just to make it less regular.
     a.add(new KeyValue(Bytes.toBytes("aa"), -1L));
     a.add(new KeyValue(Bytes.toBytes("aaa"), -1L));
-    List<Cell> b = new ArrayList<Cell>();
+    List<Cell> b = new ArrayList<>();
     KeyValue bkv = new KeyValue(Bytes.toBytes("b"), -1L);
     a.add(bkv);
-    List<Cell> c = new ArrayList<Cell>();
+    List<Cell> c = new ArrayList<>();
     KeyValue ckv = new KeyValue(Bytes.toBytes("c"), -1L);
     c.add(ckv);
-    List<List<? extends Cell>> all = new ArrayList<List<? extends Cell>>();
+    List<List<? extends Cell>> all = new ArrayList<>();
     all.add(a);
     all.add(b);
     all.add(c);
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/AbstractTestDateTieredCompactionPolicy.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/AbstractTestDateTieredCompactionPolicy.java
index 4dce696..7229c40 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/AbstractTestDateTieredCompactionPolicy.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/AbstractTestDateTieredCompactionPolicy.java
@@ -39,7 +39,7 @@ public class AbstractTestDateTieredCompactionPolicy extends TestCompactionPolicy
     EnvironmentEdgeManager.injectEdge(timeMachine);
     // Has to be > 0 and < now.
     timeMachine.setValue(1);
-    ArrayList<Long> ageInDisk = new ArrayList<Long>();
+    ArrayList<Long> ageInDisk = new ArrayList<>();
     for (int i = 0; i < sizes.length; i++) {
       ageInDisk.add(0L);
     }
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/DataBlockEncodingTool.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/DataBlockEncodingTool.java
index bbcdce4..dd20259 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/DataBlockEncodingTool.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/DataBlockEncodingTool.java
@@ -117,7 +117,7 @@ public class DataBlockEncodingTool {
   private static int benchmarkNTimes = DEFAULT_BENCHMARK_N_TIMES;
   private static int benchmarkNOmit = DEFAULT_BENCHMARK_N_OMIT;
 
-  private List<EncodedDataBlock> codecs = new ArrayList<EncodedDataBlock>();
+  private List<EncodedDataBlock> codecs = new ArrayList<>();
   private long totalPrefixLength = 0;
   private long totalKeyLength = 0;
   private long totalValueLength = 0;
@@ -236,8 +236,7 @@ public class DataBlockEncodingTool {
     KeyValue currentKv;
 
     scanner.seek(KeyValue.LOWESTKEY);
-    List<Iterator<Cell>> codecIterators =
-        new ArrayList<Iterator<Cell>>();
+    List<Iterator<Cell>> codecIterators = new ArrayList<>();
     for(EncodedDataBlock codec : codecs) {
       codecIterators.add(codec.getIterator(HFileBlock.headerSize(useHBaseChecksum)));
     }
@@ -326,7 +325,7 @@ public class DataBlockEncodingTool {
     int totalSize = 0;
 
     // decompression time
-    List<Long> durations = new ArrayList<Long>();
+    List<Long> durations = new ArrayList<>();
     for (int itTime = 0; itTime < benchmarkNTimes; ++itTime) {
       totalSize = 0;
 
@@ -352,7 +351,7 @@ public class DataBlockEncodingTool {
       prevTotalSize = totalSize;
     }
 
-    List<Long> encodingDurations = new ArrayList<Long>();
+    List<Long> encodingDurations = new ArrayList<>();
     for (int itTime = 0; itTime < benchmarkNTimes; ++itTime) {
       final long startTime = System.nanoTime();
       codec.encodeData();
@@ -390,7 +389,7 @@ public class DataBlockEncodingTool {
     System.out.println(name + ":");
 
     // compress it
-    List<Long> compressDurations = new ArrayList<Long>();
+    List<Long> compressDurations = new ArrayList<>();
     ByteArrayOutputStream compressedStream = new ByteArrayOutputStream();
     CompressionOutputStream compressingStream =
         algorithm.createPlainCompressionStream(compressedStream, compressor);
@@ -421,7 +420,7 @@ public class DataBlockEncodingTool {
     byte[] compBuffer = compressedStream.toByteArray();
 
     // uncompress it several times and measure performance
-    List<Long> durations = new ArrayList<Long>();
+    List<Long> durations = new ArrayList<>();
     for (int itTime = 0; itTime < benchmarkNTimes; ++itTime) {
       final long startTime = System.nanoTime();
       byte[] newBuf = new byte[length + 1];
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/EncodedSeekPerformanceTest.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/EncodedSeekPerformanceTest.java
index 9638e69..eb77c28 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/EncodedSeekPerformanceTest.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/EncodedSeekPerformanceTest.java
@@ -56,7 +56,7 @@ public class EncodedSeekPerformanceTest {
   }
 
   private List<Cell> prepareListOfTestSeeks(Path path) throws IOException {
-    List<Cell> allKeyValues = new ArrayList<Cell>();
+    List<Cell> allKeyValues = new ArrayList<>();
 
     // read all of the key values
     StoreFile storeFile = new StoreFile(testingUtility.getTestFileSystem(),
@@ -74,7 +74,7 @@ public class EncodedSeekPerformanceTest {
     storeFile.closeReader(cacheConf.shouldEvictOnClose());
 
     // pick seeks by random
-    List<Cell> seeks = new ArrayList<Cell>();
+    List<Cell> seeks = new ArrayList<>();
     for (int i = 0; i < numberOfSeeks; ++i) {
       Cell keyValue = allKeyValues.get(
           randomizer.nextInt(allKeyValues.size()));
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/KeyValueScanFixture.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/KeyValueScanFixture.java
index a4e7f9b..59aded8 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/KeyValueScanFixture.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/KeyValueScanFixture.java
@@ -39,7 +39,7 @@ public class KeyValueScanFixture extends CollectionBackedScanner {
   }
 
   public static List<KeyValueScanner> scanFixture(KeyValue[] ... kvArrays) {
-    ArrayList<KeyValueScanner> scanners = new ArrayList<KeyValueScanner>();
+    ArrayList<KeyValueScanner> scanners = new ArrayList<>();
     for (KeyValue [] kvs : kvArrays) {
       scanners.add(new KeyValueScanFixture(CellComparator.COMPARATOR, kvs));
     }
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/MockStoreFile.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/MockStoreFile.java
index 5b4b0c1..1169434 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/MockStoreFile.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/MockStoreFile.java
@@ -36,7 +36,7 @@ public class MockStoreFile extends StoreFile {
   boolean isRef = false;
   long ageInDisk;
   long sequenceid;
-  private Map<byte[], byte[]> metadata = new TreeMap<byte[], byte[]>(Bytes.BYTES_COMPARATOR);
+  private Map<byte[], byte[]> metadata = new TreeMap<>(Bytes.BYTES_COMPARATOR);
   byte[] splitPoint = null;
   TimeRangeTracker timeRangeTracker;
   long entryCount;
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/OOMERegionServer.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/OOMERegionServer.java
index aa2bc1a..036c11c 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/OOMERegionServer.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/OOMERegionServer.java
@@ -38,7 +38,7 @@ import org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos.MutateRequ
  * <code>${HBASE_HOME}/bin/hbase ./bin/hbase org.apache.hadoop.hbase.OOMERegionServer start</code>.
  */
 public class OOMERegionServer extends HRegionServer {
-  private List<Put> retainer = new ArrayList<Put>();
+  private List<Put> retainer = new ArrayList<>();
 
   public OOMERegionServer(HBaseConfiguration conf, CoordinatedStateManager cp)
       throws IOException, InterruptedException {
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/RegionAsTable.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/RegionAsTable.java
index d2e78b7..cfae7cb 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/RegionAsTable.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/RegionAsTable.java
@@ -144,13 +144,13 @@ public class RegionAsTable implements Table {
 
     @Override
     public Result next() throws IOException {
-      List<Cell> cells = new ArrayList<Cell>();
+      List<Cell> cells = new ArrayList<>();
       return regionScanner.next(cells)? Result.create(cells): null;
     }
 
     @Override
     public Result[] next(int nbRows) throws IOException {
-      List<Result> results = new ArrayList<Result>(nbRows);
+      List<Result> results = new ArrayList<>(nbRows);
       for (int i = 0; i < nbRows; i++) {
         Result result = next();
         if (result == null) break;
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestAtomicOperation.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestAtomicOperation.java
index d00eef1..ef3ce06 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestAtomicOperation.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestAtomicOperation.java
@@ -493,7 +493,7 @@ public class TestAtomicOperation {
                 }
               }
               long ts = timeStamps.incrementAndGet();
-              List<Mutation> mrm = new ArrayList<Mutation>();
+              List<Mutation> mrm = new ArrayList<>();
               if (op) {
                 Put p = new Put(row2, ts);
                 p.addColumn(fam1, qual1, value1);
@@ -518,7 +518,7 @@ public class TestAtomicOperation {
               // check: should always see exactly one column
               Scan s = new Scan(row);
               RegionScanner rs = region.getScanner(s);
-              List<Cell> r = new ArrayList<Cell>();
+              List<Cell> r = new ArrayList<>();
               while (rs.next(r))
                 ;
               rs.close();
@@ -610,7 +610,7 @@ public class TestAtomicOperation {
     ctx.stop();
     Scan s = new Scan();
     RegionScanner scanner = region.getScanner(s);
-    List<Cell> results = new ArrayList<Cell>();
+    List<Cell> results = new ArrayList<>();
     ScannerContext scannerContext = ScannerContext.newBuilder().setBatchLimit(2).build();
     scanner.next(results, scannerContext);
     for (Cell keyValue : results) {
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestBlocksRead.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestBlocksRead.java
index edd7847..59c256a 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestBlocksRead.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestBlocksRead.java
@@ -389,7 +389,7 @@ public class TestBlocksRead  {
       Scan scan = new Scan();
       scan.setCacheBlocks(false);
       RegionScanner rs = region.getScanner(scan);
-      List<Cell> result = new ArrayList<Cell>(2);
+      List<Cell> result = new ArrayList<>(2);
       rs.next(result);
       assertEquals(2 * BLOOM_TYPE.length, result.size());
       rs.close();
@@ -402,7 +402,7 @@ public class TestBlocksRead  {
       blocksStart = blocksEnd;
       scan.setCacheBlocks(true);
       rs = region.getScanner(scan);
-      result = new ArrayList<Cell>(2);
+      result = new ArrayList<>(2);
       rs.next(result);
       assertEquals(2 * BLOOM_TYPE.length, result.size());
       rs.close();
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestBlocksScanned.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestBlocksScanned.java
index b2ba97c..497fd03 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestBlocksScanned.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestBlocksScanned.java
@@ -101,7 +101,7 @@ public class TestBlocksScanned extends HBaseTestCase {
     scan.setMaxVersions(1);
 
     InternalScanner s = r.getScanner(scan);
-    List<Cell> results = new ArrayList<Cell>();
+    List<Cell> results = new ArrayList<>();
     while (s.next(results))
       ;
     s.close();
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestBulkLoad.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestBulkLoad.java
index 4c025c4..418aadf 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestBulkLoad.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestBulkLoad.java
@@ -106,7 +106,7 @@ public class TestBulkLoad {
     byte[] familyName = familyPaths.get(0).getFirst();
     String storeFileName = familyPaths.get(0).getSecond();
     storeFileName = (new Path(storeFileName)).getName();
-    List<String> storeFileNames = new ArrayList<String>();
+    List<String> storeFileNames = new ArrayList<>();
     storeFileNames.add(storeFileName);
     when(log.append(any(HRegionInfo.class), any(WALKey.class),
             argThat(bulkLogWalEdit(WALEdit.BULK_LOAD, tableName.toBytes(),
@@ -129,8 +129,7 @@ public class TestBulkLoad {
 
   @Test
   public void bulkHLogShouldThrowNoErrorAndWriteMarkerWithBlankInput() throws IOException {
-    testRegionWithFamilies(family1).bulkLoadHFiles(new ArrayList<Pair<byte[], String>>(),
-      false, null);
+    testRegionWithFamilies(family1).bulkLoadHFiles(new ArrayList<>(),false, null);
   }
 
   @Test
@@ -219,7 +218,7 @@ public class TestBulkLoad {
   }
 
   private Pair<byte[], String> withMissingHFileForFamily(byte[] family) {
-    return new Pair<byte[], String>(family, getNotExistFilePath());
+    return new Pair<>(family, getNotExistFilePath());
   }
 
   private String getNotExistFilePath() {
@@ -230,7 +229,7 @@ public class TestBulkLoad {
   private Pair<byte[], String> withInvalidColumnFamilyButProperHFileLocation(byte[] family)
       throws IOException {
     createHFileForFamilies(family);
-    return new Pair<byte[], String>(new byte[]{0x00, 0x01, 0x02}, getNotExistFilePath());
+    return new Pair<>(new byte[]{0x00, 0x01, 0x02}, getNotExistFilePath());
   }
 
 
@@ -258,13 +257,13 @@ public class TestBulkLoad {
   }
 
   private List<Pair<byte[], String>> getBlankFamilyPaths(){
-    return new ArrayList<Pair<byte[], String>>();
+    return new ArrayList<>();
   }
 
   private List<Pair<byte[], String>> withFamilyPathsFor(byte[]... families) throws IOException {
     List<Pair<byte[], String>> familyPaths = getBlankFamilyPaths();
     for (byte[] family : families) {
-      familyPaths.add(new Pair<byte[], String>(family, createHFileForFamilies(family)));
+      familyPaths.add(new Pair<>(family, createHFileForFamilies(family)));
     }
     return familyPaths;
   }
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestCacheOnWriteInSchema.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestCacheOnWriteInSchema.java
index 4a73eda..9fed202 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestCacheOnWriteInSchema.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestCacheOnWriteInSchema.java
@@ -137,7 +137,7 @@ public class TestCacheOnWriteInSchema {
 
   @Parameters
   public static Collection<Object[]> getParameters() {
-    List<Object[]> cowTypes = new ArrayList<Object[]>();
+    List<Object[]> cowTypes = new ArrayList<>();
     for (CacheOnWriteType cowType : CacheOnWriteType.values()) {
       cowTypes.add(new Object[] { cowType });
     }
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestColumnSeeking.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestColumnSeeking.java
index c59f64b..5cfa17d 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestColumnSeeking.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestColumnSeeking.java
@@ -90,14 +90,14 @@ public class TestColumnSeeking {
       double majorPercentage = 0.2;
       double putPercentage = 0.2;
 
-      HashMap<String, KeyValue> allKVMap = new HashMap<String, KeyValue>();
+      HashMap<String, KeyValue> allKVMap = new HashMap<>();
 
       HashMap<String, KeyValue>[] kvMaps = new HashMap[numberOfTests];
       ArrayList<String>[] columnLists = new ArrayList[numberOfTests];
 
       for (int i = 0; i < numberOfTests; i++) {
-        kvMaps[i] = new HashMap<String, KeyValue>();
-        columnLists[i] = new ArrayList<String>();
+        kvMaps[i] = new HashMap<>();
+        columnLists[i] = new ArrayList<>();
         for (String column : allColumns) {
           if (Math.random() < selectPercent) {
             columnLists[i].add(column);
@@ -162,7 +162,7 @@ public class TestColumnSeeking {
 
         }
         InternalScanner scanner = region.getScanner(scan);
-        List<Cell> results = new ArrayList<Cell>();
+        List<Cell> results = new ArrayList<>();
         while (scanner.next(results))
           ;
         assertEquals(kvSet.size(), results.size());
@@ -201,15 +201,15 @@ public class TestColumnSeeking {
     double majorPercentage = 0.2;
     double putPercentage = 0.2;
 
-    HashMap<String, KeyValue> allKVMap = new HashMap<String, KeyValue>();
+    HashMap<String, KeyValue> allKVMap = new HashMap<>();
 
     HashMap<String, KeyValue>[] kvMaps = new HashMap[numberOfTests];
     ArrayList<String>[] columnLists = new ArrayList[numberOfTests];
     String valueString = "Value";
 
     for (int i = 0; i < numberOfTests; i++) {
-      kvMaps[i] = new HashMap<String, KeyValue>();
-      columnLists[i] = new ArrayList<String>();
+      kvMaps[i] = new HashMap<>();
+      columnLists[i] = new ArrayList<>();
       for (String column : allColumns) {
         if (Math.random() < selectPercent) {
           columnLists[i].add(column);
@@ -274,7 +274,7 @@ public class TestColumnSeeking {
 
       }
       InternalScanner scanner = region.getScanner(scan);
-      List<Cell> results = new ArrayList<Cell>();
+      List<Cell> results = new ArrayList<>();
       while (scanner.next(results))
         ;
       assertEquals(kvSet.size(), results.size());
@@ -285,7 +285,7 @@ public class TestColumnSeeking {
   }
 
   List<String> generateRandomWords(int numberOfWords, String suffix) {
-    Set<String> wordSet = new HashSet<String>();
+    Set<String> wordSet = new HashSet<>();
     for (int i = 0; i < numberOfWords; i++) {
       int lengthOfWords = (int) (Math.random() * 5) + 1;
       char[] wordChar = new char[lengthOfWords];
@@ -300,7 +300,7 @@ public class TestColumnSeeking {
       }
       wordSet.add(word);
     }
-    List<String> wordList = new ArrayList<String>(wordSet);
+    List<String> wordList = new ArrayList<>(wordSet);
     return wordList;
   }
 
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestCompactingMemStore.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestCompactingMemStore.java
index 65ad956..63bbe65 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestCompactingMemStore.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestCompactingMemStore.java
@@ -191,7 +191,7 @@ public class TestCompactingMemStore extends TestDefaultMemStore {
       InternalScanner scanner = new StoreScanner(new Scan(
           Bytes.toBytes(startRowId)), scanInfo, scanType, null,
           memstore.getScanners(0));
-      List<Cell> results = new ArrayList<Cell>();
+      List<Cell> results = new ArrayList<>();
       for (int i = 0; scanner.next(results); i++) {
         int rowId = startRowId + i;
         Cell left = results.get(0);
@@ -199,7 +199,7 @@ public class TestCompactingMemStore extends TestDefaultMemStore {
         assertTrue("Row name",
             CellComparator.COMPARATOR.compareRows(left, row1, 0, row1.length) == 0);
         assertEquals("Count of columns", QUALIFIER_COUNT, results.size());
-        List<Cell> row = new ArrayList<Cell>();
+        List<Cell> row = new ArrayList<>();
         for (Cell kv : results) {
           row.add(kv);
         }
@@ -255,7 +255,7 @@ public class TestCompactingMemStore extends TestDefaultMemStore {
   public void testUpsertMemstoreSize() throws Exception {
     MemstoreSize oldSize = memstore.size();
 
-    List<Cell> l = new ArrayList<Cell>();
+    List<Cell> l = new ArrayList<>();
     KeyValue kv1 = KeyValueTestUtil.create("r", "f", "q", 100, "v");
     KeyValue kv2 = KeyValueTestUtil.create("r", "f", "q", 101, "v");
     KeyValue kv3 = KeyValueTestUtil.create("r", "f", "q", 102, "v");
@@ -313,7 +313,7 @@ public class TestCompactingMemStore extends TestDefaultMemStore {
      t = runSnapshot(memstore, true);
 
       // test the case that the timeOfOldestEdit is updated after a KV upsert
-      List<Cell> l = new ArrayList<Cell>();
+      List<Cell> l = new ArrayList<>();
       KeyValue kv1 = KeyValueTestUtil.create("r", "f", "q", 100, "v");
       kv1.setSequenceId(100);
       l.add(kv1);
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestCompaction.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestCompaction.java
index bc51c41..1bf6ea7 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestCompaction.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestCompaction.java
@@ -370,8 +370,7 @@ public class TestCompaction {
 
     // setup a region/store with some files
     int numStores = r.getStores().size();
-    List<Pair<CompactionRequest, Store>> requests =
-        new ArrayList<Pair<CompactionRequest, Store>>(numStores);
+    List<Pair<CompactionRequest, Store>> requests = new ArrayList<>(numStores);
     CountDownLatch latch = new CountDownLatch(numStores);
     // create some store files and setup requests for each store on which we want to do a
     // compaction
@@ -379,8 +378,7 @@ public class TestCompaction {
       createStoreFile(r, store.getColumnFamilyName());
       createStoreFile(r, store.getColumnFamilyName());
       createStoreFile(r, store.getColumnFamilyName());
-      requests
-          .add(new Pair<CompactionRequest, Store>(new TrackableCompactionRequest(latch), store));
+      requests.add(new Pair<>(new TrackableCompactionRequest(latch), store));
     }
 
     thread.requestCompaction(r, "test mulitple custom comapctions", Store.PRIORITY_USER,
@@ -393,8 +391,8 @@ public class TestCompaction {
   }
 
   private class StoreMockMaker extends StatefulStoreMockMaker {
-    public ArrayList<StoreFile> compacting = new ArrayList<StoreFile>();
-    public ArrayList<StoreFile> notCompacting = new ArrayList<StoreFile>();
+    public ArrayList<StoreFile> compacting = new ArrayList<>();
+    public ArrayList<StoreFile> notCompacting = new ArrayList<>();
     private ArrayList<Integer> results;
 
     public StoreMockMaker(ArrayList<Integer> results) {
@@ -410,7 +408,7 @@ public class TestCompaction {
 
       @Override
       public List<StoreFile> preSelect(List<StoreFile> filesCompacting) {
-        return new ArrayList<StoreFile>();
+        return new ArrayList<>();
       }
 
       @Override
@@ -425,13 +423,13 @@ public class TestCompaction {
       public List<Path> compact(ThroughputController throughputController, User user)
           throws IOException {
         finishCompaction(this.selectedFiles);
-        return new ArrayList<Path>();
+        return new ArrayList<>();
       }
     }
 
     @Override
     public synchronized CompactionContext selectCompaction() {
-      CompactionContext ctx = new TestCompactionContext(new ArrayList<StoreFile>(notCompacting));
+      CompactionContext ctx = new TestCompactionContext(new ArrayList<>(notCompacting));
       compacting.addAll(notCompacting);
       notCompacting.clear();
       try {
@@ -484,18 +482,18 @@ public class TestCompaction {
         } catch (InterruptedException e) {
           Assume.assumeNoException(e);
         }
-        return new ArrayList<Path>();
+        return new ArrayList<>();
       }
 
       @Override
       public List<StoreFile> preSelect(List<StoreFile> filesCompacting) {
-        return new ArrayList<StoreFile>();
+        return new ArrayList<>();
       }
 
       @Override
       public boolean select(List<StoreFile> f, boolean i, boolean m, boolean e)
           throws IOException {
-        this.request = new CompactionRequest(new ArrayList<StoreFile>());
+        this.request = new CompactionRequest(new ArrayList<>());
         return true;
       }
     }
@@ -568,7 +566,7 @@ public class TestCompaction {
     });
 
     // Set up store mocks for 2 "real" stores and the one we use for blocking CST.
-    ArrayList<Integer> results = new ArrayList<Integer>();
+    ArrayList<Integer> results = new ArrayList<>();
     StoreMockMaker sm = new StoreMockMaker(results), sm2 = new StoreMockMaker(results);
     Store store = sm.createStoreMock("store1"), store2 = sm2.createStoreMock("store2");
     BlockingStoreMockMaker blocker = new BlockingStoreMockMaker();
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestCompactionArchiveConcurrentClose.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestCompactionArchiveConcurrentClose.java
index 7c7bfd3..8e85730 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestCompactionArchiveConcurrentClose.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestCompactionArchiveConcurrentClose.java
@@ -91,7 +91,7 @@ public class TestCompactionArchiveConcurrentClose {
     HRegionInfo info = new HRegionInfo(tableName, null, null, false);
     Region region = initHRegion(htd, info);
     RegionServerServices rss = mock(RegionServerServices.class);
-    List<Region> regions = new ArrayList<Region>();
+    List<Region> regions = new ArrayList<>();
     regions.add(region);
     when(rss.getOnlineRegions()).thenReturn(regions);
 
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestCompactionArchiveIOException.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestCompactionArchiveIOException.java
index cf99258..89b2368 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestCompactionArchiveIOException.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestCompactionArchiveIOException.java
@@ -98,7 +98,7 @@ public class TestCompactionArchiveIOException {
     HRegionInfo info = new HRegionInfo(tableName, null, null, false);
     final HRegion region = initHRegion(htd, info);
     RegionServerServices rss = mock(RegionServerServices.class);
-    List<Region> regions = new ArrayList<Region>();
+    List<Region> regions = new ArrayList<>();
     regions.add(region);
     when(rss.getOnlineRegions()).thenReturn(regions);
 
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestCompactionPolicy.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestCompactionPolicy.java
index 24b3667..7154511 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestCompactionPolicy.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestCompactionPolicy.java
@@ -136,7 +136,7 @@ public class TestCompactionPolicy {
   }
 
   ArrayList<Long> toArrayList(long... numbers) {
-    ArrayList<Long> result = new ArrayList<Long>();
+    ArrayList<Long> result = new ArrayList<>();
     for (long i : numbers) {
       result.add(i);
     }
@@ -144,7 +144,7 @@ public class TestCompactionPolicy {
   }
 
   List<StoreFile> sfCreate(long... sizes) throws IOException {
-    ArrayList<Long> ageInDisk = new ArrayList<Long>();
+    ArrayList<Long> ageInDisk = new ArrayList<>();
     for (int i = 0; i < sizes.length; i++) {
       ageInDisk.add(0L);
     }
@@ -156,7 +156,7 @@ public class TestCompactionPolicy {
   }
 
   List<StoreFile> sfCreate(boolean isReference, long... sizes) throws IOException {
-    ArrayList<Long> ageInDisk = new ArrayList<Long>(sizes.length);
+    ArrayList<Long> ageInDisk = new ArrayList<>(sizes.length);
     for (int i = 0; i < sizes.length; i++) {
       ageInDisk.add(0L);
     }
@@ -196,8 +196,8 @@ public class TestCompactionPolicy {
     // Test Default compactions
     CompactionRequest result =
         ((RatioBasedCompactionPolicy) store.storeEngine.getCompactionPolicy()).selectCompaction(
-          candidates, new ArrayList<StoreFile>(), false, isOffPeak, forcemajor);
-    List<StoreFile> actual = new ArrayList<StoreFile>(result.getFiles());
+          candidates, new ArrayList<>(), false, isOffPeak, forcemajor);
+    List<StoreFile> actual = new ArrayList<>(result.getFiles());
     if (isOffPeak && !forcemajor) {
       Assert.assertTrue(result.isOffPeak());
     }
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestCompactionState.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestCompactionState.java
index 0e6fb54..8c55327 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestCompactionState.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestCompactionState.java
@@ -220,7 +220,7 @@ public class TestCompactionState {
 
   private static void loadData(final Table ht, final byte[][] families,
       final int rows, final int flushes) throws IOException {
-    List<Put> puts = new ArrayList<Put>(rows);
+    List<Put> puts = new ArrayList<>(rows);
     byte[] qualifier = Bytes.toBytes("val");
     for (int i = 0; i < flushes; i++) {
       for (int k = 0; k < rows; k++) {
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestCompoundBloomFilter.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestCompoundBloomFilter.java
index 174843e..dfea761 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestCompoundBloomFilter.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestCompoundBloomFilter.java
@@ -142,7 +142,7 @@ public class TestCompoundBloomFilter {
   }
 
   private List<KeyValue> createSortedKeyValues(Random rand, int n) {
-    List<KeyValue> kvList = new ArrayList<KeyValue>(n);
+    List<KeyValue> kvList = new ArrayList<>(n);
     for (int i = 0; i < n; ++i)
       kvList.add(RandomKeyValueUtil.randomKeyValue(rand));
     Collections.sort(kvList, CellComparator.COMPARATOR);
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestCorruptedRegionStoreFile.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestCorruptedRegionStoreFile.java
index 68b0ba3..cec5fc7 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestCorruptedRegionStoreFile.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestCorruptedRegionStoreFile.java
@@ -67,7 +67,7 @@ public class TestCorruptedRegionStoreFile {
 
   @Rule public TestTableName TEST_TABLE = new TestTableName();
 
-  private final ArrayList<Path> storeFiles = new ArrayList<Path>();
+  private final ArrayList<Path> storeFiles = new ArrayList<>();
   private Path tableDir;
   private int rowCount;
 
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestDefaultCompactSelection.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestDefaultCompactSelection.java
index 4fa18b8..3c41fc5 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestDefaultCompactSelection.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestDefaultCompactSelection.java
@@ -119,7 +119,7 @@ public class TestDefaultCompactSelection extends TestCompactionPolicy {
     compactEquals(sfCreate(true, 7, 6, 5, 4, 3, 2, 1), 7, 6, 5, 4, 3);
 
     // empty case
-    compactEquals(new ArrayList<StoreFile>() /* empty */);
+    compactEquals(new ArrayList<>() /* empty */);
     // empty case (because all files are too big)
     compactEquals(sfCreate(tooBig, tooBig) /* empty */);
   }
@@ -175,7 +175,7 @@ public class TestDefaultCompactSelection extends TestCompactionPolicy {
     // Test Default compactions
     CompactionRequest result = ((RatioBasedCompactionPolicy) store.storeEngine
         .getCompactionPolicy()).selectCompaction(candidates,
-        new ArrayList<StoreFile>(), false, false, false);
+        new ArrayList<>(), false, false, false);
     Assert.assertTrue(result.getFiles().isEmpty());
     store.setScanInfo(oldScanInfo);
   }
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestDefaultMemStore.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestDefaultMemStore.java
index 43c185a..e6d3147 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestDefaultMemStore.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestDefaultMemStore.java
@@ -150,7 +150,7 @@ public class TestDefaultMemStore {
     int rowCount = addRows(this.memstore);
     List<KeyValueScanner> memstorescanners = this.memstore.getScanners(0);
     Scan scan = new Scan();
-    List<Cell> result = new ArrayList<Cell>();
+    List<Cell> result = new ArrayList<>();
     Configuration conf = HBaseConfiguration.create();
     ScanInfo scanInfo =
         new ScanInfo(conf, null, 0, 1, HConstants.LATEST_TIMESTAMP, KeepDeletedCells.FALSE, 0,
@@ -502,7 +502,7 @@ public class TestDefaultMemStore {
     int NUM_THREADS = 8;
 
     ReadOwnWritesTester threads[] = new ReadOwnWritesTester[NUM_THREADS];
-    AtomicReference<Throwable> caught = new AtomicReference<Throwable>();
+    AtomicReference<Throwable> caught = new AtomicReference<>();
 
     for (int i = 0; i < NUM_THREADS; i++) {
       threads[i] = new ReadOwnWritesTester(i, memstore, mvcc, caught, this.startSeqNum);
@@ -589,7 +589,7 @@ public class TestDefaultMemStore {
       try (InternalScanner scanner = new StoreScanner(new Scan(
           Bytes.toBytes(startRowId)), scanInfo, scanType, null,
           memstore.getScanners(0))) {
-        List<Cell> results = new ArrayList<Cell>();
+        List<Cell> results = new ArrayList<>();
         for (int i = 0; scanner.next(results); i++) {
           int rowId = startRowId + i;
           Cell left = results.get(0);
@@ -598,7 +598,7 @@ public class TestDefaultMemStore {
               "Row name",
               CellComparator.COMPARATOR.compareRows(left, row1, 0, row1.length) == 0);
           assertEquals("Count of columns", QUALIFIER_COUNT, results.size());
-          List<Cell> row = new ArrayList<Cell>();
+          List<Cell> row = new ArrayList<>();
           for (Cell kv : results) {
             row.add(kv);
           }
@@ -660,7 +660,7 @@ public class TestDefaultMemStore {
     KeyValue del2 = new KeyValue(row, fam, qf1, ts2, KeyValue.Type.Delete, val);
     memstore.add(del2, null);
 
-    List<Cell> expected = new ArrayList<Cell>();
+    List<Cell> expected = new ArrayList<>();
     expected.add(put3);
     expected.add(del2);
     expected.add(put2);
@@ -696,7 +696,7 @@ public class TestDefaultMemStore {
       new KeyValue(row, fam, qf1, ts2, KeyValue.Type.DeleteColumn, val);
     memstore.add(del2, null);
 
-    List<Cell> expected = new ArrayList<Cell>();
+    List<Cell> expected = new ArrayList<>();
     expected.add(put3);
     expected.add(del2);
     expected.add(put2);
@@ -733,7 +733,7 @@ public class TestDefaultMemStore {
       new KeyValue(row, fam, null, ts, KeyValue.Type.DeleteFamily, val);
     memstore.add(del, null);
 
-    List<Cell> expected = new ArrayList<Cell>();
+    List<Cell> expected = new ArrayList<>();
     expected.add(del);
     expected.add(put1);
     expected.add(put2);
@@ -822,7 +822,7 @@ public class TestDefaultMemStore {
     memstore = new DefaultMemStore(conf, CellComparator.COMPARATOR);
     MemstoreSize oldSize = memstore.size();
 
-    List<Cell> l = new ArrayList<Cell>();
+    List<Cell> l = new ArrayList<>();
     KeyValue kv1 = KeyValueTestUtil.create("r", "f", "q", 100, "v");
     KeyValue kv2 = KeyValueTestUtil.create("r", "f", "q", 101, "v");
     KeyValue kv3 = KeyValueTestUtil.create("r", "f", "q", 102, "v");
@@ -880,7 +880,7 @@ public class TestDefaultMemStore {
       t = runSnapshot(memstore);
 
       // test the case that the timeOfOldestEdit is updated after a KV upsert
-      List<Cell> l = new ArrayList<Cell>();
+      List<Cell> l = new ArrayList<>();
       KeyValue kv1 = KeyValueTestUtil.create("r", "f", "q", 100, "v");
       kv1.setSequenceId(100);
       l.add(kv1);
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestEncryptionKeyRotation.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestEncryptionKeyRotation.java
index 3eb86be..b34c307 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestEncryptionKeyRotation.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestEncryptionKeyRotation.java
@@ -233,7 +233,7 @@ public class TestEncryptionKeyRotation {
   }
 
   private static List<Path> findStorefilePaths(TableName tableName) throws Exception {
-    List<Path> paths = new ArrayList<Path>();
+    List<Path> paths = new ArrayList<>();
     for (Region region:
         TEST_UTIL.getRSForFirstRegionInTable(tableName).getOnlineRegions(tableName)) {
       for (Store store: region.getStores()) {
@@ -246,7 +246,7 @@ public class TestEncryptionKeyRotation {
   }
 
   private static List<Path> findCompactedStorefilePaths(TableName tableName) throws Exception {
-    List<Path> paths = new ArrayList<Path>();
+    List<Path> paths = new ArrayList<>();
     for (Region region:
         TEST_UTIL.getRSForFirstRegionInTable(tableName).getOnlineRegions(tableName)) {
       for (Store store : region.getStores()) {
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestEncryptionRandomKeying.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestEncryptionRandomKeying.java
index 760bdac..2b0ab7b 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestEncryptionRandomKeying.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestEncryptionRandomKeying.java
@@ -52,7 +52,7 @@ public class TestEncryptionRandomKeying {
   private static HTableDescriptor htd;
 
   private static List<Path> findStorefilePaths(TableName tableName) throws Exception {
-    List<Path> paths = new ArrayList<Path>();
+    List<Path> paths = new ArrayList<>();
     for (Region region:
         TEST_UTIL.getRSForFirstRegionInTable(tableName).getOnlineRegions(htd.getTableName())) {
       for (Store store: region.getStores()) {
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestEndToEndSplitTransaction.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestEndToEndSplitTransaction.java
index 9ed0c2a..1051036 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestEndToEndSplitTransaction.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestEndToEndSplitTransaction.java
@@ -245,7 +245,7 @@ public class TestEndToEndSplitTransaction {
           verifyStartEndKeys(keys);
 
           //HTable.getRegionsInfo()
-          Set<HRegionInfo> regions = new TreeSet<HRegionInfo>();
+          Set<HRegionInfo> regions = new TreeSet<>();
           for (HRegionLocation loc : rl.getAllRegionLocations()) {
             regions.add(loc.getRegionInfo());
           }
@@ -275,7 +275,7 @@ public class TestEndToEndSplitTransaction {
         i++;
       }
 
-      Pair<byte[][], byte[][]> keys = new Pair<byte[][], byte[][]>(startKeys, endKeys);
+      Pair<byte[][], byte[][]> keys = new Pair<>(startKeys, endKeys);
       verifyStartEndKeys(keys);
     }
 
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestFSErrorsExposed.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestFSErrorsExposed.java
index 9a49c5d..9f0975d 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestFSErrorsExposed.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestFSErrorsExposed.java
@@ -233,8 +233,7 @@ public class TestFSErrorsExposed {
   }
 
   static class FaultyFileSystem extends FilterFileSystem {
-    List<SoftReference<FaultyInputStream>> inStreams =
-      new ArrayList<SoftReference<FaultyInputStream>>();
+    List<SoftReference<FaultyInputStream>> inStreams = new ArrayList<>();
 
     public FaultyFileSystem(FileSystem testFileSystem) {
       super(testFileSystem);
@@ -244,7 +243,7 @@ public class TestFSErrorsExposed {
     public FSDataInputStream open(Path p, int bufferSize) throws IOException  {
       FSDataInputStream orig = fs.open(p, bufferSize);
       FaultyInputStream faulty = new FaultyInputStream(orig);
-      inStreams.add(new SoftReference<FaultyInputStream>(faulty));
+      inStreams.add(new SoftReference<>(faulty));
       return faulty;
     }
 
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestGetClosestAtOrBefore.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestGetClosestAtOrBefore.java
index 7ee3f0b..570d2d8 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestGetClosestAtOrBefore.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestGetClosestAtOrBefore.java
@@ -101,7 +101,7 @@ public class TestGetClosestAtOrBefore  {
     }
     InternalScanner s = mr.getScanner(new Scan());
     try {
-      List<Cell> keys = new ArrayList<Cell>();
+      List<Cell> keys = new ArrayList<>();
         while (s.next(keys)) {
         LOG.info(keys);
         keys.clear();
@@ -125,7 +125,7 @@ public class TestGetClosestAtOrBefore  {
     Scan scan = new Scan(firstRowInC);
     s = mr.getScanner(scan);
     try {
-      List<Cell> keys = new ArrayList<Cell>();
+      List<Cell> keys = new ArrayList<>();
         while (s.next(keys)) {
         mr.delete(new Delete(CellUtil.cloneRow(keys.get(0))));
         keys.clear();
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestHMobStore.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestHMobStore.java
index 6877fca..b416c7d 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestHMobStore.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestHMobStore.java
@@ -100,9 +100,8 @@ public class TestHMobStore {
   private Cell seekKey1;
   private Cell seekKey2;
   private Cell seekKey3;
-  private NavigableSet<byte[]> qualifiers =
-    new ConcurrentSkipListSet<byte[]>(Bytes.BYTES_COMPARATOR);
-  private List<Cell> expected = new ArrayList<Cell>();
+  private NavigableSet<byte[]> qualifiers = new ConcurrentSkipListSet<>(Bytes.BYTES_COMPARATOR);
+  private List<Cell> expected = new ArrayList<>();
   private long id = System.currentTimeMillis();
   private Get get = new Get(row);
   private final static HBaseTestingUtility TEST_UTIL = new HBaseTestingUtility();
@@ -222,7 +221,7 @@ public class TestHMobStore {
         scan.getFamilyMap().get(store.getFamily().getName()),
         0);
 
-    List<Cell> results = new ArrayList<Cell>();
+    List<Cell> results = new ArrayList<>();
     scanner.next(results);
     Collections.sort(results, CellComparator.COMPARATOR);
     scanner.close();
@@ -267,7 +266,7 @@ public class TestHMobStore {
         scan.getFamilyMap().get(store.getFamily().getName()),
         0);
 
-    List<Cell> results = new ArrayList<Cell>();
+    List<Cell> results = new ArrayList<>();
     scanner.next(results);
     Collections.sort(results, CellComparator.COMPARATOR);
     scanner.close();
@@ -312,7 +311,7 @@ public class TestHMobStore {
       scan.getFamilyMap().get(store.getFamily().getName()),
       0);
 
-    List<Cell> results = new ArrayList<Cell>();
+    List<Cell> results = new ArrayList<>();
     scanner.next(results);
     Collections.sort(results, CellComparator.COMPARATOR);
     scanner.close();
@@ -357,7 +356,7 @@ public class TestHMobStore {
         scan.getFamilyMap().get(store.getFamily().getName()),
         0);
 
-    List<Cell> results = new ArrayList<Cell>();
+    List<Cell> results = new ArrayList<>();
     scanner.next(results);
     Collections.sort(results, CellComparator.COMPARATOR);
     scanner.close();
@@ -409,7 +408,7 @@ public class TestHMobStore {
       scan.getFamilyMap().get(store.getFamily().getName()),
       0);
 
-    List<Cell> results = new ArrayList<Cell>();
+    List<Cell> results = new ArrayList<>();
     scanner.next(results);
     Collections.sort(results, CellComparator.COMPARATOR);
     scanner.close();
@@ -525,7 +524,7 @@ public class TestHMobStore {
         scan.getFamilyMap().get(store.getFamily().getName()),
         0);
 
-    List<Cell> results = new ArrayList<Cell>();
+    List<Cell> results = new ArrayList<>();
     scanner.next(results);
     Collections.sort(results, CellComparator.COMPARATOR);
     scanner.close();
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestHRegion.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestHRegion.java
index 35f1b7d..eac3c77 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestHRegion.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestHRegion.java
@@ -608,7 +608,7 @@ public class TestHRegion {
     // open the second scanner
     RegionScanner scanner2 = region.getScanner(scan);
 
-    List<Cell> results = new ArrayList<Cell>();
+    List<Cell> results = new ArrayList<>();
 
     System.out.println("Smallest read point:" + region.getSmallestReadPoint());
 
@@ -657,7 +657,7 @@ public class TestHRegion {
     region.compact(true);
 
     scanner1.reseek(Bytes.toBytes("r2"));
-    List<Cell> results = new ArrayList<Cell>();
+    List<Cell> results = new ArrayList<>();
     scanner1.next(results);
     Cell keyValue = results.get(0);
     Assert.assertTrue(Bytes.compareTo(CellUtil.cloneRow(keyValue), Bytes.toBytes("r2")) == 0);
@@ -694,7 +694,7 @@ public class TestHRegion {
         writer.close();
       }
       MonitoredTask status = TaskMonitor.get().createStatus(method);
-      Map<byte[], Long> maxSeqIdInStores = new TreeMap<byte[], Long>(Bytes.BYTES_COMPARATOR);
+      Map<byte[], Long> maxSeqIdInStores = new TreeMap<>(Bytes.BYTES_COMPARATOR);
       for (Store store : region.getStores()) {
         maxSeqIdInStores.put(store.getColumnFamilyName().getBytes(), minSeqId - 1);
       }
@@ -746,7 +746,7 @@ public class TestHRegion {
       }
       long recoverSeqId = 1030;
       MonitoredTask status = TaskMonitor.get().createStatus(method);
-      Map<byte[], Long> maxSeqIdInStores = new TreeMap<byte[], Long>(Bytes.BYTES_COMPARATOR);
+      Map<byte[], Long> maxSeqIdInStores = new TreeMap<>(Bytes.BYTES_COMPARATOR);
       for (Store store : region.getStores()) {
         maxSeqIdInStores.put(store.getColumnFamilyName().getBytes(), recoverSeqId - 1);
       }
@@ -791,7 +791,7 @@ public class TestHRegion {
       FSDataOutputStream dos = fs.create(recoveredEdits);
       dos.close();
 
-      Map<byte[], Long> maxSeqIdInStores = new TreeMap<byte[], Long>(Bytes.BYTES_COMPARATOR);
+      Map<byte[], Long> maxSeqIdInStores = new TreeMap<>(Bytes.BYTES_COMPARATOR);
       for (Store store : region.getStores()) {
         maxSeqIdInStores.put(store.getColumnFamilyName().getBytes(), minSeqId);
       }
@@ -848,7 +848,7 @@ public class TestHRegion {
       }
 
       long recoverSeqId = 1030;
-      Map<byte[], Long> maxSeqIdInStores = new TreeMap<byte[], Long>(Bytes.BYTES_COMPARATOR);
+      Map<byte[], Long> maxSeqIdInStores = new TreeMap<>(Bytes.BYTES_COMPARATOR);
       MonitoredTask status = TaskMonitor.get().createStatus(method);
       for (Store store : region.getStores()) {
         maxSeqIdInStores.put(store.getColumnFamilyName().getBytes(), recoverSeqId - 1);
@@ -893,7 +893,7 @@ public class TestHRegion {
 
       // this will create a region with 3 files
       assertEquals(3, region.getStore(family).getStorefilesCount());
-      List<Path> storeFiles = new ArrayList<Path>(3);
+      List<Path> storeFiles = new ArrayList<>(3);
       for (StoreFile sf : region.getStore(family).getStorefiles()) {
         storeFiles.add(sf.getPath());
       }
@@ -1007,7 +1007,7 @@ public class TestHRegion {
 
       // this will create a region with 3 files from flush
       assertEquals(3, region.getStore(family).getStorefilesCount());
-      List<String> storeFiles = new ArrayList<String>(3);
+      List<String> storeFiles = new ArrayList<>(3);
       for (StoreFile sf : region.getStore(family).getStorefiles()) {
         storeFiles.add(sf.getPath().getName());
       }
@@ -1017,7 +1017,7 @@ public class TestHRegion {
       WAL.Reader reader = WALFactory.createReader(fs, AbstractFSWALProvider.getCurrentFileName(wal),
         TEST_UTIL.getConfiguration());
       try {
-        List<WAL.Entry> flushDescriptors = new ArrayList<WAL.Entry>();
+        List<WAL.Entry> flushDescriptors = new ArrayList<>();
         long lastFlushSeqId = -1;
         while (true) {
           WAL.Entry entry = reader.next();
@@ -1422,7 +1422,7 @@ public class TestHRegion {
     InternalScanner scanner = buildScanner(keyPrefix, value, r);
     int count = 0;
     boolean more = false;
-    List<Cell> results = new ArrayList<Cell>();
+    List<Cell> results = new ArrayList<>();
     do {
       more = scanner.next(results);
       if (results != null && !results.isEmpty())
@@ -1440,7 +1440,7 @@ public class TestHRegion {
   private int getNumberOfRows(String keyPrefix, String value, HRegion r) throws Exception {
     InternalScanner resultScanner = buildScanner(keyPrefix, value, r);
     int numberOfResults = 0;
-    List<Cell> results = new ArrayList<Cell>();
+    List<Cell> results = new ArrayList<>();
     boolean more = false;
     do {
       more = resultScanner.next(results);
@@ -1579,7 +1579,7 @@ public class TestHRegion {
 
 
       MultithreadedTestUtil.TestContext ctx = new MultithreadedTestUtil.TestContext(CONF);
-      final AtomicReference<OperationStatus[]> retFromThread = new AtomicReference<OperationStatus[]>();
+      final AtomicReference<OperationStatus[]> retFromThread = new AtomicReference<>();
       final CountDownLatch startingPuts = new CountDownLatch(1);
       final CountDownLatch startingClose = new CountDownLatch(1);
       TestThread putter = new TestThread(ctx) {
@@ -2112,14 +2112,13 @@ public class TestHRegion {
     // Setting up region
     this.region = initHRegion(tableName, method, CONF, fam1, fam2, fam3);
     try {
-      List<Cell> kvs = new ArrayList<Cell>();
+      List<Cell> kvs = new ArrayList<>();
       kvs.add(new KeyValue(row1, fam4, null, null));
 
       // testing existing family
       byte[] family = fam2;
       try {
-        NavigableMap<byte[], List<Cell>> deleteMap = new TreeMap<byte[], List<Cell>>(
-            Bytes.BYTES_COMPARATOR);
+        NavigableMap<byte[], List<Cell>> deleteMap = new TreeMap<>(Bytes.BYTES_COMPARATOR);
         deleteMap.put(family, kvs);
         region.delete(deleteMap, Durability.SYNC_WAL);
       } catch (Exception e) {
@@ -2130,8 +2129,7 @@ public class TestHRegion {
       boolean ok = false;
       family = fam4;
       try {
-        NavigableMap<byte[], List<Cell>> deleteMap = new TreeMap<byte[], List<Cell>>(
-            Bytes.BYTES_COMPARATOR);
+        NavigableMap<byte[], List<Cell>> deleteMap = new TreeMap<>(Bytes.BYTES_COMPARATOR);
         deleteMap.put(family, kvs);
         region.delete(deleteMap, Durability.SYNC_WAL);
       } catch (Exception e) {
@@ -2361,7 +2359,7 @@ public class TestHRegion {
       Scan scan = new Scan();
       scan.addFamily(fam1).addFamily(fam2);
       InternalScanner s = region.getScanner(scan);
-      List<Cell> results = new ArrayList<Cell>();
+      List<Cell> results = new ArrayList<>();
       s.next(results);
       assertTrue(CellUtil.matchingRow(results.get(0), rowA));
 
@@ -2488,7 +2486,7 @@ public class TestHRegion {
       scan.addColumn(fam1, qual1);
       InternalScanner s = region.getScanner(scan);
 
-      List<Cell> results = new ArrayList<Cell>();
+      List<Cell> results = new ArrayList<>();
       assertEquals(false, s.next(results));
       assertEquals(1, results.size());
       Cell kv = results.get(0);
@@ -2514,13 +2512,12 @@ public class TestHRegion {
     this.region = initHRegion(tableName, method, CONF, fam1);
     try {
       // Building checkerList
-      List<Cell> kvs = new ArrayList<Cell>();
+      List<Cell> kvs = new ArrayList<>();
       kvs.add(new KeyValue(row1, fam1, col1, null));
       kvs.add(new KeyValue(row1, fam1, col2, null));
       kvs.add(new KeyValue(row1, fam1, col3, null));
 
-      NavigableMap<byte[], List<Cell>> deleteMap = new TreeMap<byte[], List<Cell>>(
-          Bytes.BYTES_COMPARATOR);
+      NavigableMap<byte[], List<Cell>> deleteMap = new TreeMap<>(Bytes.BYTES_COMPARATOR);
       deleteMap.put(fam1, kvs);
       region.delete(deleteMap, Durability.SYNC_WAL);
 
@@ -2811,22 +2808,22 @@ public class TestHRegion {
       List<Cell> res = null;
 
       // Result 1
-      List<Cell> expected1 = new ArrayList<Cell>();
+      List<Cell> expected1 = new ArrayList<>();
       expected1.add(new KeyValue(row1, fam2, null, ts, KeyValue.Type.Put, null));
       expected1.add(new KeyValue(row1, fam4, null, ts, KeyValue.Type.Put, null));
 
-      res = new ArrayList<Cell>();
+      res = new ArrayList<>();
       is.next(res);
       for (int i = 0; i < res.size(); i++) {
         assertTrue(CellUtil.equalsIgnoreMvccVersion(expected1.get(i), res.get(i)));
       }
 
       // Result 2
-      List<Cell> expected2 = new ArrayList<Cell>();
+      List<Cell> expected2 = new ArrayList<>();
       expected2.add(new KeyValue(row2, fam2, null, ts, KeyValue.Type.Put, null));
       expected2.add(new KeyValue(row2, fam4, null, ts, KeyValue.Type.Put, null));
 
-      res = new ArrayList<Cell>();
+      res = new ArrayList<>();
       is.next(res);
       for (int i = 0; i < res.size(); i++) {
         assertTrue(CellUtil.equalsIgnoreMvccVersion(expected2.get(i), res.get(i)));
@@ -2872,14 +2869,14 @@ public class TestHRegion {
       region.put(put);
 
       // Expected
-      List<Cell> expected = new ArrayList<Cell>();
+      List<Cell> expected = new ArrayList<>();
       expected.add(kv13);
       expected.add(kv12);
 
       Scan scan = new Scan(row1);
       scan.addColumn(fam1, qf1);
       scan.setMaxVersions(MAX_VERSIONS);
-      List<Cell> actual = new ArrayList<Cell>();
+      List<Cell> actual = new ArrayList<>();
       InternalScanner scanner = region.getScanner(scan);
 
       boolean hasNext = scanner.next(actual);
@@ -2931,7 +2928,7 @@ public class TestHRegion {
       region.flush(true);
 
       // Expected
-      List<Cell> expected = new ArrayList<Cell>();
+      List<Cell> expected = new ArrayList<>();
       expected.add(kv13);
       expected.add(kv12);
       expected.add(kv23);
@@ -2941,7 +2938,7 @@ public class TestHRegion {
       scan.addColumn(fam1, qf1);
       scan.addColumn(fam1, qf2);
       scan.setMaxVersions(MAX_VERSIONS);
-      List<Cell> actual = new ArrayList<Cell>();
+      List<Cell> actual = new ArrayList<>();
       InternalScanner scanner = region.getScanner(scan);
 
       boolean hasNext = scanner.next(actual);
@@ -3010,7 +3007,7 @@ public class TestHRegion {
       region.put(put);
 
       // Expected
-      List<Cell> expected = new ArrayList<Cell>();
+      List<Cell> expected = new ArrayList<>();
       expected.add(kv14);
       expected.add(kv13);
       expected.add(kv12);
@@ -3023,7 +3020,7 @@ public class TestHRegion {
       scan.addColumn(fam1, qf2);
       int versions = 3;
       scan.setMaxVersions(versions);
-      List<Cell> actual = new ArrayList<Cell>();
+      List<Cell> actual = new ArrayList<>();
       InternalScanner scanner = region.getScanner(scan);
 
       boolean hasNext = scanner.next(actual);
@@ -3074,7 +3071,7 @@ public class TestHRegion {
       region.put(put);
 
       // Expected
-      List<Cell> expected = new ArrayList<Cell>();
+      List<Cell> expected = new ArrayList<>();
       expected.add(kv13);
       expected.add(kv12);
       expected.add(kv23);
@@ -3083,7 +3080,7 @@ public class TestHRegion {
       Scan scan = new Scan(row1);
       scan.addFamily(fam1);
       scan.setMaxVersions(MAX_VERSIONS);
-      List<Cell> actual = new ArrayList<Cell>();
+      List<Cell> actual = new ArrayList<>();
       InternalScanner scanner = region.getScanner(scan);
 
       boolean hasNext = scanner.next(actual);
@@ -3134,7 +3131,7 @@ public class TestHRegion {
       region.flush(true);
 
       // Expected
-      List<Cell> expected = new ArrayList<Cell>();
+      List<Cell> expected = new ArrayList<>();
       expected.add(kv13);
       expected.add(kv12);
       expected.add(kv23);
@@ -3143,7 +3140,7 @@ public class TestHRegion {
       Scan scan = new Scan(row1);
       scan.addFamily(fam1);
       scan.setMaxVersions(MAX_VERSIONS);
-      List<Cell> actual = new ArrayList<Cell>();
+      List<Cell> actual = new ArrayList<>();
       InternalScanner scanner = region.getScanner(scan);
 
       boolean hasNext = scanner.next(actual);
@@ -3198,7 +3195,7 @@ public class TestHRegion {
       scan.addColumn(family, col1);
       InternalScanner s = region.getScanner(scan);
 
-      List<Cell> results = new ArrayList<Cell>();
+      List<Cell> results = new ArrayList<>();
       assertEquals(false, s.next(results));
       assertEquals(0, results.size());
     } finally {
@@ -3258,7 +3255,7 @@ public class TestHRegion {
       region.put(put);
 
       // Expected
-      List<KeyValue> expected = new ArrayList<KeyValue>();
+      List<KeyValue> expected = new ArrayList<>();
       expected.add(kv14);
       expected.add(kv13);
       expected.add(kv12);
@@ -3269,7 +3266,7 @@ public class TestHRegion {
       Scan scan = new Scan(row1);
       int versions = 3;
       scan.setMaxVersions(versions);
-      List<Cell> actual = new ArrayList<Cell>();
+      List<Cell> actual = new ArrayList<>();
       InternalScanner scanner = region.getScanner(scan);
 
       boolean hasNext = scanner.next(actual);
@@ -3334,7 +3331,7 @@ public class TestHRegion {
       scan.setLoadColumnFamiliesOnDemand(true);
       InternalScanner s = region.getScanner(scan);
 
-      List<Cell> results = new ArrayList<Cell>();
+      List<Cell> results = new ArrayList<>();
       assertTrue(s.next(results));
       assertEquals(results.size(), 1);
       results.clear();
@@ -3427,7 +3424,7 @@ public class TestHRegion {
       // r8: first:a
       // r9: first:a
 
-      List<Cell> results = new ArrayList<Cell>();
+      List<Cell> results = new ArrayList<>();
       int index = 0;
       ScannerContext scannerContext = ScannerContext.newBuilder().setBatchLimit(3).build();
       while (true) {
@@ -3501,7 +3498,7 @@ public class TestHRegion {
           new BinaryComparator(Bytes.toBytes(5L))));
 
       int expectedCount = 0;
-      List<Cell> res = new ArrayList<Cell>();
+      List<Cell> res = new ArrayList<>();
 
       boolean toggle = true;
       for (long i = 0; i < numRows; i++) {
@@ -3643,7 +3640,7 @@ public class TestHRegion {
       Scan scan = new Scan(Bytes.toBytes("row0"), Bytes.toBytes("row1"));
 
       int expectedCount = numFamilies * numQualifiers;
-      List<Cell> res = new ArrayList<Cell>();
+      List<Cell> res = new ArrayList<>();
 
       long prevTimestamp = 0L;
       for (int i = 0; i < testCount; i++) {
@@ -3943,7 +3940,7 @@ public class TestHRegion {
               new BinaryComparator(Bytes.toBytes(0L))), new SingleColumnValueFilter(family, qual1,
               CompareOp.LESS_OR_EQUAL, new BinaryComparator(Bytes.toBytes(3L))))));
       InternalScanner scanner = region.getScanner(idxScan);
-      List<Cell> res = new ArrayList<Cell>();
+      List<Cell> res = new ArrayList<>();
 
       while (scanner.next(res))
         ;
@@ -4841,7 +4838,7 @@ public class TestHRegion {
       scan.addFamily(families[i]);
     InternalScanner s = r.getScanner(scan);
     try {
-      List<Cell> curVals = new ArrayList<Cell>();
+      List<Cell> curVals = new ArrayList<>();
       boolean first = true;
       OUTER_LOOP: while (s.next(curVals)) {
         for (Cell kv : curVals) {
@@ -5000,7 +4997,7 @@ public class TestHRegion {
       scan.setMaxVersions(5);
       scan.setReversed(true);
       InternalScanner scanner = region.getScanner(scan);
-      List<Cell> currRow = new ArrayList<Cell>();
+      List<Cell> currRow = new ArrayList<>();
       boolean hasNext = scanner.next(currRow);
       assertEquals(2, currRow.size());
       assertTrue(Bytes.equals(currRow.get(0).getRowArray(), currRow.get(0).getRowOffset(), currRow
@@ -5056,7 +5053,7 @@ public class TestHRegion {
       region.put(put);
 
       Scan scan = new Scan(rowD);
-      List<Cell> currRow = new ArrayList<Cell>();
+      List<Cell> currRow = new ArrayList<>();
       scan.setReversed(true);
       scan.setMaxVersions(5);
       InternalScanner scanner = region.getScanner(scan);
@@ -5113,7 +5110,7 @@ public class TestHRegion {
       put.add(kv3);
       region.put(put);
       Scan scan = new Scan();
-      List<Cell> currRow = new ArrayList<Cell>();
+      List<Cell> currRow = new ArrayList<>();
       scan.setReversed(true);
       InternalScanner scanner = region.getScanner(scan);
       boolean hasNext = scanner.next(currRow);
@@ -5184,7 +5181,7 @@ public class TestHRegion {
       Scan scan = new Scan(rowD, rowA);
       scan.addColumn(families[0], col1);
       scan.setReversed(true);
-      List<Cell> currRow = new ArrayList<Cell>();
+      List<Cell> currRow = new ArrayList<>();
       InternalScanner scanner = region.getScanner(scan);
       boolean hasNext = scanner.next(currRow);
       assertEquals(1, currRow.size());
@@ -5267,7 +5264,7 @@ public class TestHRegion {
       Scan scan = new Scan(rowD, rowA);
       scan.addColumn(families[0], col1);
       scan.setReversed(true);
-      List<Cell> currRow = new ArrayList<Cell>();
+      List<Cell> currRow = new ArrayList<>();
       InternalScanner scanner = region.getScanner(scan);
       boolean hasNext = scanner.next(currRow);
       assertEquals(1, currRow.size());
@@ -5412,7 +5409,7 @@ public class TestHRegion {
       scan.setBatch(3);
       scan.setReversed(true);
       InternalScanner scanner = region.getScanner(scan);
-      List<Cell> currRow = new ArrayList<Cell>();
+      List<Cell> currRow = new ArrayList<>();
       boolean hasNext = false;
       // 1. scan out "row4" (5 kvs), "row5" can't be scanned out since not
       // included in scan range
@@ -5519,7 +5516,7 @@ public class TestHRegion {
       scan.setReversed(true);
       scan.setBatch(10);
       InternalScanner scanner = region.getScanner(scan);
-      List<Cell> currRow = new ArrayList<Cell>();
+      List<Cell> currRow = new ArrayList<>();
       boolean hasNext = scanner.next(currRow);
       assertEquals(1, currRow.size());
       assertTrue(Bytes.equals(currRow.get(0).getRowArray(), currRow.get(0).getRowOffset(), currRow
@@ -6348,7 +6345,7 @@ public class TestHRegion {
     CONF.setInt("hbase.regionserver.wal.disruptor.event.count", 2);
     this.region = initHRegion(tableName, method, CONF, families);
     try {
-      List<Thread> threads = new ArrayList<Thread>();
+      List<Thread> threads = new ArrayList<>();
       for (int i = 0; i < numRows; i++) {
         final int count = i;
         Thread t = new Thread(new Runnable() {
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestHRegionOnCluster.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestHRegionOnCluster.java
index 23c96a2..41ad68b 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestHRegionOnCluster.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestHRegionOnCluster.java
@@ -159,7 +159,7 @@ public class TestHRegionOnCluster {
     put.addColumn(family, Bytes.toBytes("q1"), Bytes.toBytes(value));
     table.put(put);
     ResultScanner resultScanner = table.getScanner(new Scan());
-    List<Result> results = new ArrayList<Result>();
+    List<Result> results = new ArrayList<>();
     while (true) {
       Result r = resultScanner.next();
       if (r == null)
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestHRegionReplayEvents.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestHRegionReplayEvents.java
index ac10f8c..0054642 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestHRegionReplayEvents.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestHRegionReplayEvents.java
@@ -179,7 +179,7 @@ public class TestHRegionReplayEvents {
     when(rss.getExecutorService()).thenReturn(es);
     primaryRegion = HRegion.createHRegion(primaryHri, rootDir, CONF, htd, walPrimary);
     primaryRegion.close();
-    List<Region> regions = new ArrayList<Region>();
+    List<Region> regions = new ArrayList<>();
     regions.add(primaryRegion);
     when(rss.getOnlineRegions()).thenReturn(regions);
 
@@ -1391,7 +1391,7 @@ public class TestHRegionReplayEvents {
 
     // Test case 3: compact primary files
     primaryRegion.compactStores();
-    List<Region> regions = new ArrayList<Region>();
+    List<Region> regions = new ArrayList<>();
     regions.add(primaryRegion);
     when(rss.getOnlineRegions()).thenReturn(regions);
     CompactedHFilesDischarger cleaner = new CompactedHFilesDischarger(100, null, rss, false);
@@ -1486,11 +1486,10 @@ public class TestHRegionReplayEvents {
     random.nextBytes(randomValues);
     Path testPath = TEST_UTIL.getDataTestDirOnTestFS();
 
-    List<Pair<byte[], String>> familyPaths = new ArrayList<Pair<byte[], String>>();
+    List<Pair<byte[], String>> familyPaths = new ArrayList<>();
     int expectedLoadFileCount = 0;
     for (byte[] family : families) {
-      familyPaths.add(new Pair<byte[], String>(family, createHFileForFamilies(testPath, family,
-        randomValues)));
+      familyPaths.add(new Pair<>(family, createHFileForFamilies(testPath, family, randomValues)));
       expectedLoadFileCount++;
     }
     primaryRegion.bulkLoadHFiles(familyPaths, false, null);
@@ -1519,7 +1518,7 @@ public class TestHRegionReplayEvents {
     secondaryRegion.replayWALBulkLoadEventMarker(bulkloadEvent);
 
 
-    List<String> storeFileName = new ArrayList<String>();
+    List<String> storeFileName = new ArrayList<>();
     for (StoreDescriptor storeDesc : bulkloadEvent.getStoresList()) {
       storeFileName.addAll(storeDesc.getStoreFileList());
     }
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestHRegionServerBulkLoad.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestHRegionServerBulkLoad.java
index 398711e..0ac5153 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestHRegionServerBulkLoad.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestHRegionServerBulkLoad.java
@@ -108,7 +108,7 @@ public class TestHRegionServerBulkLoad {
   @Parameters
   public static final Collection<Object[]> parameters() {
     int[] sleepDurations = new int[] { 0, 30000 };
-    List<Object[]> configurations = new ArrayList<Object[]>();
+    List<Object[]> configurations = new ArrayList<>();
     for (int i : sleepDurations) {
       configurations.add(new Object[] { i });
     }
@@ -189,8 +189,7 @@ public class TestHRegionServerBulkLoad {
       // create HFiles for different column families
       FileSystem fs = UTIL.getTestFileSystem();
       byte[] val = Bytes.toBytes(String.format("%010d", iteration));
-      final List<Pair<byte[], String>> famPaths = new ArrayList<Pair<byte[], String>>(
-          NUM_CFS);
+      final List<Pair<byte[], String>> famPaths = new ArrayList<>(NUM_CFS);
       for (int i = 0; i < NUM_CFS; i++) {
         Path hfile = new Path(dir, family(i));
         byte[] fam = Bytes.toBytes(family(i));
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestHRegionServerBulkLoadWithOldClient.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestHRegionServerBulkLoadWithOldClient.java
index f68fda9..7aa1b31 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestHRegionServerBulkLoadWithOldClient.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestHRegionServerBulkLoadWithOldClient.java
@@ -80,8 +80,7 @@ public class TestHRegionServerBulkLoadWithOldClient extends TestHRegionServerBul
       // create HFiles for different column families
       FileSystem fs = UTIL.getTestFileSystem();
       byte[] val = Bytes.toBytes(String.format("%010d", iteration));
-      final List<Pair<byte[], String>> famPaths = new ArrayList<Pair<byte[], String>>(
-          NUM_CFS);
+      final List<Pair<byte[], String>> famPaths = new ArrayList<>(NUM_CFS);
       for (int i = 0; i < NUM_CFS; i++) {
         Path hfile = new Path(dir, family(i));
         byte[] fam = Bytes.toBytes(family(i));
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestJoinedScanners.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestJoinedScanners.java
index 00cc50a..83810f2 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestJoinedScanners.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestJoinedScanners.java
@@ -113,7 +113,7 @@ public class TestJoinedScanners {
 
       byte [] val_large = new byte[valueWidth];
 
-      List<Put> puts = new ArrayList<Put>();
+      List<Put> puts = new ArrayList<>();
 
       for (long i = 0; i < rows_to_insert; i++) {
         Put put = new Put(Bytes.toBytes(Long.toString (i)));
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestKeepDeletes.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestKeepDeletes.java
index 3e32772..d93152a 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestKeepDeletes.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestKeepDeletes.java
@@ -212,7 +212,7 @@ public class TestKeepDeletes {
     s.setRaw(true);
     s.setMaxVersions();
     InternalScanner scan = region.getScanner(s);
-    List<Cell> kvs = new ArrayList<Cell>();
+    List<Cell> kvs = new ArrayList<>();
     scan.next(kvs);
     assertEquals(2, kvs.size());
 
@@ -226,7 +226,7 @@ public class TestKeepDeletes {
     s.setRaw(true);
     s.setMaxVersions();
     scan = region.getScanner(s);
-    kvs = new ArrayList<Cell>();
+    kvs = new ArrayList<>();
     scan.next(kvs);
     assertTrue(kvs.isEmpty());
 
@@ -271,7 +271,7 @@ public class TestKeepDeletes {
     s.setMaxVersions();
     s.setTimeRange(0L, ts+1);
     InternalScanner scanner = region.getScanner(s);
-    List<Cell> kvs = new ArrayList<Cell>();
+    List<Cell> kvs = new ArrayList<>();
     while (scanner.next(kvs))
       ;
     assertTrue(kvs.isEmpty());
@@ -346,7 +346,7 @@ public class TestKeepDeletes {
     s.setRaw(true);
     s.setMaxVersions();
     InternalScanner scan = region.getScanner(s);
-    List<Cell> kvs = new ArrayList<Cell>();
+    List<Cell> kvs = new ArrayList<>();
     scan.next(kvs);
     assertEquals(8, kvs.size());
     assertTrue(CellUtil.isDeleteFamily(kvs.get(0)));
@@ -365,7 +365,7 @@ public class TestKeepDeletes {
     s.setMaxVersions();
     s.setTimeRange(0, 1);
     scan = region.getScanner(s);
-    kvs = new ArrayList<Cell>();
+    kvs = new ArrayList<>();
     scan.next(kvs);
     // nothing in this interval, not even delete markers
     assertTrue(kvs.isEmpty());
@@ -376,7 +376,7 @@ public class TestKeepDeletes {
     s.setMaxVersions();
     s.setTimeRange(0, ts+2);
     scan = region.getScanner(s);
-    kvs = new ArrayList<Cell>();
+    kvs = new ArrayList<>();
     scan.next(kvs);
     assertEquals(4, kvs.size());
     assertTrue(CellUtil.isDeleteFamily(kvs.get(0)));
@@ -391,7 +391,7 @@ public class TestKeepDeletes {
     s.setMaxVersions();
     s.setTimeRange(ts+3, ts+5);
     scan = region.getScanner(s);
-    kvs = new ArrayList<Cell>();
+    kvs = new ArrayList<>();
     scan.next(kvs);
     assertEquals(2, kvs.size());
     assertArrayEquals(CellUtil.cloneValue(kvs.get(0)), T3);
@@ -794,7 +794,7 @@ public class TestKeepDeletes {
     Scan s = new Scan(T1);
     s.setTimeRange(0, ts+1);
     InternalScanner scanner = region.getScanner(s);
-    List<Cell> kvs = new ArrayList<Cell>();
+    List<Cell> kvs = new ArrayList<>();
     scanner.next(kvs);
     assertEquals(4, kvs.size());
     scanner.close();
@@ -802,7 +802,7 @@ public class TestKeepDeletes {
     s = new Scan(T2);
     s.setTimeRange(0, ts+2);
     scanner = region.getScanner(s);
-    kvs = new ArrayList<Cell>();
+    kvs = new ArrayList<>();
     scanner.next(kvs);
     assertEquals(4, kvs.size());
     scanner.close();
@@ -951,7 +951,7 @@ public class TestKeepDeletes {
     // use max versions from the store(s)
     s.setMaxVersions(region.getStores().iterator().next().getScanInfo().getMaxVersions());
     InternalScanner scan = region.getScanner(s);
-    List<Cell> kvs = new ArrayList<Cell>();
+    List<Cell> kvs = new ArrayList<>();
     int res = 0;
     boolean hasMore;
     do {
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestKeyValueHeap.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestKeyValueHeap.java
index b030c74..d574e75 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestKeyValueHeap.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestKeyValueHeap.java
@@ -67,7 +67,7 @@ public class TestKeyValueHeap extends HBaseTestCase {
   TestScanner s2 = new TestScanner(Arrays.asList(kv111, kv112));
   TestScanner s3 = new TestScanner(Arrays.asList(kv113, kv114, kv121, kv122, kv213));
 
-  List<KeyValueScanner> scanners = new ArrayList<KeyValueScanner>(Arrays.asList(s1, s2, s3));
+  List<KeyValueScanner> scanners = new ArrayList<>(Arrays.asList(s1, s2, s3));
 
   /*
    * Uses {@code scanners} to build a KeyValueHeap, iterates over it and asserts that returned
@@ -136,7 +136,7 @@ public class TestKeyValueHeap extends HBaseTestCase {
   public void testScannerLeak() throws IOException {
     // Test for unclosed scanners (HBASE-1927)
 
-    TestScanner s4 = new TestScanner(new ArrayList<Cell>());
+    TestScanner s4 = new TestScanner(new ArrayList<>());
     scanners.add(s4);
 
     //Creating KeyValueHeap
@@ -163,9 +163,9 @@ public class TestKeyValueHeap extends HBaseTestCase {
     TestScanner s1 = new SeekTestScanner(Arrays.asList(kv115, kv211, kv212));
     TestScanner s2 = new SeekTestScanner(Arrays.asList(kv111, kv112));
     TestScanner s3 = new SeekTestScanner(Arrays.asList(kv113, kv114, kv121, kv122, kv213));
-    TestScanner s4 = new SeekTestScanner(new ArrayList<Cell>());
+    TestScanner s4 = new SeekTestScanner(new ArrayList<>());
 
-    List<KeyValueScanner> scanners = new ArrayList<KeyValueScanner>(Arrays.asList(s1, s2, s3, s4));
+    List<KeyValueScanner> scanners = new ArrayList<>(Arrays.asList(s1, s2, s3, s4));
 
     // Creating KeyValueHeap
     KeyValueHeap kvh = new KeyValueHeap(scanners, CellComparator.COMPARATOR);
@@ -197,13 +197,13 @@ public class TestKeyValueHeap extends HBaseTestCase {
       TestScanner scan1 = new TestScanner(Arrays.asList(kv111, kv112, kv113A), 1);
       TestScanner scan2 = new TestScanner(Arrays.asList(kv113B), 2);
       List<Cell> expected = Arrays.asList(kv111, kv112, kv113B, kv113A);
-      assertCells(expected, new ArrayList<KeyValueScanner>(Arrays.asList(scan1, scan2)));
+      assertCells(expected, new ArrayList<>(Arrays.asList(scan1, scan2)));
     }
     {
       TestScanner scan1 = new TestScanner(Arrays.asList(kv111, kv112, kv113A), 2);
       TestScanner scan2 = new TestScanner(Arrays.asList(kv113B), 1);
       List<Cell> expected = Arrays.asList(kv111, kv112, kv113A, kv113B);
-      assertCells(expected, new ArrayList<KeyValueScanner>(Arrays.asList(scan1, scan2)));
+      assertCells(expected, new ArrayList<>(Arrays.asList(scan1, scan2)));
     }
   }
 
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestMajorCompaction.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestMajorCompaction.java
index edc46ca..9d00d38 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestMajorCompaction.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestMajorCompaction.java
@@ -137,7 +137,7 @@ public class TestMajorCompaction {
     // Now delete everything.
     InternalScanner s = r.getScanner(new Scan());
     do {
-      List<Cell> results = new ArrayList<Cell>();
+      List<Cell> results = new ArrayList<>();
       boolean result = s.next(results);
       r.delete(new Delete(CellUtil.cloneRow(results.get(0))));
       if (!result) break;
@@ -150,7 +150,7 @@ public class TestMajorCompaction {
     s = r.getScanner(new Scan());
     int counter = 0;
     do {
-      List<Cell> results = new ArrayList<Cell>();
+      List<Cell> results = new ArrayList<>();
       boolean result = s.next(results);
       if (!result) break;
       counter++;
@@ -180,8 +180,7 @@ public class TestMajorCompaction {
 
   public void majorCompactionWithDataBlockEncoding(boolean inCacheOnly)
       throws Exception {
-    Map<Store, HFileDataBlockEncoder> replaceBlockCache =
-        new HashMap<Store, HFileDataBlockEncoder>();
+    Map<Store, HFileDataBlockEncoder> replaceBlockCache = new HashMap<>();
     for (Store store : r.getStores()) {
       HFileDataBlockEncoder blockEncoder = store.getDataBlockEncoder();
       replaceBlockCache.put(store, blockEncoder);
@@ -461,7 +460,7 @@ public class TestMajorCompaction {
     scan.setReversed(true);
     InternalScanner s = r.getScanner(scan);
     do {
-      List<Cell> results = new ArrayList<Cell>();
+      List<Cell> results = new ArrayList<>();
       boolean result = s.next(results);
       assertTrue(!results.isEmpty());
       r.delete(new Delete(CellUtil.cloneRow(results.get(0))));
@@ -477,7 +476,7 @@ public class TestMajorCompaction {
     s = r.getScanner(scan);
     int counter = 0;
     do {
-      List<Cell> results = new ArrayList<Cell>();
+      List<Cell> results = new ArrayList<>();
       boolean result = s.next(results);
       if (!result) break;
       counter++;
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestMemStoreLAB.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestMemStoreLAB.java
index c106c04..141b802 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestMemStoreLAB.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestMemStoreLAB.java
@@ -197,7 +197,7 @@ public class TestMemStoreLAB {
     MemStoreChunkPool.clearDisableFlag();
     mslab = new MemStoreLABImpl(conf);
     // launch multiple threads to trigger frequent chunk retirement
-    List<Thread> threads = new ArrayList<Thread>();
+    List<Thread> threads = new ArrayList<>();
     final KeyValue kv = new KeyValue(Bytes.toBytes("r"), Bytes.toBytes("f"), Bytes.toBytes("q"),
         new byte[MemStoreLABImpl.MAX_ALLOC_DEFAULT - 24]);
     for (int i = 0; i < 10; i++) {
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestMinVersions.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestMinVersions.java
index 661583e..52b5a40 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestMinVersions.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestMinVersions.java
@@ -427,7 +427,7 @@ public class TestMinVersions {
       p.addColumn(c1, c1, T3);
       region.put(p);
 
-      List<Long> tss = new ArrayList<Long>();
+      List<Long> tss = new ArrayList<>();
       tss.add(ts-1);
       tss.add(ts-2);
 
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestMiniBatchOperationInProgress.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestMiniBatchOperationInProgress.java
index 15931c6..7406e47 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestMiniBatchOperationInProgress.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestMiniBatchOperationInProgress.java
@@ -40,10 +40,10 @@ public class TestMiniBatchOperationInProgress {
     OperationStatus[] retCodeDetails = new OperationStatus[10];
     WALEdit[] walEditsFromCoprocessors = new WALEdit[10];
     for (int i = 0; i < 10; i++) {
-      operations[i] = new Pair<Mutation, Integer>(new Put(Bytes.toBytes(i)), null);
+      operations[i] = new Pair<>(new Put(Bytes.toBytes(i)), null);
     }
     MiniBatchOperationInProgress<Pair<Mutation, Integer>> miniBatch = 
-      new MiniBatchOperationInProgress<Pair<Mutation, Integer>>(operations, retCodeDetails, 
+      new MiniBatchOperationInProgress<>(operations, retCodeDetails,
       walEditsFromCoprocessors, 0, 5);
 
     assertEquals(5, miniBatch.size());
@@ -68,7 +68,7 @@ public class TestMiniBatchOperationInProgress {
     } catch (ArrayIndexOutOfBoundsException e) {
     }
 
-    miniBatch = new MiniBatchOperationInProgress<Pair<Mutation, Integer>>(operations,
+    miniBatch = new MiniBatchOperationInProgress<>(operations,
         retCodeDetails, walEditsFromCoprocessors, 7, 10);
     try {
       miniBatch.setWalEdit(-1, new WALEdit());
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestMultiColumnScanner.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestMultiColumnScanner.java
index ff6f09b..8afdec9 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestMultiColumnScanner.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestMultiColumnScanner.java
@@ -124,7 +124,7 @@ public class TestMultiColumnScanner {
 
   @Parameters
   public static final Collection<Object[]> parameters() {
-    List<Object[]> parameters = new ArrayList<Object[]>();
+    List<Object[]> parameters = new ArrayList<>();
     for (Object[] bloomAndCompressionParams :
         HBaseTestingUtility.BLOOM_AND_COMPRESSION_COMBINATIONS) {
       for (boolean useDataBlockEncoding : new boolean[]{false, true}) {
@@ -154,15 +154,15 @@ public class TestMultiColumnScanner {
     );
     List<String> rows = sequentialStrings("row", NUM_ROWS);
     List<String> qualifiers = sequentialStrings("qual", NUM_COLUMNS);
-    List<KeyValue> kvs = new ArrayList<KeyValue>();
-    Set<String> keySet = new HashSet<String>();
+    List<KeyValue> kvs = new ArrayList<>();
+    Set<String> keySet = new HashSet<>();
 
     // A map from <row>_<qualifier> to the most recent delete timestamp for
     // that column.
-    Map<String, Long> lastDelTimeMap = new HashMap<String, Long>();
+    Map<String, Long> lastDelTimeMap = new HashMap<>();
 
     Random rand = new Random(29372937L);
-    Set<String> rowQualSkip = new HashSet<String>();
+    Set<String> rowQualSkip = new HashSet<>();
 
     // Skip some columns in some rows. We need to test scanning over a set
     // of columns when some of the columns are not there.
@@ -228,7 +228,7 @@ public class TestMultiColumnScanner {
       for (int columnBitMask = 1; columnBitMask <= MAX_COLUMN_BIT_MASK; ++columnBitMask) {
         Scan scan = new Scan();
         scan.setMaxVersions(maxVersions);
-        Set<String> qualSet = new TreeSet<String>();
+        Set<String> qualSet = new TreeSet<>();
         {
           int columnMaskTmp = columnBitMask;
           for (String qual : qualifiers) {
@@ -242,7 +242,7 @@ public class TestMultiColumnScanner {
         }
 
         InternalScanner scanner = region.getScanner(scan);
-        List<Cell> results = new ArrayList<Cell>();
+        List<Cell> results = new ArrayList<>();
 
         int kvPos = 0;
         int numResults = 0;
@@ -317,7 +317,7 @@ public class TestMultiColumnScanner {
   }
 
   private static List<String> sequentialStrings(String prefix, int n) {
-    List<String> lst = new ArrayList<String>();
+    List<String> lst = new ArrayList<>();
     for (int i = 0; i < n; ++i) {
       StringBuilder sb = new StringBuilder();
       sb.append(prefix + i);
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestRegionFavoredNodes.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestRegionFavoredNodes.java
index 67be032..d8f75ed 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestRegionFavoredNodes.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestRegionFavoredNodes.java
@@ -117,8 +117,7 @@ public class TestRegionFavoredNodes {
       List<Region> regions = server.getOnlineRegions(TABLE_NAME);
       for (Region region : regions) {
         List<org.apache.hadoop.hbase.shaded.protobuf.generated.HBaseProtos.ServerName>favoredNodes =
-            new ArrayList<org.apache.hadoop.hbase.shaded.protobuf.generated.HBaseProtos.
-            ServerName>(3);
+            new ArrayList<>(3);
         String encodedRegionName = region.getRegionInfo().getEncodedName();
         for (int j = 0; j < FAVORED_NODES_NUM; j++) {
           org.apache.hadoop.hbase.shaded.protobuf.generated.HBaseProtos.ServerName.Builder b =
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestRegionIncrement.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestRegionIncrement.java
index 1583bf8..5d11c0e 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestRegionIncrement.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestRegionIncrement.java
@@ -193,7 +193,7 @@ public class TestRegionIncrement {
         threads[i].join();
       }
       RegionScanner regionScanner = region.getScanner(new Scan());
-      List<Cell> cells = new ArrayList<Cell>(THREAD_COUNT);
+      List<Cell> cells = new ArrayList<>(THREAD_COUNT);
       while(regionScanner.next(cells)) continue;
       assertEquals(THREAD_COUNT, cells.size());
       long total = 0;
@@ -230,7 +230,7 @@ public class TestRegionIncrement {
         threads[i].join();
       }
       RegionScanner regionScanner = region.getScanner(new Scan());
-      List<Cell> cells = new ArrayList<Cell>(100);
+      List<Cell> cells = new ArrayList<>(100);
       while(regionScanner.next(cells)) continue;
       assertEquals(THREAD_COUNT, cells.size());
       long total = 0;
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestRegionMergeTransactionOnCluster.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestRegionMergeTransactionOnCluster.java
index 3e6d180..358aabd 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestRegionMergeTransactionOnCluster.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestRegionMergeTransactionOnCluster.java
@@ -382,11 +382,11 @@ public class TestRegionMergeTransactionOnCluster {
     List<Pair<HRegionInfo, ServerName>> currentRegionToServers =
         MetaTableAccessor.getTableRegionsAndLocations(
             TEST_UTIL.getConnection(), tableName);
-    List<HRegionInfo> initialRegions = new ArrayList<HRegionInfo>();
+    List<HRegionInfo> initialRegions = new ArrayList<>();
     for (Pair<HRegionInfo, ServerName> p : initialRegionToServers) {
       initialRegions.add(p.getFirst());
     }
-    List<HRegionInfo> currentRegions = new ArrayList<HRegionInfo>();
+    List<HRegionInfo> currentRegions = new ArrayList<>();
     for (Pair<HRegionInfo, ServerName> p : currentRegionToServers) {
       currentRegions.add(p.getFirst());
     }
@@ -427,7 +427,7 @@ public class TestRegionMergeTransactionOnCluster {
     ADMIN.mergeRegionsAsync(
       regionA.getEncodedNameAsBytes(),
       regionB.getEncodedNameAsBytes(), false);
-    return new PairOfSameType<HRegionInfo>(regionA, regionB);
+    return new PairOfSameType<>(regionA, regionB);
   }
 
   private void waitAndVerifyRegionNum(HMaster master, TableName tablename,
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestRegionReplicaFailover.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestRegionReplicaFailover.java
index abe6c6c..69b7581 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestRegionReplicaFailover.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestRegionReplicaFailover.java
@@ -283,7 +283,7 @@ public class TestRegionReplicaFailover {
       admin.flush(table.getName());
       HTU.loadNumericRows(table, fam, 1000, 2000);
 
-      final AtomicReference<Throwable> ex = new AtomicReference<Throwable>(null);
+      final AtomicReference<Throwable> ex = new AtomicReference<>(null);
       final AtomicBoolean done = new AtomicBoolean(false);
       final AtomicInteger key = new AtomicInteger(2000);
 
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestRegionReplicas.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestRegionReplicas.java
index 9166101..642cc14 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestRegionReplicas.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestRegionReplicas.java
@@ -329,7 +329,7 @@ public class TestRegionReplicas {
       @SuppressWarnings("unchecked")
       final AtomicReference<Exception>[] exceptions = new AtomicReference[3];
       for (int i=0; i < exceptions.length; i++) {
-        exceptions[i] = new AtomicReference<Exception>();
+        exceptions[i] = new AtomicReference<>();
       }
 
       Runnable writer = new Runnable() {
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestRegionSplitPolicy.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestRegionSplitPolicy.java
index b2909e2..89f7589 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestRegionSplitPolicy.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestRegionSplitPolicy.java
@@ -63,7 +63,7 @@ public class TestRegionSplitPolicy {
     mockRegion = Mockito.mock(HRegion.class);
     Mockito.doReturn(htd).when(mockRegion).getTableDesc();
     Mockito.doReturn(hri).when(mockRegion).getRegionInfo();
-    stores = new ArrayList<Store>();
+    stores = new ArrayList<>();
     Mockito.doReturn(stores).when(mockRegion).getStores();
   }
 
@@ -103,7 +103,7 @@ public class TestRegionSplitPolicy {
     // Now make it so the mock region has a RegionServerService that will
     // return 'online regions'.
     RegionServerServices rss = Mockito.mock(RegionServerServices.class);
-    final List<Region> regions = new ArrayList<Region>();
+    final List<Region> regions = new ArrayList<>();
     Mockito.when(rss.getOnlineRegions(TABLENAME)).thenReturn(regions);
     Mockito.when(mockRegion.getRegionServerServices()).thenReturn(rss);
     // Set max size for this 'table'.
@@ -162,7 +162,7 @@ public class TestRegionSplitPolicy {
     conf.setFloat("hbase.busy.policy.blockedRequests", 0.1f);
 
     RegionServerServices rss  = Mockito.mock(RegionServerServices.class);
-    final List<Region> regions = new ArrayList<Region>();
+    final List<Region> regions = new ArrayList<>();
     Mockito.when(rss.getOnlineRegions(TABLENAME)).thenReturn(regions);
     Mockito.when(mockRegion.getRegionServerServices()).thenReturn(rss);
     Mockito.when(mockRegion.getBlockedRequestsCount()).thenReturn(0L);
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestReversibleScanners.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestReversibleScanners.java
index ea16edf..69965ba 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestReversibleScanners.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestReversibleScanners.java
@@ -435,7 +435,7 @@ public class TestReversibleScanners {
   private void verifyCountAndOrder(InternalScanner scanner,
       int expectedKVCount, int expectedRowCount, boolean forward)
       throws IOException {
-    List<Cell> kvList = new ArrayList<Cell>();
+    List<Cell> kvList = new ArrayList<>();
     Result lastResult = null;
     int rowCount = 0;
     int kvCount = 0;
@@ -502,8 +502,7 @@ public class TestReversibleScanners {
         .getScannersForStoreFiles(Lists.newArrayList(sf1, sf2), false, true,
             false, false, readPoint);
     List<KeyValueScanner> memScanners = memstore.getScanners(readPoint);
-    List<KeyValueScanner> scanners = new ArrayList<KeyValueScanner>(
-        fileScanners.size() + 1);
+    List<KeyValueScanner> scanners = new ArrayList<>(fileScanners.size() + 1);
     scanners.addAll(fileScanners);
     scanners.addAll(memScanners);
 
@@ -611,7 +610,7 @@ public class TestReversibleScanners {
     for (int i = startRowNum; i >= 0; i--) {
       for (int j = (i == startRowNum ? startQualNum : 0); j < QUALSIZE; j++) {
         if (makeMVCC(i, j) <= readPoint) {
-          nextReadableNum = new Pair<Integer, Integer>(i, j);
+          nextReadableNum = new Pair<>(i, j);
           findExpected = true;
           break;
         }
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestSCVFWithMiniCluster.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestSCVFWithMiniCluster.java
index 8a31af8..4cf2964 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestSCVFWithMiniCluster.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestSCVFWithMiniCluster.java
@@ -85,7 +85,7 @@ public class TestSCVFWithMiniCluster {
     htable = util.getConnection().getTable(HBASE_TABLE_NAME);
 
     /* Add some values */
-    List<Put> puts = new ArrayList<Put>();
+    List<Put> puts = new ArrayList<>();
 
     /* Add a row with 'a:foo' = false */
     Put put = new Put(Bytes.toBytes("1"));
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestScanWithBloomError.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestScanWithBloomError.java
index 027193f..72267be 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestScanWithBloomError.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestScanWithBloomError.java
@@ -74,7 +74,7 @@ public class TestScanWithBloomError {
   private static final String ROW = "theRow";
   private static final String QUALIFIER_PREFIX = "qual";
   private static final byte[] ROW_BYTES = Bytes.toBytes(ROW);
-  private static NavigableSet<Integer> allColIds = new TreeSet<Integer>();
+  private static NavigableSet<Integer> allColIds = new TreeSet<>();
   private Region region;
   private BloomType bloomType;
   private FileSystem fs;
@@ -84,7 +84,7 @@ public class TestScanWithBloomError {
 
   @Parameters
   public static final Collection<Object[]> parameters() {
-    List<Object[]> configurations = new ArrayList<Object[]>();
+    List<Object[]> configurations = new ArrayList<>();
     for (BloomType bloomType : BloomType.values()) {
       configurations.add(new Object[] { bloomType });
     }
@@ -160,24 +160,24 @@ public class TestScanWithBloomError {
         + lastStoreFileReader.getHFileReader().getName());
     lastStoreFileReader.disableBloomFilterForTesting();
 
-    List<Cell> allResults = new ArrayList<Cell>();
+    List<Cell> allResults = new ArrayList<>();
 
     { // Limit the scope of results.
-      List<Cell> results = new ArrayList<Cell>();
+      List<Cell> results = new ArrayList<>();
       while (scanner.next(results) || results.size() > 0) {
         allResults.addAll(results);
         results.clear();
       }
     }
 
-    List<Integer> actualIds = new ArrayList<Integer>();
+    List<Integer> actualIds = new ArrayList<>();
     for (Cell kv : allResults) {
       String qual = Bytes.toString(CellUtil.cloneQualifier(kv));
       assertTrue(qual.startsWith(QUALIFIER_PREFIX));
       actualIds.add(Integer.valueOf(qual.substring(
           QUALIFIER_PREFIX.length())));
     }
-    List<Integer> expectedIds = new ArrayList<Integer>();
+    List<Integer> expectedIds = new ArrayList<>();
     for (int expectedId : expectedResultCols)
       expectedIds.add(expectedId);
 
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestScanner.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestScanner.java
index 1b42754..d9fd3de 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestScanner.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestScanner.java
@@ -132,7 +132,7 @@ public class TestScanner {
     try {
       this.region = TEST_UTIL.createLocalHRegion(TESTTABLEDESC, null, null);
       HBaseTestCase.addContent(this.region, HConstants.CATALOG_FAMILY);
-      List<Cell> results = new ArrayList<Cell>();
+      List<Cell> results = new ArrayList<>();
       // Do simple test of getting one row only first.
       Scan scan = new Scan(Bytes.toBytes("abc"), Bytes.toBytes("abd"));
       scan.addFamily(HConstants.CATALOG_FAMILY);
@@ -151,7 +151,7 @@ public class TestScanner {
       s = region.getScanner(scan);
       count = 0;
       Cell kv = null;
-      results = new ArrayList<Cell>();
+      results = new ArrayList<>();
       for (boolean first = true; s.next(results);) {
         kv = results.get(0);
         if (first) {
@@ -170,7 +170,7 @@ public class TestScanner {
   }
 
   void rowPrefixFilter(Scan scan) throws IOException {
-    List<Cell> results = new ArrayList<Cell>();
+    List<Cell> results = new ArrayList<>();
     scan.addFamily(HConstants.CATALOG_FAMILY);
     InternalScanner s = region.getScanner(scan);
     boolean hasMore = true;
@@ -186,7 +186,7 @@ public class TestScanner {
   }
 
   void rowInclusiveStopFilter(Scan scan, byte[] stopRow) throws IOException {
-    List<Cell> results = new ArrayList<Cell>();
+    List<Cell> results = new ArrayList<>();
     scan.addFamily(HConstants.CATALOG_FAMILY);
     InternalScanner s = region.getScanner(scan);
     boolean hasMore = true;
@@ -234,7 +234,7 @@ public class TestScanner {
       HBaseTestCase.addContent(this.region, HConstants.CATALOG_FAMILY);
       Scan scan = new Scan();
       InternalScanner s = region.getScanner(scan);
-      List<Cell> results = new ArrayList<Cell>();
+      List<Cell> results = new ArrayList<>();
       try {
         s.next(results);
         s.close();
@@ -376,7 +376,7 @@ public class TestScanner {
   throws IOException {
     InternalScanner scanner = null;
     Scan scan = null;
-    List<Cell> results = new ArrayList<Cell>();
+    List<Cell> results = new ArrayList<>();
     byte [][][] scanColumns = {
         COLS,
         EXPLICIT_COLS
@@ -540,7 +540,7 @@ public class TestScanner {
       // run a major compact, column1 of firstRow will be cleaned.
       region.compact(true);
 
-      List<Cell> results = new ArrayList<Cell>();
+      List<Cell> results = new ArrayList<>();
       s.next(results);
 
       // make sure returns column2 of firstRow
@@ -549,7 +549,7 @@ public class TestScanner {
       assertTrue(CellUtil.matchingRow(results.get(0), firstRowBytes)); 
       assertTrue(CellUtil.matchingFamily(results.get(0), fam2));
 
-      results = new ArrayList<Cell>();
+      results = new ArrayList<>();
       s.next(results);
 
       // get secondRow
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestScannerRetriableFailure.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestScannerRetriableFailure.java
index ac33f15..9bda019 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestScannerRetriableFailure.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestScannerRetriableFailure.java
@@ -128,7 +128,7 @@ public class TestScannerRetriableFailure {
   }
 
   public void loadTable(final Table table, int numRows) throws IOException {
-    List<Put> puts = new ArrayList<Put>(numRows);
+    List<Put> puts = new ArrayList<>(numRows);
     for (int i = 0; i < numRows; ++i) {
       byte[] row = Bytes.toBytes(String.format("%09d", i));
       Put put = new Put(row);
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestSeekOptimizations.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestSeekOptimizations.java
index b31be9d..67f6f34 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestSeekOptimizations.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestSeekOptimizations.java
@@ -111,9 +111,9 @@ public class TestSeekOptimizations {
   private Put put;
   private Delete del;
   private Random rand;
-  private Set<Long> putTimestamps = new HashSet<Long>();
-  private Set<Long> delTimestamps = new HashSet<Long>();
-  private List<Cell> expectedKVs = new ArrayList<Cell>();
+  private Set<Long> putTimestamps = new HashSet<>();
+  private Set<Long> delTimestamps = new HashSet<>();
+  private List<Cell> expectedKVs = new ArrayList<>();
 
   private Compression.Algorithm comprAlgo;
   private BloomType bloomType;
@@ -199,7 +199,7 @@ public class TestSeekOptimizations {
       throws IOException {
     StoreScanner.enableLazySeekGlobally(lazySeekEnabled);
     final Scan scan = new Scan();
-    final Set<String> qualSet = new HashSet<String>();
+    final Set<String> qualSet = new HashSet<>();
     for (int iColumn : columnArr) {
       String qualStr = getQualStr(iColumn);
       scan.addColumn(FAMILY_BYTES, Bytes.toBytes(qualStr));
@@ -217,8 +217,8 @@ public class TestSeekOptimizations {
 
     final long initialSeekCount = StoreFileScanner.getSeekCount();
     final InternalScanner scanner = region.getScanner(scan);
-    final List<Cell> results = new ArrayList<Cell>();
-    final List<Cell> actualKVs = new ArrayList<Cell>();
+    final List<Cell> results = new ArrayList<>();
+    final List<Cell> actualKVs = new ArrayList<>();
 
     // Such a clumsy do-while loop appears to be the official way to use an
     // internalScanner. scanner.next() return value refers to the _next_
@@ -260,8 +260,8 @@ public class TestSeekOptimizations {
 
   private List<Cell> filterExpectedResults(Set<String> qualSet,
       byte[] startRow, byte[] endRow, int maxVersions) {
-    final List<Cell> filteredKVs = new ArrayList<Cell>();
-    final Map<String, Integer> verCount = new HashMap<String, Integer>();
+    final List<Cell> filteredKVs = new ArrayList<>();
+    final Map<String, Integer> verCount = new HashMap<>();
     for (Cell kv : expectedKVs) {
       if (startRow.length > 0 &&
           Bytes.compareTo(kv.getRowArray(), kv.getRowOffset(), kv.getRowLength(),
@@ -297,7 +297,7 @@ public class TestSeekOptimizations {
   }
 
   private void prepareExpectedKVs(long latestDelTS) {
-    final List<Cell> filteredKVs = new ArrayList<Cell>();
+    final List<Cell> filteredKVs = new ArrayList<>();
     for (Cell kv : expectedKVs) {
       if (kv.getTimestamp() > latestDelTS || latestDelTS == -1) {
         filteredKVs.add(kv);
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestStore.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestStore.java
index 46c1dd5..4c09810 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestStore.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestStore.java
@@ -112,11 +112,10 @@ public class TestStore {
   byte [] qf5 = Bytes.toBytes("qf5");
   byte [] qf6 = Bytes.toBytes("qf6");
 
-  NavigableSet<byte[]> qualifiers =
-    new ConcurrentSkipListSet<byte[]>(Bytes.BYTES_COMPARATOR);
+  NavigableSet<byte[]> qualifiers = new ConcurrentSkipListSet<>(Bytes.BYTES_COMPARATOR);
 
-  List<Cell> expected = new ArrayList<Cell>();
-  List<Cell> result = new ArrayList<Cell>();
+  List<Cell> expected = new ArrayList<>();
+  List<Cell> result = new ArrayList<>();
 
   long id = System.currentTimeMillis();
   Get get = new Get(row);
@@ -624,8 +623,7 @@ public class TestStore {
    * only; thereafter it will succeed.  Used by {@link TestHRegion} too.
    */
   static class FaultyFileSystem extends FilterFileSystem {
-    List<SoftReference<FaultyOutputStream>> outStreams =
-      new ArrayList<SoftReference<FaultyOutputStream>>();
+    List<SoftReference<FaultyOutputStream>> outStreams = new ArrayList<>();
     private long faultPos = 200;
     AtomicBoolean fault = new AtomicBoolean(true);
 
@@ -699,7 +697,7 @@ public class TestStore {
    */
   List<Cell> getKeyValueSet(long[] timestamps, int numRows,
       byte[] qualifier, byte[] family) {
-    List<Cell> kvList = new ArrayList<Cell>();
+    List<Cell> kvList = new ArrayList<>();
     for (int i=1;i<=numRows;i++) {
       byte[] b = Bytes.toBytes(i);
       for (long timestamp: timestamps) {
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestStoreFile.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestStoreFile.java
index 0a8dbc4..7e4ebd8 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestStoreFile.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestStoreFile.java
@@ -512,7 +512,7 @@ public class TestStoreFile extends HBaseTestCase {
     int falseNeg = 0;
     for (int i = 0; i < 2000; i++) {
       String row = String.format(localFormatter, i);
-      TreeSet<byte[]> columns = new TreeSet<byte[]>(Bytes.BYTES_COMPARATOR);
+      TreeSet<byte[]> columns = new TreeSet<>(Bytes.BYTES_COMPARATOR);
       columns.add("family:col".getBytes());
 
       Scan scan = new Scan(row.getBytes(),row.getBytes());
@@ -712,7 +712,7 @@ public class TestStoreFile extends HBaseTestCase {
         for (int j = 0; j < colCount*2; ++j) {   // column qualifiers
           String row = String.format(localFormatter, i);
           String col = String.format(localFormatter, j);
-          TreeSet<byte[]> columns = new TreeSet<byte[]>(Bytes.BYTES_COMPARATOR);
+          TreeSet<byte[]> columns = new TreeSet<>(Bytes.BYTES_COMPARATOR);
           columns.add(("col" + col).getBytes());
 
           Scan scan = new Scan(row.getBytes(),row.getBytes());
@@ -799,7 +799,7 @@ public class TestStoreFile extends HBaseTestCase {
    */
   List<KeyValue> getKeyValueSet(long[] timestamps, int numRows,
       byte[] qualifier, byte[] family) {
-    List<KeyValue> kvList = new ArrayList<KeyValue>();
+    List<KeyValue> kvList = new ArrayList<>();
     for (int i=1;i<=numRows;i++) {
       byte[] b = Bytes.toBytes(i) ;
       LOG.info(Bytes.toString(b));
@@ -851,7 +851,7 @@ public class TestStoreFile extends HBaseTestCase {
     when(store.getFamily()).thenReturn(hcd);
     StoreFileReader reader = hsf.createReader();
     StoreFileScanner scanner = getStoreFileScanner(reader, false, false);
-    TreeSet<byte[]> columns = new TreeSet<byte[]>(Bytes.BYTES_COMPARATOR);
+    TreeSet<byte[]> columns = new TreeSet<>(Bytes.BYTES_COMPARATOR);
     columns.add(qualifier);
 
     scan.setTimeRange(20, 100);
@@ -1019,7 +1019,7 @@ public class TestStoreFile extends HBaseTestCase {
   throws IOException {
     // Let's put ~5 small KVs in each block, so let's make 5*numBlocks KVs
     int numKVs = 5 * numBlocks;
-    List<KeyValue> kvs = new ArrayList<KeyValue>(numKVs);
+    List<KeyValue> kvs = new ArrayList<>(numKVs);
     byte [] b = Bytes.toBytes("x");
     int totalSize = 0;
     for (int i=numKVs;i>0;i--) {
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestStoreFileRefresherChore.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestStoreFileRefresherChore.java
index d96fd9f..3cdb227 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestStoreFileRefresherChore.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestStoreFileRefresherChore.java
@@ -170,7 +170,7 @@ public class TestStoreFileRefresherChore {
     byte[] qf = Bytes.toBytes("cq");
 
     HRegionServer regionServer = mock(HRegionServer.class);
-    List<Region> regions = new ArrayList<Region>();
+    List<Region> regions = new ArrayList<>();
     when(regionServer.getOnlineRegionsLocalContext()).thenReturn(regions);
     when(regionServer.getConfiguration()).thenReturn(TEST_UTIL.getConfiguration());
 
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestStoreScanner.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestStoreScanner.java
index 52efe63..ccbf067 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestStoreScanner.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestStoreScanner.java
@@ -173,7 +173,7 @@ public class TestStoreScanner {
    * @return
    */
   NavigableSet<byte[]> getCols(String ...strCols) {
-    NavigableSet<byte[]> cols = new TreeSet<byte[]>(Bytes.BYTES_COMPARATOR);
+    NavigableSet<byte[]> cols = new TreeSet<>(Bytes.BYTES_COMPARATOR);
     for (String col : strCols) {
       byte[] bytes = Bytes.toBytes(col);
       cols.add(bytes);
@@ -189,7 +189,7 @@ public class TestStoreScanner {
     Scan scan = new Scan(get);
     CellGridStoreScanner scanner = new CellGridStoreScanner(scan, this.scanInfo, this.scanType);
     try {
-      List<Cell> results = new ArrayList<Cell>();
+      List<Cell> results = new ArrayList<>();
       while (scanner.next(results)) {
         continue;
       }
@@ -213,7 +213,7 @@ public class TestStoreScanner {
     Scan scan = new Scan(get);
     CellGridStoreScanner scanner = new CellGridStoreScanner(scan, this.scanInfo, this.scanType);
     try {
-      List<Cell> results = new ArrayList<Cell>();
+      List<Cell> results = new ArrayList<>();
       while (scanner.next(results)) {
         continue;
       }
@@ -242,7 +242,7 @@ public class TestStoreScanner {
     scan.addColumn(CF, ONE);
     CellGridStoreScanner scanner = new CellGridStoreScanner(scan, this.scanInfo, this.scanType);
     try {
-      List<Cell> results = new ArrayList<Cell>();
+      List<Cell> results = new ArrayList<>();
       while (scanner.next(results)) {
         continue;
       }
@@ -276,7 +276,7 @@ public class TestStoreScanner {
     Scan scan = new Scan(get);
     CellGridStoreScanner scanner = new CellGridStoreScanner(scan, this.scanInfo, this.scanType);
     try {
-      List<Cell> results = new ArrayList<Cell>();
+      List<Cell> results = new ArrayList<>();
       // For a Get there should be no more next's after the first call.
       Assert.assertEquals(false, scanner.next(results));
       // Should be one result only.
@@ -307,7 +307,7 @@ public class TestStoreScanner {
     Scan scan = new Scan(get);
     CellGridStoreScanner scanner = new CellGridStoreScanner(scan, this.scanInfo, this.scanType);
     try {
-      List<Cell> results = new ArrayList<Cell>();
+      List<Cell> results = new ArrayList<>();
       // For a Get there should be no more next's after the first call.
       Assert.assertEquals(false, scanner.next(results));
       // Should be one result only.
@@ -341,7 +341,7 @@ public class TestStoreScanner {
     List<Cell> results = null;
     try (StoreScanner scan =
         new StoreScanner(scanSpec, scanInfo, scanType, getCols("a"), scanners)) {
-      results = new ArrayList<Cell>();
+      results = new ArrayList<>();
       Assert.assertEquals(true, scan.next(results));
       Assert.assertEquals(5, results.size());
       Assert.assertEquals(kvs[kvs.length - 1], results.get(0));
@@ -352,7 +352,7 @@ public class TestStoreScanner {
     scanSpec.setMaxVersions();
     try (StoreScanner scan =
         new StoreScanner(scanSpec, scanInfo, scanType, getCols("a"), scanners)) {
-      results = new ArrayList<Cell>();
+      results = new ArrayList<>();
       Assert.assertEquals(true, scan.next(results));
       Assert.assertEquals(2, results.size());
     }
@@ -362,7 +362,7 @@ public class TestStoreScanner {
     scanSpec.setMaxVersions();
     try (StoreScanner scan =
         new StoreScanner(scanSpec, scanInfo, scanType, getCols("a"), scanners)) {
-      results = new ArrayList<Cell>();
+      results = new ArrayList<>();
       Assert.assertEquals(true, scan.next(results));
       Assert.assertEquals(1, results.size());
     }
@@ -373,7 +373,7 @@ public class TestStoreScanner {
     scanSpec.setMaxVersions(3);
     try (StoreScanner scan = new StoreScanner(scanSpec, scanInfo, scanType, getCols("a"),
         scanners)) {
-      results = new ArrayList<Cell>();
+      results = new ArrayList<>();
       Assert.assertEquals(true, scan.next(results));
       Assert.assertEquals(3, results.size());
     }
@@ -395,7 +395,7 @@ public class TestStoreScanner {
     // this only uses maxVersions (default=1) and TimeRange (default=all)
     try (StoreScanner scan =
         new StoreScanner(scanSpec, scanInfo, scanType, getCols("a"), scanners)) {
-      List<Cell> results = new ArrayList<Cell>();
+      List<Cell> results = new ArrayList<>();
       Assert.assertEquals(true, scan.next(results));
       Assert.assertEquals(1, results.size());
       Assert.assertEquals(kvs[0], results.get(0));
@@ -423,7 +423,7 @@ public class TestStoreScanner {
     // this only uses maxVersions (default=1) and TimeRange (default=all)
     try (StoreScanner scan =
         new StoreScanner(scanSpec, scanInfo, scanType, getCols("a"), scanners)) {
-      List<Cell> results = new ArrayList<Cell>();
+      List<Cell> results = new ArrayList<>();
       scan.next(results);
       Assert.assertEquals(1, results.size());
       Assert.assertEquals(kvs[0], results.get(0));
@@ -451,7 +451,7 @@ public class TestStoreScanner {
     Scan scanSpec = new Scan(Bytes.toBytes("R1"));
     try (StoreScanner scan =
         new StoreScanner(scanSpec, scanInfo, scanType, getCols("a"), scanners)) {
-      List<Cell> results = new ArrayList<Cell>();
+      List<Cell> results = new ArrayList<>();
       Assert.assertFalse(scan.next(results));
       Assert.assertEquals(0, results.size());
     }
@@ -472,7 +472,7 @@ public class TestStoreScanner {
     Scan scanSpec = new Scan(Bytes.toBytes("R1"));
     try (StoreScanner scan =
         new StoreScanner(scanSpec, scanInfo, scanType, getCols("a"), scanners)) {
-      List<Cell> results = new ArrayList<Cell>();
+      List<Cell> results = new ArrayList<>();
       Assert.assertEquals(true, scan.next(results));
       Assert.assertEquals(0, results.size());
 
@@ -499,7 +499,7 @@ public class TestStoreScanner {
 
     try (StoreScanner scan = new StoreScanner(new Scan(Bytes.toBytes("R1")), scanInfo, scanType,
         getCols("a"), scanners)) {
-      List<Cell> results = new ArrayList<Cell>();
+      List<Cell> results = new ArrayList<>();
       // the two put at ts=now will be masked by the 1 delete, and
       // since the scan default returns 1 version we'll return the newest
       // key, which is kvs[2], now-100.
@@ -525,7 +525,7 @@ public class TestStoreScanner {
     Scan scanSpec = new Scan(Bytes.toBytes("R1")).setMaxVersions(2);
     try (StoreScanner scan = new StoreScanner(scanSpec, scanInfo, scanType,
         getCols("a"), scanners)) {
-      List<Cell> results = new ArrayList<Cell>();
+      List<Cell> results = new ArrayList<>();
       Assert.assertEquals(true, scan.next(results));
       Assert.assertEquals(2, results.size());
       Assert.assertEquals(kvs2[1], results.get(0));
@@ -543,7 +543,7 @@ public class TestStoreScanner {
     List<KeyValueScanner> scanners = scanFixture(kvs);
     try (StoreScanner scan =
         new StoreScanner(new Scan(Bytes.toBytes("R1")), scanInfo, scanType, null, scanners)) {
-      List<Cell> results = new ArrayList<Cell>();
+      List<Cell> results = new ArrayList<>();
       Assert.assertEquals(true, scan.next(results));
       Assert.assertEquals(2, results.size());
       Assert.assertEquals(kvs[0], results.get(0));
@@ -574,7 +574,7 @@ public class TestStoreScanner {
     List<KeyValueScanner> scanners = scanFixture(kvs);
     try (StoreScanner scan = new StoreScanner(new Scan().setMaxVersions(2),
         scanInfo, scanType, null, scanners)) {
-      List<Cell> results = new ArrayList<Cell>();
+      List<Cell> results = new ArrayList<>();
       Assert.assertEquals(true, scan.next(results));
       Assert.assertEquals(5, results.size());
       Assert.assertEquals(kvs[0], results.get(0));
@@ -605,7 +605,7 @@ public class TestStoreScanner {
     try (StoreScanner scan =
         new StoreScanner(new Scan().setMaxVersions(Integer.MAX_VALUE), scanInfo, scanType, null,
             scanners)) {
-      List<Cell> results = new ArrayList<Cell>();
+      List<Cell> results = new ArrayList<>();
       Assert.assertEquals(true, scan.next(results));
       Assert.assertEquals(0, results.size());
       Assert.assertEquals(true, scan.next(results));
@@ -627,7 +627,7 @@ public class TestStoreScanner {
     List<KeyValueScanner> scanners = scanFixture(kvs);
     try (StoreScanner scan = new StoreScanner(new Scan(), scanInfo, scanType, null,
         scanners)) {
-      List<Cell> results = new ArrayList<Cell>();
+      List<Cell> results = new ArrayList<>();
       Assert.assertEquals(true, scan.next(results));
       Assert.assertEquals(1, results.size());
       Assert.assertEquals(kvs[3], results.get(0));
@@ -652,7 +652,7 @@ public class TestStoreScanner {
     List<KeyValueScanner> scanners = scanFixture(kvs);
     try (StoreScanner scan =
         new StoreScanner(new Scan(), scanInfo, scanType, getCols("a", "d"), scanners)) {
-      List<Cell> results = new ArrayList<Cell>();
+      List<Cell> results = new ArrayList<>();
       Assert.assertEquals(true, scan.next(results));
       Assert.assertEquals(2, results.size());
       Assert.assertEquals(kvs[0], results.get(0));
@@ -692,7 +692,7 @@ public class TestStoreScanner {
         CellComparator.COMPARATOR);
     ScanType scanType = ScanType.USER_SCAN;
     try (StoreScanner scanner = new StoreScanner(scan, scanInfo, scanType, null, scanners)) {
-      List<Cell> results = new ArrayList<Cell>();
+      List<Cell> results = new ArrayList<>();
       Assert.assertEquals(true, scanner.next(results));
       Assert.assertEquals(2, results.size());
       Assert.assertEquals(kvs[1], results.get(0));
@@ -720,9 +720,9 @@ public class TestStoreScanner {
       // normally cause an NPE because scan.store is null.  So as long as we get through these
       // two calls we are good and the bug was quashed.
 
-      scan.updateReaders(new ArrayList<StoreFile>());
+      scan.updateReaders(new ArrayList<>());
 
-      scan.updateReaders(new ArrayList<StoreFile>());
+      scan.updateReaders(new ArrayList<>());
 
       scan.peek();
     }
@@ -767,7 +767,7 @@ public class TestStoreScanner {
     try (StoreScanner scanner =
         new StoreScanner(scan, scanInfo, scanType, null, scanners)) {
 
-      List<Cell> results = new ArrayList<Cell>();
+      List<Cell> results = new ArrayList<>();
       Assert.assertEquals(true, scanner.next(results));
       Assert.assertEquals(1, results.size());
       Assert.assertEquals(kvs[1], results.get(0));
@@ -833,8 +833,8 @@ public class TestStoreScanner {
         new StoreScanner(scan, scanInfo,
           ScanType.COMPACT_DROP_DELETES, null, scanners,
           HConstants.OLDEST_TIMESTAMP)) {
-        List<Cell> results = new ArrayList<Cell>();
-        results = new ArrayList<Cell>();
+        List<Cell> results = new ArrayList<>();
+        results = new ArrayList<>();
         Assert.assertEquals(true, scanner.next(results));
         Assert.assertEquals(kvs[0], results.get(0));
         Assert.assertEquals(kvs[2], results.get(1));
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestStripeStoreEngine.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestStripeStoreEngine.java
index 3e3eef9..b2739e1 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestStripeStoreEngine.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestStripeStoreEngine.java
@@ -78,7 +78,7 @@ public class TestStripeStoreEngine {
       mockCompactor.compact(any(CompactionRequest.class), anyInt(), anyLong(), any(byte[].class),
         any(byte[].class), any(byte[].class), any(byte[].class),
         any(ThroughputController.class), any(User.class)))
-        .thenReturn(new ArrayList<Path>());
+        .thenReturn(new ArrayList<>());
 
     // Produce 3 L0 files.
     StoreFile sf = createFile();
@@ -118,6 +118,6 @@ public class TestStripeStoreEngine {
   }
 
   private static ArrayList<StoreFile> al(StoreFile... sfs) {
-    return new ArrayList<StoreFile>(Arrays.asList(sfs));
+    return new ArrayList<>(Arrays.asList(sfs));
   }
 }
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestStripeStoreFileManager.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestStripeStoreFileManager.java
index c533257..a6ce270 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestStripeStoreFileManager.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestStripeStoreFileManager.java
@@ -115,7 +115,7 @@ public class TestStripeStoreFileManager {
   }
 
   private static ArrayList<StoreFile> dumpIterator(Iterator<StoreFile> iter) {
-    ArrayList<StoreFile> result = new ArrayList<StoreFile>();
+    ArrayList<StoreFile> result = new ArrayList<>();
     for (; iter.hasNext(); result.add(iter.next()));
     return result;
   }
@@ -233,7 +233,7 @@ public class TestStripeStoreFileManager {
   private void verifySplitPointScenario(int splitPointAfter, boolean shouldSplitStripe,
       float splitRatioToVerify, int... sizes) throws Exception {
     assertTrue(sizes.length > 1);
-    ArrayList<StoreFile> sfs = new ArrayList<StoreFile>();
+    ArrayList<StoreFile> sfs = new ArrayList<>();
     for (int sizeIx = 0; sizeIx < sizes.length; ++sizeIx) {
       byte[] startKey = (sizeIx == 0) ? OPEN_KEY : Bytes.toBytes(sizeIx - 1);
       byte[] endKey = (sizeIx == sizes.length - 1) ? OPEN_KEY : Bytes.toBytes(sizeIx);
@@ -525,7 +525,7 @@ public class TestStripeStoreFileManager {
       sfm.insertNewFiles(al(createFile()));
     }
     for (int i = 0; i < filesInStripe; ++i) {
-      ArrayList<StoreFile> stripe = new ArrayList<StoreFile>();
+      ArrayList<StoreFile> stripe = new ArrayList<>();
       for (int j = 0; j < stripes; ++j) {
         stripe.add(createFile(
             (j == 0) ? OPEN_KEY : keys[j - 1], (j == stripes - 1) ? OPEN_KEY : keys[j]));
@@ -597,7 +597,7 @@ public class TestStripeStoreFileManager {
   }
 
   private static StripeStoreFileManager createManager() throws Exception {
-    return createManager(new ArrayList<StoreFile>());
+    return createManager(new ArrayList<>());
   }
 
   private static StripeStoreFileManager createManager(ArrayList<StoreFile> sfs) throws Exception {
@@ -615,11 +615,11 @@ public class TestStripeStoreFileManager {
   }
 
   private static ArrayList<StoreFile> al(StoreFile... sfs) {
-    return new ArrayList<StoreFile>(Arrays.asList(sfs));
+    return new ArrayList<>(Arrays.asList(sfs));
   }
 
   private static ArrayList<StoreFile> flattenLists(ArrayList<StoreFile>... sfls) {
-    ArrayList<StoreFile> result = new ArrayList<StoreFile>();
+    ArrayList<StoreFile> result = new ArrayList<>();
     for (ArrayList<StoreFile> sfl : sfls) {
       result.addAll(sfl);
     }
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestTags.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestTags.java
index 4d531ac..40eebb6 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestTags.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestTags.java
@@ -421,7 +421,7 @@ public class TestTags {
       assertEquals(5L, Bytes.toLong(kv.getValueArray(), kv.getValueOffset(), kv.getValueLength()));
       assertEquals(2, tags.size());
       // We cannot assume the ordering of tags
-      List<String> tagValues = new ArrayList<String>();
+      List<String> tagValues = new ArrayList<>();
       for (Tag tag: tags) {
         tagValues.add(Bytes.toString(TagUtil.cloneValue(tag)));
       }
@@ -557,7 +557,7 @@ public class TestTags {
     private void updateMutationAddingTags(final Mutation m) {
       byte[] attribute = m.getAttribute("visibility");
       byte[] cf = null;
-      List<Cell> updatedCells = new ArrayList<Cell>();
+      List<Cell> updatedCells = new ArrayList<>();
       if (attribute != null) {
         for (List<? extends Cell> edits : m.getFamilyCellMap().values()) {
           for (Cell cell : edits) {
@@ -566,7 +566,7 @@ public class TestTags {
               cf = CellUtil.cloneFamily(kv);
             }
             Tag tag = new ArrayBackedTag((byte) 1, attribute);
-            List<Tag> tagList = new ArrayList<Tag>();
+            List<Tag> tagList = new ArrayList<>();
             tagList.add(tag);
 
             KeyValue newKV = new KeyValue(CellUtil.cloneRow(kv), 0, kv.getRowLength(),
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestWALLockup.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestWALLockup.java
index 4821c74..4f247b0 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestWALLockup.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestWALLockup.java
@@ -218,7 +218,7 @@ public class TestWALLockup {
     HTableDescriptor htd = new HTableDescriptor(TableName.META_TABLE_NAME);
     final HRegion region = initHRegion(tableName, null, null, dodgyWAL);
     byte [] bytes = Bytes.toBytes(getName());
-    NavigableMap<byte[], Integer> scopes = new TreeMap<byte[], Integer>(
+    NavigableMap<byte[], Integer> scopes = new TreeMap<>(
         Bytes.BYTES_COMPARATOR);
     scopes.put(COLUMN_FAMILY_BYTES, 0);
     MultiVersionConcurrencyControl mvcc = new MultiVersionConcurrencyControl();
@@ -398,7 +398,7 @@ public class TestWALLockup {
     HTableDescriptor htd = new HTableDescriptor(TableName.META_TABLE_NAME);
     final HRegion region = initHRegion(tableName, null, null, dodgyWAL1);
     byte[] bytes = Bytes.toBytes(getName());
-    NavigableMap<byte[], Integer> scopes = new TreeMap<byte[], Integer>(
+    NavigableMap<byte[], Integer> scopes = new TreeMap<>(
         Bytes.BYTES_COMPARATOR);
     scopes.put(COLUMN_FAMILY_BYTES, 0);
     MultiVersionConcurrencyControl mvcc = new MultiVersionConcurrencyControl();
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestWideScanner.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestWideScanner.java
index f598a8d..cdf84d2 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestWideScanner.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestWideScanner.java
@@ -94,7 +94,7 @@ public class TestWideScanner extends HBaseTestCase {
     try {
       this.r = createNewHRegion(TESTTABLEDESC, null, null);
       int inserted = addWideContent(this.r);
-      List<Cell> results = new ArrayList<Cell>();
+      List<Cell> results = new ArrayList<>();
       Scan scan = new Scan();
       scan.addFamily(A);
       scan.addFamily(B);
@@ -130,7 +130,7 @@ public class TestWideScanner extends HBaseTestCase {
           ((HRegion.RegionScannerImpl)s).storeHeap.getHeap().iterator();
         while (scanners.hasNext()) {
           StoreScanner ss = (StoreScanner)scanners.next();
-          ss.updateReaders(new ArrayList<StoreFile>());
+          ss.updateReaders(new ArrayList<>());
         }
       } while (more);
 
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/compactions/ConstantSizeFileListGenerator.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/compactions/ConstantSizeFileListGenerator.java
index 68d57af..5014b41 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/compactions/ConstantSizeFileListGenerator.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/compactions/ConstantSizeFileListGenerator.java
@@ -49,7 +49,7 @@ class ConstantSizeFileListGenerator extends StoreFileListGenerator {
       @Override
       public List<StoreFile> next() {
         count += 1;
-        ArrayList<StoreFile> files = new ArrayList<StoreFile>(NUM_FILES_GEN);
+        ArrayList<StoreFile> files = new ArrayList<>(NUM_FILES_GEN);
         for (int i = 0; i < NUM_FILES_GEN; i++) {
           files.add(createMockStoreFile(FILESIZE));
         }
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/compactions/EverythingPolicy.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/compactions/EverythingPolicy.java
index 9a4bb8e..46bb639 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/compactions/EverythingPolicy.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/compactions/EverythingPolicy.java
@@ -46,9 +46,9 @@ public class EverythingPolicy extends RatioBasedCompactionPolicy {
     final boolean mayUseOffPeak, final boolean mayBeStuck) throws IOException {
 
     if (candidates.size() < comConf.getMinFilesToCompact()) {
-      return new ArrayList<StoreFile>(0);
+      return new ArrayList<>(0);
     }
 
-    return new ArrayList<StoreFile>(candidates);
+    return new ArrayList<>(candidates);
   }
 }
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/compactions/GaussianFileListGenerator.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/compactions/GaussianFileListGenerator.java
index a19e9ad..fb8c30a 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/compactions/GaussianFileListGenerator.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/compactions/GaussianFileListGenerator.java
@@ -47,7 +47,7 @@ class GaussianFileListGenerator extends StoreFileListGenerator {
       @Override
       public List<StoreFile> next() {
         count += 1;
-        ArrayList<StoreFile> files = new ArrayList<StoreFile>(NUM_FILES_GEN);
+        ArrayList<StoreFile> files = new ArrayList<>(NUM_FILES_GEN);
         for (int i = 0; i < NUM_FILES_GEN; i++) {
           files.add(createMockStoreFile(
               (int) Math.ceil(Math.max(0, gen.nextNormalizedDouble() * 32 + 32)))
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/compactions/MockStoreFileGenerator.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/compactions/MockStoreFileGenerator.java
index 663714a..cb97d27 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/compactions/MockStoreFileGenerator.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/compactions/MockStoreFileGenerator.java
@@ -46,7 +46,7 @@ class MockStoreFileGenerator {
   }
 
   protected List<StoreFile> createStoreFileList(final int[] fs) {
-    List<StoreFile> storeFiles = new LinkedList<StoreFile>();
+    List<StoreFile> storeFiles = new LinkedList<>();
     for (int fileSize : fs) {
       storeFiles.add(createMockStoreFile(fileSize));
     }
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/compactions/PerfTestCompactionPolicies.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/compactions/PerfTestCompactionPolicies.java
index 3fcd3fe..0a84fe9 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/compactions/PerfTestCompactionPolicies.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/compactions/PerfTestCompactionPolicies.java
@@ -82,7 +82,7 @@ public class PerfTestCompactionPolicies extends MockStoreFileGenerator {
     int[] minFilesValues = new int[] {3};
     float[] ratioValues = new float[] {1.2f};
 
-    List<Object[]> params = new ArrayList<Object[]>(
+    List<Object[]> params = new ArrayList<>(
         maxFileValues.length
         * minFilesValues.length
         * fileListGenClasses.length
@@ -152,7 +152,7 @@ public class PerfTestCompactionPolicies extends MockStoreFileGenerator {
   public final void testSelection() throws Exception {
     long fileDiff = 0;
     for (List<StoreFile> storeFileList : generator) {
-      List<StoreFile> currentFiles = new ArrayList<StoreFile>(18);
+      List<StoreFile> currentFiles = new ArrayList<>(18);
       for (StoreFile file : storeFileList) {
         currentFiles.add(file);
         currentFiles = runIteration(currentFiles);
@@ -175,16 +175,16 @@ public class PerfTestCompactionPolicies extends MockStoreFileGenerator {
 
   private List<StoreFile> runIteration(List<StoreFile> startingStoreFiles) throws IOException {
 
-    List<StoreFile> storeFiles = new ArrayList<StoreFile>(startingStoreFiles);
+    List<StoreFile> storeFiles = new ArrayList<>(startingStoreFiles);
     CompactionRequest req = cp.selectCompaction(
-        storeFiles, new ArrayList<StoreFile>(), false, false, false);
+        storeFiles, new ArrayList<>(), false, false, false);
     long newFileSize = 0;
 
     Collection<StoreFile> filesToCompact = req.getFiles();
 
     if (!filesToCompact.isEmpty()) {
 
-      storeFiles = new ArrayList<StoreFile>(storeFiles);
+      storeFiles = new ArrayList<>(storeFiles);
       storeFiles.removeAll(filesToCompact);
 
       for (StoreFile storeFile : filesToCompact) {
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/compactions/SemiConstantSizeFileListGenerator.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/compactions/SemiConstantSizeFileListGenerator.java
index ed4531a..5fe47f3 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/compactions/SemiConstantSizeFileListGenerator.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/compactions/SemiConstantSizeFileListGenerator.java
@@ -42,7 +42,7 @@ class SemiConstantSizeFileListGenerator extends StoreFileListGenerator {
       @Override
       public List<StoreFile> next() {
         count += 1;
-        ArrayList<StoreFile> files = new ArrayList<StoreFile>(NUM_FILES_GEN);
+        ArrayList<StoreFile> files = new ArrayList<>(NUM_FILES_GEN);
         for (int i = 0; i < NUM_FILES_GEN; i++) {
           files.add(createMockStoreFile(random.nextInt(5) + 30));
         }
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/compactions/SinusoidalFileListGenerator.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/compactions/SinusoidalFileListGenerator.java
index 6afbb2f..f5f36ac 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/compactions/SinusoidalFileListGenerator.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/compactions/SinusoidalFileListGenerator.java
@@ -43,7 +43,7 @@ class SinusoidalFileListGenerator extends StoreFileListGenerator {
       @Override
       public List<StoreFile> next() {
         count += 1;
-        ArrayList<StoreFile> files = new ArrayList<StoreFile>(NUM_FILES_GEN);
+        ArrayList<StoreFile> files = new ArrayList<>(NUM_FILES_GEN);
         for (int x = 0; x < NUM_FILES_GEN; x++) {
           int fileSize = (int) Math.abs(64 * Math.sin((Math.PI * x) / 50.0)) + 1;
           files.add(createMockStoreFile(fileSize));
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/compactions/SpikyFileListGenerator.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/compactions/SpikyFileListGenerator.java
index ebaa711..5201eb7 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/compactions/SpikyFileListGenerator.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/compactions/SpikyFileListGenerator.java
@@ -43,7 +43,7 @@ class SpikyFileListGenerator extends StoreFileListGenerator {
       @Override
       public List<StoreFile> next() {
         count += 1;
-        ArrayList<StoreFile> files = new ArrayList<StoreFile>(NUM_FILES_GEN);
+        ArrayList<StoreFile> files = new ArrayList<>(NUM_FILES_GEN);
         for (int x = 0; x < NUM_FILES_GEN; x++) {
           int fileSize = random.nextInt(5) + 1;
           if ( x % 10 == 0) {
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/compactions/TestCompactedHFilesDischarger.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/compactions/TestCompactedHFilesDischarger.java
index aa5a20e..08fc7bf 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/compactions/TestCompactedHFilesDischarger.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/compactions/TestCompactedHFilesDischarger.java
@@ -77,7 +77,7 @@ public class TestCompactedHFilesDischarger {
     Path path = testUtil.getDataTestDir(getClass().getSimpleName());
     region = HBaseTestingUtility.createRegionAndWAL(info, path, testUtil.getConfiguration(), htd);
     rss = mock(RegionServerServices.class);
-    List<Region> regions = new ArrayList<Region>(1);
+    List<Region> regions = new ArrayList<>(1);
     regions.add(region);
     when(rss.getOnlineRegions()).thenReturn(regions);
   }
@@ -379,7 +379,7 @@ public class TestCompactedHFilesDischarger {
       RegionScanner resScanner = null;
       try {
         resScanner = region.getScanner(scan);
-        List<Cell> results = new ArrayList<Cell>();
+        List<Cell> results = new ArrayList<>();
         boolean next = resScanner.next(results);
         try {
           counter.incrementAndGet();
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/compactions/TestCompactor.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/compactions/TestCompactor.java
index 89f61d0..dff6919 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/compactions/TestCompactor.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/compactions/TestCompactor.java
@@ -82,12 +82,12 @@ public class TestCompactor {
   public static class StoreFileWritersCapture
       implements Answer<StoreFileWriter>, StripeMultiFileWriter.WriterFactory {
     public static class Writer {
-      public ArrayList<KeyValue> kvs = new ArrayList<KeyValue>();
-      public TreeMap<byte[], byte[]> data = new TreeMap<byte[], byte[]>(Bytes.BYTES_COMPARATOR);
+      public ArrayList<KeyValue> kvs = new ArrayList<>();
+      public TreeMap<byte[], byte[]> data = new TreeMap<>(Bytes.BYTES_COMPARATOR);
       public boolean hasMetadata;
     }
 
-    private List<Writer> writers = new ArrayList<Writer>();
+    private List<Writer> writers = new ArrayList<>();
 
     @Override
     public StoreFileWriter createWriter() throws IOException {
@@ -192,7 +192,7 @@ public class TestCompactor {
     private final ArrayList<KeyValue> kvs;
 
     public Scanner(KeyValue... kvs) {
-      this.kvs = new ArrayList<KeyValue>(Arrays.asList(kvs));
+      this.kvs = new ArrayList<>(Arrays.asList(kvs));
     }
 
     @Override
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/compactions/TestDateTieredCompactor.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/compactions/TestDateTieredCompactor.java
index 38d9f99..e590639 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/compactions/TestDateTieredCompactor.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/compactions/TestDateTieredCompactor.java
@@ -156,7 +156,7 @@ public class TestDateTieredCompactor {
     StoreFileWritersCapture writers = new StoreFileWritersCapture();
     CompactionRequest request = createDummyRequest();
     DateTieredCompactor dtc = createCompactor(writers, new KeyValue[0],
-      new ArrayList<StoreFile>(request.getFiles()));
+      new ArrayList<>(request.getFiles()));
     List<Path> paths = dtc.compact(request, Arrays.asList(Long.MIN_VALUE, Long.MAX_VALUE),
       NoLimitThroughputController.INSTANCE, null);
     assertEquals(1, paths.size());
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/compactions/TestStripeCompactionPolicy.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/compactions/TestStripeCompactionPolicy.java
index 5fadee8..f2d00b3 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/compactions/TestStripeCompactionPolicy.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/compactions/TestStripeCompactionPolicy.java
@@ -415,7 +415,7 @@ public class TestStripeCompactionPolicy {
   private static StripeCompactionPolicy.StripeInformationProvider createStripesWithFiles(
       List<StoreFile>... stripeFiles) throws Exception {
     return createStripesWithFiles(createBoundaries(stripeFiles.length),
-        Lists.newArrayList(stripeFiles), new ArrayList<StoreFile>());
+        Lists.newArrayList(stripeFiles), new ArrayList<>());
   }
 
   @Test
@@ -433,7 +433,7 @@ public class TestStripeCompactionPolicy {
     verifySingleStripeCompaction(policy, si, 0, false);
     // Unless there are enough to cause L0 compaction.
     si = createStripesWithSizes(6, 2, stripes);
-    ConcatenatedLists<StoreFile> sfs = new ConcatenatedLists<StoreFile>();
+    ConcatenatedLists<StoreFile> sfs = new ConcatenatedLists<>();
     sfs.addSublist(si.getLevel0Files());
     sfs.addSublist(si.getStripes().get(0));
     verifyCompaction(
@@ -446,7 +446,7 @@ public class TestStripeCompactionPolicy {
     // if all files of stripe aren't selected, delete must not be dropped.
     stripes = new Long[][] { new Long[] { 100L, 3L, 2L, 2L, 2L }, new Long[] { 6L } };
     si = createStripesWithSizes(0, 0, stripes);
-    List<StoreFile> compact_file = new ArrayList<StoreFile>();
+    List<StoreFile> compact_file = new ArrayList<>();
     Iterator<StoreFile> iter = si.getStripes().get(0).listIterator(1);
     while (iter.hasNext()) {
         compact_file.add(iter.next());
@@ -472,7 +472,7 @@ public class TestStripeCompactionPolicy {
   }
 
   private static ArrayList<StoreFile> al(StoreFile... sfs) {
-    return new ArrayList<StoreFile>(Arrays.asList(sfs));
+    return new ArrayList<>(Arrays.asList(sfs));
   }
 
   private void verifyMergeCompatcion(StripeCompactionPolicy policy, StripeInformationProvider si,
@@ -619,7 +619,7 @@ public class TestStripeCompactionPolicy {
 
   private static List<StoreFile> getAllFiles(
       StripeInformationProvider si, int fromStripe, int toStripe) {
-    ArrayList<StoreFile> expected = new ArrayList<StoreFile>();
+    ArrayList<StoreFile> expected = new ArrayList<>();
     for (int i = fromStripe; i <= toStripe; ++i) {
       expected.addAll(si.getStripes().get(i));
     }
@@ -633,11 +633,11 @@ public class TestStripeCompactionPolicy {
    */
   private static StripeInformationProvider createStripes(
       int l0Count, byte[]... boundaries) throws Exception {
-    List<Long> l0Sizes = new ArrayList<Long>();
+    List<Long> l0Sizes = new ArrayList<>();
     for (int i = 0; i < l0Count; ++i) {
       l0Sizes.add(5L);
     }
-    List<List<Long>> sizes = new ArrayList<List<Long>>();
+    List<List<Long>> sizes = new ArrayList<>();
     for (int i = 0; i <= boundaries.length; ++i) {
       sizes.add(Arrays.asList(Long.valueOf(5)));
     }
@@ -651,11 +651,11 @@ public class TestStripeCompactionPolicy {
    */
   private static StripeInformationProvider createStripesL0Only(
       int l0Count, long l0Size) throws Exception {
-    List<Long> l0Sizes = new ArrayList<Long>();
+    List<Long> l0Sizes = new ArrayList<>();
     for (int i = 0; i < l0Count; ++i) {
       l0Sizes.add(l0Size);
     }
-    return createStripes(null, new ArrayList<List<Long>>(), l0Sizes);
+    return createStripes(null, new ArrayList<>(), l0Sizes);
   }
 
   /**
@@ -666,7 +666,7 @@ public class TestStripeCompactionPolicy {
    */
   private static StripeInformationProvider createStripesWithSizes(
       int l0Count, long l0Size, Long[]... sizes) throws Exception {
-    ArrayList<List<Long>> sizeList = new ArrayList<List<Long>>(sizes.length);
+    ArrayList<List<Long>> sizeList = new ArrayList<>(sizes.length);
     for (Long[] size : sizes) {
       sizeList.add(Arrays.asList(size));
     }
@@ -676,7 +676,7 @@ public class TestStripeCompactionPolicy {
   private static StripeInformationProvider createStripesWithSizes(
       int l0Count, long l0Size, List<List<Long>> sizes) throws Exception {
     List<byte[]> boundaries = createBoundaries(sizes.size());
-    List<Long> l0Sizes = new ArrayList<Long>();
+    List<Long> l0Sizes = new ArrayList<>();
     for (int i = 0; i < l0Count; ++i) {
       l0Sizes.add(l0Size);
     }
@@ -686,22 +686,22 @@ public class TestStripeCompactionPolicy {
   private static List<byte[]> createBoundaries(int stripeCount) {
     byte[][] keys = new byte[][] { KEY_A, KEY_B, KEY_C, KEY_D, KEY_E };
     assert stripeCount <= keys.length + 1;
-    List<byte[]> boundaries = new ArrayList<byte[]>();
+    List<byte[]> boundaries = new ArrayList<>();
     boundaries.addAll(Arrays.asList(keys).subList(0, stripeCount - 1));
     return boundaries;
   }
 
   private static StripeInformationProvider createStripes(List<byte[]> boundaries,
       List<List<Long>> stripeSizes, List<Long> l0Sizes) throws Exception {
-    List<List<StoreFile>> stripeFiles = new ArrayList<List<StoreFile>>(stripeSizes.size());
+    List<List<StoreFile>> stripeFiles = new ArrayList<>(stripeSizes.size());
     for (List<Long> sizes : stripeSizes) {
-      List<StoreFile> sfs = new ArrayList<StoreFile>(sizes.size());
+      List<StoreFile> sfs = new ArrayList<>(sizes.size());
       for (Long size : sizes) {
         sfs.add(createFile(size));
       }
       stripeFiles.add(sfs);
     }
-    List<StoreFile> l0Files = new ArrayList<StoreFile>();
+    List<StoreFile> l0Files = new ArrayList<>();
     for (Long size : l0Sizes) {
       l0Files.add(createFile(size));
     }
@@ -713,8 +713,8 @@ public class TestStripeCompactionPolicy {
    */
   private static StripeInformationProvider createStripesWithFiles(List<byte[]> boundaries,
       List<List<StoreFile>> stripeFiles, List<StoreFile> l0Files) throws Exception {
-    ArrayList<ImmutableList<StoreFile>> stripes = new ArrayList<ImmutableList<StoreFile>>();
-    ArrayList<byte[]> boundariesList = new ArrayList<byte[]>();
+    ArrayList<ImmutableList<StoreFile>> stripes = new ArrayList<>();
+    ArrayList<byte[]> boundariesList = new ArrayList<>();
     StripeInformationProvider si = mock(StripeInformationProvider.class);
     if (!stripeFiles.isEmpty()) {
       assert stripeFiles.size() == (boundaries.size() + 1);
@@ -731,7 +731,7 @@ public class TestStripeCompactionPolicy {
         when(si.getEndRow(eq(i))).thenReturn(endKey);
       }
     }
-    ConcatenatedLists<StoreFile> sfs = new ConcatenatedLists<StoreFile>();
+    ConcatenatedLists<StoreFile> sfs = new ConcatenatedLists<>();
     sfs.addAllSublists(stripes);
     sfs.addSublist(l0Files);
     when(si.getStorefiles()).thenReturn(sfs);
@@ -803,7 +803,7 @@ public class TestStripeCompactionPolicy {
     private final ArrayList<KeyValue> kvs;
 
     public Scanner(KeyValue... kvs) {
-      this.kvs = new ArrayList<KeyValue>(Arrays.asList(kvs));
+      this.kvs = new ArrayList<>(Arrays.asList(kvs));
     }
 
     @Override
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/compactions/TestStripeCompactor.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/compactions/TestStripeCompactor.java
index 4b82940..088c958 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/compactions/TestStripeCompactor.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/compactions/TestStripeCompactor.java
@@ -177,7 +177,7 @@ public class TestStripeCompactor {
       null, NoLimitThroughputController.INSTANCE, null);
     assertEquals(output.length, paths.size());
     writers.verifyKvs(output, true, true);
-    List<byte[]> boundaries = new ArrayList<byte[]>(output.length + 2);
+    List<byte[]> boundaries = new ArrayList<>(output.length + 2);
     boundaries.add(left);
     for (int i = 1; i < output.length; ++i) {
       boundaries.add(CellUtil.cloneRow(output[i][0]));
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/querymatcher/TestCompactionScanQueryMatcher.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/querymatcher/TestCompactionScanQueryMatcher.java
index 055fe1c..af8c27d 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/querymatcher/TestCompactionScanQueryMatcher.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/querymatcher/TestCompactionScanQueryMatcher.java
@@ -79,8 +79,7 @@ public class TestCompactionScanQueryMatcher extends AbstractTestScanQueryMatcher
     CompactionScanQueryMatcher qm = CompactionScanQueryMatcher.create(scanInfo,
       ScanType.COMPACT_RETAIN_DELETES, Long.MAX_VALUE, HConstants.OLDEST_TIMESTAMP,
       HConstants.OLDEST_TIMESTAMP, now, from, to, null);
-    List<ScanQueryMatcher.MatchCode> actual = new ArrayList<ScanQueryMatcher.MatchCode>(
-        rows.length);
+    List<ScanQueryMatcher.MatchCode> actual = new ArrayList<>(rows.length);
     byte[] prevRow = null;
     for (byte[] row : rows) {
       if (prevRow == null || !Bytes.equals(prevRow, row)) {
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/querymatcher/TestExplicitColumnTracker.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/querymatcher/TestExplicitColumnTracker.java
index 3480597..4e07f80 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/querymatcher/TestExplicitColumnTracker.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/querymatcher/TestExplicitColumnTracker.java
@@ -49,7 +49,7 @@ public class TestExplicitColumnTracker {
     ColumnTracker exp = new ExplicitColumnTracker(trackColumns, 0, maxVersions, Long.MIN_VALUE);
 
     // Initialize result
-    List<ScanQueryMatcher.MatchCode> result = new ArrayList<ScanQueryMatcher.MatchCode>(scannerColumns.size());
+    List<ScanQueryMatcher.MatchCode> result = new ArrayList<>(scannerColumns.size());
 
     long timestamp = 0;
     // "Match"
@@ -67,11 +67,11 @@ public class TestExplicitColumnTracker {
   @Test
   public void testGetSingleVersion() throws IOException {
     // Create tracker
-    TreeSet<byte[]> columns = new TreeSet<byte[]>(Bytes.BYTES_COMPARATOR);
+    TreeSet<byte[]> columns = new TreeSet<>(Bytes.BYTES_COMPARATOR);
     // Looking for every other
     columns.add(col2);
     columns.add(col4);
-    List<MatchCode> expected = new ArrayList<ScanQueryMatcher.MatchCode>(5);
+    List<MatchCode> expected = new ArrayList<>(5);
     expected.add(ScanQueryMatcher.MatchCode.SEEK_NEXT_COL); // col1
     expected.add(ScanQueryMatcher.MatchCode.INCLUDE_AND_SEEK_NEXT_COL); // col2
     expected.add(ScanQueryMatcher.MatchCode.SEEK_NEXT_COL); // col3
@@ -80,7 +80,7 @@ public class TestExplicitColumnTracker {
     int maxVersions = 1;
 
     // Create "Scanner"
-    List<byte[]> scanner = new ArrayList<byte[]>(5);
+    List<byte[]> scanner = new ArrayList<>(5);
     scanner.add(col1);
     scanner.add(col2);
     scanner.add(col3);
@@ -93,12 +93,12 @@ public class TestExplicitColumnTracker {
   @Test
   public void testGetMultiVersion() throws IOException {
     // Create tracker
-    TreeSet<byte[]> columns = new TreeSet<byte[]>(Bytes.BYTES_COMPARATOR);
+    TreeSet<byte[]> columns = new TreeSet<>(Bytes.BYTES_COMPARATOR);
     // Looking for every other
     columns.add(col2);
     columns.add(col4);
 
-    List<ScanQueryMatcher.MatchCode> expected = new ArrayList<ScanQueryMatcher.MatchCode>(15);
+    List<ScanQueryMatcher.MatchCode> expected = new ArrayList<>(15);
     expected.add(ScanQueryMatcher.MatchCode.SEEK_NEXT_COL);
     expected.add(ScanQueryMatcher.MatchCode.SEEK_NEXT_COL);
     expected.add(ScanQueryMatcher.MatchCode.SEEK_NEXT_COL);
@@ -121,7 +121,7 @@ public class TestExplicitColumnTracker {
     int maxVersions = 2;
 
     // Create "Scanner"
-    List<byte[]> scanner = new ArrayList<byte[]>(15);
+    List<byte[]> scanner = new ArrayList<>(15);
     scanner.add(col1);
     scanner.add(col1);
     scanner.add(col1);
@@ -148,7 +148,7 @@ public class TestExplicitColumnTracker {
   @Test
   public void testStackOverflow() throws IOException {
     int maxVersions = 1;
-    TreeSet<byte[]> columns = new TreeSet<byte[]>(Bytes.BYTES_COMPARATOR);
+    TreeSet<byte[]> columns = new TreeSet<>(Bytes.BYTES_COMPARATOR);
     for (int i = 0; i < 100000; i++) {
       columns.add(Bytes.toBytes("col" + i));
     }
@@ -173,7 +173,7 @@ public class TestExplicitColumnTracker {
    */
   @Test
   public void testInfiniteLoop() throws IOException {
-    TreeSet<byte[]> columns = new TreeSet<byte[]>(Bytes.BYTES_COMPARATOR);
+    TreeSet<byte[]> columns = new TreeSet<>(Bytes.BYTES_COMPARATOR);
     columns.addAll(Arrays.asList(new byte[][] { col2, col3, col5 }));
     List<byte[]> scanner = Arrays.<byte[]> asList(new byte[][] { col1, col4 });
     List<ScanQueryMatcher.MatchCode> expected = Arrays.<ScanQueryMatcher.MatchCode> asList(
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/querymatcher/TestScanWildcardColumnTracker.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/querymatcher/TestScanWildcardColumnTracker.java
index 2852947..6d6e58e 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/querymatcher/TestScanWildcardColumnTracker.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/querymatcher/TestScanWildcardColumnTracker.java
@@ -44,20 +44,20 @@ public class TestScanWildcardColumnTracker {
     ScanWildcardColumnTracker tracker = new ScanWildcardColumnTracker(0, VERSIONS, Long.MIN_VALUE);
 
     // Create list of qualifiers
-    List<byte[]> qualifiers = new ArrayList<byte[]>(4);
+    List<byte[]> qualifiers = new ArrayList<>(4);
     qualifiers.add(Bytes.toBytes("qualifier1"));
     qualifiers.add(Bytes.toBytes("qualifier2"));
     qualifiers.add(Bytes.toBytes("qualifier3"));
     qualifiers.add(Bytes.toBytes("qualifier4"));
 
     // Setting up expected result
-    List<MatchCode> expected = new ArrayList<MatchCode>(4);
+    List<MatchCode> expected = new ArrayList<>(4);
     expected.add(ScanQueryMatcher.MatchCode.INCLUDE);
     expected.add(ScanQueryMatcher.MatchCode.INCLUDE);
     expected.add(ScanQueryMatcher.MatchCode.INCLUDE);
     expected.add(ScanQueryMatcher.MatchCode.INCLUDE);
 
-    List<ScanQueryMatcher.MatchCode> actual = new ArrayList<MatchCode>(qualifiers.size());
+    List<ScanQueryMatcher.MatchCode> actual = new ArrayList<>(qualifiers.size());
 
     for (byte[] qualifier : qualifiers) {
       ScanQueryMatcher.MatchCode mc = ScanQueryMatcher.checkColumn(tracker, qualifier, 0,
@@ -76,20 +76,20 @@ public class TestScanWildcardColumnTracker {
     ScanWildcardColumnTracker tracker = new ScanWildcardColumnTracker(0, VERSIONS, Long.MIN_VALUE);
 
     // Create list of qualifiers
-    List<byte[]> qualifiers = new ArrayList<byte[]>(4);
+    List<byte[]> qualifiers = new ArrayList<>(4);
     qualifiers.add(Bytes.toBytes("qualifier1"));
     qualifiers.add(Bytes.toBytes("qualifier1"));
     qualifiers.add(Bytes.toBytes("qualifier1"));
     qualifiers.add(Bytes.toBytes("qualifier2"));
 
     // Setting up expected result
-    List<ScanQueryMatcher.MatchCode> expected = new ArrayList<MatchCode>(4);
+    List<ScanQueryMatcher.MatchCode> expected = new ArrayList<>(4);
     expected.add(ScanQueryMatcher.MatchCode.INCLUDE);
     expected.add(ScanQueryMatcher.MatchCode.INCLUDE);
     expected.add(ScanQueryMatcher.MatchCode.SEEK_NEXT_COL);
     expected.add(ScanQueryMatcher.MatchCode.INCLUDE);
 
-    List<MatchCode> actual = new ArrayList<ScanQueryMatcher.MatchCode>(qualifiers.size());
+    List<MatchCode> actual = new ArrayList<>(qualifiers.size());
 
     long timestamp = 0;
     for (byte[] qualifier : qualifiers) {
@@ -109,7 +109,7 @@ public class TestScanWildcardColumnTracker {
     ScanWildcardColumnTracker tracker = new ScanWildcardColumnTracker(0, VERSIONS, Long.MIN_VALUE);
 
     // Create list of qualifiers
-    List<byte[]> qualifiers = new ArrayList<byte[]>(2);
+    List<byte[]> qualifiers = new ArrayList<>(2);
     qualifiers.add(Bytes.toBytes("qualifier2"));
     qualifiers.add(Bytes.toBytes("qualifier1"));
 
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/querymatcher/TestUserScanQueryMatcher.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/querymatcher/TestUserScanQueryMatcher.java
index 0831404..b4e4311 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/querymatcher/TestUserScanQueryMatcher.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/querymatcher/TestUserScanQueryMatcher.java
@@ -69,7 +69,7 @@ public class TestUserScanQueryMatcher extends AbstractTestScanQueryMatcher {
     // of just byte []
 
     // Expected result
-    List<MatchCode> expected = new ArrayList<ScanQueryMatcher.MatchCode>(6);
+    List<MatchCode> expected = new ArrayList<>(6);
     expected.add(ScanQueryMatcher.MatchCode.SEEK_NEXT_COL);
     expected.add(ScanQueryMatcher.MatchCode.INCLUDE_AND_SEEK_NEXT_COL);
     expected.add(ScanQueryMatcher.MatchCode.SEEK_NEXT_COL);
@@ -83,7 +83,7 @@ public class TestUserScanQueryMatcher extends AbstractTestScanQueryMatcher {
       new ScanInfo(this.conf, fam2, 0, 1, ttl, KeepDeletedCells.FALSE, 0, rowComparator),
       get.getFamilyMap().get(fam2), now - ttl, now, null);
 
-    List<KeyValue> memstore = new ArrayList<KeyValue>(6);
+    List<KeyValue> memstore = new ArrayList<>(6);
     memstore.add(new KeyValue(row1, fam2, col1, 1, data));
     memstore.add(new KeyValue(row1, fam2, col2, 1, data));
     memstore.add(new KeyValue(row1, fam2, col3, 1, data));
@@ -92,7 +92,7 @@ public class TestUserScanQueryMatcher extends AbstractTestScanQueryMatcher {
 
     memstore.add(new KeyValue(row2, fam1, col1, data));
 
-    List<ScanQueryMatcher.MatchCode> actual = new ArrayList<ScanQueryMatcher.MatchCode>(memstore.size());
+    List<ScanQueryMatcher.MatchCode> actual = new ArrayList<>(memstore.size());
     KeyValue k = memstore.get(0);
     qm.setToNewRow(k);
 
@@ -113,7 +113,7 @@ public class TestUserScanQueryMatcher extends AbstractTestScanQueryMatcher {
     // of just byte []
 
     // Expected result
-    List<MatchCode> expected = new ArrayList<ScanQueryMatcher.MatchCode>(6);
+    List<MatchCode> expected = new ArrayList<>(6);
     expected.add(ScanQueryMatcher.MatchCode.INCLUDE);
     expected.add(ScanQueryMatcher.MatchCode.INCLUDE);
     expected.add(ScanQueryMatcher.MatchCode.INCLUDE);
@@ -126,7 +126,7 @@ public class TestUserScanQueryMatcher extends AbstractTestScanQueryMatcher {
       new ScanInfo(this.conf, fam2, 0, 1, ttl, KeepDeletedCells.FALSE, 0, rowComparator), null,
       now - ttl, now, null);
 
-    List<KeyValue> memstore = new ArrayList<KeyValue>(6);
+    List<KeyValue> memstore = new ArrayList<>(6);
     memstore.add(new KeyValue(row1, fam2, col1, 1, data));
     memstore.add(new KeyValue(row1, fam2, col2, 1, data));
     memstore.add(new KeyValue(row1, fam2, col3, 1, data));
@@ -134,7 +134,7 @@ public class TestUserScanQueryMatcher extends AbstractTestScanQueryMatcher {
     memstore.add(new KeyValue(row1, fam2, col5, 1, data));
     memstore.add(new KeyValue(row2, fam1, col1, 1, data));
 
-    List<ScanQueryMatcher.MatchCode> actual = new ArrayList<ScanQueryMatcher.MatchCode>(memstore.size());
+    List<ScanQueryMatcher.MatchCode> actual = new ArrayList<>(memstore.size());
 
     KeyValue k = memstore.get(0);
     qm.setToNewRow(k);
@@ -181,7 +181,7 @@ public class TestUserScanQueryMatcher extends AbstractTestScanQueryMatcher {
     KeyValue k = kvs[0];
     qm.setToNewRow(k);
 
-    List<MatchCode> actual = new ArrayList<MatchCode>(kvs.length);
+    List<MatchCode> actual = new ArrayList<>(kvs.length);
     for (KeyValue kv : kvs) {
       actual.add(qm.match(kv));
     }
@@ -222,7 +222,7 @@ public class TestUserScanQueryMatcher extends AbstractTestScanQueryMatcher {
     KeyValue k = kvs[0];
     qm.setToNewRow(k);
 
-    List<ScanQueryMatcher.MatchCode> actual = new ArrayList<ScanQueryMatcher.MatchCode>(kvs.length);
+    List<ScanQueryMatcher.MatchCode> actual = new ArrayList<>(kvs.length);
     for (KeyValue kv : kvs) {
       actual.add(qm.match(kv));
     }
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/wal/AbstractTestFSWAL.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/wal/AbstractTestFSWAL.java
index fb0b514..0be7b31 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/wal/AbstractTestFSWAL.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/wal/AbstractTestFSWAL.java
@@ -255,11 +255,11 @@ public abstract class AbstractTestFSWAL {
         new HRegionInfo(t2.getTableName(), HConstants.EMPTY_START_ROW, HConstants.EMPTY_END_ROW);
     // add edits and roll the wal
     MultiVersionConcurrencyControl mvcc = new MultiVersionConcurrencyControl();
-    NavigableMap<byte[], Integer> scopes1 = new TreeMap<byte[], Integer>(Bytes.BYTES_COMPARATOR);
+    NavigableMap<byte[], Integer> scopes1 = new TreeMap<>(Bytes.BYTES_COMPARATOR);
     for (byte[] fam : t1.getFamiliesKeys()) {
       scopes1.put(fam, 0);
     }
-    NavigableMap<byte[], Integer> scopes2 = new TreeMap<byte[], Integer>(Bytes.BYTES_COMPARATOR);
+    NavigableMap<byte[], Integer> scopes2 = new TreeMap<>(Bytes.BYTES_COMPARATOR);
     for (byte[] fam : t2.getFamiliesKeys()) {
       scopes2.put(fam, 0);
     }
@@ -362,7 +362,7 @@ public abstract class AbstractTestFSWAL {
     HBaseTestingUtility.closeRegionAndWAL(r);
     final int countPerFamily = 10;
     final AtomicBoolean goslow = new AtomicBoolean(false);
-    NavigableMap<byte[], Integer> scopes = new TreeMap<byte[], Integer>(Bytes.BYTES_COMPARATOR);
+    NavigableMap<byte[], Integer> scopes = new TreeMap<>(Bytes.BYTES_COMPARATOR);
     for (byte[] fam : htd.getFamiliesKeys()) {
       scopes.put(fam, 0);
     }
@@ -402,7 +402,7 @@ public abstract class AbstractTestFSWAL {
         }
       }
       // Add any old cluster id.
-      List<UUID> clusterIds = new ArrayList<UUID>(1);
+      List<UUID> clusterIds = new ArrayList<>(1);
       clusterIds.add(UUID.randomUUID());
       // Now make appends run slow.
       goslow.set(true);
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/wal/AbstractTestLogRollPeriod.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/wal/AbstractTestLogRollPeriod.java
index f70bcc8..04a4bbc 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/wal/AbstractTestLogRollPeriod.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/wal/AbstractTestLogRollPeriod.java
@@ -130,7 +130,7 @@ public abstract class AbstractTestLogRollPeriod {
 
   private void checkMinLogRolls(final WAL log, final int minRolls)
       throws Exception {
-    final List<Path> paths = new ArrayList<Path>();
+    final List<Path> paths = new ArrayList<>();
     log.registerWALActionsListener(new WALActionsListener.Base() {
       @Override
       public void postLogRoll(Path oldFile, Path newFile) {
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/wal/AbstractTestWALReplay.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/wal/AbstractTestWALReplay.java
index 90eacf0..237d24a 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/wal/AbstractTestWALReplay.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/wal/AbstractTestWALReplay.java
@@ -299,8 +299,7 @@ public abstract class AbstractTestWALReplay {
     // Add 1k to each family.
     final int countPerFamily = 1000;
 
-    NavigableMap<byte[], Integer> scopes = new TreeMap<byte[], Integer>(
-        Bytes.BYTES_COMPARATOR);
+    NavigableMap<byte[], Integer> scopes = new TreeMap<>(Bytes.BYTES_COMPARATOR);
     for(byte[] fam : htd.getFamiliesKeys()) {
       scopes.put(fam, 0);
     }
@@ -366,7 +365,7 @@ public abstract class AbstractTestWALReplay {
     Path f =  new Path(basedir, "hfile");
     HFileTestUtil.createHFile(this.conf, fs, f, family, family, Bytes.toBytes(""),
         Bytes.toBytes("z"), 10);
-    List<Pair<byte[], String>> hfs = new ArrayList<Pair<byte[], String>>(1);
+    List<Pair<byte[], String>> hfs = new ArrayList<>(1);
     hfs.add(Pair.newPair(family, f.toString()));
     region.bulkLoadHFiles(hfs, true, null);
 
@@ -434,7 +433,7 @@ public abstract class AbstractTestWALReplay {
     region.put((new Put(row)).addColumn(family, family, family));
     wal.sync();
 
-    List <Pair<byte[],String>>  hfs= new ArrayList<Pair<byte[],String>>(1);
+    List <Pair<byte[],String>>  hfs= new ArrayList<>(1);
     for (int i = 0; i < 3; i++) {
       Path f = new Path(basedir, "hfile"+i);
       HFileTestUtil.createHFile(this.conf, fs, f, family, family, Bytes.toBytes(i + "00"),
@@ -700,8 +699,7 @@ public abstract class AbstractTestWALReplay {
     HRegion region =
       HRegion.openHRegion(this.hbaseRootDir, hri, htd, wal, customConf, rsServices, null);
     int writtenRowCount = 10;
-    List<HColumnDescriptor> families = new ArrayList<HColumnDescriptor>(
-        htd.getFamilies());
+    List<HColumnDescriptor> families = new ArrayList<>(htd.getFamilies());
     for (int i = 0; i < writtenRowCount; i++) {
       Put put = new Put(Bytes.toBytes(tableName + Integer.toString(i)));
       put.addColumn(families.get(i % families.size()).getName(), Bytes.toBytes("q"),
@@ -759,7 +757,7 @@ public abstract class AbstractTestWALReplay {
 
   private int getScannedCount(RegionScanner scanner) throws IOException {
     int scannedCount = 0;
-    List<Cell> results = new ArrayList<Cell>();
+    List<Cell> results = new ArrayList<>();
     while (true) {
       boolean existMore = scanner.next(results);
       if (!results.isEmpty())
@@ -794,9 +792,8 @@ public abstract class AbstractTestWALReplay {
 
     // Add 1k to each family.
     final int countPerFamily = 1000;
-    Set<byte[]> familyNames = new HashSet<byte[]>();
-    NavigableMap<byte[], Integer> scopes = new TreeMap<byte[], Integer>(
-        Bytes.BYTES_COMPARATOR);
+    Set<byte[]> familyNames = new HashSet<>();
+    NavigableMap<byte[], Integer> scopes = new TreeMap<>(Bytes.BYTES_COMPARATOR);
     for(byte[] fam : htd.getFamiliesKeys()) {
       scopes.put(fam, 0);
     }
@@ -1048,7 +1045,7 @@ public abstract class AbstractTestWALReplay {
     deleteDir(basedir);
 
     final HTableDescriptor htd = createBasic1FamilyHTD(tableName);
-    NavigableMap<byte[], Integer> scopes = new TreeMap<byte[], Integer>(Bytes.BYTES_COMPARATOR);
+    NavigableMap<byte[], Integer> scopes = new TreeMap<>(Bytes.BYTES_COMPARATOR);
     for (byte[] fam : htd.getFamiliesKeys()) {
       scopes.put(fam, 0);
     }
@@ -1196,7 +1193,7 @@ public abstract class AbstractTestWALReplay {
 
   static List<Put> addRegionEdits(final byte[] rowName, final byte[] family, final int count,
       EnvironmentEdge ee, final Region r, final String qualifierPrefix) throws IOException {
-    List<Put> puts = new ArrayList<Put>();
+    List<Put> puts = new ArrayList<>();
     for (int j = 0; j < count; j++) {
       byte[] qualifier = Bytes.toBytes(qualifierPrefix + Integer.toString(j));
       Put p = new Put(rowName);
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/wal/FaultyProtobufLogReader.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/wal/FaultyProtobufLogReader.java
index c654c16..1af21d2 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/wal/FaultyProtobufLogReader.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/wal/FaultyProtobufLogReader.java
@@ -31,7 +31,7 @@ public class FaultyProtobufLogReader extends ProtobufLogReader {
     BEGINNING, MIDDLE, END, NONE
   }
 
-  Queue<Entry> nextQueue = new LinkedList<Entry>();
+  Queue<Entry> nextQueue = new LinkedList<>();
   int numberOfFileEntries = 0;
 
   FailureType getFailureType() {
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/wal/TestFSHLog.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/wal/TestFSHLog.java
index 9e546e6..8847c4c 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/wal/TestFSHLog.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/wal/TestFSHLog.java
@@ -102,7 +102,7 @@ public class TestFSHLog extends AbstractTestFSWAL {
       syncRunnerIndexField.set(ringBufferEventHandler, Integer.MAX_VALUE - 1);
       HTableDescriptor htd =
           new HTableDescriptor(TableName.valueOf(this.name.getMethodName())).addFamily(new HColumnDescriptor("row"));
-      NavigableMap<byte[], Integer> scopes = new TreeMap<byte[], Integer>(Bytes.BYTES_COMPARATOR);
+      NavigableMap<byte[], Integer> scopes = new TreeMap<>(Bytes.BYTES_COMPARATOR);
       for (byte[] fam : htd.getFamiliesKeys()) {
         scopes.put(fam, 0);
       }
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/wal/TestKeyValueCompression.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/wal/TestKeyValueCompression.java
index 104f897..4a256a6 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/wal/TestKeyValueCompression.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/wal/TestKeyValueCompression.java
@@ -107,7 +107,7 @@ public class TestKeyValueCompression {
     byte[] cf = Bytes.toBytes("myCF");
     byte[] q = Bytes.toBytes("myQualifier");
     byte[] value = Bytes.toBytes("myValue");
-    List<Tag> tags = new ArrayList<Tag>(noOfTags);
+    List<Tag> tags = new ArrayList<>(noOfTags);
     for (int i = 1; i <= noOfTags; i++) {
       tags.add(new ArrayBackedTag((byte) i, Bytes.toBytes("tagValue" + i)));
     }
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/wal/TestLogRollAbort.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/wal/TestLogRollAbort.java
index 22395c8..ccb00c7 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/wal/TestLogRollAbort.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/wal/TestLogRollAbort.java
@@ -199,8 +199,7 @@ public class TestLogRollAbort {
         kvs.add(new KeyValue(Bytes.toBytes(i), tableName.getName(), tableName.getName()));
         HTableDescriptor htd = new HTableDescriptor(tableName);
         htd.addFamily(new HColumnDescriptor("column"));
-        NavigableMap<byte[], Integer> scopes = new TreeMap<byte[], Integer>(
-            Bytes.BYTES_COMPARATOR);
+        NavigableMap<byte[], Integer> scopes = new TreeMap<>(Bytes.BYTES_COMPARATOR);
         for(byte[] fam : htd.getFamiliesKeys()) {
           scopes.put(fam, 0);
         }
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/wal/TestLogRolling.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/wal/TestLogRolling.java
index 19c534e..5bc4c9b 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/wal/TestLogRolling.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/wal/TestLogRolling.java
@@ -245,8 +245,8 @@ public class TestLogRolling extends AbstractTestLogRolling {
       server = TEST_UTIL.getRSForFirstRegionInTable(desc.getTableName());
       HRegionInfo region = server.getOnlineRegions(desc.getTableName()).get(0).getRegionInfo();
       final WAL log = server.getWAL(region);
-      final List<Path> paths = new ArrayList<Path>(1);
-      final List<Integer> preLogRolledCalled = new ArrayList<Integer>();
+      final List<Path> paths = new ArrayList<>(1);
+      final List<Integer> preLogRolledCalled = new ArrayList<>();
 
       paths.add(AbstractFSWALProvider.getCurrentFileName(log));
       log.registerWALActionsListener(new WALActionsListener.Base() {
@@ -307,7 +307,7 @@ public class TestLogRolling extends AbstractTestLogRolling {
         preLogRolledCalled.size() >= 1);
 
       // read back the data written
-      Set<String> loggedRows = new HashSet<String>();
+      Set<String> loggedRows = new HashSet<>();
       FSUtils fsUtils = FSUtils.getInstance(fs, TEST_UTIL.getConfiguration());
       for (Path p : paths) {
         LOG.debug("recovering lease for " + p);
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/wal/TestLogRollingNoCluster.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/wal/TestLogRollingNoCluster.java
index 7412128..d3d582c 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/wal/TestLogRollingNoCluster.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/wal/TestLogRollingNoCluster.java
@@ -157,8 +157,7 @@ public class TestLogRollingNoCluster {
           edit.add(new KeyValue(bytes, bytes, bytes, now, EMPTY_1K_ARRAY));
           final HRegionInfo hri = HRegionInfo.FIRST_META_REGIONINFO;
           final HTableDescriptor htd = TEST_UTIL.getMetaTableDescriptor();
-          NavigableMap<byte[], Integer> scopes = new TreeMap<byte[], Integer>(
-              Bytes.BYTES_COMPARATOR);
+          NavigableMap<byte[], Integer> scopes = new TreeMap<>(Bytes.BYTES_COMPARATOR);
           for(byte[] fam : htd.getFamiliesKeys()) {
             scopes.put(fam, 0);
           }
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/wal/TestSequenceIdAccounting.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/wal/TestSequenceIdAccounting.java
index 9fd0cb1..9f5acbd 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/wal/TestSequenceIdAccounting.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/wal/TestSequenceIdAccounting.java
@@ -38,7 +38,7 @@ public class TestSequenceIdAccounting {
   private static final byte [] FAMILY_NAME = Bytes.toBytes("cf");
   private static final Set<byte[]> FAMILIES;
   static {
-    FAMILIES = new HashSet<byte[]>();
+    FAMILIES = new HashSet<>();
     FAMILIES.add(FAMILY_NAME);
   }
 
@@ -46,7 +46,7 @@ public class TestSequenceIdAccounting {
   public void testStartCacheFlush() {
     SequenceIdAccounting sida = new SequenceIdAccounting();
     sida.getOrCreateLowestSequenceIds(ENCODED_REGION_NAME);
-    Map<byte[], Long> m = new HashMap<byte[], Long>();
+    Map<byte[], Long> m = new HashMap<>();
     m.put(ENCODED_REGION_NAME, HConstants.NO_SEQNUM);
     assertEquals(HConstants.NO_SEQNUM, (long)sida.startCacheFlush(ENCODED_REGION_NAME, FAMILIES));
     sida.completeCacheFlush(ENCODED_REGION_NAME);
@@ -57,7 +57,7 @@ public class TestSequenceIdAccounting {
     sida.completeCacheFlush(ENCODED_REGION_NAME);
     long currentSequenceId = sequenceid;
     sida.update(ENCODED_REGION_NAME, FAMILIES, sequenceid, true);
-    final Set<byte[]> otherFamily = new HashSet<byte[]>(1);
+    final Set<byte[]> otherFamily = new HashSet<>(1);
     otherFamily.add(Bytes.toBytes("otherCf"));
     sida.update(ENCODED_REGION_NAME, FAMILIES, ++sequenceid, true);
     // Should return oldest sequence id in the region.
@@ -69,7 +69,7 @@ public class TestSequenceIdAccounting {
   public void testAreAllLower() {
     SequenceIdAccounting sida = new SequenceIdAccounting();
     sida.getOrCreateLowestSequenceIds(ENCODED_REGION_NAME);
-    Map<byte[], Long> m = new HashMap<byte[], Long>();
+    Map<byte[], Long> m = new HashMap<>();
     m.put(ENCODED_REGION_NAME, HConstants.NO_SEQNUM);
     assertTrue(sida.areAllLower(m));
     long sequenceid = 1;
@@ -117,7 +117,7 @@ public class TestSequenceIdAccounting {
   public void testFindLower() {
     SequenceIdAccounting sida = new SequenceIdAccounting();
     sida.getOrCreateLowestSequenceIds(ENCODED_REGION_NAME);
-    Map<byte[], Long> m = new HashMap<byte[], Long>();
+    Map<byte[], Long> m = new HashMap<>();
     m.put(ENCODED_REGION_NAME, HConstants.NO_SEQNUM);
     long sequenceid = 1;
     sida.update(ENCODED_REGION_NAME, FAMILIES, sequenceid, true);
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/wal/TestWALActionsListener.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/wal/TestWALActionsListener.java
index 9ac9f0e..9f9e2df 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/wal/TestWALActionsListener.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/wal/TestWALActionsListener.java
@@ -95,7 +95,7 @@ public class TestWALActionsListener {
   @Test
   public void testActionListener() throws Exception {
     DummyWALActionsListener observer = new DummyWALActionsListener();
-    List<WALActionsListener> list = new ArrayList<WALActionsListener>(1);
+    List<WALActionsListener> list = new ArrayList<>(1);
     list.add(observer);
     final WALFactory wals = new WALFactory(conf, list, "testActionListener");
     DummyWALActionsListener laterobserver = new DummyWALActionsListener();
@@ -110,8 +110,7 @@ public class TestWALActionsListener {
       edit.add(kv);
       HTableDescriptor htd = new HTableDescriptor(TableName.valueOf(SOME_BYTES));
       htd.addFamily(new HColumnDescriptor(b));
-      NavigableMap<byte[], Integer> scopes = new TreeMap<byte[], Integer>(
-          Bytes.BYTES_COMPARATOR);
+      NavigableMap<byte[], Integer> scopes = new TreeMap<>(Bytes.BYTES_COMPARATOR);
       for(byte[] fam : htd.getFamiliesKeys()) {
         scopes.put(fam, 0);
       }
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/wal/TestWALCellCodecWithCompression.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/wal/TestWALCellCodecWithCompression.java
index c4329b8..8a246be 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/wal/TestWALCellCodecWithCompression.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/wal/TestWALCellCodecWithCompression.java
@@ -102,7 +102,7 @@ public class TestWALCellCodecWithCompression {
     byte[] cf = Bytes.toBytes("myCF");
     byte[] q = Bytes.toBytes("myQualifier");
     byte[] value = Bytes.toBytes("myValue");
-    List<Tag> tags = new ArrayList<Tag>(noOfTags);
+    List<Tag> tags = new ArrayList<>(noOfTags);
     for (int i = 1; i <= noOfTags; i++) {
       tags.add(new ArrayBackedTag((byte) i, Bytes.toBytes("tagValue" + i)));
     }
@@ -114,7 +114,7 @@ public class TestWALCellCodecWithCompression {
     byte[] cf = Bytes.toBytes("myCF");
     byte[] q = Bytes.toBytes("myQualifier");
     byte[] value = Bytes.toBytes("myValue");
-    List<Tag> tags = new ArrayList<Tag>(noOfTags);
+    List<Tag> tags = new ArrayList<>(noOfTags);
     for (int i = 1; i <= noOfTags; i++) {
       tags.add(new ArrayBackedTag((byte) i, Bytes.toBytes("tagValue" + i)));
     }
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/replication/TestNamespaceReplication.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/replication/TestNamespaceReplication.java
index 3814562..e296f87 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/replication/TestNamespaceReplication.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/replication/TestNamespaceReplication.java
@@ -165,7 +165,7 @@ public class TestNamespaceReplication extends TestReplicationBase {
     namespaces.add(ns2);
     rpc.setNamespaces(namespaces);
     Map<TableName, List<String>> tableCfs = new HashMap<>();
-    tableCfs.put(tabAName, new ArrayList<String>());
+    tableCfs.put(tabAName, new ArrayList<>());
     tableCfs.get(tabAName).add("f1");
     rpc.setTableCFsMap(tableCfs);
     admin.updatePeerConfig("2", rpc);
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/replication/TestPerTableCFReplication.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/replication/TestPerTableCFReplication.java
index a7a4cd8..abf2db3 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/replication/TestPerTableCFReplication.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/replication/TestPerTableCFReplication.java
@@ -283,7 +283,7 @@ public class TestPerTableCFReplication {
     // 1. null or empty string, result should be null
     assertNull(ReplicationSerDeHelper.convert(tabCFsMap));
 
-    tabCFsMap = new HashMap<TableName, List<String>>();
+    tabCFsMap = new HashMap<>();
     tableCFs = ReplicationSerDeHelper.convert(tabCFsMap);
     assertEquals(0, tableCFs.length);
 
@@ -301,7 +301,7 @@ public class TestPerTableCFReplication {
     assertEquals(0, tableCFs[0].getFamiliesCount());
 
     tabCFsMap.clear();
-    tabCFsMap.put(tableName2, new ArrayList<String>());
+    tabCFsMap.put(tableName2, new ArrayList<>());
     tabCFsMap.get(tableName2).add("cf1");
     tableCFs = ReplicationSerDeHelper.convert(tabCFsMap);
     assertEquals(1, tableCFs.length); // only one table
@@ -311,7 +311,7 @@ public class TestPerTableCFReplication {
     assertEquals("cf1", tableCFs[0].getFamilies(0).toStringUtf8());
 
     tabCFsMap.clear();
-    tabCFsMap.put(tableName3, new ArrayList<String>());
+    tabCFsMap.put(tableName3, new ArrayList<>());
     tabCFsMap.get(tableName3).add("cf1");
     tabCFsMap.get(tableName3).add("cf3");
     tableCFs = ReplicationSerDeHelper.convert(tabCFsMap);
@@ -324,9 +324,9 @@ public class TestPerTableCFReplication {
 
     tabCFsMap.clear();
     tabCFsMap.put(tableName1, null);
-    tabCFsMap.put(tableName2, new ArrayList<String>());
+    tabCFsMap.put(tableName2, new ArrayList<>());
     tabCFsMap.get(tableName2).add("cf1");
-    tabCFsMap.put(tableName3, new ArrayList<String>());
+    tabCFsMap.put(tableName3, new ArrayList<>());
     tabCFsMap.get(tableName3).add("cf1");
     tabCFsMap.get(tableName3).add("cf3");
 
@@ -406,7 +406,7 @@ public class TestPerTableCFReplication {
       rpc2.setClusterKey(utility2.getClusterKey());
       Map<TableName, List<String>> tableCFs = new HashMap<>();
       tableCFs.put(tabCName, null);
-      tableCFs.put(tabBName, new ArrayList<String>());
+      tableCFs.put(tabBName, new ArrayList<>());
       tableCFs.get(tabBName).add("f1");
       tableCFs.get(tabBName).add("f3");
       replicationAdmin.addPeer("2", rpc2, tableCFs);
@@ -415,7 +415,7 @@ public class TestPerTableCFReplication {
       rpc3.setClusterKey(utility3.getClusterKey());
       tableCFs.clear();
       tableCFs.put(tabAName, null);
-      tableCFs.put(tabBName, new ArrayList<String>());
+      tableCFs.put(tabBName, new ArrayList<>());
       tableCFs.get(tabBName).add("f1");
       tableCFs.get(tabBName).add("f2");
       replicationAdmin.addPeer("3", rpc3, tableCFs);
@@ -462,17 +462,17 @@ public class TestPerTableCFReplication {
 
       // B. change peers' replicable table-cf config
       tableCFs.clear();
-      tableCFs.put(tabAName, new ArrayList<String>());
+      tableCFs.put(tabAName, new ArrayList<>());
       tableCFs.get(tabAName).add("f1");
       tableCFs.get(tabAName).add("f2");
-      tableCFs.put(tabCName, new ArrayList<String>());
+      tableCFs.put(tabCName, new ArrayList<>());
       tableCFs.get(tabCName).add("f2");
       tableCFs.get(tabCName).add("f3");
       replicationAdmin.setPeerTableCFs("2", tableCFs);
 
       tableCFs.clear();
       tableCFs.put(tabBName, null);
-      tableCFs.put(tabCName, new ArrayList<String>());
+      tableCFs.put(tabCName, new ArrayList<>());
       tableCFs.get(tabCName).add("f3");
       replicationAdmin.setPeerTableCFs("3", tableCFs);
 
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/replication/TestReplicationBase.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/replication/TestReplicationBase.java
index 474039b..caad544 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/replication/TestReplicationBase.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/replication/TestReplicationBase.java
@@ -142,8 +142,7 @@ public class TestReplicationBase {
     table.addFamily(fam);
     fam = new HColumnDescriptor(noRepfamName);
     table.addFamily(fam);
-    scopes = new TreeMap<byte[], Integer>(
-        Bytes.BYTES_COMPARATOR);
+    scopes = new TreeMap<>(Bytes.BYTES_COMPARATOR);
     for(HColumnDescriptor f : table.getColumnFamilies()) {
       scopes.put(f.getName(), f.getScope());
     }
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/replication/TestReplicationEndpoint.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/replication/TestReplicationEndpoint.java
index 5e8d569..4925aab 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/replication/TestReplicationEndpoint.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/replication/TestReplicationEndpoint.java
@@ -106,7 +106,7 @@ public class TestReplicationEndpoint extends TestReplicationBase {
 
       @Override
       public String explainFailure() throws Exception {
-        List<String> logRollInProgressRsList = new ArrayList<String>();
+        List<String> logRollInProgressRsList = new ArrayList<>();
         for (RegionServerThread rs : rsThreads) {
           if (!rs.getRegionServer().walRollRequestFinished()) {
             logRollInProgressRsList.add(rs.getRegionServer().toString());
@@ -462,7 +462,7 @@ public class TestReplicationEndpoint extends TestReplicationBase {
 
   public static class ReplicationEndpointReturningFalse extends ReplicationEndpointForTest {
     static int COUNT = 10;
-    static AtomicReference<Exception> ex = new AtomicReference<Exception>(null);
+    static AtomicReference<Exception> ex = new AtomicReference<>(null);
     static AtomicBoolean replicated = new AtomicBoolean(false);
     @Override
     public boolean replicate(ReplicateContext replicateContext) {
@@ -483,7 +483,7 @@ public class TestReplicationEndpoint extends TestReplicationBase {
 
   // return a WALEntry filter which only accepts "row", but not other rows
   public static class ReplicationEndpointWithWALEntryFilter extends ReplicationEndpointForTest {
-    static AtomicReference<Exception> ex = new AtomicReference<Exception>(null);
+    static AtomicReference<Exception> ex = new AtomicReference<>(null);
 
     @Override
     public boolean replicate(ReplicateContext replicateContext) {
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/replication/TestReplicationSmallTests.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/replication/TestReplicationSmallTests.java
index 9536f9f..1c5a994 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/replication/TestReplicationSmallTests.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/replication/TestReplicationSmallTests.java
@@ -413,7 +413,7 @@ public class TestReplicationSmallTests extends TestReplicationBase {
   @Test(timeout=300000)
   public void testLoading() throws Exception {
     LOG.info("Writing out rows to table1 in testLoading");
-    List<Put> puts = new ArrayList<Put>(NB_ROWS_IN_BIG_BATCH);
+    List<Put> puts = new ArrayList<>(NB_ROWS_IN_BIG_BATCH);
     for (int i = 0; i < NB_ROWS_IN_BIG_BATCH; i++) {
       Put put = new Put(Bytes.toBytes(i));
       put.addColumn(famName, row, row);
@@ -519,8 +519,7 @@ public class TestReplicationSmallTests extends TestReplicationBase {
       fam.setMaxVersions(100);
       fam.setScope(HConstants.REPLICATION_SCOPE_GLOBAL);
       table.addFamily(fam);
-      scopes = new TreeMap<byte[], Integer>(
-          Bytes.BYTES_COMPARATOR);
+      scopes = new TreeMap<>(Bytes.BYTES_COMPARATOR);
       for (HColumnDescriptor f : table.getColumnFamilies()) {
         scopes.put(f.getName(), f.getScope());
       }
@@ -818,7 +817,7 @@ public class TestReplicationSmallTests extends TestReplicationBase {
 
     HRegion region = utility1.getMiniHBaseCluster().getRegions(tableName).get(0);
     HRegionInfo hri = region.getRegionInfo();
-    NavigableMap<byte[], Integer> scopes = new TreeMap<byte[], Integer>(Bytes.BYTES_COMPARATOR);
+    NavigableMap<byte[], Integer> scopes = new TreeMap<>(Bytes.BYTES_COMPARATOR);
     for (byte[] fam : htable1.getTableDescriptor().getFamiliesKeys()) {
       scopes.put(fam, 1);
     }
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/replication/TestReplicationStateBasic.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/replication/TestReplicationStateBasic.java
index c7c1b89..15d15b3 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/replication/TestReplicationStateBasic.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/replication/TestReplicationStateBasic.java
@@ -205,9 +205,9 @@ public abstract class TestReplicationStateBasic {
     rqc.init();
 
     List<Pair<Path, Path>> files1 = new ArrayList<>(3);
-    files1.add(new Pair<Path, Path>(null, new Path("file_1")));
-    files1.add(new Pair<Path, Path>(null, new Path("file_2")));
-    files1.add(new Pair<Path, Path>(null, new Path("file_3")));
+    files1.add(new Pair<>(null, new Path("file_1")));
+    files1.add(new Pair<>(null, new Path("file_2")));
+    files1.add(new Pair<>(null, new Path("file_3")));
     assertNull(rqc.getReplicableHFiles(ID_ONE));
     assertEquals(0, rqc.getAllPeersFromHFileRefsQueue().size());
     rp.registerPeer(ID_ONE, new ReplicationPeerConfig().setClusterKey(KEY_ONE));
@@ -241,9 +241,9 @@ public abstract class TestReplicationStateBasic {
     rq1.addPeerToHFileRefs(ID_TWO);
 
     List<Pair<Path, Path>> files1 = new ArrayList<>(3);
-    files1.add(new Pair<Path, Path>(null, new Path("file_1")));
-    files1.add(new Pair<Path, Path>(null, new Path("file_2")));
-    files1.add(new Pair<Path, Path>(null, new Path("file_3")));
+    files1.add(new Pair<>(null, new Path("file_1")));
+    files1.add(new Pair<>(null, new Path("file_2")));
+    files1.add(new Pair<>(null, new Path("file_3")));
     rq1.addHFileRefs(ID_ONE, files1);
     rq1.addHFileRefs(ID_TWO, files1);
     assertEquals(2, rqc.getAllPeersFromHFileRefsQueue().size());
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/replication/TestReplicationSyncUpTool.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/replication/TestReplicationSyncUpTool.java
index e61ceb2..9ec9b99 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/replication/TestReplicationSyncUpTool.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/replication/TestReplicationSyncUpTool.java
@@ -277,7 +277,7 @@ public class TestReplicationSyncUpTool extends TestReplicationBase {
     LOG.debug("mimicSyncUpAfterDelete");
     utility2.shutdownMiniHBaseCluster();
 
-    List<Delete> list = new ArrayList<Delete>();
+    List<Delete> list = new ArrayList<>();
     // delete half of the rows
     for (int i = 0; i < NB_ROWS_IN_BATCH / 2; i++) {
       String rowKey = "row" + i;
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/replication/TestReplicationTrackerZKImpl.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/replication/TestReplicationTrackerZKImpl.java
index 0222513..388b6cc 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/replication/TestReplicationTrackerZKImpl.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/replication/TestReplicationTrackerZKImpl.java
@@ -99,7 +99,7 @@ public class TestReplicationTrackerZKImpl {
     rsRemovedCount = new AtomicInteger(0);
     rsRemovedData = "";
     plChangedCount = new AtomicInteger(0);
-    plChangedData = new ArrayList<String>();
+    plChangedData = new ArrayList<>();
     peerRemovedCount = new AtomicInteger(0);
     peerRemovedData = "";
   }
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/replication/TestReplicationWALEntryFilters.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/replication/TestReplicationWALEntryFilters.java
index 2dbacaf..8ea0bae 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/replication/TestReplicationWALEntryFilters.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/replication/TestReplicationWALEntryFilters.java
@@ -93,19 +93,19 @@ public class TestReplicationWALEntryFilters {
     assertEquals(null, filter.filter(userEntry));
 
     // empty scopes
-    TreeMap<byte[], Integer> scopes = new TreeMap<byte[], Integer>(Bytes.BYTES_COMPARATOR);
+    TreeMap<byte[], Integer> scopes = new TreeMap<>(Bytes.BYTES_COMPARATOR);
     userEntry = createEntry(scopes, a, b);
     assertEquals(null, filter.filter(userEntry));
 
     // different scope
-    scopes = new TreeMap<byte[], Integer>(Bytes.BYTES_COMPARATOR);
+    scopes = new TreeMap<>(Bytes.BYTES_COMPARATOR);
     scopes.put(c, HConstants.REPLICATION_SCOPE_GLOBAL);
     userEntry = createEntry(scopes, a, b);
     // all kvs should be filtered
     assertEquals(userEntryEmpty, filter.filter(userEntry));
 
     // local scope
-    scopes = new TreeMap<byte[], Integer>(Bytes.BYTES_COMPARATOR);
+    scopes = new TreeMap<>(Bytes.BYTES_COMPARATOR);
     scopes.put(a, HConstants.REPLICATION_SCOPE_LOCAL);
     userEntry = createEntry(scopes, a, b);
     assertEquals(userEntryEmpty, filter.filter(userEntry));
@@ -113,7 +113,7 @@ public class TestReplicationWALEntryFilters {
     assertEquals(userEntryEmpty, filter.filter(userEntry));
 
     // only scope a
-    scopes = new TreeMap<byte[], Integer>(Bytes.BYTES_COMPARATOR);
+    scopes = new TreeMap<>(Bytes.BYTES_COMPARATOR);
     scopes.put(a, HConstants.REPLICATION_SCOPE_GLOBAL);
     userEntry = createEntry(scopes, a, b);
     assertEquals(userEntryA, filter.filter(userEntry));
@@ -121,7 +121,7 @@ public class TestReplicationWALEntryFilters {
     assertEquals(userEntryA, filter.filter(userEntry));
 
     // only scope b
-    scopes = new TreeMap<byte[], Integer>(Bytes.BYTES_COMPARATOR);
+    scopes = new TreeMap<>(Bytes.BYTES_COMPARATOR);
     scopes.put(b, HConstants.REPLICATION_SCOPE_GLOBAL);
     userEntry = createEntry(scopes, a, b);
     assertEquals(userEntryB, filter.filter(userEntry));
@@ -129,7 +129,7 @@ public class TestReplicationWALEntryFilters {
     assertEquals(userEntryB, filter.filter(userEntry));
 
     // scope a and b
-    scopes = new TreeMap<byte[], Integer>(Bytes.BYTES_COMPARATOR);
+    scopes = new TreeMap<>(Bytes.BYTES_COMPARATOR);
     scopes.put(b, HConstants.REPLICATION_SCOPE_GLOBAL);
     userEntry = createEntry(scopes, a, b);
     assertEquals(userEntryB, filter.filter(userEntry));
@@ -213,14 +213,14 @@ public class TestReplicationWALEntryFilters {
     // 2. Only config table-cfs in peer
     // empty map
     userEntry = createEntry(null, a, b, c);
-    Map<TableName, List<String>> tableCfs = new HashMap<TableName, List<String>>();
+    Map<TableName, List<String>> tableCfs = new HashMap<>();
     when(peer.getTableCFs()).thenReturn(tableCfs);
     filter = new ChainWALEntryFilter(new NamespaceTableCfWALEntryFilter(peer));
     assertEquals(null, filter.filter(userEntry));
 
     // table bar
     userEntry = createEntry(null, a, b, c);
-    tableCfs = new HashMap<TableName, List<String>>();
+    tableCfs = new HashMap<>();
     tableCfs.put(TableName.valueOf("bar"), null);
     when(peer.getTableCFs()).thenReturn(tableCfs);
     filter = new ChainWALEntryFilter(new NamespaceTableCfWALEntryFilter(peer));
@@ -228,7 +228,7 @@ public class TestReplicationWALEntryFilters {
 
     // table foo:a
     userEntry = createEntry(null, a, b, c);
-    tableCfs = new HashMap<TableName, List<String>>();
+    tableCfs = new HashMap<>();
     tableCfs.put(TableName.valueOf("foo"), Lists.newArrayList("a"));
     when(peer.getTableCFs()).thenReturn(tableCfs);
     filter = new ChainWALEntryFilter(new NamespaceTableCfWALEntryFilter(peer));
@@ -236,7 +236,7 @@ public class TestReplicationWALEntryFilters {
 
     // table foo:a,c
     userEntry = createEntry(null, a, b, c, d);
-    tableCfs = new HashMap<TableName, List<String>>();
+    tableCfs = new HashMap<>();
     tableCfs.put(TableName.valueOf("foo"), Lists.newArrayList("a", "c"));
     when(peer.getTableCFs()).thenReturn(tableCfs);
     filter = new ChainWALEntryFilter(new NamespaceTableCfWALEntryFilter(peer));
@@ -245,7 +245,7 @@ public class TestReplicationWALEntryFilters {
     // 3. Only config namespaces in peer
     when(peer.getTableCFs()).thenReturn(null);
     // empty set
-    Set<String> namespaces = new HashSet<String>();
+    Set<String> namespaces = new HashSet<>();
     when(peer.getNamespaces()).thenReturn(namespaces);
     userEntry = createEntry(null, a, b, c);
     filter = new ChainWALEntryFilter(new NamespaceTableCfWALEntryFilter(peer));
@@ -259,7 +259,7 @@ public class TestReplicationWALEntryFilters {
     assertEquals(createEntry(null, a,b,c), filter.filter(userEntry));
 
     // namespace ns1
-    namespaces = new HashSet<String>();;
+    namespaces = new HashSet<>();
     namespaces.add("ns1");
     when(peer.getNamespaces()).thenReturn(namespaces);
     userEntry = createEntry(null, a, b, c);
@@ -268,8 +268,8 @@ public class TestReplicationWALEntryFilters {
 
     // 4. Config namespaces and table-cfs both
     // Namespaces config should not confict with table-cfs config
-    namespaces = new HashSet<String>();
-    tableCfs = new HashMap<TableName, List<String>>();
+    namespaces = new HashSet<>();
+    tableCfs = new HashMap<>();
     namespaces.add("ns1");
     when(peer.getNamespaces()).thenReturn(namespaces);
     tableCfs.put(TableName.valueOf("foo"), Lists.newArrayList("a", "c"));
@@ -278,8 +278,8 @@ public class TestReplicationWALEntryFilters {
     filter = new ChainWALEntryFilter(new NamespaceTableCfWALEntryFilter(peer));
     assertEquals(createEntry(null, a, c), filter.filter(userEntry));
 
-    namespaces = new HashSet<String>();;
-    tableCfs = new HashMap<TableName, List<String>>();
+    namespaces = new HashSet<>();
+    tableCfs = new HashMap<>();
     namespaces.add("default");
     when(peer.getNamespaces()).thenReturn(namespaces);
     tableCfs.put(TableName.valueOf("ns1:foo"), Lists.newArrayList("a", "c"));
@@ -288,8 +288,8 @@ public class TestReplicationWALEntryFilters {
     filter = new ChainWALEntryFilter(new NamespaceTableCfWALEntryFilter(peer));
     assertEquals(createEntry(null, a, b, c), filter.filter(userEntry));
 
-    namespaces = new HashSet<String>();;
-    tableCfs = new HashMap<TableName, List<String>>();
+    namespaces = new HashSet<>();
+    tableCfs = new HashMap<>();
     namespaces.add("ns1");
     when(peer.getNamespaces()).thenReturn(namespaces);
     tableCfs.put(TableName.valueOf("bar"), null);
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/replication/TestReplicationWithTags.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/replication/TestReplicationWithTags.java
index ebf00e3..4369e5e 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/replication/TestReplicationWithTags.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/replication/TestReplicationWithTags.java
@@ -203,7 +203,7 @@ public class TestReplicationWithTags {
         final WALEdit edit, final Durability durability) throws IOException {
       byte[] attribute = put.getAttribute("visibility");
       byte[] cf = null;
-      List<Cell> updatedCells = new ArrayList<Cell>();
+      List<Cell> updatedCells = new ArrayList<>();
       if (attribute != null) {
         for (List<? extends Cell> edits : put.getFamilyCellMap().values()) {
           for (Cell cell : edits) {
@@ -212,7 +212,7 @@ public class TestReplicationWithTags {
               cf = CellUtil.cloneFamily(kv);
             }
             Tag tag = new ArrayBackedTag(TAG_TYPE, attribute);
-            List<Tag> tagList = new ArrayList<Tag>(1);
+            List<Tag> tagList = new ArrayList<>(1);
             tagList.add(tag);
 
             KeyValue newKV = new KeyValue(CellUtil.cloneRow(kv), 0, kv.getRowLength(),
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/replication/regionserver/TestRegionReplicaReplicationEndpointNoMaster.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/replication/regionserver/TestRegionReplicaReplicationEndpointNoMaster.java
index dcfa736..ad5063a 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/replication/regionserver/TestRegionReplicaReplicationEndpointNoMaster.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/replication/regionserver/TestRegionReplicaReplicationEndpointNoMaster.java
@@ -146,7 +146,7 @@ public class TestRegionReplicaReplicationEndpointNoMaster {
   public void after() throws Exception {
   }
 
-  static ConcurrentLinkedQueue<Entry> entries = new ConcurrentLinkedQueue<Entry>();
+  static ConcurrentLinkedQueue<Entry> entries = new ConcurrentLinkedQueue<>();
 
   public static class WALEditCopro implements WALObserver {
     public WALEditCopro() {
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/replication/regionserver/TestReplicationSink.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/replication/regionserver/TestReplicationSink.java
index 4ae14b7..0e08c90 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/replication/regionserver/TestReplicationSink.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/replication/regionserver/TestReplicationSink.java
@@ -153,8 +153,8 @@ public class TestReplicationSink {
    */
   @Test
   public void testBatchSink() throws Exception {
-    List<WALEntry> entries = new ArrayList<WALEntry>(BATCH_SIZE);
-    List<Cell> cells = new ArrayList<Cell>();
+    List<WALEntry> entries = new ArrayList<>(BATCH_SIZE);
+    List<Cell> cells = new ArrayList<>();
     for(int i = 0; i < BATCH_SIZE; i++) {
       entries.add(createEntry(TABLE_NAME1, i, KeyValue.Type.Put, cells));
     }
@@ -171,16 +171,16 @@ public class TestReplicationSink {
    */
   @Test
   public void testMixedPutDelete() throws Exception {
-    List<WALEntry> entries = new ArrayList<WALEntry>(BATCH_SIZE/2);
-    List<Cell> cells = new ArrayList<Cell>();
+    List<WALEntry> entries = new ArrayList<>(BATCH_SIZE/2);
+    List<Cell> cells = new ArrayList<>();
     for(int i = 0; i < BATCH_SIZE/2; i++) {
       entries.add(createEntry(TABLE_NAME1, i, KeyValue.Type.Put, cells));
     }
     SINK.replicateEntries(entries, CellUtil.createCellScanner(cells), replicationClusterId,
       baseNamespaceDir, hfileArchiveDir);
 
-    entries = new ArrayList<WALEntry>(BATCH_SIZE);
-    cells = new ArrayList<Cell>();
+    entries = new ArrayList<>(BATCH_SIZE);
+    cells = new ArrayList<>();
     for(int i = 0; i < BATCH_SIZE; i++) {
       entries.add(createEntry(TABLE_NAME1, i,
           i % 2 != 0 ? KeyValue.Type.Put: KeyValue.Type.DeleteColumn, cells));
@@ -199,8 +199,8 @@ public class TestReplicationSink {
    */
   @Test
   public void testMixedPutTables() throws Exception {
-    List<WALEntry> entries = new ArrayList<WALEntry>(BATCH_SIZE/2);
-    List<Cell> cells = new ArrayList<Cell>();
+    List<WALEntry> entries = new ArrayList<>(BATCH_SIZE/2);
+    List<Cell> cells = new ArrayList<>();
     for(int i = 0; i < BATCH_SIZE; i++) {
       entries.add(createEntry( i % 2 == 0 ? TABLE_NAME2 : TABLE_NAME1,
               i, KeyValue.Type.Put, cells));
@@ -221,15 +221,15 @@ public class TestReplicationSink {
    */
   @Test
   public void testMixedDeletes() throws Exception {
-    List<WALEntry> entries = new ArrayList<WALEntry>(3);
-    List<Cell> cells = new ArrayList<Cell>();
+    List<WALEntry> entries = new ArrayList<>(3);
+    List<Cell> cells = new ArrayList<>();
     for(int i = 0; i < 3; i++) {
       entries.add(createEntry(TABLE_NAME1, i, KeyValue.Type.Put, cells));
     }
     SINK.replicateEntries(entries, CellUtil.createCellScanner(cells.iterator()),
       replicationClusterId, baseNamespaceDir, hfileArchiveDir);
-    entries = new ArrayList<WALEntry>(3);
-    cells = new ArrayList<Cell>();
+    entries = new ArrayList<>(3);
+    cells = new ArrayList<>();
     entries.add(createEntry(TABLE_NAME1, 0, KeyValue.Type.DeleteColumn, cells));
     entries.add(createEntry(TABLE_NAME1, 1, KeyValue.Type.DeleteFamily, cells));
     entries.add(createEntry(TABLE_NAME1, 2, KeyValue.Type.DeleteColumn, cells));
@@ -249,8 +249,8 @@ public class TestReplicationSink {
    */
   @Test
   public void testApplyDeleteBeforePut() throws Exception {
-    List<WALEntry> entries = new ArrayList<WALEntry>(5);
-    List<Cell> cells = new ArrayList<Cell>();
+    List<WALEntry> entries = new ArrayList<>(5);
+    List<Cell> cells = new ArrayList<>();
     for(int i = 0; i < 2; i++) {
       entries.add(createEntry(TABLE_NAME1, i, KeyValue.Type.Put, cells));
     }
@@ -284,7 +284,7 @@ public class TestReplicationSink {
     }
     List<Integer> numberList = new ArrayList<>(numbers);
     Collections.sort(numberList);
-    Map<String, Long> storeFilesSize = new HashMap<String, Long>(1);
+    Map<String, Long> storeFilesSize = new HashMap<>(1);
 
     // 2. Create 25 hfiles
     Configuration conf = TEST_UTIL.getConfiguration();
@@ -313,7 +313,7 @@ public class TestReplicationSink {
               storeFiles, storeFilesSize, 1);
       edit = WALEdit.createBulkLoadEvent(regionInfo, loadDescriptor);
     }
-    List<WALEntry> entries = new ArrayList<WALEntry>(1);
+    List<WALEntry> entries = new ArrayList<>(1);
 
     // 4. Create a WALEntryBuilder
     WALEntry.Builder builder = createWALEntryBuilder(TABLE_NAME1);
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/replication/regionserver/TestReplicationSourceManager.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/replication/regionserver/TestReplicationSourceManager.java
index af3bf83..026f8e4 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/replication/regionserver/TestReplicationSourceManager.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/replication/regionserver/TestReplicationSourceManager.java
@@ -144,7 +144,7 @@ public abstract class TestReplicationSourceManager {
 
   protected static CountDownLatch latch;
 
-  protected static List<String> files = new ArrayList<String>();
+  protected static List<String> files = new ArrayList<>();
   protected static NavigableMap<byte[], Integer> scopes;
 
   protected static void setupZkAndReplication() throws Exception {
@@ -182,8 +182,7 @@ public abstract class TestReplicationSourceManager {
     col.setScope(HConstants.REPLICATION_SCOPE_LOCAL);
     htd.addFamily(col);
 
-    scopes = new TreeMap<byte[], Integer>(
-        Bytes.BYTES_COMPARATOR);
+    scopes = new TreeMap<>(Bytes.BYTES_COMPARATOR);
     for(byte[] fam : htd.getFamiliesKeys()) {
       scopes.put(fam, 0);
     }
@@ -225,7 +224,7 @@ public abstract class TestReplicationSourceManager {
     WALEdit edit = new WALEdit();
     edit.add(kv);
 
-    List<WALActionsListener> listeners = new ArrayList<WALActionsListener>(1);
+    List<WALActionsListener> listeners = new ArrayList<>(1);
     listeners.add(replication);
     final WALFactory wals = new WALFactory(utility.getConfiguration(), listeners,
         URLEncoder.encode("regionserver:60020", "UTF8"));
@@ -233,8 +232,7 @@ public abstract class TestReplicationSourceManager {
     manager.init();
     HTableDescriptor htd = new HTableDescriptor(TableName.valueOf("tableame"));
     htd.addFamily(new HColumnDescriptor(f1));
-    NavigableMap<byte[], Integer> scopes = new TreeMap<byte[], Integer>(
-        Bytes.BYTES_COMPARATOR);
+    NavigableMap<byte[], Integer> scopes = new TreeMap<>(Bytes.BYTES_COMPARATOR);
     for(byte[] fam : htd.getFamiliesKeys()) {
       scopes.put(fam, 0);
     }
@@ -341,7 +339,7 @@ public abstract class TestReplicationSourceManager {
           server.getZooKeeper()));
     rq.init(server.getServerName().toString());
     // populate some znodes in the peer znode
-    SortedSet<String> files = new TreeSet<String>();
+    SortedSet<String> files = new TreeSet<>();
     String group = "testgroup";
     String file1 = group + ".log1";
     String file2 = group + ".log2";
@@ -393,7 +391,7 @@ public abstract class TestReplicationSourceManager {
 
   @Test
   public void testBulkLoadWALEditsWithoutBulkLoadReplicationEnabled() throws Exception {
-    NavigableMap<byte[], Integer> scope = new TreeMap<byte[], Integer>(Bytes.BYTES_COMPARATOR);
+    NavigableMap<byte[], Integer> scope = new TreeMap<>(Bytes.BYTES_COMPARATOR);
     // 1. Get the bulk load wal edit event
     WALEdit logEdit = getBulkLoadWALEdit(scope);
     // 2. Create wal key
@@ -410,7 +408,7 @@ public abstract class TestReplicationSourceManager {
   @Test
   public void testBulkLoadWALEdits() throws Exception {
     // 1. Get the bulk load wal edit event
-    NavigableMap<byte[], Integer> scope = new TreeMap<byte[], Integer>(Bytes.BYTES_COMPARATOR);
+    NavigableMap<byte[], Integer> scope = new TreeMap<>(Bytes.BYTES_COMPARATOR);
     WALEdit logEdit = getBulkLoadWALEdit(scope);
     // 2. Create wal key
     WALKey logKey = new WALKey(scope);
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/security/access/HbaseObjectWritableFor96Migration.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/security/access/HbaseObjectWritableFor96Migration.java
index 3739d75..adf09d4 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/security/access/HbaseObjectWritableFor96Migration.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/security/access/HbaseObjectWritableFor96Migration.java
@@ -127,10 +127,8 @@ class HbaseObjectWritableFor96Migration implements Writable, WritableWithSize, C
   // Here we maintain two static maps of classes to code and vice versa.
   // Add new classes+codes as wanted or figure way to auto-generate these
   // maps.
-  static final Map<Integer, Class<?>> CODE_TO_CLASS =
-    new HashMap<Integer, Class<?>>();
-  static final Map<Class<?>, Integer> CLASS_TO_CODE =
-    new HashMap<Class<?>, Integer>();
+  static final Map<Integer, Class<?>> CODE_TO_CLASS = new HashMap<>();
+  static final Map<Class<?>, Integer> CLASS_TO_CODE = new HashMap<>();
   // Special code that means 'not-encoded'; in this case we do old school
   // sending of the class name using reflection, etc.
   private static final byte NOT_ENCODED = 0;
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/security/access/TestAccessControlFilter.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/security/access/TestAccessControlFilter.java
index cf01463..06389ab 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/security/access/TestAccessControlFilter.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/security/access/TestAccessControlFilter.java
@@ -112,7 +112,7 @@ public class TestAccessControlFilter extends SecureTestUtil {
       Permission.Action.READ);
 
     // put some test data
-    List<Put> puts = new ArrayList<Put>(100);
+    List<Put> puts = new ArrayList<>(100);
     for (int i=0; i<100; i++) {
       Put p = new Put(Bytes.toBytes(i));
       p.addColumn(FAMILY, PRIVATE_COL, Bytes.toBytes("secret " + i));
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/security/access/TestAccessController.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/security/access/TestAccessController.java
index 0377190..8bf2c5c 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/security/access/TestAccessController.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/security/access/TestAccessController.java
@@ -640,7 +640,7 @@ public class TestAccessController extends SecureTestUtil {
     AccessTestAction listProceduresAction = new AccessTestAction() {
       @Override
       public Object run() throws Exception {
-        List<ProcedureInfo> procInfoListClone = new ArrayList<ProcedureInfo>(procInfoList.size());
+        List<ProcedureInfo> procInfoListClone = new ArrayList<>(procInfoList.size());
         for(ProcedureInfo pi : procInfoList) {
           procInfoListClone.add(pi.clone());
         }
@@ -1763,7 +1763,7 @@ public class TestAccessController extends SecureTestUtil {
     }
 
     List<String> superUsers = Superusers.getSuperUsers();
-    List<UserPermission> adminPerms = new ArrayList<UserPermission>(superUsers.size() + 1);
+    List<UserPermission> adminPerms = new ArrayList<>(superUsers.size() + 1);
     adminPerms.add(new UserPermission(Bytes.toBytes(USER_ADMIN.getShortName()),
       AccessControlLists.ACL_TABLE_NAME, null, null, Bytes.toBytes("ACRW")));
 
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/security/access/TestCellACLWithMultipleVersions.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/security/access/TestCellACLWithMultipleVersions.java
index bbc6ad0..88cdf1d 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/security/access/TestCellACLWithMultipleVersions.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/security/access/TestCellACLWithMultipleVersions.java
@@ -248,7 +248,7 @@ public class TestCellACLWithMultipleVersions extends SecureTestUtil {
   }
 
   private Map<String, Permission> prepareCellPermissions(String[] users, Action... action) {
-    Map<String, Permission> perms = new HashMap<String, Permission>(2);
+    Map<String, Permission> perms = new HashMap<>(2);
     for (String user : users) {
       perms.put(user, new Permission(action));
     }
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/security/access/TestCellACLs.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/security/access/TestCellACLs.java
index 29bbbbb..102b28a 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/security/access/TestCellACLs.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/security/access/TestCellACLs.java
@@ -447,7 +447,7 @@ public class TestCellACLs extends SecureTestUtil {
   }
 
   private Map<String, Permission> prepareCellPermissions(String[] users, Action... action) {
-    Map<String, Permission> perms = new HashMap<String, Permission>(2);
+    Map<String, Permission> perms = new HashMap<>(2);
     for (String user : users) {
       perms.put(user, new Permission(action));
     }
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/security/access/TestWithDisabledAuthorization.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/security/access/TestWithDisabledAuthorization.java
index 7c60f68..08c8107 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/security/access/TestWithDisabledAuthorization.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/security/access/TestWithDisabledAuthorization.java
@@ -931,7 +931,7 @@ public class TestWithDisabledAuthorization extends SecureTestUtil {
       @Override
       public Object run() throws Exception {
         ACCESS_CONTROLLER.preBatchMutate(ObserverContext.createAndPrepare(RCP_ENV, null),
-          new MiniBatchOperationInProgress<Mutation>(null, null, null, 0, 0));
+          new MiniBatchOperationInProgress<>(null, null, null, 0, 0));
         return null;
       }
     }, SUPERUSER, USER_ADMIN, USER_RW, USER_RO, USER_OWNER, USER_CREATE, USER_QUAL, USER_NONE);
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/security/access/TestZKPermissionsWatcher.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/security/access/TestZKPermissionsWatcher.java
index 6582751..cb36246 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/security/access/TestZKPermissionsWatcher.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/security/access/TestZKPermissionsWatcher.java
@@ -111,7 +111,7 @@ public class TestZKPermissionsWatcher {
       TablePermission.Action.WRITE));
 
     // update ACL: george RW
-    List<TablePermission> acl = new ArrayList<TablePermission>(1);
+    List<TablePermission> acl = new ArrayList<>(1);
     acl.add(new TablePermission(TEST_TABLE, null, TablePermission.Action.READ,
       TablePermission.Action.WRITE));
     final long mtimeB = AUTH_B.getMTime();
@@ -144,7 +144,7 @@ public class TestZKPermissionsWatcher {
       TablePermission.Action.WRITE));
 
     // update ACL: hubert R
-    acl = new ArrayList<TablePermission>(1);
+    acl = new ArrayList<>(1);
     acl.add(new TablePermission(TEST_TABLE, null, TablePermission.Action.READ));
     final long mtimeA = AUTH_A.getMTime();
     AUTH_B.setTableUserPermissions("hubert", TEST_TABLE, acl);
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/security/token/TestTokenAuthentication.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/security/token/TestTokenAuthentication.java
index 5b46af5..0324359 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/security/token/TestTokenAuthentication.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/security/token/TestTokenAuthentication.java
@@ -137,7 +137,7 @@ public class TestTokenAuthentication {
       if (initialIsa.getAddress() == null) {
         throw new IllegalArgumentException("Failed resolve of " + initialIsa);
       }
-      final List<BlockingServiceAndInterface> sai = new ArrayList<BlockingServiceAndInterface>(1);
+      final List<BlockingServiceAndInterface> sai = new ArrayList<>(1);
       // Make a proxy to go between the shaded Service that rpc expects and the
       // non-shaded Service this CPEP is providing. This is because this test does a neat
       // little trick of testing the CPEP Service by inserting it as RpcServer Service. This
@@ -351,8 +351,7 @@ public class TestTokenAuthentication {
       // Ignore above passed in controller -- it is always null
       ServerRpcController serverController = new ServerRpcController();
       final NonShadedBlockingRpcCallback<AuthenticationProtos.GetAuthenticationTokenResponse>
-        callback =
-          new NonShadedBlockingRpcCallback<AuthenticationProtos.GetAuthenticationTokenResponse>();
+        callback = new NonShadedBlockingRpcCallback<>();
       getAuthenticationToken((RpcController)null, request, callback);
       try {
         serverController.checkFailed();
@@ -370,7 +369,7 @@ public class TestTokenAuthentication {
       // Ignore above passed in controller -- it is always null
       ServerRpcController serverController = new ServerRpcController();
       NonShadedBlockingRpcCallback<AuthenticationProtos.WhoAmIResponse> callback =
-          new NonShadedBlockingRpcCallback<AuthenticationProtos.WhoAmIResponse>();
+          new NonShadedBlockingRpcCallback<>();
       whoAmI(null, request, callback);
       try {
         serverController.checkFailed();
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/security/visibility/ExpAsStringVisibilityLabelServiceImpl.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/security/visibility/ExpAsStringVisibilityLabelServiceImpl.java
index 8cef21e..d8d6f1e 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/security/visibility/ExpAsStringVisibilityLabelServiceImpl.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/security/visibility/ExpAsStringVisibilityLabelServiceImpl.java
@@ -152,7 +152,7 @@ public class ExpAsStringVisibilityLabelServiceImpl implements VisibilityLabelSer
   @Override
   public List<String> getUserAuths(byte[] user, boolean systemCall) throws IOException {
     assert (labelsRegion != null || systemCall);
-    List<String> auths = new ArrayList<String>();
+    List<String> auths = new ArrayList<>();
     Get get = new Get(user);
     List<Cell> cells = null;
     if (labelsRegion == null) {
@@ -187,7 +187,7 @@ public class ExpAsStringVisibilityLabelServiceImpl implements VisibilityLabelSer
   @Override
   public List<String> getGroupAuths(String[] groups, boolean systemCall) throws IOException {
     assert (labelsRegion != null || systemCall);
-    List<String> auths = new ArrayList<String>();
+    List<String> auths = new ArrayList<>();
     if (groups != null && groups.length > 0) {
       for (String group : groups) {
         Get get = new Get(Bytes.toBytes(AuthUtil.toGroupEntry(group)));
@@ -224,7 +224,7 @@ public class ExpAsStringVisibilityLabelServiceImpl implements VisibilityLabelSer
   @Override
   public List<String> listLabels(String regex) throws IOException {
     // return an empty list for this implementation.
-    return new ArrayList<String>();
+    return new ArrayList<>();
   }
 
   @Override
@@ -237,7 +237,7 @@ public class ExpAsStringVisibilityLabelServiceImpl implements VisibilityLabelSer
       throw new IOException(e);
     }
     node = this.expressionExpander.expand(node);
-    List<Tag> tags = new ArrayList<Tag>();
+    List<Tag> tags = new ArrayList<>();
     if (withSerializationFormat) {
       tags.add(STRING_SERIALIZATION_FORMAT_TAG);
     }
@@ -270,7 +270,7 @@ public class ExpAsStringVisibilityLabelServiceImpl implements VisibilityLabelSer
       try {
         // null authorizations to be handled inside SLG impl.
         authLabels = scanLabelGenerator.getLabels(VisibilityUtils.getActiveUser(), authorizations);
-        authLabels = (authLabels == null) ? new ArrayList<String>() : authLabels;
+        authLabels = (authLabels == null) ? new ArrayList<>() : authLabels;
         authorizations = new Authorizations(authLabels);
       } catch (Throwable t) {
         LOG.error(t);
@@ -334,8 +334,8 @@ public class ExpAsStringVisibilityLabelServiceImpl implements VisibilityLabelSer
   private Tag createTag(ExpressionNode node) throws IOException {
     ByteArrayOutputStream baos = new ByteArrayOutputStream();
     DataOutputStream dos = new DataOutputStream(baos);
-    List<String> labels = new ArrayList<String>();
-    List<String> notLabels = new ArrayList<String>();
+    List<String> labels = new ArrayList<>();
+    List<String> notLabels = new ArrayList<>();
     extractLabels(node, labels, notLabels);
     Collections.sort(labels);
     Collections.sort(notLabels);
@@ -402,7 +402,7 @@ public class ExpAsStringVisibilityLabelServiceImpl implements VisibilityLabelSer
     if (Superusers.isSuperUser(user)) {
       return true;
     }
-    Set<String> auths = new HashSet<String>();
+    Set<String> auths = new HashSet<>();
     auths.addAll(this.getUserAuths(Bytes.toBytes(user.getShortName()), true));
     auths.addAll(this.getGroupAuths(user.getGroupNames(), true));
     return auths.contains(SYSTEM_LABEL);
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/security/visibility/LabelFilteringScanLabelGenerator.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/security/visibility/LabelFilteringScanLabelGenerator.java
index df77400..ed90274 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/security/visibility/LabelFilteringScanLabelGenerator.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/security/visibility/LabelFilteringScanLabelGenerator.java
@@ -44,7 +44,7 @@ public class LabelFilteringScanLabelGenerator implements ScanLabelGenerator {
   public List<String> getLabels(User user, Authorizations authorizations) {
     if (authorizations != null) {
       if (labelToFilter == null) return authorizations.getLabels();
-      List<String> newAuths = new ArrayList<String>();
+      List<String> newAuths = new ArrayList<>();
       for (String auth : authorizations.getLabels()) {
         if (!labelToFilter.equals(auth)) newAuths.add(auth);
       }
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/security/visibility/TestVisibilityLabels.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/security/visibility/TestVisibilityLabels.java
index 2111229..8348679 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/security/visibility/TestVisibilityLabels.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/security/visibility/TestVisibilityLabels.java
@@ -438,7 +438,7 @@ public abstract class TestVisibilityLabels {
       scan.setAuthorizations(new Authorizations(VisibilityUtils.SYSTEM_LABEL));
       ResultScanner scanner = ht.getScanner(scan);
       Result result = null;
-      List<Result> results = new ArrayList<Result>();
+      List<Result> results = new ArrayList<>();
       while ((result = scanner.next()) != null) {
         results.add(result);
       }
@@ -456,7 +456,7 @@ public abstract class TestVisibilityLabels {
         } catch (Throwable e) {
           fail("Should not have failed");
         }
-        List<String> authsList = new ArrayList<String>(authsResponse.getAuthList().size());
+        List<String> authsList = new ArrayList<>(authsResponse.getAuthList().size());
         for (ByteString authBS : authsResponse.getAuthList()) {
           authsList.add(Bytes.toString(authBS.toByteArray()));
         }
@@ -482,7 +482,7 @@ public abstract class TestVisibilityLabels {
           }
         } catch (Throwable e) {
         }
-        List<String> authsList = new ArrayList<String>(authsResponse.getAuthList().size());
+        List<String> authsList = new ArrayList<>(authsResponse.getAuthList().size());
         for (ByteString authBS : authsResponse.getAuthList()) {
           authsList.add(Bytes.toString(authBS.toByteArray()));
         }
@@ -496,7 +496,7 @@ public abstract class TestVisibilityLabels {
   }
 
   protected List<String> extractAuths(String user, List<Result> results) {
-    List<String> auths = new ArrayList<String>();
+    List<String> auths = new ArrayList<>();
     for (Result result : results) {
       Cell labelCell = result.getColumnLatestCell(LABELS_TABLE_FAMILY, LABEL_QUALIFIER);
       Cell userAuthCell = result.getColumnLatestCell(LABELS_TABLE_FAMILY, user.getBytes());
@@ -542,7 +542,7 @@ public abstract class TestVisibilityLabels {
              Table ht = connection.getTable(LABELS_TABLE_NAME)) {
           ResultScanner scanner = ht.getScanner(new Scan());
           Result result = null;
-          List<Result> results = new ArrayList<Result>();
+          List<Result> results = new ArrayList<>();
           while ((result = scanner.next()) != null) {
             results.add(result);
           }
@@ -557,7 +557,7 @@ public abstract class TestVisibilityLabels {
         } catch (Throwable e) {
           fail("Should not have failed");
         }
-        List<String> authsList = new ArrayList<String>(authsResponse.getAuthList().size());
+        List<String> authsList = new ArrayList<>(authsResponse.getAuthList().size());
         for (ByteString authBS : authsResponse.getAuthList()) {
           authsList.add(Bytes.toString(authBS.toByteArray()));
         }
@@ -853,7 +853,7 @@ public abstract class TestVisibilityLabels {
 
   static Table createTableAndWriteDataWithLabels(TableName tableName, String... labelExps)
       throws Exception {
-    List<Put> puts = new ArrayList<Put>(labelExps.length);
+    List<Put> puts = new ArrayList<>(labelExps.length);
     for (int i = 0; i < labelExps.length; i++) {
       Put put = new Put(Bytes.toBytes("row" + (i+1)));
       put.addColumn(fam, qual, HConstants.LATEST_TIMESTAMP, value);
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/security/visibility/TestVisibilityLabelsOpWithDifferentUsersNoACL.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/security/visibility/TestVisibilityLabelsOpWithDifferentUsersNoACL.java
index 307bd00..a3c926e 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/security/visibility/TestVisibilityLabelsOpWithDifferentUsersNoACL.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/security/visibility/TestVisibilityLabelsOpWithDifferentUsersNoACL.java
@@ -128,7 +128,7 @@ public class TestVisibilityLabelsOpWithDifferentUsersNoACL {
     authsResponse = NORMAL_USER1.runAs(action1);
     assertTrue(authsResponse.getAuthList().isEmpty());
     authsResponse = SUPERUSER.runAs(action1);
-    List<String> authsList = new ArrayList<String>(authsResponse.getAuthList().size());
+    List<String> authsList = new ArrayList<>(authsResponse.getAuthList().size());
     for (ByteString authBS : authsResponse.getAuthList()) {
       authsList.add(Bytes.toString(authBS.toByteArray()));
     }
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/security/visibility/TestVisibilityLabelsReplication.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/security/visibility/TestVisibilityLabelsReplication.java
index a10e3a9..d79e30d 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/security/visibility/TestVisibilityLabelsReplication.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/security/visibility/TestVisibilityLabelsReplication.java
@@ -378,7 +378,7 @@ public class TestVisibilityLabelsReplication {
   static Table writeData(TableName tableName, String... labelExps) throws Exception {
     Table table = TEST_UTIL.getConnection().getTable(TABLE_NAME);
     int i = 1;
-    List<Put> puts = new ArrayList<Put>(labelExps.length);
+    List<Put> puts = new ArrayList<>(labelExps.length);
     for (String labelExp : labelExps) {
       Put put = new Put(Bytes.toBytes("row" + i));
       put.addColumn(fam, qual, HConstants.LATEST_TIMESTAMP, value);
@@ -399,7 +399,7 @@ public class TestVisibilityLabelsReplication {
         Durability durability) throws IOException {
       byte[] attribute = m.getAttribute(NON_VISIBILITY);
       byte[] cf = null;
-      List<Cell> updatedCells = new ArrayList<Cell>();
+      List<Cell> updatedCells = new ArrayList<>();
       if (attribute != null) {
         for (List<? extends Cell> edits : m.getFamilyCellMap().values()) {
           for (Cell cell : edits) {
@@ -408,7 +408,7 @@ public class TestVisibilityLabelsReplication {
               cf = CellUtil.cloneFamily(kv);
             }
             Tag tag = new ArrayBackedTag((byte) NON_VIS_TAG_TYPE, attribute);
-            List<Tag> tagList = new ArrayList<Tag>(kv.getTags().size() + 1);
+            List<Tag> tagList = new ArrayList<>(kv.getTags().size() + 1);
             tagList.add(tag);
             tagList.addAll(kv.getTags());
             Cell newcell = CellUtil.createCell(kv, tagList);
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/security/visibility/TestVisibilityLabelsWithACL.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/security/visibility/TestVisibilityLabelsWithACL.java
index e236be2..f6ff640 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/security/visibility/TestVisibilityLabelsWithACL.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/security/visibility/TestVisibilityLabelsWithACL.java
@@ -300,7 +300,7 @@ public class TestVisibilityLabelsWithACL {
     GetAuthsResponse authsResponse = NORMAL_USER1.runAs(action1);
     assertNull(authsResponse);
     authsResponse = SUPERUSER.runAs(action1);
-    List<String> authsList = new ArrayList<String>(authsResponse.getAuthList().size());
+    List<String> authsList = new ArrayList<>(authsResponse.getAuthList().size());
     for (ByteString authBS : authsResponse.getAuthList()) {
       authsList.add(Bytes.toString(authBS.toByteArray()));
     }
@@ -315,7 +315,7 @@ public class TestVisibilityLabelsWithACL {
     try {
       table = TEST_UTIL.createTable(tableName, fam);
       int i = 1;
-      List<Put> puts = new ArrayList<Put>(labelExps.length);
+      List<Put> puts = new ArrayList<>(labelExps.length);
       for (String labelExp : labelExps) {
         Put put = new Put(Bytes.toBytes("row" + i));
         put.addColumn(fam, qual, HConstants.LATEST_TIMESTAMP, value);
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/security/visibility/TestVisibilityLabelsWithCustomVisLabService.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/security/visibility/TestVisibilityLabelsWithCustomVisLabService.java
index 5cc72d2..7b5a5b7 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/security/visibility/TestVisibilityLabelsWithCustomVisLabService.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/security/visibility/TestVisibilityLabelsWithCustomVisLabService.java
@@ -66,7 +66,7 @@ public class TestVisibilityLabelsWithCustomVisLabService extends TestVisibilityL
   }
 
   protected List<String> extractAuths(String user, List<Result> results) {
-    List<String> auths = new ArrayList<String>();
+    List<String> auths = new ArrayList<>();
     for (Result result : results) {
       if (Bytes.equals(result.getRow(), Bytes.toBytes(user))) {
         NavigableMap<byte[], byte[]> familyMap = result.getFamilyMap(LABELS_TABLE_FAMILY);
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/security/visibility/TestVisibilityLabelsWithDeletes.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/security/visibility/TestVisibilityLabelsWithDeletes.java
index 9853fa2..9e244ab 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/security/visibility/TestVisibilityLabelsWithDeletes.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/security/visibility/TestVisibilityLabelsWithDeletes.java
@@ -1146,7 +1146,7 @@ public class TestVisibilityLabelsWithDeletes {
     desc.addFamily(colDesc);
     hBaseAdmin.createTable(desc);
 
-    List<Put> puts = new ArrayList<Put>(5);
+    List<Put> puts = new ArrayList<>(5);
     Put put = new Put(Bytes.toBytes("row1"));
     put.addColumn(fam, qual, 123l, value);
     put.setCellVisibility(new CellVisibility(CONFIDENTIAL));
@@ -3225,7 +3225,7 @@ public class TestVisibilityLabelsWithDeletes {
     Table table = null;
     table = TEST_UTIL.createTable(tableName, fam);
     int i = 1;
-    List<Put> puts = new ArrayList<Put>(labelExps.length);
+    List<Put> puts = new ArrayList<>(labelExps.length);
     for (String labelExp : labelExps) {
       Put put = new Put(Bytes.toBytes("row" + i));
       put.addColumn(fam, qual, HConstants.LATEST_TIMESTAMP, value);
@@ -3243,7 +3243,7 @@ public class TestVisibilityLabelsWithDeletes {
     Table table = null;
     table = TEST_UTIL.createTable(tableName, fam);
     int i = 1;
-    List<Put> puts = new ArrayList<Put>(labelExps.length);
+    List<Put> puts = new ArrayList<>(labelExps.length);
     for (String labelExp : labelExps) {
       Put put = new Put(Bytes.toBytes("row" + i));
       put.addColumn(fam, qual, timestamp[i - 1], value);
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/security/visibility/TestVisibilityLablesWithGroups.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/security/visibility/TestVisibilityLablesWithGroups.java
index 940d6dc..9f24f6c 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/security/visibility/TestVisibilityLablesWithGroups.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/security/visibility/TestVisibilityLablesWithGroups.java
@@ -185,7 +185,7 @@ public class TestVisibilityLablesWithGroups {
         } catch (Throwable e) {
           fail("Should not have failed");
         }
-        List<String> authsList = new ArrayList<String>(authsResponse.getAuthList().size());
+        List<String> authsList = new ArrayList<>(authsResponse.getAuthList().size());
         for (ByteString authBS : authsResponse.getAuthList()) {
           authsList.add(Bytes.toString(authBS.toByteArray()));
         }
@@ -302,7 +302,7 @@ public class TestVisibilityLablesWithGroups {
         } catch (Throwable e) {
           fail("Should not have failed");
         }
-        List<String> authsList = new ArrayList<String>(authsResponse.getAuthList().size());
+        List<String> authsList = new ArrayList<>(authsResponse.getAuthList().size());
         for (ByteString authBS : authsResponse.getAuthList()) {
           authsList.add(Bytes.toString(authBS.toByteArray()));
         }
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/security/visibility/TestWithDisabledAuthorization.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/security/visibility/TestWithDisabledAuthorization.java
index ff348db..3d53a1e 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/security/visibility/TestWithDisabledAuthorization.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/security/visibility/TestWithDisabledAuthorization.java
@@ -142,7 +142,7 @@ public class TestWithDisabledAuthorization {
           } catch (Throwable t) {
             fail("Should not have failed");
           }
-          List<String> authsList = new ArrayList<String>(authsResponse.getAuthList().size());
+          List<String> authsList = new ArrayList<>(authsResponse.getAuthList().size());
           for (ByteString authBS : authsResponse.getAuthList()) {
             authsList.add(Bytes.toString(authBS.toByteArray()));
           }
@@ -227,7 +227,7 @@ public class TestWithDisabledAuthorization {
 
   static Table createTableAndWriteDataWithLabels(TableName tableName, String... labelExps)
       throws Exception {
-    List<Put> puts = new ArrayList<Put>(labelExps.length + 1);
+    List<Put> puts = new ArrayList<>(labelExps.length + 1);
     for (int i = 0; i < labelExps.length; i++) {
       Put put = new Put(Bytes.toBytes("row" + (i+1)));
       put.addColumn(TEST_FAMILY, TEST_QUALIFIER, HConstants.LATEST_TIMESTAMP, ZERO);
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/snapshot/SnapshotTestingUtils.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/snapshot/SnapshotTestingUtils.java
index 74f4974..ccad85b 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/snapshot/SnapshotTestingUtils.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/snapshot/SnapshotTestingUtils.java
@@ -112,7 +112,7 @@ public final class SnapshotTestingUtils {
     // list the snapshot
     List<SnapshotDescription> snapshots = admin.listSnapshots();
 
-    List<SnapshotDescription> returnedSnapshots = new ArrayList<SnapshotDescription>();
+    List<SnapshotDescription> returnedSnapshots = new ArrayList<>();
     for (SnapshotDescription sd : snapshots) {
       if (snapshotName.equals(sd.getName()) && tableName.equals(sd.getTableName())) {
         returnedSnapshots.add(sd);
@@ -213,7 +213,7 @@ public final class SnapshotTestingUtils {
     HBaseProtos.SnapshotDescription desc = SnapshotDescriptionUtils.readSnapshotInfo(fs, snapshotDir);
 
     // Extract regions and families with store files
-    final Set<byte[]> snapshotFamilies = new TreeSet<byte[]>(Bytes.BYTES_COMPARATOR);
+    final Set<byte[]> snapshotFamilies = new TreeSet<>(Bytes.BYTES_COMPARATOR);
 
     SnapshotManifest manifest = SnapshotManifest.open(conf, fs, snapshotDir, desc);
     Map<String, SnapshotRegionManifest> regionManifests = manifest.getRegionManifestsMap();
@@ -355,7 +355,7 @@ public final class SnapshotTestingUtils {
    */
   public static ArrayList<String> listHFileNames(final FileSystem fs, final Path tableDir)
       throws IOException {
-    final ArrayList<String> hfiles = new ArrayList<String>();
+    final ArrayList<String> hfiles = new ArrayList<>();
     FSVisitor.visitTableStoreFiles(fs, tableDir, new FSVisitor.StoreFileVisitor() {
       @Override
       public void storeFile(final String region, final String family, final String hfileName)
@@ -376,7 +376,7 @@ public final class SnapshotTestingUtils {
       TableName tableName, String familyName, String snapshotNameString,
       Path rootDir, FileSystem fs, boolean onlineSnapshot)
       throws Exception {
-    ArrayList<byte[]> nonEmptyFamilyNames = new ArrayList<byte[]>(1);
+    ArrayList<byte[]> nonEmptyFamilyNames = new ArrayList<>(1);
     nonEmptyFamilyNames.add(Bytes.toBytes(familyName));
     createSnapshotAndValidate(admin, tableName, nonEmptyFamilyNames, /* emptyFamilyNames= */ null,
                               snapshotNameString, rootDir, fs, onlineSnapshot);
@@ -869,7 +869,7 @@ public final class SnapshotTestingUtils {
   public static void verifyReplicasCameOnline(TableName tableName, Admin admin,
       int regionReplication) throws IOException {
     List<HRegionInfo> regions = admin.getTableRegions(tableName);
-    HashSet<HRegionInfo> set = new HashSet<HRegionInfo>();
+    HashSet<HRegionInfo> set = new HashSet<>();
     for (HRegionInfo hri : regions) {
       set.add(RegionReplicaUtil.getRegionInfoForDefaultReplica(hri));
       for (int i = 0; i < regionReplication; i++) {
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/snapshot/TestExportSnapshot.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/snapshot/TestExportSnapshot.java
index 47601ca..1beb518 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/snapshot/TestExportSnapshot.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/snapshot/TestExportSnapshot.java
@@ -205,7 +205,7 @@ public class TestExportSnapshot {
     FileSystem fs = FileSystem.get(copyDir.toUri(), new Configuration());
     copyDir = copyDir.makeQualified(fs);
 
-    List<String> opts = new ArrayList<String>();
+    List<String> opts = new ArrayList<>();
     opts.add("--snapshot");
     opts.add(Bytes.toString(snapshotName));
     opts.add("--copy-to");
@@ -302,7 +302,7 @@ public class TestExportSnapshot {
       final RegionPredicate bypassregionPredicate) throws IOException {
     final Path exportedSnapshot = new Path(rootDir,
       new Path(HConstants.SNAPSHOT_DIR_NAME, snapshotName));
-    final Set<String> snapshotFiles = new HashSet<String>();
+    final Set<String> snapshotFiles = new HashSet<>();
     final Path exportedArchive = new Path(rootDir, HConstants.HFILE_ARCHIVE_DIRECTORY);
     SnapshotReferenceUtil.visitReferencedFiles(conf, fs, exportedSnapshot,
           new SnapshotReferenceUtil.SnapshotVisitor() {
@@ -338,7 +338,7 @@ public class TestExportSnapshot {
 
   private static Set<String> listFiles(final FileSystem fs, final Path root, final Path dir)
       throws IOException {
-    Set<String> files = new HashSet<String>();
+    Set<String> files = new HashSet<>();
     int rootPrefix = root.makeQualified(fs).toString().length();
     FileStatus[] list = FSUtils.listStatus(fs, dir);
     if (list != null) {
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/snapshot/TestExportSnapshotHelpers.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/snapshot/TestExportSnapshotHelpers.java
index 77cfbcc..e31e81e 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/snapshot/TestExportSnapshotHelpers.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/snapshot/TestExportSnapshotHelpers.java
@@ -47,13 +47,13 @@ public class TestExportSnapshotHelpers {
   @Test
   public void testBalanceSplit() throws Exception {
     // Create a list of files
-    List<Pair<SnapshotFileInfo, Long>> files = new ArrayList<Pair<SnapshotFileInfo, Long>>(21);
+    List<Pair<SnapshotFileInfo, Long>> files = new ArrayList<>(21);
     for (long i = 0; i <= 20; i++) {
       SnapshotFileInfo fileInfo = SnapshotFileInfo.newBuilder()
         .setType(SnapshotFileInfo.Type.HFILE)
         .setHfile("file-" + i)
         .build();
-      files.add(new Pair<SnapshotFileInfo, Long>(fileInfo, i));
+      files.add(new Pair<>(fileInfo, i));
     }
 
     // Create 5 groups (total size 210)
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/snapshot/TestFlushSnapshotFromClient.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/snapshot/TestFlushSnapshotFromClient.java
index deb3320..86405dc 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/snapshot/TestFlushSnapshotFromClient.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/snapshot/TestFlushSnapshotFromClient.java
@@ -224,7 +224,7 @@ public class TestFlushSnapshotFromClient {
     // take a snapshot of the enabled table
     String snapshotString = "offlineTableSnapshot";
     byte[] snapshot = Bytes.toBytes(snapshotString);
-    Map<String, String> props = new HashMap<String, String>();
+    Map<String, String> props = new HashMap<>();
     props.put("table", TABLE_NAME.getNameAsString());
     admin.execProcedure(SnapshotManager.ONLINE_SNAPSHOT_CONTROLLER_DESCRIPTION,
         snapshotString, props);
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/util/BaseTestHBaseFsck.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/util/BaseTestHBaseFsck.java
index b7fb9f7..02bd49b 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/util/BaseTestHBaseFsck.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/util/BaseTestHBaseFsck.java
@@ -259,7 +259,7 @@ public class BaseTestHBaseFsck {
     createTable(TEST_UTIL, desc, SPLITS);
 
     tbl = connection.getTable(tablename, tableExecutorService);
-    List<Put> puts = new ArrayList<Put>(ROWKEYS.length);
+    List<Put> puts = new ArrayList<>(ROWKEYS.length);
     for (byte[] row : ROWKEYS) {
       Put p = new Put(row);
       p.addColumn(FAM, Bytes.toBytes("val"), row);
@@ -283,7 +283,7 @@ public class BaseTestHBaseFsck {
     createTable(TEST_UTIL, desc, SPLITS);
 
     tbl = connection.getTable(tablename, tableExecutorService);
-    List<Put> puts = new ArrayList<Put>(ROWKEYS.length);
+    List<Put> puts = new ArrayList<>(ROWKEYS.length);
     for (byte[] row : ROWKEYS) {
       Put p = new Put(row);
       p.addColumn(FAM, Bytes.toBytes("val"), row);
@@ -328,8 +328,7 @@ public class BaseTestHBaseFsck {
   Map<ServerName, List<String>> getDeployedHRIs(final Admin admin) throws IOException {
     ClusterStatus status = admin.getClusterStatus();
     Collection<ServerName> regionServers = status.getServers();
-    Map<ServerName, List<String>> mm =
-        new HashMap<ServerName, List<String>>();
+    Map<ServerName, List<String>> mm = new HashMap<>();
     for (ServerName hsi : regionServers) {
       AdminProtos.AdminService.BlockingInterface server = connection.getAdmin(hsi);
 
@@ -525,7 +524,7 @@ public class BaseTestHBaseFsck {
     @Override
     public ArrayList<ERROR_CODE> getErrorList() {
       calledCount++;
-      return new ArrayList<ERROR_CODE>();
+      return new ArrayList<>();
     }
 
     @Override
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/util/ConstantDelayQueue.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/util/ConstantDelayQueue.java
index 73ce71a..ddf26a0 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/util/ConstantDelayQueue.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/util/ConstantDelayQueue.java
@@ -57,7 +57,7 @@ public class ConstantDelayQueue<E> implements BlockingQueue<E> {
   private final long delayMs;
 
   // backing DelayQueue
-  private DelayQueue<DelayedElement<E>> queue = new DelayQueue<DelayedElement<E>>();
+  private DelayQueue<DelayedElement<E>> queue = new DelayQueue<>();
 
   public ConstantDelayQueue(TimeUnit timeUnit, long delay) {
     this.delayMs = TimeUnit.MILLISECONDS.convert(delay, timeUnit);
@@ -139,22 +139,22 @@ public class ConstantDelayQueue<E> implements BlockingQueue<E> {
 
   @Override
   public boolean add(E e) {
-    return queue.add(new DelayedElement<E>(e, delayMs));
+    return queue.add(new DelayedElement<>(e, delayMs));
   }
 
   @Override
   public boolean offer(E e) {
-    return queue.offer(new DelayedElement<E>(e, delayMs));
+    return queue.offer(new DelayedElement<>(e, delayMs));
   }
 
   @Override
   public void put(E e) throws InterruptedException {
-    queue.put(new DelayedElement<E>(e, delayMs));
+    queue.put(new DelayedElement<>(e, delayMs));
   }
 
   @Override
   public boolean offer(E e, long timeout, TimeUnit unit) throws InterruptedException {
-    return queue.offer(new DelayedElement<E>(e, delayMs), timeout, unit);
+    return queue.offer(new DelayedElement<>(e, delayMs), timeout, unit);
   }
 
   @Override
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/util/HFileArchiveTestingUtil.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/util/HFileArchiveTestingUtil.java
index d68c578..bf95a9e 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/util/HFileArchiveTestingUtil.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/util/HFileArchiveTestingUtil.java
@@ -143,14 +143,14 @@ public class HFileArchiveTestingUtil {
    * @return <expected, gotten, backup>, where each is sorted
    */
   private static List<List<String>> getFileLists(FileStatus[] previous, FileStatus[] archived) {
-    List<List<String>> files = new ArrayList<List<String>>(3);
+    List<List<String>> files = new ArrayList<>(3);
 
     // copy over the original files
     List<String> originalFileNames = convertToString(previous);
     files.add(originalFileNames);
 
-    List<String> currentFiles = new ArrayList<String>(previous.length);
-    List<FileStatus> backedupFiles = new ArrayList<FileStatus>(previous.length);
+    List<String> currentFiles = new ArrayList<>(previous.length);
+    List<FileStatus> backedupFiles = new ArrayList<>(previous.length);
     for (FileStatus f : archived) {
       String name = f.getPath().getName();
       // if the file has been backed up
@@ -177,7 +177,7 @@ public class HFileArchiveTestingUtil {
   }
 
   private static List<String> convertToString(List<FileStatus> files) {
-    List<String> originalFileNames = new ArrayList<String>(files.size());
+    List<String> originalFileNames = new ArrayList<>(files.size());
     for (FileStatus f : files) {
       originalFileNames.add(f.getPath().getName());
     }
@@ -188,7 +188,7 @@ public class HFileArchiveTestingUtil {
   private static String compareFileLists(List<String> expected, List<String> gotten) {
     StringBuilder sb = new StringBuilder("Expected (" + expected.size() + "): \t\t Gotten ("
         + gotten.size() + "):\n");
-    List<String> notFound = new ArrayList<String>();
+    List<String> notFound = new ArrayList<>();
     for (String s : expected) {
       if (gotten.contains(s)) sb.append(s + "\t\t" + s + "\n");
       else notFound.add(s);
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/util/LoadTestDataGeneratorWithTags.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/util/LoadTestDataGeneratorWithTags.java
index 87cb070..2ea01bb 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/util/LoadTestDataGeneratorWithTags.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/util/LoadTestDataGeneratorWithTags.java
@@ -64,7 +64,7 @@ public class LoadTestDataGeneratorWithTags extends DefaultDataGenerator {
   @Override
   public Mutation beforeMutate(long rowkeyBase, Mutation m) throws IOException {
     if (m instanceof Put) {
-      List<Cell> updatedCells = new ArrayList<Cell>();
+      List<Cell> updatedCells = new ArrayList<>();
       int numTags;
       if (minNumTags == maxNumTags) {
         numTags = minNumTags;
@@ -76,7 +76,7 @@ public class LoadTestDataGeneratorWithTags extends DefaultDataGenerator {
         Cell cell = cellScanner.current();
         byte[] tag = LoadTestTool.generateData(random,
             minTagLength + random.nextInt(maxTagLength - minTagLength));
-        tags = new ArrayList<Tag>();
+        tags = new ArrayList<>();
         for (int n = 0; n < numTags; n++) {
           tags.add(new ArrayBackedTag((byte) 127, tag));
         }
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/util/LoadTestTool.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/util/LoadTestTool.java
index 9d62693..9a5e6f1 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/util/LoadTestTool.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/util/LoadTestTool.java
@@ -864,7 +864,7 @@ public class LoadTestTool extends AbstractHBaseTool {
     }
 
     // starting to load multiple tables
-    List<WorkerThread> workers = new ArrayList<WorkerThread>();
+    List<WorkerThread> workers = new ArrayList<>();
     for (int i = 0; i < numTables; i++) {
       String[] workerArgs = newArgs.clone();
       workerArgs[tableNameValueIndex] = tableName + "_" + (i+1);
@@ -892,7 +892,7 @@ public class LoadTestTool extends AbstractHBaseTool {
 
   // If an exception is thrown by one of worker threads, it will be
   // stored here.
-  protected AtomicReference<Throwable> thrown = new AtomicReference<Throwable>();
+  protected AtomicReference<Throwable> thrown = new AtomicReference<>();
 
   private void workerThreadError(Throwable t) {
     thrown.compareAndSet(null, t);
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/util/MultiThreadedAction.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/util/MultiThreadedAction.java
index db42659..8da92b2 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/util/MultiThreadedAction.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/util/MultiThreadedAction.java
@@ -533,7 +533,7 @@ public abstract class MultiThreadedAction {
 
   // Parse mutate info into a map of <column name> => <update action>
   private Map<String, MutationType> parseMutateInfo(byte[] mutateInfo) {
-    Map<String, MutationType> mi = new HashMap<String, MutationType>();
+    Map<String, MutationType> mi = new HashMap<>();
     if (mutateInfo != null) {
       String mutateInfoStr = Bytes.toString(mutateInfo);
       String[] mutations = mutateInfoStr.split("#");
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/util/MultiThreadedReader.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/util/MultiThreadedReader.java
index 77443e1..e6de33d 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/util/MultiThreadedReader.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/util/MultiThreadedReader.java
@@ -41,7 +41,7 @@ public class MultiThreadedReader extends MultiThreadedAction
 {
   private static final Log LOG = LogFactory.getLog(MultiThreadedReader.class);
 
-  protected Set<HBaseReaderThread> readers = new HashSet<HBaseReaderThread>();
+  protected Set<HBaseReaderThread> readers = new HashSet<>();
   private final double verifyPercent;
   protected volatile boolean aborted;
 
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/util/MultiThreadedReaderWithACL.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/util/MultiThreadedReaderWithACL.java
index cdf814c..1e7e341 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/util/MultiThreadedReaderWithACL.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/util/MultiThreadedReaderWithACL.java
@@ -43,8 +43,8 @@ public class MultiThreadedReaderWithACL extends MultiThreadedReader {
    * Maps user with Table instance. Because the table instance has to be created
    * per user inorder to work in that user's context
    */
-  private Map<String, Table> userVsTable = new HashMap<String, Table>();
-  private Map<String, User> users = new HashMap<String, User>();
+  private Map<String, Table> userVsTable = new HashMap<>();
+  private Map<String, User> users = new HashMap<>();
   private String[] userNames;
 
   public MultiThreadedReaderWithACL(LoadTestDataGenerator dataGen, Configuration conf,
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/util/MultiThreadedUpdater.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/util/MultiThreadedUpdater.java
index dbcfddb..1505fc1 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/util/MultiThreadedUpdater.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/util/MultiThreadedUpdater.java
@@ -57,7 +57,7 @@ import com.google.common.base.Preconditions;
 public class MultiThreadedUpdater extends MultiThreadedWriterBase {
   private static final Log LOG = LogFactory.getLog(MultiThreadedUpdater.class);
 
-  protected Set<HBaseUpdaterThread> updaters = new HashSet<HBaseUpdaterThread>();
+  protected Set<HBaseUpdaterThread> updaters = new HashSet<>();
 
   private MultiThreadedWriterBase writer = null;
   private boolean isBatchUpdate = false;
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/util/MultiThreadedUpdaterWithACL.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/util/MultiThreadedUpdaterWithACL.java
index bf27dde..40e23fb 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/util/MultiThreadedUpdaterWithACL.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/util/MultiThreadedUpdaterWithACL.java
@@ -53,8 +53,8 @@ public class MultiThreadedUpdaterWithACL extends MultiThreadedUpdater {
    * Maps user with Table instance. Because the table instance has to be created
    * per user inorder to work in that user's context
    */
-  private Map<String, Table> userVsTable = new HashMap<String, Table>();
-  private Map<String, User> users = new HashMap<String, User>();
+  private Map<String, Table> userVsTable = new HashMap<>();
+  private Map<String, User> users = new HashMap<>();
   private String[] userNames;
 
   public MultiThreadedUpdaterWithACL(LoadTestDataGenerator dataGen, Configuration conf,
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/util/MultiThreadedWriter.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/util/MultiThreadedWriter.java
index d53ab25..d62f72d 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/util/MultiThreadedWriter.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/util/MultiThreadedWriter.java
@@ -43,7 +43,7 @@ import org.apache.hadoop.util.StringUtils;
 public class MultiThreadedWriter extends MultiThreadedWriterBase {
   private static final Log LOG = LogFactory.getLog(MultiThreadedWriter.class);
 
-  protected Set<HBaseWriterThread> writers = new HashSet<HBaseWriterThread>();
+  protected Set<HBaseWriterThread> writers = new HashSet<>();
 
   protected boolean isMultiPut = false;
 
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/util/MultiThreadedWriterBase.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/util/MultiThreadedWriterBase.java
index 1bbd410..fbf745f 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/util/MultiThreadedWriterBase.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/util/MultiThreadedWriterBase.java
@@ -60,7 +60,7 @@ public abstract class MultiThreadedWriterBase extends MultiThreadedAction {
   protected AtomicLong wroteUpToKey = new AtomicLong();
 
   /** The sorted set of keys NOT inserted/updated by the writers */
-  protected Set<Long> failedKeySet = new ConcurrentSkipListSet<Long>();
+  protected Set<Long> failedKeySet = new ConcurrentSkipListSet<>();
 
   /**
    * The total size of the temporary inserted/updated key set that have not yet lined
@@ -79,7 +79,7 @@ public abstract class MultiThreadedWriterBase extends MultiThreadedAction {
   }
 
   protected BlockingQueue<Long> createWriteKeysQueue(Configuration conf) {
-    return new ArrayBlockingQueue<Long>(10000);
+    return new ArrayBlockingQueue<>(10000);
   }
 
   @Override
@@ -129,7 +129,7 @@ public abstract class MultiThreadedWriterBase extends MultiThreadedAction {
       Thread.currentThread().setName(getClass().getSimpleName());
       try {
         long expectedKey = startKey;
-        Queue<Long> sortedKeys = new PriorityQueue<Long>();
+        Queue<Long> sortedKeys = new PriorityQueue<>();
         while (expectedKey < endKey) {
           // Block until a new element is available.
           Long k;
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/util/ProcessBasedLocalHBaseCluster.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/util/ProcessBasedLocalHBaseCluster.java
index 6d992a5..a5cf0bd 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/util/ProcessBasedLocalHBaseCluster.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/util/ProcessBasedLocalHBaseCluster.java
@@ -140,7 +140,7 @@ public class ProcessBasedLocalHBaseCluster {
    * in the returned array, e.g. server #0, #1, etc.
    */
   private static List<Integer> sortedPorts(int n) {
-    List<Integer> ports = new ArrayList<Integer>(n);
+    List<Integer> ports = new ArrayList<>(n);
     for (int i = 0; i < n; ++i) {
       ports.add(HBaseTestingUtility.randomFreePort());
     }
@@ -216,8 +216,7 @@ public class ProcessBasedLocalHBaseCluster {
     try {
       String [] envp = null;
       if (envOverrides != null) {
-        Map<String, String> map = new HashMap<String, String>(
-            System.getenv());
+        Map<String, String> map = new HashMap<>(System.getenv());
         map.putAll(envOverrides);
         envp = new String[map.size()];
         int idx = 0;
@@ -250,7 +249,7 @@ public class ProcessBasedLocalHBaseCluster {
 
   private void shutdownAllProcesses() {
     LOG.info("Killing daemons using pid files");
-    final List<String> pidFiles = new ArrayList<String>(daemonPidFiles);
+    final List<String> pidFiles = new ArrayList<>(daemonPidFiles);
     for (String pidFile : pidFiles) {
       int pid = 0;
       try {
@@ -359,7 +358,7 @@ public class ProcessBasedLocalHBaseCluster {
         "HBASE_ZOOKEEPER_JMX_OPTS=' '\n",
         dir + "/hbase-env.sh");
 
-    Map<String, String> envOverrides = new HashMap<String, String>();
+    Map<String, String> envOverrides = new HashMap<>();
     envOverrides.put("HBASE_LOG_DIR", dir);
     envOverrides.put("HBASE_PID_DIR", dir);
     try {
@@ -379,7 +378,7 @@ public class ProcessBasedLocalHBaseCluster {
   private final String generateConfig(ServerType serverType, int rpcPort,
       String daemonDir) {
     StringBuilder sb = new StringBuilder();
-    Map<String, Object> confMap = new TreeMap<String, Object>();
+    Map<String, Object> confMap = new TreeMap<>();
     confMap.put(HConstants.CLUSTER_DISTRIBUTED, true);
 
     if (serverType == ServerType.MASTER) {
@@ -446,8 +445,8 @@ public class ProcessBasedLocalHBaseCluster {
   }
 
   private final class LocalDaemonLogTailer implements Runnable {
-    private final Set<String> tailedFiles = new HashSet<String>();
-    private final List<String> dirList = new ArrayList<String>();
+    private final Set<String> tailedFiles = new HashSet<>();
+    private final List<String> dirList = new ArrayList<>();
     private final Object printLock = new Object();
 
     private final FilenameFilter LOG_FILES = new FilenameFilter() {
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/util/TestBoundedPriorityBlockingQueue.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/util/TestBoundedPriorityBlockingQueue.java
index 34c4ec0..7112d50 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/util/TestBoundedPriorityBlockingQueue.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/util/TestBoundedPriorityBlockingQueue.java
@@ -73,7 +73,7 @@ public class TestBoundedPriorityBlockingQueue {
 
   @Before
   public void setUp() throws Exception {
-    this.queue = new BoundedPriorityBlockingQueue<TestObject>(CAPACITY, new TestObjectComparator());
+    this.queue = new BoundedPriorityBlockingQueue<>(CAPACITY, new TestObjectComparator());
   }
 
   @After
@@ -186,7 +186,7 @@ public class TestBoundedPriorityBlockingQueue {
   @Test
   public void testPoll() {
     assertNull(queue.poll());
-    PriorityQueue<TestObject> testList = new PriorityQueue<TestObject>(CAPACITY, new TestObjectComparator());
+    PriorityQueue<TestObject> testList = new PriorityQueue<>(CAPACITY, new TestObjectComparator());
 
     for (int i = 0; i < CAPACITY; ++i) {
       TestObject obj = new TestObject(i, i);
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/util/TestCoprocessorScanPolicy.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/util/TestCoprocessorScanPolicy.java
index 304717a..caf8de9 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/util/TestCoprocessorScanPolicy.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/util/TestCoprocessorScanPolicy.java
@@ -210,10 +210,8 @@ public class TestCoprocessorScanPolicy {
   }
 
   public static class ScanObserver implements RegionObserver {
-    private Map<TableName, Long> ttls =
-        new HashMap<TableName, Long>();
-    private Map<TableName, Integer> versions =
-        new HashMap<TableName, Integer>();
+    private Map<TableName, Long> ttls = new HashMap<>();
+    private Map<TableName, Integer> versions = new HashMap<>();
 
     // lame way to communicate with the coprocessor,
     // since it is loaded by a different class loader
@@ -301,4 +299,4 @@ public class TestCoprocessorScanPolicy {
       }
     }
   }
-}
\ No newline at end of file
+}
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/util/TestFSVisitor.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/util/TestFSVisitor.java
index 4a870f8..e455b0a 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/util/TestFSVisitor.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/util/TestFSVisitor.java
@@ -59,9 +59,9 @@ public class TestFSVisitor {
     fs = FileSystem.get(TEST_UTIL.getConfiguration());
     rootDir = TEST_UTIL.getDataTestDir("hbase");
 
-    tableFamilies = new HashSet<String>();
-    tableRegions = new HashSet<String>();
-    tableHFiles = new HashSet<String>();
+    tableFamilies = new HashSet<>();
+    tableRegions = new HashSet<>();
+    tableHFiles = new HashSet<>();
     tableDir = createTableFiles(rootDir, TABLE_NAME, tableRegions, tableFamilies, tableHFiles);
     FSUtils.logFileSystemState(fs, rootDir, LOG);
   }
@@ -73,9 +73,9 @@ public class TestFSVisitor {
 
   @Test
   public void testVisitStoreFiles() throws IOException {
-    final Set<String> regions = new HashSet<String>();
-    final Set<String> families = new HashSet<String>();
-    final Set<String> hfiles = new HashSet<String>();
+    final Set<String> regions = new HashSet<>();
+    final Set<String> families = new HashSet<>();
+    final Set<String> hfiles = new HashSet<>();
     FSVisitor.visitTableStoreFiles(fs, tableDir, new FSVisitor.StoreFileVisitor() {
       public void storeFile(final String region, final String family, final String hfileName)
           throws IOException {
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/util/TestHBaseFsckEncryption.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/util/TestHBaseFsckEncryption.java
index 501bfc4..54f310d 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/util/TestHBaseFsckEncryption.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/util/TestHBaseFsckEncryption.java
@@ -139,7 +139,7 @@ public class TestHBaseFsckEncryption {
   }
 
   private List<Path> findStorefilePaths(TableName tableName) throws Exception {
-    List<Path> paths = new ArrayList<Path>();
+    List<Path> paths = new ArrayList<>();
     for (Region region:
         TEST_UTIL.getRSForFirstRegionInTable(tableName).getOnlineRegions(htd.getTableName())) {
       for (Store store: region.getStores()) {
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/util/TestHBaseFsckMOB.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/util/TestHBaseFsckMOB.java
index b04689c..b6a185b 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/util/TestHBaseFsckMOB.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/util/TestHBaseFsckMOB.java
@@ -62,7 +62,7 @@ public class TestHBaseFsckMOB extends BaseTestHBaseFsck {
     TEST_UTIL.startMiniCluster(1);
 
     tableExecutorService = new ThreadPoolExecutor(1, POOL_SIZE, 60, TimeUnit.SECONDS,
-        new SynchronousQueue<Runnable>(), Threads.newDaemonThreadFactory("testhbck"));
+        new SynchronousQueue<>(), Threads.newDaemonThreadFactory("testhbck"));
 
     hbfsckExecutorService = new ScheduledThreadPoolExecutor(POOL_SIZE);
 
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/util/TestHBaseFsckOneRS.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/util/TestHBaseFsckOneRS.java
index 0e3355a..1d09dfa 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/util/TestHBaseFsckOneRS.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/util/TestHBaseFsckOneRS.java
@@ -112,7 +112,7 @@ public class TestHBaseFsckOneRS extends BaseTestHBaseFsck {
     TEST_UTIL.startMiniCluster(1);
 
     tableExecutorService = new ThreadPoolExecutor(1, POOL_SIZE, 60, TimeUnit.SECONDS,
-        new SynchronousQueue<Runnable>(), Threads.newDaemonThreadFactory("testhbck"));
+        new SynchronousQueue<>(), Threads.newDaemonThreadFactory("testhbck"));
 
     hbfsckExecutorService = new ScheduledThreadPoolExecutor(POOL_SIZE);
 
@@ -1402,7 +1402,7 @@ public class TestHBaseFsckOneRS extends BaseTestHBaseFsck {
       setupTable(tableName);
 
       // Mess it up by removing the RegionInfo for one region.
-      final List<Delete> deletes = new LinkedList<Delete>();
+      final List<Delete> deletes = new LinkedList<>();
       Table meta = connection.getTable(TableName.META_TABLE_NAME, hbfsckExecutorService);
       MetaTableAccessor.fullScanRegions(connection, new MetaTableAccessor.Visitor() {
 
@@ -1630,7 +1630,7 @@ public class TestHBaseFsckOneRS extends BaseTestHBaseFsck {
         am.regionOffline(state.getRegion());
       }
 
-      Map<HRegionInfo, ServerName> regionsMap = new HashMap<HRegionInfo, ServerName>();
+      Map<HRegionInfo, ServerName> regionsMap = new HashMap<>();
       regionsMap.put(regions.get(0).getRegionInfo(), regionServer.getServerName());
       am.assign(regionsMap);
       am.waitForAssignment(regions.get(0).getRegionInfo());
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/util/TestHBaseFsckReplicas.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/util/TestHBaseFsckReplicas.java
index 9b92a69..7956d40 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/util/TestHBaseFsckReplicas.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/util/TestHBaseFsckReplicas.java
@@ -74,7 +74,7 @@ public class TestHBaseFsckReplicas extends BaseTestHBaseFsck {
     TEST_UTIL.startMiniCluster(3);
 
     tableExecutorService = new ThreadPoolExecutor(1, POOL_SIZE, 60, TimeUnit.SECONDS,
-        new SynchronousQueue<Runnable>(), Threads.newDaemonThreadFactory("testhbck"));
+        new SynchronousQueue<>(), Threads.newDaemonThreadFactory("testhbck"));
 
     hbfsckExecutorService = new ScheduledThreadPoolExecutor(POOL_SIZE);
 
@@ -255,7 +255,7 @@ public class TestHBaseFsckReplicas extends BaseTestHBaseFsck {
       }
       // get all the online regions in the regionservers
       Collection<ServerName> servers = admin.getClusterStatus().getServers();
-      Set<HRegionInfo> onlineRegions = new HashSet<HRegionInfo>();
+      Set<HRegionInfo> onlineRegions = new HashSet<>();
       for (ServerName s : servers) {
         List<HRegionInfo> list = admin.getOnlineRegions(s);
         onlineRegions.addAll(list);
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/util/TestHBaseFsckTwoRS.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/util/TestHBaseFsckTwoRS.java
index 99a41f5..91a71c7 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/util/TestHBaseFsckTwoRS.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/util/TestHBaseFsckTwoRS.java
@@ -86,7 +86,7 @@ public class TestHBaseFsckTwoRS extends BaseTestHBaseFsck {
     TEST_UTIL.startMiniCluster(2);
 
     tableExecutorService = new ThreadPoolExecutor(1, POOL_SIZE, 60, TimeUnit.SECONDS,
-        new SynchronousQueue<Runnable>(), Threads.newDaemonThreadFactory("testhbck"));
+        new SynchronousQueue<>(), Threads.newDaemonThreadFactory("testhbck"));
 
     hbfsckExecutorService = new ScheduledThreadPoolExecutor(POOL_SIZE);
 
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/util/TestIdLock.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/util/TestIdLock.java
index fbfbb47..c3f934d 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/util/TestIdLock.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/util/TestIdLock.java
@@ -50,7 +50,7 @@ public class TestIdLock {
 
   private IdLock idLock = new IdLock();
 
-  private Map<Long, String> idOwner = new ConcurrentHashMap<Long, String>();
+  private Map<Long, String> idOwner = new ConcurrentHashMap<>();
 
   private class IdLockTestThread implements Callable<Boolean> {
 
@@ -95,8 +95,7 @@ public class TestIdLock {
   public void testMultipleClients() throws Exception {
     ExecutorService exec = Executors.newFixedThreadPool(NUM_THREADS);
     try {
-      ExecutorCompletionService<Boolean> ecs =
-          new ExecutorCompletionService<Boolean>(exec);
+      ExecutorCompletionService<Boolean> ecs = new ExecutorCompletionService<>(exec);
       for (int i = 0; i < NUM_THREADS; ++i)
         ecs.submit(new IdLockTestThread("client_" + i));
       for (int i = 0; i < NUM_THREADS; ++i) {
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/util/TestIdReadWriteLock.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/util/TestIdReadWriteLock.java
index 66f6d4b..2ccfad8 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/util/TestIdReadWriteLock.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/util/TestIdReadWriteLock.java
@@ -53,7 +53,7 @@ public class TestIdReadWriteLock {
 
   private IdReadWriteLock idLock = new IdReadWriteLock();
 
-  private Map<Long, String> idOwner = new ConcurrentHashMap<Long, String>();
+  private Map<Long, String> idOwner = new ConcurrentHashMap<>();
 
   private class IdLockTestThread implements Callable<Boolean> {
 
@@ -104,8 +104,7 @@ public class TestIdReadWriteLock {
   public void testMultipleClients() throws Exception {
     ExecutorService exec = Executors.newFixedThreadPool(NUM_THREADS);
     try {
-      ExecutorCompletionService<Boolean> ecs =
-          new ExecutorCompletionService<Boolean>(exec);
+      ExecutorCompletionService<Boolean> ecs = new ExecutorCompletionService<>(exec);
       for (int i = 0; i < NUM_THREADS; ++i)
         ecs.submit(new IdLockTestThread("client_" + i));
       for (int i = 0; i < NUM_THREADS; ++i) {
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/util/TestMiniClusterLoadEncoded.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/util/TestMiniClusterLoadEncoded.java
index 0cf4609..865cd11 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/util/TestMiniClusterLoadEncoded.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/util/TestMiniClusterLoadEncoded.java
@@ -40,7 +40,7 @@ public class TestMiniClusterLoadEncoded extends TestMiniClusterLoadParallel {
 
   @Parameters
   public static Collection<Object[]> parameters() {
-    List<Object[]> parameters = new ArrayList<Object[]>();
+    List<Object[]> parameters = new ArrayList<>();
     for (DataBlockEncoding dataBlockEncoding : DataBlockEncoding.values() ) {
       parameters.add(new Object[]{dataBlockEncoding});
     }
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/util/TestMiniClusterLoadSequential.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/util/TestMiniClusterLoadSequential.java
index 726a450..f765221 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/util/TestMiniClusterLoadSequential.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/util/TestMiniClusterLoadSequential.java
@@ -88,7 +88,7 @@ public class TestMiniClusterLoadSequential {
 
   @Parameters
   public static Collection<Object[]> parameters() {
-    List<Object[]> parameters = new ArrayList<Object[]>();
+    List<Object[]> parameters = new ArrayList<>();
     for (boolean multiPut : new boolean[]{false, true}) {
       for (DataBlockEncoding dataBlockEncoding : new DataBlockEncoding[] {
           DataBlockEncoding.NONE, DataBlockEncoding.PREFIX }) {
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/util/TestPoolMap.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/util/TestPoolMap.java
index b229e91..7fc09d2 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/util/TestPoolMap.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/util/TestPoolMap.java
@@ -46,7 +46,7 @@ public class TestPoolMap {
 
     @Override
     protected void setUp() throws Exception {
-      this.poolMap = new PoolMap<String, String>(getPoolType(), POOL_SIZE);
+      this.poolMap = new PoolMap<>(getPoolType(), POOL_SIZE);
     }
 
     protected abstract PoolType getPoolType();
@@ -117,7 +117,7 @@ public class TestPoolMap {
 
     public void testPoolCap() throws InterruptedException, ExecutionException {
       String randomKey = String.valueOf(random.nextInt());
-      List<String> randomValues = new ArrayList<String>();
+      List<String> randomValues = new ArrayList<>();
       for (int i = 0; i < POOL_SIZE * 2; i++) {
         String randomValue = String.valueOf(random.nextInt());
         randomValues.add(randomValue);
@@ -219,7 +219,7 @@ public class TestPoolMap {
     public void testPoolCap() throws InterruptedException, ExecutionException {
       // As long as we poll values we put, the pool size should remain zero
       String randomKey = String.valueOf(random.nextInt());
-      List<String> randomValues = new ArrayList<String>();
+      List<String> randomValues = new ArrayList<>();
       for (int i = 0; i < POOL_SIZE * 2; i++) {
         String randomValue = String.valueOf(random.nextInt());
         randomValues.add(randomValue);
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/util/TestRegionSizeCalculator.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/util/TestRegionSizeCalculator.java
index e7a6500..51dc238 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/util/TestRegionSizeCalculator.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/util/TestRegionSizeCalculator.java
@@ -135,7 +135,7 @@ public class TestRegionSizeCalculator {
   */
   private Admin mockAdmin(RegionLoad... regionLoadArray) throws Exception {
     Admin mockAdmin = Mockito.mock(Admin.class);
-    Map<byte[], RegionLoad> regionLoads = new TreeMap<byte[], RegionLoad>(Bytes.BYTES_COMPARATOR);
+    Map<byte[], RegionLoad> regionLoads = new TreeMap<>(Bytes.BYTES_COMPARATOR);
     for (RegionLoad regionLoad : regionLoadArray) {
       regionLoads.put(regionLoad.getName(), regionLoad);
     }
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/util/TestRegionSplitCalculator.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/util/TestRegionSplitCalculator.java
index ea2bc7a..931830f 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/util/TestRegionSplitCalculator.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/util/TestRegionSplitCalculator.java
@@ -126,8 +126,7 @@ public class TestRegionSplitCalculator {
     SimpleRange a = new SimpleRange(Bytes.toBytes("A"), Bytes.toBytes("B"));
     SimpleRange b = new SimpleRange(Bytes.toBytes("B"), Bytes.toBytes("C"));
     SimpleRange c = new SimpleRange(Bytes.toBytes("C"), Bytes.toBytes("D"));
-    RegionSplitCalculator<SimpleRange> sc = new RegionSplitCalculator<SimpleRange>(
-        cmp);
+    RegionSplitCalculator<SimpleRange> sc = new RegionSplitCalculator<>(cmp);
     sc.add(a);
     sc.add(b);
     sc.add(c);
@@ -142,8 +141,7 @@ public class TestRegionSplitCalculator {
 
   @Test
   public void testSplitCalculatorNoEdge() {
-    RegionSplitCalculator<SimpleRange> sc = new RegionSplitCalculator<SimpleRange>(
-        cmp);
+    RegionSplitCalculator<SimpleRange> sc = new RegionSplitCalculator<>(cmp);
 
     Multimap<byte[], SimpleRange> regions = sc.calcCoverage();
     LOG.info("Empty");
@@ -155,8 +153,7 @@ public class TestRegionSplitCalculator {
   @Test
   public void testSplitCalculatorSingleEdge() {
     SimpleRange a = new SimpleRange(Bytes.toBytes("A"), Bytes.toBytes("B"));
-    RegionSplitCalculator<SimpleRange> sc = new RegionSplitCalculator<SimpleRange>(
-        cmp);
+    RegionSplitCalculator<SimpleRange> sc = new RegionSplitCalculator<>(cmp);
     sc.add(a);
 
     Multimap<byte[], SimpleRange> regions = sc.calcCoverage();
@@ -169,8 +166,7 @@ public class TestRegionSplitCalculator {
   @Test
   public void testSplitCalculatorDegenerateEdge() {
     SimpleRange a = new SimpleRange(Bytes.toBytes("A"), Bytes.toBytes("A"));
-    RegionSplitCalculator<SimpleRange> sc = new RegionSplitCalculator<SimpleRange>(
-        cmp);
+    RegionSplitCalculator<SimpleRange> sc = new RegionSplitCalculator<>(cmp);
     sc.add(a);
 
     Multimap<byte[], SimpleRange> regions = sc.calcCoverage();
@@ -185,8 +181,7 @@ public class TestRegionSplitCalculator {
     SimpleRange a = new SimpleRange(Bytes.toBytes("A"), Bytes.toBytes("B"));
     SimpleRange b = new SimpleRange(Bytes.toBytes("B"), Bytes.toBytes("C"));
     SimpleRange c = new SimpleRange(Bytes.toBytes("A"), Bytes.toBytes("C"));
-    RegionSplitCalculator<SimpleRange> sc = new RegionSplitCalculator<SimpleRange>(
-        cmp);
+    RegionSplitCalculator<SimpleRange> sc = new RegionSplitCalculator<>(cmp);
     sc.add(a);
     sc.add(b);
     sc.add(c);
@@ -204,8 +199,7 @@ public class TestRegionSplitCalculator {
     SimpleRange a = new SimpleRange(Bytes.toBytes("A"), Bytes.toBytes("B"));
     SimpleRange b = new SimpleRange(Bytes.toBytes("B"), Bytes.toBytes("C"));
     SimpleRange c = new SimpleRange(Bytes.toBytes("B"), Bytes.toBytes("D"));
-    RegionSplitCalculator<SimpleRange> sc = new RegionSplitCalculator<SimpleRange>(
-        cmp);
+    RegionSplitCalculator<SimpleRange> sc = new RegionSplitCalculator<>(cmp);
     sc.add(a);
     sc.add(b);
     sc.add(c);
@@ -223,8 +217,7 @@ public class TestRegionSplitCalculator {
     SimpleRange a = new SimpleRange(Bytes.toBytes("A"), Bytes.toBytes("B"));
     SimpleRange b = new SimpleRange(Bytes.toBytes("B"), Bytes.toBytes("C"));
     SimpleRange c = new SimpleRange(Bytes.toBytes("E"), Bytes.toBytes("F"));
-    RegionSplitCalculator<SimpleRange> sc = new RegionSplitCalculator<SimpleRange>(
-        cmp);
+    RegionSplitCalculator<SimpleRange> sc = new RegionSplitCalculator<>(cmp);
     sc.add(a);
     sc.add(b);
     sc.add(c);
@@ -241,8 +234,7 @@ public class TestRegionSplitCalculator {
   public void testSplitCalculatorOverreach() {
     SimpleRange a = new SimpleRange(Bytes.toBytes("A"), Bytes.toBytes("C"));
     SimpleRange b = new SimpleRange(Bytes.toBytes("B"), Bytes.toBytes("D"));
-    RegionSplitCalculator<SimpleRange> sc = new RegionSplitCalculator<SimpleRange>(
-        cmp);
+    RegionSplitCalculator<SimpleRange> sc = new RegionSplitCalculator<>(cmp);
     sc.add(a);
     sc.add(b);
 
@@ -258,8 +250,7 @@ public class TestRegionSplitCalculator {
   public void testSplitCalculatorFloor() {
     SimpleRange a = new SimpleRange(Bytes.toBytes("A"), Bytes.toBytes("C"));
     SimpleRange b = new SimpleRange(Bytes.toBytes("A"), Bytes.toBytes("B"));
-    RegionSplitCalculator<SimpleRange> sc = new RegionSplitCalculator<SimpleRange>(
-        cmp);
+    RegionSplitCalculator<SimpleRange> sc = new RegionSplitCalculator<>(cmp);
     sc.add(a);
     sc.add(b);
 
@@ -274,8 +265,7 @@ public class TestRegionSplitCalculator {
   public void testSplitCalculatorCeil() {
     SimpleRange a = new SimpleRange(Bytes.toBytes("A"), Bytes.toBytes("C"));
     SimpleRange b = new SimpleRange(Bytes.toBytes("B"), Bytes.toBytes("C"));
-    RegionSplitCalculator<SimpleRange> sc = new RegionSplitCalculator<SimpleRange>(
-        cmp);
+    RegionSplitCalculator<SimpleRange> sc = new RegionSplitCalculator<>(cmp);
     sc.add(a);
     sc.add(b);
 
@@ -292,8 +282,7 @@ public class TestRegionSplitCalculator {
     SimpleRange b = new SimpleRange(Bytes.toBytes("A"), Bytes.toBytes("C"));
 
     LOG.info(a.tiebreaker + " - " + b.tiebreaker);
-    RegionSplitCalculator<SimpleRange> sc = new RegionSplitCalculator<SimpleRange>(
-        cmp);
+    RegionSplitCalculator<SimpleRange> sc = new RegionSplitCalculator<>(cmp);
     sc.add(a);
     sc.add(b);
 
@@ -307,8 +296,7 @@ public class TestRegionSplitCalculator {
   @Test
   public void testSplitCalculatorBackwards() {
     SimpleRange a = new SimpleRange(Bytes.toBytes("C"), Bytes.toBytes("A"));
-    RegionSplitCalculator<SimpleRange> sc = new RegionSplitCalculator<SimpleRange>(
-        cmp);
+    RegionSplitCalculator<SimpleRange> sc = new RegionSplitCalculator<>(cmp);
     sc.add(a);
 
     Multimap<byte[], SimpleRange> regions = sc.calcCoverage();
@@ -320,8 +308,7 @@ public class TestRegionSplitCalculator {
 
   @Test
   public void testComplex() {
-    RegionSplitCalculator<SimpleRange> sc = new RegionSplitCalculator<SimpleRange>(
-        cmp);
+    RegionSplitCalculator<SimpleRange> sc = new RegionSplitCalculator<>(cmp);
     sc.add(new SimpleRange(Bytes.toBytes("A"), Bytes.toBytes("Am")));
     sc.add(new SimpleRange(Bytes.toBytes("A"), Bytes.toBytes("C")));
     sc.add(new SimpleRange(Bytes.toBytes("Am"), Bytes.toBytes("C")));
@@ -344,8 +331,7 @@ public class TestRegionSplitCalculator {
 
   @Test
   public void testBeginEndMarker() {
-    RegionSplitCalculator<SimpleRange> sc = new RegionSplitCalculator<SimpleRange>(
-        cmp);
+    RegionSplitCalculator<SimpleRange> sc = new RegionSplitCalculator<>(cmp);
     sc.add(new SimpleRange(Bytes.toBytes(""), Bytes.toBytes("A")));
     sc.add(new SimpleRange(Bytes.toBytes("A"), Bytes.toBytes("B")));
     sc.add(new SimpleRange(Bytes.toBytes("B"), Bytes.toBytes("")));
@@ -364,7 +350,7 @@ public class TestRegionSplitCalculator {
     SimpleRange ae = new SimpleRange(Bytes.toBytes("A"), Bytes.toBytes("E"));
     SimpleRange ac = new SimpleRange(Bytes.toBytes("A"), Bytes.toBytes("C"));
 
-    Collection<SimpleRange> bigOverlap = new ArrayList<SimpleRange>(8);
+    Collection<SimpleRange> bigOverlap = new ArrayList<>(8);
     bigOverlap.add(new SimpleRange(Bytes.toBytes("A"), Bytes.toBytes("E")));
     bigOverlap.add(new SimpleRange(Bytes.toBytes("A"), Bytes.toBytes("C")));
     bigOverlap.add(new SimpleRange(Bytes.toBytes("A"), Bytes.toBytes("B")));
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/util/TestRegionSplitter.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/util/TestRegionSplitter.java
index c195762..0c5b980 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/util/TestRegionSplitter.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/util/TestRegionSplitter.java
@@ -78,7 +78,7 @@ public class TestRegionSplitter {
      */
     @Test
     public void testCreatePresplitTableHex() throws Exception {
-      final List<byte[]> expectedBounds = new ArrayList<byte[]>(17);
+      final List<byte[]> expectedBounds = new ArrayList<>(17);
       expectedBounds.add(ArrayUtils.EMPTY_BYTE_ARRAY);
       expectedBounds.add("10000000".getBytes());
       expectedBounds.add("20000000".getBytes());
@@ -108,7 +108,7 @@ public class TestRegionSplitter {
      */
     @Test
     public void testCreatePresplitTableUniform() throws Exception {
-      List<byte[]> expectedBounds = new ArrayList<byte[]>(17);
+      List<byte[]> expectedBounds = new ArrayList<>(17);
       expectedBounds.add(ArrayUtils.EMPTY_BYTE_ARRAY);
       expectedBounds.add(new byte[] {      0x10, 0, 0, 0, 0, 0, 0, 0});
       expectedBounds.add(new byte[] {      0x20, 0, 0, 0, 0, 0, 0, 0});
@@ -293,7 +293,7 @@ public class TestRegionSplitter {
 
   @Test
   public void noopRollingSplit() throws Exception {
-    final List<byte[]> expectedBounds = new ArrayList<byte[]>(1);
+    final List<byte[]> expectedBounds = new ArrayList<>(1);
     expectedBounds.add(ArrayUtils.EMPTY_BYTE_ARRAY);
     rollingSplitAndVerify(TableName.valueOf(TestRegionSplitter.class.getSimpleName()),
         "UniformSplit", expectedBounds);
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/util/TestSortedCopyOnWriteSet.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/util/TestSortedCopyOnWriteSet.java
index 839d1cc..0efa6da 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/util/TestSortedCopyOnWriteSet.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/util/TestSortedCopyOnWriteSet.java
@@ -34,7 +34,7 @@ public class TestSortedCopyOnWriteSet {
 
   @Test
   public void testSorting() throws Exception {
-    SortedCopyOnWriteSet<String> set = new SortedCopyOnWriteSet<String>();
+    SortedCopyOnWriteSet<String> set = new SortedCopyOnWriteSet<>();
     set.add("c");
     set.add("d");
     set.add("a");
@@ -52,8 +52,7 @@ public class TestSortedCopyOnWriteSet {
 
   @Test
   public void testIteratorIsolation() throws Exception {
-    SortedCopyOnWriteSet<String> set = new SortedCopyOnWriteSet<String>(
-        Lists.newArrayList("a", "b", "c", "d", "e"));
+    SortedCopyOnWriteSet<String> set = new SortedCopyOnWriteSet<>(Lists.newArrayList("a", "b", "c", "d", "e"));
 
     // isolation of remove()
     Iterator<String> iter = set.iterator();
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/util/TestSortedList.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/util/TestSortedList.java
index 454435b..bdae0e5 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/util/TestSortedList.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/util/TestSortedList.java
@@ -45,7 +45,7 @@ public class TestSortedList {
 
   @Test
   public void testSorting() throws Exception {
-    SortedList<String> list = new SortedList<String>(new StringComparator());
+    SortedList<String> list = new SortedList<>(new StringComparator());
     list.add("c");
     list.add("d");
     list.add("a");
@@ -72,8 +72,7 @@ public class TestSortedList {
 
   @Test
   public void testReadOnlyIterators() throws Exception {
-    SortedList<String> list = new SortedList<String>(
-        Lists.newArrayList("a", "b", "c", "d", "e"), new StringComparator());
+    SortedList<String> list = new SortedList<>(Lists.newArrayList("a", "b", "c", "d", "e"), new StringComparator());
 
     Iterator<String> i = list.iterator();
     i.next();
@@ -108,8 +107,7 @@ public class TestSortedList {
 
   @Test
   public void testIteratorIsolation() throws Exception {
-    SortedList<String> list = new SortedList<String>(
-        Lists.newArrayList("a", "b", "c", "d", "e"), new StringComparator());
+    SortedList<String> list = new SortedList<>(Lists.newArrayList("a", "b", "c", "d", "e"), new StringComparator());
 
     // isolation of remove()
     Iterator<String> iter = list.iterator();
@@ -161,8 +159,7 @@ public class TestSortedList {
 
   @Test
   public void testRandomAccessIsolation() throws Exception {
-    SortedList<String> list = new SortedList<String>(
-        Lists.newArrayList("a", "b", "c"), new StringComparator());
+    SortedList<String> list = new SortedList<>(Lists.newArrayList("a", "b", "c"), new StringComparator());
     List<String> innerList = list.get();
     assertEquals("a", innerList.get(0));
     assertEquals("b", innerList.get(1));
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/util/hbck/OfflineMetaRebuildTestCore.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/util/hbck/OfflineMetaRebuildTestCore.java
index 3701094..a7c4ad1 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/util/hbck/OfflineMetaRebuildTestCore.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/util/hbck/OfflineMetaRebuildTestCore.java
@@ -228,7 +228,7 @@ public class OfflineMetaRebuildTestCore {
     Scan s = new Scan();
     Table meta = TEST_UTIL.getConnection().getTable(TableName.META_TABLE_NAME);
     ResultScanner scanner = meta.getScanner(s);
-    List<Delete> dels = new ArrayList<Delete>();
+    List<Delete> dels = new ArrayList<>();
     for (Result r : scanner) {
       HRegionInfo info =
           MetaTableAccessor.getHRegionInfo(r);
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/wal/IOTestProvider.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/wal/IOTestProvider.java
index 2d32b5e..e2ea838 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/wal/IOTestProvider.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/wal/IOTestProvider.java
@@ -108,7 +108,7 @@ public class IOTestProvider implements WALProvider {
 
   @Override
   public List<WAL> getWALs() throws IOException {
-    List<WAL> wals = new ArrayList<WAL>(1);
+    List<WAL> wals = new ArrayList<>(1);
     wals.add(log);
     return wals;
   }
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/wal/TestBoundedRegionGroupingStrategy.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/wal/TestBoundedRegionGroupingStrategy.java
index 8523e69..73725bb 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/wal/TestBoundedRegionGroupingStrategy.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/wal/TestBoundedRegionGroupingStrategy.java
@@ -178,7 +178,7 @@ public class TestBoundedRegionGroupingStrategy {
       FSUtils.setRootDir(CONF, TEST_UTIL.getDataTestDirOnTestFS());
 
       wals = new WALFactory(CONF, null, "setMembershipDedups");
-      final Set<WAL> seen = new HashSet<WAL>(temp * 4);
+      final Set<WAL> seen = new HashSet<>(temp * 4);
       final Random random = new Random();
       int count = 0;
       // we know that this should see one of the wals more than once
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/wal/TestFSHLogProvider.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/wal/TestFSHLogProvider.java
index d82c3b6..f752735 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/wal/TestFSHLogProvider.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/wal/TestFSHLogProvider.java
@@ -193,12 +193,12 @@ public class TestFSHLogProvider {
     final HTableDescriptor htd2 =
         new HTableDescriptor(TableName.valueOf(currentTest.getMethodName() + "2"))
             .addFamily(new HColumnDescriptor("row"));
-    NavigableMap<byte[], Integer> scopes1 = new TreeMap<byte[], Integer>(
+    NavigableMap<byte[], Integer> scopes1 = new TreeMap<>(
         Bytes.BYTES_COMPARATOR);
     for(byte[] fam : htd.getFamiliesKeys()) {
       scopes1.put(fam, 0);
     }
-    NavigableMap<byte[], Integer> scopes2 = new TreeMap<byte[], Integer>(
+    NavigableMap<byte[], Integer> scopes2 = new TreeMap<>(
         Bytes.BYTES_COMPARATOR);
     for(byte[] fam : htd2.getFamiliesKeys()) {
       scopes2.put(fam, 0);
@@ -275,12 +275,12 @@ public class TestFSHLogProvider {
         new HTableDescriptor(TableName.valueOf(currentTest.getMethodName() + "1")).addFamily(new HColumnDescriptor("row"));
     HTableDescriptor table2 =
         new HTableDescriptor(TableName.valueOf(currentTest.getMethodName() + "2")).addFamily(new HColumnDescriptor("row"));
-    NavigableMap<byte[], Integer> scopes1 = new TreeMap<byte[], Integer>(
+    NavigableMap<byte[], Integer> scopes1 = new TreeMap<>(
         Bytes.BYTES_COMPARATOR);
     for(byte[] fam : table1.getFamiliesKeys()) {
       scopes1.put(fam, 0);
     }
-    NavigableMap<byte[], Integer> scopes2 = new TreeMap<byte[], Integer>(
+    NavigableMap<byte[], Integer> scopes2 = new TreeMap<>(
         Bytes.BYTES_COMPARATOR);
     for(byte[] fam : table2.getFamiliesKeys()) {
       scopes2.put(fam, 0);
@@ -370,7 +370,7 @@ public class TestFSHLogProvider {
     localConf.set(WALFactory.WAL_PROVIDER, FSHLogProvider.class.getName());
     final WALFactory wals = new WALFactory(localConf, null, currentTest.getMethodName());
     try {
-      final Set<WAL> seen = new HashSet<WAL>(1);
+      final Set<WAL> seen = new HashSet<>(1);
       final Random random = new Random();
       assertTrue("first attempt to add WAL from default provider should work.",
           seen.add(wals.getWAL(Bytes.toBytes(random.nextInt()), null)));
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/wal/TestSecureWAL.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/wal/TestSecureWAL.java
index 913ea48..7497d67 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/wal/TestSecureWAL.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/wal/TestSecureWAL.java
@@ -116,8 +116,7 @@ public class TestSecureWAL {
     TableName tableName = TableName.valueOf(name.getMethodName().replaceAll("[^a-zA-Z0-9]", "_"));
     HTableDescriptor htd = new HTableDescriptor(tableName);
     htd.addFamily(new HColumnDescriptor(tableName.getName()));
-    NavigableMap<byte[], Integer> scopes = new TreeMap<byte[], Integer>(
-        Bytes.BYTES_COMPARATOR);
+    NavigableMap<byte[], Integer> scopes = new TreeMap<>(Bytes.BYTES_COMPARATOR);
     for(byte[] fam : htd.getFamiliesKeys()) {
       scopes.put(fam, 0);
     }
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/wal/TestWALFactory.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/wal/TestWALFactory.java
index 3318f61..f02e244 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/wal/TestWALFactory.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/wal/TestWALFactory.java
@@ -180,8 +180,7 @@ public class TestWALFactory {
     }
     HTableDescriptor htd = new HTableDescriptor(tableName);
     htd.addFamily(new HColumnDescriptor("column"));
-    NavigableMap<byte[], Integer> scopes = new TreeMap<byte[], Integer>(
-        Bytes.BYTES_COMPARATOR);
+    NavigableMap<byte[], Integer> scopes = new TreeMap<>(Bytes.BYTES_COMPARATOR);
     for(byte[] fam : htd.getFamiliesKeys()) {
       scopes.put(fam, 0);
     }
@@ -259,8 +258,7 @@ public class TestWALFactory {
                   null,null, false);
       HTableDescriptor htd = new HTableDescriptor(tableName);
       htd.addFamily(new HColumnDescriptor(tableName.getName()));
-      NavigableMap<byte[], Integer> scopes = new TreeMap<byte[], Integer>(
-          Bytes.BYTES_COMPARATOR);
+      NavigableMap<byte[], Integer> scopes = new TreeMap<>(Bytes.BYTES_COMPARATOR);
       for(byte[] fam : htd.getFamiliesKeys()) {
         scopes.put(fam, 0);
       }
@@ -385,8 +383,7 @@ public class TestWALFactory {
 
     HTableDescriptor htd = new HTableDescriptor(tableName);
     htd.addFamily(new HColumnDescriptor(tableName.getName()));
-    NavigableMap<byte[], Integer> scopes = new TreeMap<byte[], Integer>(
-        Bytes.BYTES_COMPARATOR);
+    NavigableMap<byte[], Integer> scopes = new TreeMap<>(Bytes.BYTES_COMPARATOR);
     for(byte[] fam : htd.getFamiliesKeys()) {
       scopes.put(fam, 0);
     }
@@ -637,8 +634,7 @@ public class TestWALFactory {
     long timestamp = System.currentTimeMillis();
     HTableDescriptor htd = new HTableDescriptor(tableName);
     htd.addFamily(new HColumnDescriptor("column"));
-    NavigableMap<byte[], Integer> scopes = new TreeMap<byte[], Integer>(
-        Bytes.BYTES_COMPARATOR);
+    NavigableMap<byte[], Integer> scopes = new TreeMap<>(Bytes.BYTES_COMPARATOR);
     for(byte[] fam : htd.getFamiliesKeys()) {
       scopes.put(fam, 0);
     }
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/wal/TestWALFiltering.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/wal/TestWALFiltering.java
index c69150f..65401de 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/wal/TestWALFiltering.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/wal/TestWALFiltering.java
@@ -107,8 +107,7 @@ public class TestWALFiltering {
   public void testFlushedSequenceIdsSentToHMaster()
   throws IOException, InterruptedException,
   org.apache.hadoop.hbase.shaded.com.google.protobuf.ServiceException, ServiceException {
-    SortedMap<byte[], Long> allFlushedSequenceIds =
-        new TreeMap<byte[], Long>(Bytes.BYTES_COMPARATOR);
+    SortedMap<byte[], Long> allFlushedSequenceIds = new TreeMap<>(Bytes.BYTES_COMPARATOR);
     for (int i = 0; i < NUM_RS; ++i) {
       flushAllRegions(i);
     }
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/wal/TestWALReaderOnSecureWAL.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/wal/TestWALReaderOnSecureWAL.java
index 6f4a797..ecde00d 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/wal/TestWALReaderOnSecureWAL.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/wal/TestWALReaderOnSecureWAL.java
@@ -104,8 +104,7 @@ public class TestWALReaderOnSecureWAL {
       TableName tableName = TableName.valueOf(tblName);
       HTableDescriptor htd = new HTableDescriptor(tableName);
       htd.addFamily(new HColumnDescriptor(tableName.getName()));
-      NavigableMap<byte[], Integer> scopes = new TreeMap<byte[], Integer>(
-          Bytes.BYTES_COMPARATOR);
+      NavigableMap<byte[], Integer> scopes = new TreeMap<>(Bytes.BYTES_COMPARATOR);
       for(byte[] fam : htd.getFamiliesKeys()) {
         scopes.put(fam, 0);
       }
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/wal/TestWALSplit.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/wal/TestWALSplit.java
index 3b15cef..611f8c3 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/wal/TestWALSplit.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/wal/TestWALSplit.java
@@ -135,7 +135,7 @@ public class TestWALSplit {
   private static final byte[] QUALIFIER = "q1".getBytes();
   private static final byte[] VALUE = "v1".getBytes();
   private static final String WAL_FILE_PREFIX = "wal.dat.";
-  private static List<String> REGIONS = new ArrayList<String>();
+  private static List<String> REGIONS = new ArrayList<>();
   private static final String HBASE_SKIP_ERRORS = "hbase.hlog.split.skip.errors";
   private static String ROBBER;
   private static String ZOMBIE;
@@ -158,7 +158,7 @@ public class TestWALSplit {
     // This is how you turn off shortcircuit read currently.  TODO: Fix.  Should read config.
     System.setProperty("hbase.tests.use.shortcircuit.reads", "false");
     // Create fake maping user to group and set it to the conf.
-    Map<String, String []> u2g_map = new HashMap<String, String []>(2);
+    Map<String, String []> u2g_map = new HashMap<>(2);
     ROBBER = User.getCurrent().getName() + "-robber";
     ZOMBIE = User.getCurrent().getName() + "-zombie";
     u2g_map.put(ROBBER, GROUP);
@@ -585,7 +585,7 @@ public class TestWALSplit {
         .filter(x -> x != FaultyProtobufLogReader.FailureType.NONE).collect(Collectors.toList());
     for (FaultyProtobufLogReader.FailureType failureType : failureTypes) {
       final Set<String> walDirContents = splitCorruptWALs(failureType);
-      final Set<String> archivedLogs = new HashSet<String>();
+      final Set<String> archivedLogs = new HashSet<>();
       final StringBuilder archived = new StringBuilder("Archived logs in CORRUPTDIR:");
       for (FileStatus log : fs.listStatus(CORRUPTDIR)) {
         archived.append("\n\t").append(log.toString());
@@ -630,7 +630,7 @@ public class TestWALSplit {
       wals = new WALFactory(conf, null, name.getMethodName());
       generateWALs(-1);
       // Our reader will render all of these files corrupt.
-      final Set<String> walDirContents = new HashSet<String>();
+      final Set<String> walDirContents = new HashSet<>();
       for (FileStatus status : fs.listStatus(WALDIR)) {
         walDirContents.add(status.getPath().getName());
       }
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/wal/WALPerformanceEvaluation.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/wal/WALPerformanceEvaluation.java
index 9bb3d7d..53cc49b 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/wal/WALPerformanceEvaluation.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/wal/WALPerformanceEvaluation.java
@@ -141,8 +141,7 @@ public final class WALPerformanceEvaluation extends Configured implements Tool {
       this.numFamilies = htd.getColumnFamilyCount();
       this.region = region;
       this.htd = htd;
-      scopes = new TreeMap<byte[], Integer>(
-          Bytes.BYTES_COMPARATOR);
+      scopes = new TreeMap<>(Bytes.BYTES_COMPARATOR);
       for(byte[] fam : htd.getFamiliesKeys()) {
         scopes.put(fam, 0);
       }
@@ -420,7 +419,7 @@ public final class WALPerformanceEvaluation extends Configured implements Tool {
       throws IOException {
     WAL.Reader reader = wals.createReader(wal.getFileSystem(getConf()), wal);
     long count = 0;
-    Map<String, Long> sequenceIds = new HashMap<String, Long>();
+    Map<String, Long> sequenceIds = new HashMap<>();
     try {
       while (true) {
         WAL.Entry e = reader.next();
@@ -490,7 +489,7 @@ public final class WALPerformanceEvaluation extends Configured implements Tool {
     System.exit(1);
   }
 
-  private final Set<WAL> walsListenedTo = new HashSet<WAL>();
+  private final Set<WAL> walsListenedTo = new HashSet<>();
 
   private HRegion openRegion(final FileSystem fs, final Path dir, final HTableDescriptor htd,
       final WALFactory wals, final long whenToRoll, final LogRoller roller) throws IOException {
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/zookeeper/TestZKMulti.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/zookeeper/TestZKMulti.java
index 6350af8..b4ac59c 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/zookeeper/TestZKMulti.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/zookeeper/TestZKMulti.java
@@ -85,24 +85,24 @@ public class TestZKMulti {
     ZKUtil.multiOrSequential(zkw, null, false);
 
     // empty multi
-    ZKUtil.multiOrSequential(zkw, new LinkedList<ZKUtilOp>(), false);
+    ZKUtil.multiOrSequential(zkw, new LinkedList<>(), false);
 
     // single create
     String path = ZKUtil.joinZNode(zkw.znodePaths.baseZNode, "testSimpleMulti");
-    LinkedList<ZKUtilOp> singleCreate = new LinkedList<ZKUtilOp>();
+    LinkedList<ZKUtilOp> singleCreate = new LinkedList<>();
     singleCreate.add(ZKUtilOp.createAndFailSilent(path, new byte[0]));
     ZKUtil.multiOrSequential(zkw, singleCreate, false);
     assertTrue(ZKUtil.checkExists(zkw, path) != -1);
 
     // single setdata
-    LinkedList<ZKUtilOp> singleSetData = new LinkedList<ZKUtilOp>();
+    LinkedList<ZKUtilOp> singleSetData = new LinkedList<>();
     byte [] data = Bytes.toBytes("foobar");
     singleSetData.add(ZKUtilOp.setData(path, data));
     ZKUtil.multiOrSequential(zkw, singleSetData, false);
     assertTrue(Bytes.equals(ZKUtil.getData(zkw, path), data));
 
     // single delete
-    LinkedList<ZKUtilOp> singleDelete = new LinkedList<ZKUtilOp>();
+    LinkedList<ZKUtilOp> singleDelete = new LinkedList<>();
     singleDelete.add(ZKUtilOp.deleteNodeFailSilent(path));
     ZKUtil.multiOrSequential(zkw, singleDelete, false);
     assertTrue(ZKUtil.checkExists(zkw, path) == -1);
@@ -117,7 +117,7 @@ public class TestZKMulti {
     String path5 = ZKUtil.joinZNode(zkw.znodePaths.baseZNode, "testComplexMulti5");
     String path6 = ZKUtil.joinZNode(zkw.znodePaths.baseZNode, "testComplexMulti6");
     // create 4 nodes that we'll setData on or delete later
-    LinkedList<ZKUtilOp> create4Nodes = new LinkedList<ZKUtilOp>();
+    LinkedList<ZKUtilOp> create4Nodes = new LinkedList<>();
     create4Nodes.add(ZKUtilOp.createAndFailSilent(path1, Bytes.toBytes(path1)));
     create4Nodes.add(ZKUtilOp.createAndFailSilent(path2, Bytes.toBytes(path2)));
     create4Nodes.add(ZKUtilOp.createAndFailSilent(path3, Bytes.toBytes(path3)));
@@ -129,7 +129,7 @@ public class TestZKMulti {
     assertTrue(Bytes.equals(ZKUtil.getData(zkw, path4), Bytes.toBytes(path4)));
 
     // do multiple of each operation (setData, delete, create)
-    LinkedList<ZKUtilOp> ops = new LinkedList<ZKUtilOp>();
+    LinkedList<ZKUtilOp> ops = new LinkedList<>();
     // setData
     ops.add(ZKUtilOp.setData(path1, Bytes.add(Bytes.toBytes(path1), Bytes.toBytes(path1))));
     ops.add(ZKUtilOp.setData(path2, Bytes.add(Bytes.toBytes(path2), Bytes.toBytes(path2))));
@@ -155,7 +155,7 @@ public class TestZKMulti {
     // try to delete a node that doesn't exist
     boolean caughtNoNode = false;
     String path = ZKUtil.joinZNode(zkw.znodePaths.baseZNode, "testSingleFailureZ");
-    LinkedList<ZKUtilOp> ops = new LinkedList<ZKUtilOp>();
+    LinkedList<ZKUtilOp> ops = new LinkedList<>();
     ops.add(ZKUtilOp.deleteNodeFailSilent(path));
     try {
       ZKUtil.multiOrSequential(zkw, ops, false);
@@ -166,7 +166,7 @@ public class TestZKMulti {
 
     // try to setData on a node that doesn't exist
     caughtNoNode = false;
-    ops = new LinkedList<ZKUtilOp>();
+    ops = new LinkedList<>();
     ops.add(ZKUtilOp.setData(path, Bytes.toBytes(path)));
     try {
       ZKUtil.multiOrSequential(zkw, ops, false);
@@ -177,7 +177,7 @@ public class TestZKMulti {
 
     // try to create on a node that already exists
     boolean caughtNodeExists = false;
-    ops = new LinkedList<ZKUtilOp>();
+    ops = new LinkedList<>();
     ops.add(ZKUtilOp.createAndFailSilent(path, Bytes.toBytes(path)));
     ZKUtil.multiOrSequential(zkw, ops, false);
     try {
@@ -194,7 +194,7 @@ public class TestZKMulti {
     String pathA = ZKUtil.joinZNode(zkw.znodePaths.baseZNode, "testSingleFailureInMultiA");
     String pathB = ZKUtil.joinZNode(zkw.znodePaths.baseZNode, "testSingleFailureInMultiB");
     String pathC = ZKUtil.joinZNode(zkw.znodePaths.baseZNode, "testSingleFailureInMultiC");
-    LinkedList<ZKUtilOp> ops = new LinkedList<ZKUtilOp>();
+    LinkedList<ZKUtilOp> ops = new LinkedList<>();
     ops.add(ZKUtilOp.createAndFailSilent(pathA, Bytes.toBytes(pathA)));
     ops.add(ZKUtilOp.createAndFailSilent(pathB, Bytes.toBytes(pathB)));
     ops.add(ZKUtilOp.deleteNodeFailSilent(pathC));
@@ -217,14 +217,14 @@ public class TestZKMulti {
     String pathY = ZKUtil.joinZNode(zkw.znodePaths.baseZNode, "testMultiFailureY");
     String pathZ = ZKUtil.joinZNode(zkw.znodePaths.baseZNode, "testMultiFailureZ");
     // create X that we will use to fail create later
-    LinkedList<ZKUtilOp> ops = new LinkedList<ZKUtilOp>();
+    LinkedList<ZKUtilOp> ops = new LinkedList<>();
     ops.add(ZKUtilOp.createAndFailSilent(pathX, Bytes.toBytes(pathX)));
     ZKUtil.multiOrSequential(zkw, ops, false);
 
     // fail one of each create ,setData, delete
     String pathV = ZKUtil.joinZNode(zkw.znodePaths.baseZNode, "testMultiFailureV");
     String pathW = ZKUtil.joinZNode(zkw.znodePaths.baseZNode, "testMultiFailureW");
-    ops = new LinkedList<ZKUtilOp>();
+    ops = new LinkedList<>();
     ops.add(ZKUtilOp.createAndFailSilent(pathX, Bytes.toBytes(pathX))); // fail  -- already exists
     ops.add(ZKUtilOp.setData(pathY, Bytes.toBytes(pathY))); // fail -- doesn't exist
     ops.add(ZKUtilOp.deleteNodeFailSilent(pathZ)); // fail -- doesn't exist
@@ -246,7 +246,7 @@ public class TestZKMulti {
     assertTrue(ZKUtil.checkExists(zkw, pathV) == -1);
 
     // test that with multiple failures, throws an exception corresponding to first failure in list
-    ops = new LinkedList<ZKUtilOp>();
+    ops = new LinkedList<>();
     ops.add(ZKUtilOp.setData(pathY, Bytes.toBytes(pathY))); // fail -- doesn't exist
     ops.add(ZKUtilOp.createAndFailSilent(pathX, Bytes.toBytes(pathX))); // fail -- exists
     boolean caughtNoNode = false;
@@ -273,14 +273,14 @@ public class TestZKMulti {
     String path4 = ZKUtil.joinZNode(zkw.znodePaths.baseZNode, "runSequential4");
 
     // create some nodes that we will use later
-    LinkedList<ZKUtilOp> ops = new LinkedList<ZKUtilOp>();
+    LinkedList<ZKUtilOp> ops = new LinkedList<>();
     ops.add(ZKUtilOp.createAndFailSilent(path1, Bytes.toBytes(path1)));
     ops.add(ZKUtilOp.createAndFailSilent(path2, Bytes.toBytes(path2)));
     ZKUtil.multiOrSequential(zkw, ops, false);
 
     // test that, even with operations that fail, the ones that would pass will pass
     // with runSequentialOnMultiFailure
-    ops = new LinkedList<ZKUtilOp>();
+    ops = new LinkedList<>();
     ops.add(ZKUtilOp.setData(path1, Bytes.add(Bytes.toBytes(path1), Bytes.toBytes(path1)))); // pass
     ops.add(ZKUtilOp.deleteNodeFailSilent(path2)); // pass
     ops.add(ZKUtilOp.deleteNodeFailSilent(path3)); // fail -- node doesn't exist
@@ -368,7 +368,7 @@ public class TestZKMulti {
 
   private void createZNodeTree(String rootZNode) throws KeeperException,
       InterruptedException {
-    List<Op> opList = new ArrayList<Op>();
+    List<Op> opList = new ArrayList<>();
     opList.add(Op.create(rootZNode, new byte[0], Ids.OPEN_ACL_UNSAFE,
         CreateMode.PERSISTENT));
     int level = 0;
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/zookeeper/TestZooKeeperACL.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/zookeeper/TestZooKeeperACL.java
index 26329f6..89164f4 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/zookeeper/TestZooKeeperACL.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/zookeeper/TestZooKeeperACL.java
@@ -325,7 +325,7 @@ public class TestZooKeeperACL {
     if (!secureZKAvailable) {
       return;
     }
-    List<ServerName> drainingServers = new ArrayList<ServerName>(1);
+    List<ServerName> drainingServers = new ArrayList<>(1);
     drainingServers.add(ServerName.parseServerName("ZZZ,123,123"));
 
     // If unable to connect to secure ZK cluster then this operation would fail.
diff --git a/hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift/CallQueue.java b/hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift/CallQueue.java
index 59e5856..315d6b0 100644
--- a/hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift/CallQueue.java
+++ b/hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift/CallQueue.java
@@ -133,7 +133,7 @@ public class CallQueue implements BlockingQueue<Runnable> {
       throw new IllegalArgumentException(
           "A BlockingQueue cannot drain to itself.");
     }
-    List<Call> drained = new ArrayList<Call>();
+    List<Call> drained = new ArrayList<>();
     underlyingQueue.drainTo(drained, maxElements);
     for (Call r : drained) {
       updateMetrics(r);
diff --git a/hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift/IncrementCoalescer.java b/hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift/IncrementCoalescer.java
index 2f4336b..221786a 100644
--- a/hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift/IncrementCoalescer.java
+++ b/hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift/IncrementCoalescer.java
@@ -154,7 +154,7 @@ public class IncrementCoalescer implements IncrementCoalescerMBean {
   private final AtomicLong successfulCoalescings = new AtomicLong();
   private final AtomicLong totalIncrements = new AtomicLong();
   private final ConcurrentMap<FullyQualifiedRow, Long> countersMap =
-      new ConcurrentHashMap<FullyQualifiedRow, Long>(100000, 0.75f, 1500);
+      new ConcurrentHashMap<>(100000, 0.75f, 1500);
   private final ThreadPoolExecutor pool;
   private final HBaseHandler handler;
 
@@ -166,7 +166,7 @@ public class IncrementCoalescer implements IncrementCoalescerMBean {
   @SuppressWarnings("deprecation")
   public IncrementCoalescer(HBaseHandler hand) {
     this.handler = hand;
-    LinkedBlockingQueue<Runnable> queue = new LinkedBlockingQueue<Runnable>();
+    LinkedBlockingQueue<Runnable> queue = new LinkedBlockingQueue<>();
     pool =
         new ThreadPoolExecutor(CORE_POOL_SIZE, CORE_POOL_SIZE, 50, TimeUnit.MILLISECONDS, queue,
             Threads.newDaemonThreadFactory("IncrementCoalescer"));
diff --git a/hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift/TBoundedThreadPoolServer.java b/hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift/TBoundedThreadPoolServer.java
index 2a1a398..b01bacf 100644
--- a/hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift/TBoundedThreadPoolServer.java
+++ b/hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift/TBoundedThreadPoolServer.java
@@ -146,10 +146,10 @@ public class TBoundedThreadPoolServer extends TServer {
     int maxWorkerThreads = options.maxWorkerThreads;
     if (options.maxQueuedRequests > 0) {
       this.callQueue = new CallQueue(
-          new LinkedBlockingQueue<Call>(options.maxQueuedRequests), metrics);
+          new LinkedBlockingQueue<>(options.maxQueuedRequests), metrics);
       minWorkerThreads = maxWorkerThreads;
     } else {
-      this.callQueue = new CallQueue(new SynchronousQueue<Call>(), metrics);
+      this.callQueue = new CallQueue(new SynchronousQueue<>(), metrics);
     }
 
     ThreadFactoryBuilder tfb = new ThreadFactoryBuilder();
diff --git a/hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift/ThriftServerRunner.java b/hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift/ThriftServerRunner.java
index 3eacfb9..0829188 100644
--- a/hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift/ThriftServerRunner.java
+++ b/hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift/ThriftServerRunner.java
@@ -299,7 +299,7 @@ public class ThriftServerRunner implements Runnable {
     }
 
     public static List<String> serversThatCannotSpecifyBindIP() {
-      List<String> l = new ArrayList<String>();
+      List<String> l = new ArrayList<>();
       for (ImplType t : values()) {
         if (!t.canSpecifyBindIP) {
           l.add(t.simpleClassName());
@@ -396,7 +396,7 @@ public class ThriftServerRunner implements Runnable {
 
   private void setupHTTPServer() throws IOException {
     TProtocolFactory protocolFactory = new TBinaryProtocol.Factory();
-    TProcessor processor = new Hbase.Processor<Hbase.Iface>(handler);
+    TProcessor processor = new Hbase.Processor<>(handler);
     TServlet thriftHttpServlet = new ThriftHttpServlet(processor, protocolFactory, realUser,
         conf, hbaseHandler, securityEnabled, doAsEnabled);
 
@@ -496,7 +496,7 @@ public class ThriftServerRunner implements Runnable {
       protocolFactory = new TBinaryProtocol.Factory();
     }
 
-    final TProcessor p = new Hbase.Processor<Hbase.Iface>(handler);
+    final TProcessor p = new Hbase.Processor<>(handler);
     ImplType implType = ImplType.getServerImpl(conf);
     TProcessor processor = p;
 
@@ -516,7 +516,7 @@ public class ThriftServerRunner implements Runnable {
       // Extract the name from the principal
       String name = SecurityUtil.getUserFromPrincipal(
         conf.get("hbase.thrift.kerberos.principal"));
-      Map<String, String> saslProperties = new HashMap<String, String>();
+      Map<String, String> saslProperties = new HashMap<>();
       saslProperties.put(Sasl.QOP, qop);
       TSaslServerTransport.Factory saslFactory = new TSaslServerTransport.Factory();
       saslFactory.addServerDefinition("GSSAPI", name, host, saslProperties,
@@ -591,8 +591,7 @@ public class ThriftServerRunner implements Runnable {
         tserver = new TNonblockingServer(serverArgs);
       } else if (implType == ImplType.HS_HA) {
         THsHaServer.Args serverArgs = new THsHaServer.Args(serverTransport);
-        CallQueue callQueue =
-            new CallQueue(new LinkedBlockingQueue<Call>(), metrics);
+        CallQueue callQueue = new CallQueue(new LinkedBlockingQueue<>(), metrics);
         ExecutorService executorService = createExecutor(
             callQueue, serverArgs.getMaxWorkerThreads(), serverArgs.getMaxWorkerThreads());
         serverArgs.executorService(executorService)
@@ -603,8 +602,7 @@ public class ThriftServerRunner implements Runnable {
       } else { // THREADED_SELECTOR
         TThreadedSelectorServer.Args serverArgs =
             new HThreadedSelectorServerArgs(serverTransport, conf);
-        CallQueue callQueue =
-            new CallQueue(new LinkedBlockingQueue<Call>(), metrics);
+        CallQueue callQueue = new CallQueue(new LinkedBlockingQueue<>(), metrics);
         ExecutorService executorService = createExecutor(
             callQueue, serverArgs.getWorkerThreads(), serverArgs.getWorkerThreads());
         serverArgs.executorService(executorService)
@@ -781,7 +779,7 @@ public class ThriftServerRunner implements Runnable {
     protected HBaseHandler(final Configuration c,
         final UserProvider userProvider) throws IOException {
       this.conf = c;
-      scannerMap = new HashMap<Integer, ResultScannerWrapper>();
+      scannerMap = new HashMap<>();
       this.coalescer = new IncrementCoalescer(this);
 
       int cleanInterval = conf.getInt(CLEANUP_INTERVAL, 10 * 1000);
@@ -869,7 +867,7 @@ public class ThriftServerRunner implements Runnable {
     public List<ByteBuffer> getTableNames() throws IOError {
       try {
         TableName[] tableNames = this.getAdmin().listTableNames();
-        ArrayList<ByteBuffer> list = new ArrayList<ByteBuffer>(tableNames.length);
+        ArrayList<ByteBuffer> list = new ArrayList<>(tableNames.length);
         for (int i = 0; i < tableNames.length; i++) {
           list.add(ByteBuffer.wrap(tableNames[i].getName()));
         }
@@ -888,7 +886,7 @@ public class ThriftServerRunner implements Runnable {
     throws IOError {
       try (RegionLocator locator = connectionCache.getRegionLocator(getBytes(tableName))) {
         List<HRegionLocation> regionLocations = locator.getAllRegionLocations();
-        List<TRegionInfo> results = new ArrayList<TRegionInfo>(regionLocations.size());
+        List<TRegionInfo> results = new ArrayList<>(regionLocations.size());
         for (HRegionLocation regionLocation : regionLocations) {
           HRegionInfo info = regionLocation.getRegionInfo();
           ServerName serverName = regionLocation.getServerName();
@@ -1151,7 +1149,7 @@ public class ThriftServerRunner implements Runnable {
       
       Table table= null;
       try {
-        List<Get> gets = new ArrayList<Get>(rows.size());
+        List<Get> gets = new ArrayList<>(rows.size());
         table = getTable(tableName);
         if (metrics != null) {
           metrics.incNumRowKeysInBatchGet(rows.size());
@@ -1363,8 +1361,8 @@ public class ThriftServerRunner implements Runnable {
         ByteBuffer tableName, List<BatchMutation> rowBatches, long timestamp,
         Map<ByteBuffer, ByteBuffer> attributes)
         throws IOError, IllegalArgument, TException {
-      List<Put> puts = new ArrayList<Put>();
-      List<Delete> deletes = new ArrayList<Delete>();
+      List<Put> puts = new ArrayList<>();
+      List<Delete> deletes = new ArrayList<>();
 
       for (BatchMutation batch : rowBatches) {
         byte[] row = getBytes(batch.row);
@@ -1479,7 +1477,7 @@ public class ThriftServerRunner implements Runnable {
       try {
         results = resultScannerWrapper.getScanner().next(nbRows);
         if (null == results) {
-          return new ArrayList<TRowResult>();
+          return new ArrayList<>();
         }
       } catch (IOException e) {
         LOG.warn(e.getMessage(), e);
@@ -1709,8 +1707,7 @@ public class ThriftServerRunner implements Runnable {
       
       Table table = null;
       try {
-        TreeMap<ByteBuffer, ColumnDescriptor> columns =
-          new TreeMap<ByteBuffer, ColumnDescriptor>();
+        TreeMap<ByteBuffer, ColumnDescriptor> columns = new TreeMap<>();
 
         table = getTable(tableName);
         HTableDescriptor desc = table.getTableDescriptor();
diff --git a/hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift/ThriftUtilities.java b/hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift/ThriftUtilities.java
index d2a95ce..7ec49fb 100644
--- a/hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift/ThriftUtilities.java
+++ b/hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift/ThriftUtilities.java
@@ -107,7 +107,7 @@ public class ThriftUtilities {
    * @return Thrift TCell array
    */
   static public List<TCell> cellFromHBase(Cell in) {
-    List<TCell> list = new ArrayList<TCell>(1);
+    List<TCell> list = new ArrayList<>(1);
     if (in != null) {
       list.add(new TCell(ByteBuffer.wrap(CellUtil.cloneValue(in)), in.getTimestamp()));
     }
@@ -123,12 +123,12 @@ public class ThriftUtilities {
   static public List<TCell> cellFromHBase(Cell[] in) {
     List<TCell> list = null;
     if (in != null) {
-      list = new ArrayList<TCell>(in.length);
+      list = new ArrayList<>(in.length);
       for (int i = 0; i < in.length; i++) {
         list.add(new TCell(ByteBuffer.wrap(CellUtil.cloneValue(in[i])), in[i].getTimestamp()));
       }
     } else {
-      list = new ArrayList<TCell>(0);
+      list = new ArrayList<>(0);
     }
     return list;
   }
@@ -149,7 +149,7 @@ public class ThriftUtilities {
    * @return Thrift TRowResult array
    */
   static public List<TRowResult> rowResultFromHBase(Result[] in, boolean sortColumns) {
-    List<TRowResult> results = new ArrayList<TRowResult>(in.length);
+    List<TRowResult> results = new ArrayList<>(in.length);
     for ( Result result_ : in) {
         if(result_ == null || result_.isEmpty()) {
             continue;
@@ -157,7 +157,7 @@ public class ThriftUtilities {
         TRowResult result = new TRowResult();
         result.row = ByteBuffer.wrap(result_.getRow());
         if (sortColumns) {
-          result.sortedColumns = new ArrayList<TColumn>();
+          result.sortedColumns = new ArrayList<>();
           for (Cell kv : result_.rawCells()) {
             result.sortedColumns.add(new TColumn(
                 ByteBuffer.wrap(KeyValue.makeColumn(CellUtil.cloneFamily(kv),
@@ -165,7 +165,7 @@ public class ThriftUtilities {
                 new TCell(ByteBuffer.wrap(CellUtil.cloneValue(kv)), kv.getTimestamp())));
           }
         } else {
-          result.columns = new TreeMap<ByteBuffer, TCell>();
+          result.columns = new TreeMap<>();
           for (Cell kv : result_.rawCells()) {
             result.columns.put(
                 ByteBuffer.wrap(KeyValue.makeColumn(CellUtil.cloneFamily(kv),
diff --git a/hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift2/ThriftHBaseServiceHandler.java b/hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift2/ThriftHBaseServiceHandler.java
index 5a68147..acad62c 100644
--- a/hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift2/ThriftHBaseServiceHandler.java
+++ b/hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift2/ThriftHBaseServiceHandler.java
@@ -87,8 +87,7 @@ public class ThriftHBaseServiceHandler implements THBaseService.Iface {
   // nextScannerId and scannerMap are used to manage scanner state
   // TODO: Cleanup thread for Scanners, Scanner id wrap
   private final AtomicInteger nextScannerId = new AtomicInteger(0);
-  private final Map<Integer, ResultScanner> scannerMap =
-      new ConcurrentHashMap<Integer, ResultScanner>();
+  private final Map<Integer, ResultScanner> scannerMap = new ConcurrentHashMap<>();
 
   private final ConnectionCache connectionCache;
 
diff --git a/hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift2/ThriftServer.java b/hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift2/ThriftServer.java
index d027c77..560ae64 100644
--- a/hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift2/ThriftServer.java
+++ b/hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift2/ThriftServer.java
@@ -195,7 +195,7 @@ public class ThriftServer extends Configured implements Tool {
     } else if (qop == null) {
       return new TTransportFactory();
     } else {
-      Map<String, String> saslProperties = new HashMap<String, String>();
+      Map<String, String> saslProperties = new HashMap<>();
       saslProperties.put(Sasl.QOP, qop.getSaslQop());
       TSaslServerTransport.Factory saslFactory = new TSaslServerTransport.Factory();
       saslFactory.addServerDefinition("GSSAPI", name, host, saslProperties,
@@ -306,9 +306,9 @@ public class ThriftServer extends Configured implements Tool {
       int workerThreads, int maxCallQueueSize, ThriftMetrics metrics) {
     CallQueue callQueue;
     if (maxCallQueueSize > 0) {
-      callQueue = new CallQueue(new LinkedBlockingQueue<Call>(maxCallQueueSize), metrics);
+      callQueue = new CallQueue(new LinkedBlockingQueue<>(maxCallQueueSize), metrics);
     } else {
-      callQueue = new CallQueue(new LinkedBlockingQueue<Call>(), metrics);
+      callQueue = new CallQueue(new LinkedBlockingQueue<>(), metrics);
     }
 
     ThreadFactoryBuilder tfb = new ThreadFactoryBuilder();
diff --git a/hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift2/ThriftUtilities.java b/hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift2/ThriftUtilities.java
index 0001b3f..7b4a82b 100644
--- a/hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift2/ThriftUtilities.java
+++ b/hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift2/ThriftUtilities.java
@@ -139,7 +139,7 @@ public class ThriftUtilities {
    * @see #getFromThrift(TGet)
    */
   public static List<Get> getsFromThrift(List<TGet> in) throws IOException {
-    List<Get> out = new ArrayList<Get>(in.size());
+    List<Get> out = new ArrayList<>(in.size());
     for (TGet get : in) {
       out.add(getFromThrift(get));
     }
@@ -160,7 +160,7 @@ public class ThriftUtilities {
     if (row != null) {
       out.setRow(in.getRow());
     }
-    List<TColumnValue> columnValues = new ArrayList<TColumnValue>(raw.length);
+    List<TColumnValue> columnValues = new ArrayList<>(raw.length);
     for (Cell kv : raw) {
       TColumnValue col = new TColumnValue();
       col.setFamily(CellUtil.cloneFamily(kv));
@@ -186,7 +186,7 @@ public class ThriftUtilities {
    * @see #resultFromHBase(Result)
    */
   public static List<TResult> resultsFromHBase(Result[] in) {
-    List<TResult> out = new ArrayList<TResult>(in.length);
+    List<TResult> out = new ArrayList<>(in.length);
     for (Result result : in) {
       out.add(resultFromHBase(result));
     }
@@ -245,7 +245,7 @@ public class ThriftUtilities {
    * @see #putFromThrift(TPut)
    */
   public static List<Put> putsFromThrift(List<TPut> in) {
-    List<Put> out = new ArrayList<Put>(in.size());
+    List<Put> out = new ArrayList<>(in.size());
     for (TPut put : in) {
       out.add(putFromThrift(put));
     }
@@ -318,7 +318,7 @@ public class ThriftUtilities {
    */
 
   public static List<Delete> deletesFromThrift(List<TDelete> in) {
-    List<Delete> out = new ArrayList<Delete>(in.size());
+    List<Delete> out = new ArrayList<>(in.size());
     for (TDelete delete : in) {
       out.add(deleteFromThrift(delete));
     }
@@ -328,7 +328,7 @@ public class ThriftUtilities {
   public static TDelete deleteFromHBase(Delete in) {
     TDelete out = new TDelete(ByteBuffer.wrap(in.getRow()));
 
-    List<TColumn> columns = new ArrayList<TColumn>(in.getFamilyCellMap().entrySet().size());
+    List<TColumn> columns = new ArrayList<>(in.getFamilyCellMap().entrySet().size());
     long rowTimestamp = in.getTimeStamp();
     if (rowTimestamp != HConstants.LATEST_TIMESTAMP) {
       out.setTimestamp(rowTimestamp);
@@ -505,7 +505,7 @@ public class ThriftUtilities {
   }
 
   public static List<THRegionLocation> regionLocationsFromHBase(List<HRegionLocation> locations) {
-    List<THRegionLocation> tlocations = new ArrayList<THRegionLocation>(locations.size());
+    List<THRegionLocation> tlocations = new ArrayList<>(locations.size());
     for (HRegionLocation hrl:locations) {
       tlocations.add(regionLocationFromHBase(hrl));
     }
diff --git a/hbase-thrift/src/test/java/org/apache/hadoop/hbase/thrift/TestCallQueue.java b/hbase-thrift/src/test/java/org/apache/hadoop/hbase/thrift/TestCallQueue.java
index b646009..e595847 100644
--- a/hbase-thrift/src/test/java/org/apache/hadoop/hbase/thrift/TestCallQueue.java
+++ b/hbase-thrift/src/test/java/org/apache/hadoop/hbase/thrift/TestCallQueue.java
@@ -56,7 +56,7 @@ public class TestCallQueue {
 
   @Parameters
   public static Collection<Object[]> getParameters() {
-    Collection<Object[]> parameters = new ArrayList<Object[]>();
+    Collection<Object[]> parameters = new ArrayList<>();
     for (int elementsAdded : new int[] {100, 200, 300}) {
       for (int elementsRemoved : new int[] {0, 20, 100}) {
         parameters.add(new Object[]{new Integer(elementsAdded),
@@ -77,8 +77,7 @@ public class TestCallQueue {
   @Test(timeout = 60000)
   public void testPutTake() throws Exception {
     ThriftMetrics metrics = createMetrics();
-    CallQueue callQueue = new CallQueue(
-        new LinkedBlockingQueue<Call>(), metrics);
+    CallQueue callQueue = new CallQueue(new LinkedBlockingQueue<>(), metrics);
     for (int i = 0; i < elementsAdded; ++i) {
       callQueue.put(createDummyRunnable());
     }
@@ -91,8 +90,7 @@ public class TestCallQueue {
   @Test(timeout = 60000)
   public void testOfferPoll() throws Exception {
     ThriftMetrics metrics = createMetrics();
-    CallQueue callQueue = new CallQueue(
-        new LinkedBlockingQueue<Call>(), metrics);
+    CallQueue callQueue = new CallQueue(new LinkedBlockingQueue<>(), metrics);
     for (int i = 0; i < elementsAdded; ++i) {
       callQueue.offer(createDummyRunnable());
     }
diff --git a/hbase-thrift/src/test/java/org/apache/hadoop/hbase/thrift/TestThriftHttpServer.java b/hbase-thrift/src/test/java/org/apache/hadoop/hbase/thrift/TestThriftHttpServer.java
index 26019be..c04b36f 100644
--- a/hbase-thrift/src/test/java/org/apache/hadoop/hbase/thrift/TestThriftHttpServer.java
+++ b/hbase-thrift/src/test/java/org/apache/hadoop/hbase/thrift/TestThriftHttpServer.java
@@ -127,7 +127,7 @@ public class TestThriftHttpServer {
   }
 
   private void runThriftServer(int customHeaderSize) throws Exception {
-    List<String> args = new ArrayList<String>(3);
+    List<String> args = new ArrayList<>(3);
     port = HBaseTestingUtility.randomFreePort();
     args.add("-" + ThriftServer.PORT_OPTION);
     args.add(String.valueOf(port));
diff --git a/hbase-thrift/src/test/java/org/apache/hadoop/hbase/thrift/TestThriftServer.java b/hbase-thrift/src/test/java/org/apache/hadoop/hbase/thrift/TestThriftServer.java
index ff4bc6a..d0052e5 100644
--- a/hbase-thrift/src/test/java/org/apache/hadoop/hbase/thrift/TestThriftServer.java
+++ b/hbase-thrift/src/test/java/org/apache/hadoop/hbase/thrift/TestThriftServer.java
@@ -280,13 +280,13 @@ public class TestThriftServer {
   }
 
   public static void doTestIncrements(HBaseHandler handler) throws Exception {
-    List<Mutation> mutations = new ArrayList<Mutation>(1);
+    List<Mutation> mutations = new ArrayList<>(1);
     mutations.add(new Mutation(false, columnAAname, valueEname, true));
     mutations.add(new Mutation(false, columnAname, valueEname, true));
     handler.mutateRow(tableAname, rowAname, mutations, null);
     handler.mutateRow(tableAname, rowBname, mutations, null);
 
-    List<TIncrement> increments = new ArrayList<TIncrement>(3);
+    List<TIncrement> increments = new ArrayList<>(3);
     increments.add(new TIncrement(tableAname, rowBname, columnAAname, 7));
     increments.add(new TIncrement(tableAname, rowBname, columnAAname, 7));
     increments.add(new TIncrement(tableAname, rowBname, columnAAname, 7));
@@ -377,7 +377,7 @@ public class TestThriftServer {
     assertEquals(0, size);
 
     // Try null mutation
-    List<Mutation> mutations = new ArrayList<Mutation>(1);
+    List<Mutation> mutations = new ArrayList<>(1);
     mutations.add(new Mutation(false, columnAname, null, true));
     handler.mutateRow(tableAname, rowAname, mutations, null);
     TRowResult rowResult3 = handler.getRow(tableAname, rowAname, null).get(0);
@@ -436,7 +436,7 @@ public class TestThriftServer {
     // ColumnAname has been deleted, and will never be visible even with a getRowTs()
     assertFalse(rowResult2.columns.containsKey(columnAname));
 
-    List<ByteBuffer> columns = new ArrayList<ByteBuffer>(1);
+    List<ByteBuffer> columns = new ArrayList<>(1);
     columns.add(columnBname);
 
     rowResult1 = handler.getRowWithColumns(tableAname, rowAname, columns, null).get(0);
@@ -555,7 +555,7 @@ public class TestThriftServer {
     assertEquals(rowResult6.sortedColumns.size(), 1);
     assertEquals(rowResult6.sortedColumns.get(0).getCell().value, valueCname);
 
-    List<Mutation> rowBmutations = new ArrayList<Mutation>(20);
+    List<Mutation> rowBmutations = new ArrayList<>(20);
     for (int i = 0; i < 20; i++) {
       rowBmutations.add(new Mutation(false, asByteBuffer("columnA:" + i), valueCname, true));
     }
@@ -668,13 +668,13 @@ public class TestThriftServer {
         UserProvider.instantiate(UTIL.getConfiguration()));
     handler.createTable(tableAname, getColumnDescriptors());
     try {
-      List<Mutation> mutations = new ArrayList<Mutation>(1);
+      List<Mutation> mutations = new ArrayList<>(1);
       mutations.add(new Mutation(false, columnAname, valueAname, true));
       handler.mutateRow(tableAname, rowAname, mutations, null);
 
-      List<ByteBuffer> columnList = new ArrayList<ByteBuffer>(1);
+      List<ByteBuffer> columnList = new ArrayList<>(1);
       columnList.add(columnAname);
-      List<ByteBuffer> valueList = new ArrayList<ByteBuffer>(1);
+      List<ByteBuffer> valueList = new ArrayList<>(1);
       valueList.add(valueBname);
 
       TAppend append = new TAppend(tableAname, rowAname, columnList, valueList);
@@ -702,7 +702,7 @@ public class TestThriftServer {
         UserProvider.instantiate(UTIL.getConfiguration()));
     handler.createTable(tableAname, getColumnDescriptors());
     try {
-      List<Mutation> mutations = new ArrayList<Mutation>(1);
+      List<Mutation> mutations = new ArrayList<>(1);
       mutations.add(new Mutation(false, columnAname, valueAname, true));
       Mutation putB = (new Mutation(false, columnBname, valueBname, true));
 
@@ -796,7 +796,7 @@ public class TestThriftServer {
    * default ColumnDescriptor and one ColumnDescriptor with fewer versions
    */
   private static List<ColumnDescriptor> getColumnDescriptors() {
-    ArrayList<ColumnDescriptor> cDescriptors = new ArrayList<ColumnDescriptor>(2);
+    ArrayList<ColumnDescriptor> cDescriptors = new ArrayList<>(2);
 
     // A default ColumnDescriptor
     ColumnDescriptor cDescA = new ColumnDescriptor();
@@ -818,7 +818,7 @@ public class TestThriftServer {
    * @return a List of column names for use in retrieving a scanner
    */
   private List<ByteBuffer> getColumnList(boolean includeA, boolean includeB) {
-    List<ByteBuffer> columnList = new ArrayList<ByteBuffer>();
+    List<ByteBuffer> columnList = new ArrayList<>();
     if (includeA) columnList.add(columnAname);
     if (includeB) columnList.add(columnBname);
     return columnList;
@@ -830,7 +830,7 @@ public class TestThriftServer {
    * and columnB having valueB
    */
   private static List<Mutation> getMutations() {
-    List<Mutation> mutations = new ArrayList<Mutation>(2);
+    List<Mutation> mutations = new ArrayList<>(2);
     mutations.add(new Mutation(false, columnAname, valueAname, true));
     mutations.add(new Mutation(false, columnBname, valueBname, true));
     return mutations;
@@ -845,19 +845,19 @@ public class TestThriftServer {
    * (rowB, columnB): place valueD
    */
   private static List<BatchMutation> getBatchMutations() {
-    List<BatchMutation> batchMutations = new ArrayList<BatchMutation>(3);
+    List<BatchMutation> batchMutations = new ArrayList<>(3);
 
     // Mutations to rowA.  You can't mix delete and put anymore.
-    List<Mutation> rowAmutations = new ArrayList<Mutation>(1);
+    List<Mutation> rowAmutations = new ArrayList<>(1);
     rowAmutations.add(new Mutation(true, columnAname, null, true));
     batchMutations.add(new BatchMutation(rowAname, rowAmutations));
 
-    rowAmutations = new ArrayList<Mutation>(1);
+    rowAmutations = new ArrayList<>(1);
     rowAmutations.add(new Mutation(false, columnBname, valueCname, true));
     batchMutations.add(new BatchMutation(rowAname, rowAmutations));
 
     // Mutations to rowB
-    List<Mutation> rowBmutations = new ArrayList<Mutation>(2);
+    List<Mutation> rowBmutations = new ArrayList<>(2);
     rowBmutations.add(new Mutation(false, columnAname, valueCname, true));
     rowBmutations.add(new Mutation(false, columnBname, valueDname, true));
     batchMutations.add(new BatchMutation(rowBname, rowBmutations));
diff --git a/hbase-thrift/src/test/java/org/apache/hadoop/hbase/thrift/TestThriftServerCmdLine.java b/hbase-thrift/src/test/java/org/apache/hadoop/hbase/thrift/TestThriftServerCmdLine.java
index 457273e..87998da 100644
--- a/hbase-thrift/src/test/java/org/apache/hadoop/hbase/thrift/TestThriftServerCmdLine.java
+++ b/hbase-thrift/src/test/java/org/apache/hadoop/hbase/thrift/TestThriftServerCmdLine.java
@@ -82,7 +82,7 @@ public class TestThriftServerCmdLine {
 
   @Parameters
   public static Collection<Object[]> getParameters() {
-    Collection<Object[]> parameters = new ArrayList<Object[]>();
+    Collection<Object[]> parameters = new ArrayList<>();
     for (ImplType implType : ImplType.values()) {
       for (boolean specifyFramed : new boolean[] {false, true}) {
         for (boolean specifyBindIP : new boolean[] {false, true}) {
@@ -151,7 +151,7 @@ public class TestThriftServerCmdLine {
 
   @Test(timeout=600000)
   public void testRunThriftServer() throws Exception {
-    List<String> args = new ArrayList<String>();
+    List<String> args = new ArrayList<>();
     if (implType != null) {
       String serverTypeOption = implType.toString();
       assertTrue(serverTypeOption.startsWith("-"));
diff --git a/hbase-thrift/src/test/java/org/apache/hadoop/hbase/thrift2/TestThriftHBaseServiceHandler.java b/hbase-thrift/src/test/java/org/apache/hadoop/hbase/thrift2/TestThriftHBaseServiceHandler.java
index 3fe8537..db5bdf2 100644
--- a/hbase-thrift/src/test/java/org/apache/hadoop/hbase/thrift2/TestThriftHBaseServiceHandler.java
+++ b/hbase-thrift/src/test/java/org/apache/hadoop/hbase/thrift2/TestThriftHBaseServiceHandler.java
@@ -195,7 +195,7 @@ public class TestThriftHBaseServiceHandler {
     TGet get = new TGet(wrap(rowName));
     assertFalse(handler.exists(table, get));
 
-    List<TColumnValue> columnValues = new ArrayList<TColumnValue>(2);
+    List<TColumnValue> columnValues = new ArrayList<>(2);
     columnValues.add(new TColumnValue(wrap(familyAname), wrap(qualifierAname), wrap(valueAname)));
     columnValues.add(new TColumnValue(wrap(familyBname), wrap(qualifierBname), wrap(valueBname)));
     TPut put = new TPut(wrap(rowName), columnValues);
@@ -240,7 +240,7 @@ public class TestThriftHBaseServiceHandler {
     byte[] rowName = "testPutGet".getBytes();
     ByteBuffer table = wrap(tableAname);
 
-    List<TColumnValue> columnValues = new ArrayList<TColumnValue>(2);
+    List<TColumnValue> columnValues = new ArrayList<>(2);
     columnValues.add(new TColumnValue(wrap(familyAname), wrap(qualifierAname), wrap(valueAname)));
     columnValues.add(new TColumnValue(wrap(familyBname), wrap(qualifierBname), wrap(valueBname)));
     TPut put = new TPut(wrap(rowName), columnValues);
@@ -264,16 +264,16 @@ public class TestThriftHBaseServiceHandler {
     byte[] rowName1 = "testPutGetMultiple1".getBytes();
     byte[] rowName2 = "testPutGetMultiple2".getBytes();
 
-    List<TColumnValue> columnValues = new ArrayList<TColumnValue>(2);
+    List<TColumnValue> columnValues = new ArrayList<>(2);
     columnValues.add(new TColumnValue(wrap(familyAname), wrap(qualifierAname), wrap(valueAname)));
     columnValues.add(new TColumnValue(wrap(familyBname), wrap(qualifierBname), wrap(valueBname)));
-    List<TPut> puts = new ArrayList<TPut>(2);
+    List<TPut> puts = new ArrayList<>(2);
     puts.add(new TPut(wrap(rowName1), columnValues));
     puts.add(new TPut(wrap(rowName2), columnValues));
 
     handler.putMultiple(table, puts);
 
-    List<TGet> gets = new ArrayList<TGet>(2);
+    List<TGet> gets = new ArrayList<>(2);
     gets.add(new TGet(wrap(rowName1)));
     gets.add(new TGet(wrap(rowName2)));
 
@@ -294,16 +294,16 @@ public class TestThriftHBaseServiceHandler {
     byte[] rowName1 = "testDeleteMultiple1".getBytes();
     byte[] rowName2 = "testDeleteMultiple2".getBytes();
 
-    List<TColumnValue> columnValues = new ArrayList<TColumnValue>(2);
+    List<TColumnValue> columnValues = new ArrayList<>(2);
     columnValues.add(new TColumnValue(wrap(familyAname), wrap(qualifierAname), wrap(valueAname)));
     columnValues.add(new TColumnValue(wrap(familyBname), wrap(qualifierBname), wrap(valueBname)));
-    List<TPut> puts = new ArrayList<TPut>(2);
+    List<TPut> puts = new ArrayList<>(2);
     puts.add(new TPut(wrap(rowName1), columnValues));
     puts.add(new TPut(wrap(rowName2), columnValues));
 
     handler.putMultiple(table, puts);
 
-    List<TDelete> deletes = new ArrayList<TDelete>(2);
+    List<TDelete> deletes = new ArrayList<>(2);
     deletes.add(new TDelete(wrap(rowName1)));
     deletes.add(new TDelete(wrap(rowName2)));
 
@@ -321,7 +321,7 @@ public class TestThriftHBaseServiceHandler {
     byte[] rowName = "testDelete".getBytes();
     ByteBuffer table = wrap(tableAname);
 
-    List<TColumnValue> columnValues = new ArrayList<TColumnValue>(2);
+    List<TColumnValue> columnValues = new ArrayList<>(2);
     TColumnValue columnValueA = new TColumnValue(wrap(familyAname), wrap(qualifierAname),
       wrap(valueAname));
     TColumnValue columnValueB = new TColumnValue(wrap(familyBname), wrap(qualifierBname),
@@ -335,7 +335,7 @@ public class TestThriftHBaseServiceHandler {
     handler.put(table, put);
 
     TDelete delete = new TDelete(wrap(rowName));
-    List<TColumn> deleteColumns = new ArrayList<TColumn>(1);
+    List<TColumn> deleteColumns = new ArrayList<>(1);
     TColumn deleteColumn = new TColumn(wrap(familyAname));
     deleteColumn.setQualifier(qualifierAname);
     deleteColumns.add(deleteColumn);
@@ -347,7 +347,7 @@ public class TestThriftHBaseServiceHandler {
     TResult result = handler.get(table, get);
     assertArrayEquals(rowName, result.getRow());
     List<TColumnValue> returnedColumnValues = result.getColumnValues();
-    List<TColumnValue> expectedColumnValues = new ArrayList<TColumnValue>(1);
+    List<TColumnValue> expectedColumnValues = new ArrayList<>(1);
     expectedColumnValues.add(columnValueB);
     assertTColumnValuesEqual(expectedColumnValues, returnedColumnValues);
   }
@@ -358,7 +358,7 @@ public class TestThriftHBaseServiceHandler {
     byte[] rowName = "testDeleteAllTimestamps".getBytes();
     ByteBuffer table = wrap(tableAname);
 
-    List<TColumnValue> columnValues = new ArrayList<TColumnValue>(1);
+    List<TColumnValue> columnValues = new ArrayList<>(1);
     TColumnValue columnValueA = new TColumnValue(wrap(familyAname), wrap(qualifierAname),
       wrap(valueAname));
     columnValueA.setTimestamp(System.currentTimeMillis() - 10);
@@ -377,7 +377,7 @@ public class TestThriftHBaseServiceHandler {
     assertEquals(2, result.getColumnValuesSize());
 
     TDelete delete = new TDelete(wrap(rowName));
-    List<TColumn> deleteColumns = new ArrayList<TColumn>(1);
+    List<TColumn> deleteColumns = new ArrayList<>(1);
     TColumn deleteColumn = new TColumn(wrap(familyAname));
     deleteColumn.setQualifier(qualifierAname);
     deleteColumns.add(deleteColumn);
@@ -401,7 +401,7 @@ public class TestThriftHBaseServiceHandler {
     long timestamp1 = System.currentTimeMillis() - 10;
     long timestamp2 = System.currentTimeMillis();
 
-    List<TColumnValue> columnValues = new ArrayList<TColumnValue>(1);
+    List<TColumnValue> columnValues = new ArrayList<>(1);
     TColumnValue columnValueA = new TColumnValue(wrap(familyAname), wrap(qualifierAname),
       wrap(valueAname));
     columnValueA.setTimestamp(timestamp1);
@@ -420,7 +420,7 @@ public class TestThriftHBaseServiceHandler {
     assertEquals(2, result.getColumnValuesSize());
 
     TDelete delete = new TDelete(wrap(rowName));
-    List<TColumn> deleteColumns = new ArrayList<TColumn>(1);
+    List<TColumn> deleteColumns = new ArrayList<>(1);
     TColumn deleteColumn = new TColumn(wrap(familyAname));
     deleteColumn.setQualifier(qualifierAname);
     deleteColumns.add(deleteColumn);
@@ -443,14 +443,14 @@ public class TestThriftHBaseServiceHandler {
     byte[] rowName = "testIncrement".getBytes();
     ByteBuffer table = wrap(tableAname);
 
-    List<TColumnValue> columnValues = new ArrayList<TColumnValue>(1);
+    List<TColumnValue> columnValues = new ArrayList<>(1);
     columnValues.add(new TColumnValue(wrap(familyAname), wrap(qualifierAname),
       wrap(Bytes.toBytes(1L))));
     TPut put = new TPut(wrap(rowName), columnValues);
     put.setColumnValues(columnValues);
     handler.put(table, put);
 
-    List<TColumnIncrement> incrementColumns = new ArrayList<TColumnIncrement>(1);
+    List<TColumnIncrement> incrementColumns = new ArrayList<>(1);
     incrementColumns.add(new TColumnIncrement(wrap(familyAname), wrap(qualifierAname)));
     TIncrement increment = new TIncrement(wrap(rowName), incrementColumns);
     handler.increment(table, increment);
@@ -471,13 +471,13 @@ public class TestThriftHBaseServiceHandler {
     ByteBuffer table = wrap(tableAname);
     byte[] v1 = Bytes.toBytes("42");
     byte[] v2 = Bytes.toBytes("23");
-    List<TColumnValue> columnValues = new ArrayList<TColumnValue>(1);
+    List<TColumnValue> columnValues = new ArrayList<>(1);
     columnValues.add(new TColumnValue(wrap(familyAname), wrap(qualifierAname), wrap(v1)));
     TPut put = new TPut(wrap(rowName), columnValues);
     put.setColumnValues(columnValues);
     handler.put(table, put);
 
-    List<TColumnValue> appendColumns = new ArrayList<TColumnValue>(1);
+    List<TColumnValue> appendColumns = new ArrayList<>(1);
     appendColumns.add(new TColumnValue(wrap(familyAname), wrap(qualifierAname), wrap(v2)));
     TAppend append = new TAppend(wrap(rowName), appendColumns);
     handler.append(table, append);
@@ -503,14 +503,14 @@ public class TestThriftHBaseServiceHandler {
     byte[] rowName = "testCheckAndPut".getBytes();
     ByteBuffer table = wrap(tableAname);
 
-    List<TColumnValue> columnValuesA = new ArrayList<TColumnValue>(1);
+    List<TColumnValue> columnValuesA = new ArrayList<>(1);
     TColumnValue columnValueA = new TColumnValue(wrap(familyAname), wrap(qualifierAname),
       wrap(valueAname));
     columnValuesA.add(columnValueA);
     TPut putA = new TPut(wrap(rowName), columnValuesA);
     putA.setColumnValues(columnValuesA);
 
-    List<TColumnValue> columnValuesB = new ArrayList<TColumnValue>(1);
+    List<TColumnValue> columnValuesB = new ArrayList<>(1);
     TColumnValue columnValueB = new TColumnValue(wrap(familyBname), wrap(qualifierBname),
       wrap(valueBname));
     columnValuesB.add(columnValueB);
@@ -532,7 +532,7 @@ public class TestThriftHBaseServiceHandler {
     result = handler.get(table, get);
     assertArrayEquals(rowName, result.getRow());
     List<TColumnValue> returnedColumnValues = result.getColumnValues();
-    List<TColumnValue> expectedColumnValues = new ArrayList<TColumnValue>(2);
+    List<TColumnValue> expectedColumnValues = new ArrayList<>(2);
     expectedColumnValues.add(columnValueA);
     expectedColumnValues.add(columnValueB);
     assertTColumnValuesEqual(expectedColumnValues, returnedColumnValues);
@@ -550,14 +550,14 @@ public class TestThriftHBaseServiceHandler {
     byte[] rowName = "testCheckAndDelete".getBytes();
     ByteBuffer table = wrap(tableAname);
 
-    List<TColumnValue> columnValuesA = new ArrayList<TColumnValue>(1);
+    List<TColumnValue> columnValuesA = new ArrayList<>(1);
     TColumnValue columnValueA = new TColumnValue(wrap(familyAname), wrap(qualifierAname),
       wrap(valueAname));
     columnValuesA.add(columnValueA);
     TPut putA = new TPut(wrap(rowName), columnValuesA);
     putA.setColumnValues(columnValuesA);
 
-    List<TColumnValue> columnValuesB = new ArrayList<TColumnValue>(1);
+    List<TColumnValue> columnValuesB = new ArrayList<>(1);
     TColumnValue columnValueB = new TColumnValue(wrap(familyBname), wrap(qualifierBname),
       wrap(valueBname));
     columnValuesB.add(columnValueB);
@@ -595,7 +595,7 @@ public class TestThriftHBaseServiceHandler {
     // insert data
     TColumnValue columnValue = new TColumnValue(wrap(familyAname), wrap(qualifierAname),
       wrap(valueAname));
-    List<TColumnValue> columnValues = new ArrayList<TColumnValue>(1);
+    List<TColumnValue> columnValues = new ArrayList<>(1);
     columnValues.add(columnValue);
     for (int i = 0; i < 10; i++) {
       TPut put = new TPut(wrap(("testScan" + i).getBytes()), columnValues);
@@ -604,7 +604,7 @@ public class TestThriftHBaseServiceHandler {
 
     // create scan instance
     TScan scan = new TScan();
-    List<TColumn> columns = new ArrayList<TColumn>(1);
+    List<TColumn> columns = new ArrayList<>(1);
     TColumn column = new TColumn();
     column.setFamily(familyAname);
     column.setQualifier(qualifierAname);
@@ -656,7 +656,7 @@ public class TestThriftHBaseServiceHandler {
     // insert data
     TColumnValue columnValue = new TColumnValue(wrap(familyAname), wrap(qualifierAname),
         wrap(valueAname));
-    List<TColumnValue> columnValues = new ArrayList<TColumnValue>(1);
+    List<TColumnValue> columnValues = new ArrayList<>(1);
     columnValues.add(columnValue);
     for (int i = 0; i < numTrials; i++) {
       TPut put = new TPut(wrap(("testScan" + i).getBytes()), columnValues);
@@ -665,7 +665,7 @@ public class TestThriftHBaseServiceHandler {
 
     // create scan instance
     TScan scan = new TScan();
-    List<TColumn> columns = new ArrayList<TColumn>(1);
+    List<TColumn> columns = new ArrayList<>(1);
     TColumn column = new TColumn();
     column.setFamily(familyAname);
     column.setQualifier(qualifierAname);
@@ -694,7 +694,7 @@ public class TestThriftHBaseServiceHandler {
     // insert data
     TColumnValue columnValue = new TColumnValue(wrap(familyAname), wrap(qualifierAname),
       wrap(valueAname));
-    List<TColumnValue> columnValues = new ArrayList<TColumnValue>(1);
+    List<TColumnValue> columnValues = new ArrayList<>(1);
     columnValues.add(columnValue);
     for (int i = 0; i < 10; i++) {
       TPut put = new TPut(wrap(("testReverseScan" + i).getBytes()), columnValues);
@@ -704,7 +704,7 @@ public class TestThriftHBaseServiceHandler {
     // create reverse scan instance
     TScan scan = new TScan();
     scan.setReversed(true);
-    List<TColumn> columns = new ArrayList<TColumn>(1);
+    List<TColumn> columns = new ArrayList<>(1);
     TColumn column = new TColumn();
     column.setFamily(familyAname);
     column.setQualifier(qualifierAname);
@@ -743,7 +743,7 @@ public class TestThriftHBaseServiceHandler {
     // insert data
     TColumnValue columnValue = new TColumnValue(wrap(familyAname), wrap(qualifierAname),
       wrap(valueAname));
-    List<TColumnValue> columnValues = new ArrayList<TColumnValue>(1);
+    List<TColumnValue> columnValues = new ArrayList<>(1);
     columnValues.add(columnValue);
     for (int i = 0; i < 10; i++) {
       TPut put = new TPut(wrap(("testScanWithFilter" + i).getBytes()), columnValues);
@@ -752,7 +752,7 @@ public class TestThriftHBaseServiceHandler {
 
     // create scan instance with filter
     TScan scan = new TScan();
-    List<TColumn> columns = new ArrayList<TColumn>(1);
+    List<TColumn> columns = new ArrayList<>(1);
     TColumn column = new TColumn();
     column.setFamily(familyAname);
     column.setQualifier(qualifierAname);
@@ -792,7 +792,7 @@ public class TestThriftHBaseServiceHandler {
     ThriftHBaseServiceHandler handler = createHandler();
     byte[] rowName = "testPutTTL".getBytes();
     ByteBuffer table = wrap(tableAname);
-    List<TColumnValue> columnValues = new ArrayList<TColumnValue>(1);
+    List<TColumnValue> columnValues = new ArrayList<>(1);
 
     // Add some dummy data
     columnValues.add(
@@ -856,7 +856,7 @@ public class TestThriftHBaseServiceHandler {
     ByteBuffer table = wrap(tableAname);
 
     // insert data
-    List<TColumnValue> columnValues = new ArrayList<TColumnValue>(100);
+    List<TColumnValue> columnValues = new ArrayList<>(100);
     for (int i = 0; i < 100; i++) {
       String colNum = pad(i, (byte) 3);
       TColumnValue columnValue = new TColumnValue(wrap(familyAname),
@@ -868,7 +868,7 @@ public class TestThriftHBaseServiceHandler {
 
     // create scan instance
     TScan scan = new TScan();
-    List<TColumn> columns = new ArrayList<TColumn>(1);
+    List<TColumn> columns = new ArrayList<>(1);
     TColumn column = new TColumn();
     column.setFamily(familyAname);
     columns.add(column);
@@ -917,7 +917,7 @@ public class TestThriftHBaseServiceHandler {
     // insert data
     TColumnValue columnValue =
         new TColumnValue(wrap(familyAname), wrap(qualifierAname), wrap(valueAname));
-    List<TColumnValue> columnValues = new ArrayList<TColumnValue>(1);
+    List<TColumnValue> columnValues = new ArrayList<>(1);
     columnValues.add(columnValue);
     for (int i = 0; i < 20; i++) {
       TPut put =
@@ -927,7 +927,7 @@ public class TestThriftHBaseServiceHandler {
 
     // create scan instance
     TScan scan = new TScan();
-    List<TColumn> columns = new ArrayList<TColumn>(1);
+    List<TColumn> columns = new ArrayList<>(1);
     TColumn column = new TColumn();
     column.setFamily(familyAname);
     column.setQualifier(qualifierAname);
@@ -1002,7 +1002,7 @@ public class TestThriftHBaseServiceHandler {
     TGet get = new TGet(wrap(rowName));
     assertFalse(handler.exists(table, get));
 
-    List<TColumnValue> columnValues = new ArrayList<TColumnValue>(2);
+    List<TColumnValue> columnValues = new ArrayList<>(2);
     columnValues.add(new TColumnValue(wrap(familyAname), wrap(qualifierAname), wrap(valueAname)));
     columnValues.add(new TColumnValue(wrap(familyBname), wrap(qualifierBname),  wrap(valueBname)));
     TPut put = new TPut(wrap(rowName), columnValues);
@@ -1144,7 +1144,7 @@ public class TestThriftHBaseServiceHandler {
     byte[] rowName = "testAttribute".getBytes();
     byte[] attributeKey = "attribute1".getBytes();
     byte[] attributeValue = "value1".getBytes();
-    Map<ByteBuffer, ByteBuffer> attributes = new HashMap<ByteBuffer, ByteBuffer>();
+    Map<ByteBuffer, ByteBuffer> attributes = new HashMap<>();
     attributes.put(wrap(attributeKey), wrap(attributeValue));
 
     TGet tGet = new TGet(wrap(rowName));
@@ -1152,7 +1152,7 @@ public class TestThriftHBaseServiceHandler {
     Get get = getFromThrift(tGet);
     assertArrayEquals(get.getAttribute("attribute1"), attributeValue);
 
-    List<TColumnValue> columnValues = new ArrayList<TColumnValue>(1);
+    List<TColumnValue> columnValues = new ArrayList<>(1);
     columnValues.add(new TColumnValue(wrap(familyAname), wrap(qualifierAname), wrap(valueAname)));
     TPut tPut = new TPut(wrap(rowName) , columnValues);
     tPut.setAttributes(attributes);
@@ -1164,7 +1164,7 @@ public class TestThriftHBaseServiceHandler {
     Scan scan = scanFromThrift(tScan);
     assertArrayEquals(scan.getAttribute("attribute1"), attributeValue);
 
-    List<TColumnIncrement> incrementColumns = new ArrayList<TColumnIncrement>(1);
+    List<TColumnIncrement> incrementColumns = new ArrayList<>(1);
     incrementColumns.add(new TColumnIncrement(wrap(familyAname), wrap(qualifierAname)));
     TIncrement tIncrement = new TIncrement(wrap(rowName), incrementColumns);
     tIncrement.setAttributes(attributes);
@@ -1189,7 +1189,7 @@ public class TestThriftHBaseServiceHandler {
     byte[] rowName = "testMutateRow".getBytes();
     ByteBuffer table = wrap(tableAname);
 
-    List<TColumnValue> columnValuesA = new ArrayList<TColumnValue>(1);
+    List<TColumnValue> columnValuesA = new ArrayList<>(1);
     TColumnValue columnValueA = new TColumnValue(wrap(familyAname), wrap(qualifierAname),
         wrap(valueAname));
     columnValuesA.add(columnValueA);
@@ -1203,11 +1203,11 @@ public class TestThriftHBaseServiceHandler {
     assertArrayEquals(rowName, result.getRow());
     List<TColumnValue> returnedColumnValues = result.getColumnValues();
 
-    List<TColumnValue> expectedColumnValues = new ArrayList<TColumnValue>(1);
+    List<TColumnValue> expectedColumnValues = new ArrayList<>(1);
     expectedColumnValues.add(columnValueA);
     assertTColumnValuesEqual(expectedColumnValues, returnedColumnValues);
 
-    List<TColumnValue> columnValuesB = new ArrayList<TColumnValue>(1);
+    List<TColumnValue> columnValuesB = new ArrayList<>(1);
     TColumnValue columnValueB = new TColumnValue(wrap(familyAname), wrap(qualifierBname),
         wrap(valueBname));
     columnValuesB.add(columnValueB);
@@ -1215,13 +1215,13 @@ public class TestThriftHBaseServiceHandler {
     putB.setColumnValues(columnValuesB);
 
     TDelete delete = new TDelete(wrap(rowName));
-    List<TColumn> deleteColumns = new ArrayList<TColumn>(1);
+    List<TColumn> deleteColumns = new ArrayList<>(1);
     TColumn deleteColumn = new TColumn(wrap(familyAname));
     deleteColumn.setQualifier(qualifierAname);
     deleteColumns.add(deleteColumn);
     delete.setColumns(deleteColumns);
 
-    List<TMutation> mutations = new ArrayList<TMutation>(2);
+    List<TMutation> mutations = new ArrayList<>(2);
     TMutation mutationA = TMutation.put(putB);
     mutations.add(mutationA);
 
@@ -1235,7 +1235,7 @@ public class TestThriftHBaseServiceHandler {
     assertArrayEquals(rowName, result.getRow());
     returnedColumnValues = result.getColumnValues();
 
-    expectedColumnValues = new ArrayList<TColumnValue>(1);
+    expectedColumnValues = new ArrayList<>(1);
     expectedColumnValues.add(columnValueB);
     assertTColumnValuesEqual(expectedColumnValues, returnedColumnValues);
   }
@@ -1250,10 +1250,10 @@ public class TestThriftHBaseServiceHandler {
   @Test
   public void testDurability() throws Exception {
     byte[] rowName = "testDurability".getBytes();
-    List<TColumnValue> columnValues = new ArrayList<TColumnValue>(1);
+    List<TColumnValue> columnValues = new ArrayList<>(1);
     columnValues.add(new TColumnValue(wrap(familyAname), wrap(qualifierAname), wrap(valueAname)));
 
-    List<TColumnIncrement> incrementColumns = new ArrayList<TColumnIncrement>(1);
+    List<TColumnIncrement> incrementColumns = new ArrayList<>(1);
     incrementColumns.add(new TColumnIncrement(wrap(familyAname), wrap(qualifierAname)));
 
     TDelete tDelete = new TDelete(wrap(rowName));
@@ -1319,7 +1319,7 @@ public class TestThriftHBaseServiceHandler {
     ByteBuffer value = wrap(valueAname);
 
     // Create a mutation to write to 'B', our "mutate" of "checkAndMutate"
-    List<TColumnValue> columnValuesB = new ArrayList<TColumnValue>(1);
+    List<TColumnValue> columnValuesB = new ArrayList<>(1);
     TColumnValue columnValueB = new TColumnValue(family, wrap(qualifierBname), wrap(valueBname));
     columnValuesB.add(columnValueB);
     TPut putB = new TPut(row, columnValuesB);
@@ -1337,7 +1337,7 @@ public class TestThriftHBaseServiceHandler {
         handler.checkAndMutate(table, row, family, qualifier, TCompareOp.EQUAL, value,
             tRowMutations));
 
-    List<TColumnValue> columnValuesA = new ArrayList<TColumnValue>(1);
+    List<TColumnValue> columnValuesA = new ArrayList<>(1);
     TColumnValue columnValueA = new TColumnValue(family, qualifier, value);
     columnValuesA.add(columnValueA);
 
diff --git a/hbase-thrift/src/test/java/org/apache/hadoop/hbase/thrift2/TestThriftHBaseServiceHandlerWithLabels.java b/hbase-thrift/src/test/java/org/apache/hadoop/hbase/thrift2/TestThriftHBaseServiceHandlerWithLabels.java
index f729908..d672ab4 100644
--- a/hbase-thrift/src/test/java/org/apache/hadoop/hbase/thrift2/TestThriftHBaseServiceHandlerWithLabels.java
+++ b/hbase-thrift/src/test/java/org/apache/hadoop/hbase/thrift2/TestThriftHBaseServiceHandlerWithLabels.java
@@ -197,7 +197,7 @@ public void testScanWithVisibilityLabels() throws Exception {
   // insert data
   TColumnValue columnValue = new TColumnValue(wrap(familyAname),
       wrap(qualifierAname), wrap(valueAname));
-  List<TColumnValue> columnValues = new ArrayList<TColumnValue>(1);
+  List<TColumnValue> columnValues = new ArrayList<>(1);
   columnValues.add(columnValue);
   for (int i = 0; i < 10; i++) {
     TPut put = new TPut(wrap(("testScan" + i).getBytes()), columnValues);
@@ -212,7 +212,7 @@ public void testScanWithVisibilityLabels() throws Exception {
 
   // create scan instance
   TScan scan = new TScan();
-  List<TColumn> columns = new ArrayList<TColumn>(1);
+  List<TColumn> columns = new ArrayList<>(1);
   TColumn column = new TColumn();
   column.setFamily(familyAname);
   column.setQualifier(qualifierAname);
@@ -222,7 +222,7 @@ public void testScanWithVisibilityLabels() throws Exception {
   scan.setStopRow("testScan\uffff".getBytes());
 
   TAuthorization tauth = new TAuthorization();
-  List<String> labels = new ArrayList<String>(2);
+  List<String> labels = new ArrayList<>(2);
   labels.add(SECRET);
   labels.add(PRIVATE);
   tauth.setLabels(labels);
@@ -265,7 +265,7 @@ public void testGetScannerResultsWithAuthorizations() throws Exception {
   // insert data
   TColumnValue columnValue = new TColumnValue(wrap(familyAname),
       wrap(qualifierAname), wrap(valueAname));
-  List<TColumnValue> columnValues = new ArrayList<TColumnValue>(1);
+  List<TColumnValue> columnValues = new ArrayList<>(1);
   columnValues.add(columnValue);
   for (int i = 0; i < 20; i++) {
     TPut put = new TPut(
@@ -282,7 +282,7 @@ public void testGetScannerResultsWithAuthorizations() throws Exception {
 
   // create scan instance
   TScan scan = new TScan();
-  List<TColumn> columns = new ArrayList<TColumn>(1);
+  List<TColumn> columns = new ArrayList<>(1);
   TColumn column = new TColumn();
   column.setFamily(familyAname);
   column.setQualifier(qualifierAname);
@@ -293,7 +293,7 @@ public void testGetScannerResultsWithAuthorizations() throws Exception {
   // get 5 rows and check the returned results
   scan.setStopRow("testGetScannerResults05".getBytes());
   TAuthorization tauth = new TAuthorization();
-  List<String> labels = new ArrayList<String>(2);
+  List<String> labels = new ArrayList<>(2);
   labels.add(SECRET);
   labels.add(PRIVATE);
   tauth.setLabels(labels);
@@ -321,7 +321,7 @@ public void testGetsWithLabels() throws Exception {
   byte[] rowName = "testPutGet".getBytes();
   ByteBuffer table = wrap(tableAname);
 
-  List<TColumnValue> columnValues = new ArrayList<TColumnValue>(2);
+  List<TColumnValue> columnValues = new ArrayList<>(2);
   columnValues.add(new TColumnValue(wrap(familyAname), wrap(qualifierAname),
       wrap(valueAname)));
   columnValues.add(new TColumnValue(wrap(familyBname), wrap(qualifierBname),
@@ -334,7 +334,7 @@ public void testGetsWithLabels() throws Exception {
   handler.put(table, put);
   TGet get = new TGet(wrap(rowName));
   TAuthorization tauth = new TAuthorization();
-  List<String> labels = new ArrayList<String>(2);
+  List<String> labels = new ArrayList<>(2);
   labels.add(SECRET);
   labels.add(PRIVATE);
   tauth.setLabels(labels);
@@ -351,7 +351,7 @@ public void testIncrementWithTags() throws Exception {
   byte[] rowName = "testIncrementWithTags".getBytes();
   ByteBuffer table = wrap(tableAname);
 
-  List<TColumnValue> columnValues = new ArrayList<TColumnValue>(1);
+  List<TColumnValue> columnValues = new ArrayList<>(1);
   columnValues.add(new TColumnValue(wrap(familyAname), wrap(qualifierAname),
       wrap(Bytes.toBytes(1L))));
   TPut put = new TPut(wrap(rowName), columnValues);
@@ -359,7 +359,7 @@ public void testIncrementWithTags() throws Exception {
   put.setCellVisibility(new TCellVisibility().setExpression(PRIVATE));
   handler.put(table, put);
 
-  List<TColumnIncrement> incrementColumns = new ArrayList<TColumnIncrement>(1);
+  List<TColumnIncrement> incrementColumns = new ArrayList<>(1);
   incrementColumns.add(new TColumnIncrement(wrap(familyAname),
       wrap(qualifierAname)));
   TIncrement increment = new TIncrement(wrap(rowName), incrementColumns);
@@ -368,7 +368,7 @@ public void testIncrementWithTags() throws Exception {
 
   TGet get = new TGet(wrap(rowName));
   TAuthorization tauth = new TAuthorization();
-  List<String> labels = new ArrayList<String>(1);
+  List<String> labels = new ArrayList<>(1);
   labels.add(SECRET);
   tauth.setLabels(labels);
   get.setAuthorizations(tauth);
@@ -386,7 +386,7 @@ public void testIncrementWithTagsWithNotMatchLabels() throws Exception {
   byte[] rowName = "testIncrementWithTagsWithNotMatchLabels".getBytes();
   ByteBuffer table = wrap(tableAname);
 
-  List<TColumnValue> columnValues = new ArrayList<TColumnValue>(1);
+  List<TColumnValue> columnValues = new ArrayList<>(1);
   columnValues.add(new TColumnValue(wrap(familyAname), wrap(qualifierAname),
       wrap(Bytes.toBytes(1L))));
   TPut put = new TPut(wrap(rowName), columnValues);
@@ -394,7 +394,7 @@ public void testIncrementWithTagsWithNotMatchLabels() throws Exception {
   put.setCellVisibility(new TCellVisibility().setExpression(PRIVATE));
   handler.put(table, put);
 
-  List<TColumnIncrement> incrementColumns = new ArrayList<TColumnIncrement>(1);
+  List<TColumnIncrement> incrementColumns = new ArrayList<>(1);
   incrementColumns.add(new TColumnIncrement(wrap(familyAname),
       wrap(qualifierAname)));
   TIncrement increment = new TIncrement(wrap(rowName), incrementColumns);
@@ -403,7 +403,7 @@ public void testIncrementWithTagsWithNotMatchLabels() throws Exception {
 
   TGet get = new TGet(wrap(rowName));
   TAuthorization tauth = new TAuthorization();
-  List<String> labels = new ArrayList<String>(1);
+  List<String> labels = new ArrayList<>(1);
   labels.add(PUBLIC);
   tauth.setLabels(labels);
   get.setAuthorizations(tauth);
@@ -418,7 +418,7 @@ public void testAppend() throws Exception {
   ByteBuffer table = wrap(tableAname);
   byte[] v1 = Bytes.toBytes(1L);
   byte[] v2 = Bytes.toBytes(5L);
-  List<TColumnValue> columnValues = new ArrayList<TColumnValue>(1);
+  List<TColumnValue> columnValues = new ArrayList<>(1);
   columnValues.add(new TColumnValue(wrap(familyAname), wrap(qualifierAname),
       wrap(Bytes.toBytes(1L))));
   TPut put = new TPut(wrap(rowName), columnValues);
@@ -426,7 +426,7 @@ public void testAppend() throws Exception {
   put.setCellVisibility(new TCellVisibility().setExpression(PRIVATE));
   handler.put(table, put);
 
-  List<TColumnValue> appendColumns = new ArrayList<TColumnValue>(1);
+  List<TColumnValue> appendColumns = new ArrayList<>(1);
   appendColumns.add(new TColumnValue(wrap(familyAname), wrap(qualifierAname),
       wrap(v2)));
   TAppend append = new TAppend(wrap(rowName), appendColumns);
@@ -435,7 +435,7 @@ public void testAppend() throws Exception {
 
   TGet get = new TGet(wrap(rowName));
   TAuthorization tauth = new TAuthorization();
-  List<String> labels = new ArrayList<String>(1);
+  List<String> labels = new ArrayList<>(1);
   labels.add(SECRET);
   tauth.setLabels(labels);
   get.setAuthorizations(tauth);
-- 
2.8.4 (Apple Git-73)
