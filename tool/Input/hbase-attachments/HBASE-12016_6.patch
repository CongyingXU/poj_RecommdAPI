diff --git a/hbase-client/src/main/java/org/apache/hadoop/hbase/HTableDescriptor.java b/hbase-client/src/main/java/org/apache/hadoop/hbase/HTableDescriptor.java
index 0e9e25c..5d7f4ce 100644
--- a/hbase-client/src/main/java/org/apache/hadoop/hbase/HTableDescriptor.java
+++ b/hbase-client/src/main/java/org/apache/hadoop/hbase/HTableDescriptor.java
@@ -38,6 +38,7 @@ import org.apache.commons.logging.Log;
 import org.apache.commons.logging.LogFactory;
 import org.apache.hadoop.classification.InterfaceAudience;
 import org.apache.hadoop.classification.InterfaceStability;
+import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.fs.Path;
 import org.apache.hadoop.hbase.client.Durability;
 import org.apache.hadoop.hbase.exceptions.DeserializationException;
@@ -1243,34 +1244,6 @@ public class HTableDescriptor implements Comparable<HTableDescriptor> {
               new Path(name.getNamespaceAsString(), new Path(name.getQualifierAsString()))));
   }
 
-  /** Table descriptor for <code>hbase:meta</code> catalog table */
-  public static final HTableDescriptor META_TABLEDESC = new HTableDescriptor(
-      TableName.META_TABLE_NAME,
-      new HColumnDescriptor[] {
-          new HColumnDescriptor(HConstants.CATALOG_FAMILY)
-              // Ten is arbitrary number.  Keep versions to help debugging.
-              .setMaxVersions(10)
-              .setInMemory(true)
-              .setBlocksize(8 * 1024)
-              .setScope(HConstants.REPLICATION_SCOPE_LOCAL)
-              // Disable blooms for meta.  Needs work.  Seems to mess w/ getClosestOrBefore.
-              .setBloomFilterType(BloomType.NONE)
-              // Enable cache of data blocks in L1 if more than one caching tier deployed:
-              // e.g. if using CombinedBlockCache (BucketCache).
-              .setCacheDataInL1(true)
-      });
-
-  static {
-    try {
-      META_TABLEDESC.addCoprocessor(
-          "org.apache.hadoop.hbase.coprocessor.MultiRowMutationEndpoint",
-          null, Coprocessor.PRIORITY_SYSTEM, null);
-    } catch (IOException ex) {
-      //LOG.warn("exception in loading coprocessor for the hbase:meta table");
-      throw new RuntimeException(ex);
-    }
-  }
-
   public final static String NAMESPACE_FAMILY_INFO = "info";
   public final static byte[] NAMESPACE_FAMILY_INFO_BYTES = Bytes.toBytes(NAMESPACE_FAMILY_INFO);
   public final static byte[] NAMESPACE_COL_DESC_BYTES = Bytes.toBytes("d");
diff --git a/hbase-client/src/main/java/org/apache/hadoop/hbase/MetaTableAccessor.java b/hbase-client/src/main/java/org/apache/hadoop/hbase/MetaTableAccessor.java
index af9b587..04979b5 100644
--- a/hbase-client/src/main/java/org/apache/hadoop/hbase/MetaTableAccessor.java
+++ b/hbase-client/src/main/java/org/apache/hadoop/hbase/MetaTableAccessor.java
@@ -328,7 +328,7 @@ public class MetaTableAccessor {
   public static boolean tableExists(HConnection hConnection,
       final TableName tableName)
   throws IOException {
-    if (tableName.equals(HTableDescriptor.META_TABLEDESC.getTableName())) {
+    if (tableName.equals(TableName.META_TABLE_NAME)) {
       // Catalog tables always exist.
       return true;
     }
diff --git a/hbase-client/src/main/java/org/apache/hadoop/hbase/client/ConnectionFactory.java b/hbase-client/src/main/java/org/apache/hadoop/hbase/client/ConnectionFactory.java
index 0c054ed..6349005 100644
--- a/hbase-client/src/main/java/org/apache/hadoop/hbase/client/ConnectionFactory.java
+++ b/hbase-client/src/main/java/org/apache/hadoop/hbase/client/ConnectionFactory.java
@@ -30,13 +30,12 @@ import org.apache.hadoop.hbase.security.UserProvider;
 
 
 /**
- * A non-instantiable class that manages creation of {@link Connection}s.
+ * A non-instantiable class that manages creation of {@link Connection} s.
  * Managing the lifecycle of the {@link Connection}s to the cluster is the responsibility of
  * the caller.
  * From this {@link Connection} {@link Table} implementations are retrieved
  * with {@link Connection#getTable(TableName)}. Example:
  * <pre>
- * {@code
  * Connection connection = ConnectionFactory.createConnection(config);
  * Table table = connection.getTable(TableName.valueOf("table1"));
  * try {
@@ -46,7 +45,7 @@ import org.apache.hadoop.hbase.security.UserProvider;
  *   connection.close();
  * }
  * </pre>
- * 
+ *
  * Similarly, {@link Connection} also returns {@link RegionLocator} implementations.
  *
  * This class replaces {@link HConnectionManager}, which is now deprecated.
diff --git a/hbase-client/src/main/java/org/apache/hadoop/hbase/client/ConnectionManager.java b/hbase-client/src/main/java/org/apache/hadoop/hbase/client/ConnectionManager.java
index bbf180e..f05b3aa 100644
--- a/hbase-client/src/main/java/org/apache/hadoop/hbase/client/ConnectionManager.java
+++ b/hbase-client/src/main/java/org/apache/hadoop/hbase/client/ConnectionManager.java
@@ -2482,9 +2482,6 @@ class ConnectionManager {
     public HTableDescriptor getHTableDescriptor(final TableName tableName)
     throws IOException {
       if (tableName == null) return null;
-      if (tableName.equals(TableName.META_TABLE_NAME)) {
-        return HTableDescriptor.META_TABLEDESC;
-      }
       MasterKeepAliveConnection master = getKeepAliveMasterService();
       GetTableDescriptorsResponse htds;
       try {
diff --git a/hbase-common/src/main/java/org/apache/hadoop/hbase/HConstants.java b/hbase-common/src/main/java/org/apache/hadoop/hbase/HConstants.java
index b08ff46..dae09e1 100644
--- a/hbase-common/src/main/java/org/apache/hadoop/hbase/HConstants.java
+++ b/hbase-common/src/main/java/org/apache/hadoop/hbase/HConstants.java
@@ -694,6 +694,26 @@ public final class HConstants {
   public static int DEFAULT_HBASE_META_SCANNER_CACHING = 100;
 
   /**
+   * Parameter name for number of versions, kept by meta table.
+   */
+  public static String HBASE_META_VERSIONS = "hbase.meta.versions";
+
+  /**
+   * Default value of {@link #HBASE_META_VERSIONS}.
+   */
+  public static int DEFAULT_HBASE_META_VERSIONS = 3;
+
+  /**
+   * Parameter name for number of versions, kept by meta table.
+   */
+  public static String HBASE_META_BLOCK_SIZE = "hbase.meta.blocksize";
+
+  /**
+   * Default value of {@link #HBASE_META_BLOCK_SIZE}.
+   */
+  public static int DEFAULT_HBASE_META_BLOCK_SIZE = 8 * 1024;
+
+  /**
    * Parameter name for unique identifier for this {@link org.apache.hadoop.conf.Configuration}
    * instance. If there are two or more {@link org.apache.hadoop.conf.Configuration} instances that,
    * for all intents and purposes, are the same except for their instance ids, then they will not be
diff --git a/hbase-common/src/main/java/org/apache/hadoop/hbase/security/User.java b/hbase-common/src/main/java/org/apache/hadoop/hbase/security/User.java
index fd12e47..804883c 100644
--- a/hbase-common/src/main/java/org/apache/hadoop/hbase/security/User.java
+++ b/hbase-common/src/main/java/org/apache/hadoop/hbase/security/User.java
@@ -167,7 +167,7 @@ public abstract class User {
   /**
    * Executes the given action as the login user
    * @param action
-   * @return
+   * @return result of action execution
    * @throws IOException
    * @throws InterruptedException
    */
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/master/MasterFileSystem.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/master/MasterFileSystem.java
index 49225c4..ecb2907 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/master/MasterFileSystem.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/master/MasterFileSystem.java
@@ -457,8 +457,11 @@ public class MasterFileSystem {
 
     // Create tableinfo-s for hbase:meta if not already there.
     // assume, created table descriptor is for enabling table
-    new FSTableDescriptors(fs, rd).createTableDescriptor(
-        new TableDescriptor(HTableDescriptor.META_TABLEDESC, TableState.State.ENABLING));
+    // meta table is a system table, so descriptors are predefined,
+    // we should get them from registry.
+    FSTableDescriptors fsd = new FSTableDescriptors(c, fs, rd);
+    fsd.createTableDescriptor(
+        new TableDescriptor(fsd.get(TableName.META_TABLE_NAME), TableState.State.ENABLING));
 
     return rd;
   }
@@ -498,10 +501,10 @@ public class MasterFileSystem {
       // not make it in first place.  Turn off block caching for bootstrap.
       // Enable after.
       HRegionInfo metaHRI = new HRegionInfo(HRegionInfo.FIRST_META_REGIONINFO);
-      setInfoFamilyCachingForMeta(false);
-      HRegion meta = HRegion.createHRegion(metaHRI, rd, c,
-          HTableDescriptor.META_TABLEDESC);
-      setInfoFamilyCachingForMeta(true);
+      HTableDescriptor metaDescriptor = new FSTableDescriptors(c).get(TableName.META_TABLE_NAME);
+      setInfoFamilyCachingForMeta(metaDescriptor, false);
+      HRegion meta = HRegion.createHRegion(metaHRI, rd, c, metaDescriptor);
+      setInfoFamilyCachingForMeta(metaDescriptor, true);
       HRegion.closeHRegion(meta);
     } catch (IOException e) {
         e = e instanceof RemoteException ?
@@ -514,9 +517,8 @@ public class MasterFileSystem {
   /**
    * Enable in memory caching for hbase:meta
    */
-  public static void setInfoFamilyCachingForMeta(final boolean b) {
-    for (HColumnDescriptor hcd:
-        HTableDescriptor.META_TABLEDESC.getColumnFamilies()) {
+  public static void setInfoFamilyCachingForMeta(HTableDescriptor metaDescriptor, final boolean b) {
+    for (HColumnDescriptor hcd: metaDescriptor.getColumnFamilies()) {
       if (Bytes.equals(hcd.getName(), HConstants.CATALOG_FAMILY)) {
         hcd.setBlockCacheEnabled(b);
         hcd.setInMemory(b);
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/migration/NamespaceUpgrade.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/migration/NamespaceUpgrade.java
index 1649c4e..e7ea4b9 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/migration/NamespaceUpgrade.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/migration/NamespaceUpgrade.java
@@ -385,8 +385,9 @@ public class NamespaceUpgrade implements Tool {
     HLog metaHLog = HLogFactory.createMetaHLog(fs, rootDir,
         metaLogName, conf, null,
         fakeServer.toString());
+    FSTableDescriptors fst = new FSTableDescriptors(conf);
     HRegion meta = HRegion.openHRegion(rootDir, HRegionInfo.FIRST_META_REGIONINFO,
-        HTableDescriptor.META_TABLEDESC, metaHLog, conf);
+        fst.get(TableName.META_TABLE_NAME), metaHLog, conf);
     HRegion region = null;
     try {
       for(Path regionDir : FSUtils.getRegionDirs(fs, oldTablePath)) {
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java
index 399f13d..1a38526 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java
@@ -139,6 +139,7 @@ import org.apache.hadoop.hbase.util.ClassSize;
 import org.apache.hadoop.hbase.util.CompressionTest;
 import org.apache.hadoop.hbase.util.Counter;
 import org.apache.hadoop.hbase.util.EnvironmentEdgeManager;
+import org.apache.hadoop.hbase.util.FSTableDescriptors;
 import org.apache.hadoop.hbase.util.FSUtils;
 import org.apache.hadoop.hbase.util.HashedBytes;
 import org.apache.hadoop.hbase.util.Pair;
@@ -5807,10 +5808,12 @@ public class HRegion implements HeapSize { // , Writable{
       final boolean majorCompact)
   throws IOException {
     HRegion region;
+    FSTableDescriptors fst = new FSTableDescriptors(c);
     // Currently expects tables have one region only.
     if (FSUtils.getTableName(p).equals(TableName.META_TABLE_NAME)) {
       region = HRegion.newHRegion(p, log, fs, c,
-        HRegionInfo.FIRST_META_REGIONINFO, HTableDescriptor.META_TABLEDESC, null);
+        HRegionInfo.FIRST_META_REGIONINFO,
+          fst.get(TableName.META_TABLE_NAME), null);
     } else {
       throw new IOException("Not a known catalog table: " + p.toString());
     }
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java
index b60befd..cd47b7b 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java
@@ -492,7 +492,7 @@ public class HRegionServer extends HasThread implements
     boolean useHBaseChecksum = conf.getBoolean(HConstants.HBASE_CHECKSUM_VERIFICATION, true);
     this.fs = new HFileSystem(this.conf, useHBaseChecksum);
     this.rootDir = FSUtils.getRootDir(this.conf);
-    this.tableDescriptors = new FSTableDescriptors(
+    this.tableDescriptors = new FSTableDescriptors(this.conf,
       this.fs, this.rootDir, !canUpdateTableDescriptor());
 
     service = new ExecutorService(getServerName().toShortString());
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/wal/FSHLog.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/wal/FSHLog.java
index f2b12b2..d9dda8b 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/wal/FSHLog.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/wal/FSHLog.java
@@ -69,6 +69,7 @@ import org.apache.hadoop.hbase.util.Bytes;
 import org.apache.hadoop.hbase.util.ClassSize;
 import org.apache.hadoop.hbase.util.DrainBarrier;
 import org.apache.hadoop.hbase.util.EnvironmentEdgeManager;
+import org.apache.hadoop.hbase.util.FSTableDescriptors;
 import org.apache.hadoop.hbase.util.FSUtils;
 import org.apache.hadoop.hbase.util.HasThread;
 import org.apache.hadoop.hbase.util.Threads;
@@ -2036,9 +2037,11 @@ class FSHLog implements HLog, Syncable {
       WALEdit walEdit = new WALEdit();
       walEdit.add(new KeyValue(Bytes.toBytes("row"), Bytes.toBytes("family"),
         Bytes.toBytes("qualifier"), -1, new byte [1000]));
+      FSTableDescriptors fst = new FSTableDescriptors(conf);
       for (AtomicLong i = new AtomicLong(0); i.get() < count; i.incrementAndGet()) {
-        wal.append(HRegionInfo.FIRST_META_REGIONINFO, TableName.META_TABLE_NAME, walEdit, start,
-          HTableDescriptor.META_TABLEDESC, i);
+        wal.append(HRegionInfo.FIRST_META_REGIONINFO,
+            TableName.META_TABLE_NAME, walEdit, start,
+          fst.get(TableName.META_TABLE_NAME), i);
         wal.sync();
       }
       wal.close();
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/snapshot/SnapshotManifest.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/snapshot/SnapshotManifest.java
index d6d4f71..bb9f3b7 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/snapshot/SnapshotManifest.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/snapshot/SnapshotManifest.java
@@ -355,7 +355,7 @@ public class SnapshotManifest {
       Path rootDir = FSUtils.getRootDir(conf);
       LOG.info("Using old Snapshot Format");
       // write a copy of descriptor to the snapshot directory
-      new FSTableDescriptors(fs, rootDir)
+      new FSTableDescriptors(conf, fs, rootDir)
         .createTableDescriptorForTableDirectory(workingDir, new TableDescriptor(
             htd, TableState.State.ENABLED), false);
     } else {
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/util/FSTableDescriptors.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/util/FSTableDescriptors.java
index f05575c..8233008 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/util/FSTableDescriptors.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/util/FSTableDescriptors.java
@@ -28,6 +28,8 @@ import java.util.concurrent.ConcurrentHashMap;
 import java.util.regex.Matcher;
 import java.util.regex.Pattern;
 
+import com.google.common.annotations.VisibleForTesting;
+import com.google.common.primitives.Ints;
 import org.apache.commons.lang.NotImplementedException;
 import org.apache.commons.logging.Log;
 import org.apache.commons.logging.LogFactory;
@@ -39,19 +41,18 @@ import org.apache.hadoop.fs.FileStatus;
 import org.apache.hadoop.fs.FileSystem;
 import org.apache.hadoop.fs.Path;
 import org.apache.hadoop.fs.PathFilter;
-import org.apache.hadoop.hbase.TableDescriptor;
-import org.apache.hadoop.hbase.TableName;
-import org.apache.hadoop.hbase.client.TableState;
-import org.apache.hadoop.hbase.exceptions.DeserializationException;
+import org.apache.hadoop.hbase.Coprocessor;
+import org.apache.hadoop.hbase.HColumnDescriptor;
 import org.apache.hadoop.hbase.HConstants;
 import org.apache.hadoop.hbase.HTableDescriptor;
+import org.apache.hadoop.hbase.TableDescriptor;
 import org.apache.hadoop.hbase.TableDescriptors;
 import org.apache.hadoop.hbase.TableInfoMissingException;
+import org.apache.hadoop.hbase.TableName;
+import org.apache.hadoop.hbase.client.TableState;
+import org.apache.hadoop.hbase.exceptions.DeserializationException;
 import org.apache.hadoop.hbase.protobuf.ProtobufUtil;
-
-import com.google.common.annotations.VisibleForTesting;
-import com.google.common.primitives.Ints;
-
+import org.apache.hadoop.hbase.regionserver.BloomType;
 
 /**
  * Implementation of {@link TableDescriptors} that reads descriptors from the
@@ -92,6 +93,11 @@ public class FSTableDescriptors implements TableDescriptors {
     new ConcurrentHashMap<TableName, TableDescriptorAndModtime>();
 
   /**
+   * Table descriptor for <code>hbase:meta</code> catalog table
+   */
+  private final HTableDescriptor metaTableDescritor;
+
+  /**
    * Data structure to hold modification time and table descriptor.
    */
   private static class TableDescriptorAndModtime {
@@ -126,23 +132,44 @@ public class FSTableDescriptors implements TableDescriptors {
    * This instance can do write operations (is not read only).
    */
   public FSTableDescriptors(final Configuration conf) throws IOException {
-    this(FSUtils.getCurrentFileSystem(conf), FSUtils.getRootDir(conf));
+    this(conf, FSUtils.getCurrentFileSystem(conf), FSUtils.getRootDir(conf));
   }
   
-  public FSTableDescriptors(final FileSystem fs, final Path rootdir) {
-    this(fs, rootdir, false);
+  public FSTableDescriptors(final Configuration conf, final FileSystem fs, final Path rootdir)
+      throws IOException {
+    this(conf, fs, rootdir, false);
   }
 
   /**
    * @param fsreadonly True if we are read-only when it comes to filesystem
    * operations; i.e. on remove, we do not do delete in fs.
    */
-  public FSTableDescriptors(final FileSystem fs,
-      final Path rootdir, final boolean fsreadonly) {
+  public FSTableDescriptors(final Configuration conf, final FileSystem fs,
+      final Path rootdir, final boolean fsreadonly) throws IOException {
     super();
     this.fs = fs;
     this.rootdir = rootdir;
     this.fsreadonly = fsreadonly;
+
+    this.metaTableDescritor = new HTableDescriptor(
+        TableName.META_TABLE_NAME,
+        new HColumnDescriptor[] {
+            new HColumnDescriptor(HConstants.CATALOG_FAMILY)
+                .setMaxVersions(conf.getInt(HConstants.HBASE_META_VERSIONS,
+                    HConstants.DEFAULT_HBASE_META_VERSIONS))
+                .setInMemory(true)
+                .setBlocksize(conf.getInt(HConstants.HBASE_META_BLOCK_SIZE,
+                    HConstants.DEFAULT_HBASE_META_BLOCK_SIZE))
+                .setScope(HConstants.REPLICATION_SCOPE_LOCAL)
+                    // Disable blooms for meta.  Needs work.  Seems to mess w/ getClosestOrBefore.
+                .setBloomFilterType(BloomType.NONE)
+                    // Enable cache of data blocks in L1 if more than one caching tier deployed:
+                    // e.g. if using CombinedBlockCache (BucketCache).
+                .setCacheDataInL1(true)
+        }){};
+    this.metaTableDescritor.addCoprocessor(
+        "org.apache.hadoop.hbase.coprocessor.MultiRowMutationEndpoint",
+        null, Coprocessor.PRIORITY_SYSTEM, null);
   }
 
   /**
@@ -156,9 +183,9 @@ public class FSTableDescriptors implements TableDescriptors {
   public TableDescriptor getDescriptor(final TableName tablename)
   throws IOException {
     invocations++;
-    if (HTableDescriptor.META_TABLEDESC.getTableName().equals(tablename)) {
+    if (TableName.META_TABLE_NAME.equals(tablename)) {
       cachehits++;
-      return new TableDescriptor(HTableDescriptor.META_TABLEDESC, TableState.State.ENABLED);
+      return new TableDescriptor(metaTableDescritor, TableState.State.ENABLED);
     }
     // hbase:meta is already handled. If some one tries to get the descriptor for
     // .logs, .oldlogs or .corrupt throw an exception.
@@ -202,9 +229,9 @@ public class FSTableDescriptors implements TableDescriptors {
    */
   @Override
   public HTableDescriptor get(TableName tableName) throws IOException {
-    if (HTableDescriptor.META_TABLEDESC.getTableName().equals(tableName)) {
+    if (TableName.META_TABLE_NAME.equals(tableName)) {
       cachehits++;
-      return HTableDescriptor.META_TABLEDESC;
+      return metaTableDescritor;
     }
     TableDescriptor descriptor = getDescriptor(tableName);
     return descriptor == null ? null : descriptor.getHTableDescriptor();
@@ -826,6 +853,6 @@ public class FSTableDescriptors implements TableDescriptors {
     Path p = writeTableDescriptor(fs, htd, tableDir, status);
     return p != null;
   }
-  
+
 }
 
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/util/HBaseFsck.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/util/HBaseFsck.java
index 4ca5c72..d0b8834 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/util/HBaseFsck.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/util/HBaseFsck.java
@@ -1100,10 +1100,10 @@ public class HBaseFsck extends Configured {
       Path rootdir = FSUtils.getRootDir(getConf());
     Configuration c = getConf();
     HRegionInfo metaHRI = new HRegionInfo(HRegionInfo.FIRST_META_REGIONINFO);
-    MasterFileSystem.setInfoFamilyCachingForMeta(false);
-    HRegion meta = HRegion.createHRegion(metaHRI, rootdir, c,
-        HTableDescriptor.META_TABLEDESC);
-    MasterFileSystem.setInfoFamilyCachingForMeta(true);
+    HTableDescriptor metaDescriptor = new FSTableDescriptors(c).get(TableName.META_TABLE_NAME);
+    MasterFileSystem.setInfoFamilyCachingForMeta(metaDescriptor, false);
+    HRegion meta = HRegion.createHRegion(metaHRI, rootdir, c, metaDescriptor);
+    MasterFileSystem.setInfoFamilyCachingForMeta(metaDescriptor, true);
     return meta;
   }
 
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/util/MetaUtils.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/util/MetaUtils.java
index 57dcf54..96e8448 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/util/MetaUtils.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/util/MetaUtils.java
@@ -33,6 +33,7 @@ import org.apache.hadoop.hbase.HBaseConfiguration;
 import org.apache.hadoop.hbase.HConstants;
 import org.apache.hadoop.hbase.HRegionInfo;
 import org.apache.hadoop.hbase.HTableDescriptor;
+import org.apache.hadoop.hbase.TableName;
 import org.apache.hadoop.hbase.regionserver.HRegion;
 import org.apache.hadoop.hbase.regionserver.wal.HLog;
 import org.apache.hadoop.hbase.regionserver.wal.HLogFactory;
@@ -48,6 +49,7 @@ import org.apache.hadoop.hbase.regionserver.wal.HLogFactory;
 public class MetaUtils {
   private static final Log LOG = LogFactory.getLog(MetaUtils.class);
   private final Configuration conf;
+  private final FSTableDescriptors descriptors;
   private FileSystem fs;
   private HLog log;
   private HRegion metaRegion;
@@ -69,6 +71,7 @@ public class MetaUtils {
     this.conf = conf;
     conf.setInt(HConstants.HBASE_CLIENT_RETRIES_NUMBER, 1);
     this.metaRegion = null;
+    this.descriptors = new FSTableDescriptors(conf);
     initialize();
   }
 
@@ -147,7 +150,7 @@ public class MetaUtils {
       return this.metaRegion;
     }
     this.metaRegion = HRegion.openHRegion(HRegionInfo.FIRST_META_REGIONINFO,
-      HTableDescriptor.META_TABLEDESC, getLog(),
+      descriptors.get(TableName.META_TABLE_NAME), getLog(),
       this.conf);
     this.metaRegion.compactStores();
     return this.metaRegion;
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/HBaseTestCase.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/HBaseTestCase.java
index db89a45..18bc731 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/HBaseTestCase.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/HBaseTestCase.java
@@ -43,6 +43,7 @@ import org.apache.hadoop.hbase.client.Table;
 import org.apache.hadoop.hbase.regionserver.HRegion;
 import org.apache.hadoop.hbase.regionserver.InternalScanner;
 import org.apache.hadoop.hbase.util.Bytes;
+import org.apache.hadoop.hbase.util.FSTableDescriptors;
 import org.apache.hadoop.hbase.util.FSUtils;
 import org.apache.hadoop.hdfs.MiniDFSCluster;
 
@@ -74,6 +75,14 @@ public abstract class HBaseTestCase extends TestCase {
   protected final HBaseTestingUtility testUtil = new HBaseTestingUtility();
 
   public volatile Configuration conf = HBaseConfiguration.create();
+  public final FSTableDescriptors fsTableDescriptors;
+  {
+    try {
+      fsTableDescriptors = new FSTableDescriptors(conf);
+    } catch (IOException e) {
+      throw new RuntimeException("Failed to init descriptors", e);
+    }
+  }
 
   /** constructor */
   public HBaseTestCase() {
@@ -630,8 +639,9 @@ public abstract class HBaseTestCase extends TestCase {
    * @throws IOException
    */
   protected void createMetaRegion() throws IOException {
+    FSTableDescriptors fsTableDescriptors = new FSTableDescriptors(conf);
     meta = HRegion.createHRegion(HRegionInfo.FIRST_META_REGIONINFO, testDir,
-        conf, HTableDescriptor.META_TABLEDESC);
+        conf, fsTableDescriptors.get(TableName.META_TABLE_NAME) );
   }
 
   protected void closeRootAndMeta() throws IOException {
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/HBaseTestingUtility.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/HBaseTestingUtility.java
index e8d8762..5aed779 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/HBaseTestingUtility.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/HBaseTestingUtility.java
@@ -92,6 +92,7 @@ import org.apache.hadoop.hbase.regionserver.wal.HLog;
 import org.apache.hadoop.hbase.security.User;
 import org.apache.hadoop.hbase.tool.Canary;
 import org.apache.hadoop.hbase.util.Bytes;
+import org.apache.hadoop.hbase.util.FSTableDescriptors;
 import org.apache.hadoop.hbase.util.FSUtils;
 import org.apache.hadoop.hbase.util.JVMClusterUtil;
 import org.apache.hadoop.hbase.util.JVMClusterUtil.MasterThread;
@@ -201,6 +202,8 @@ public class HBaseTestingUtility extends HBaseCommonTestingUtility {
       Compression.Algorithm.NONE, Compression.Algorithm.GZ
     };
 
+  public final FSTableDescriptors fsTableDescriptors;
+
   /**
    * Create all combinations of Bloom filters and compression algorithms for
    * testing.
@@ -240,6 +243,11 @@ public class HBaseTestingUtility extends HBaseCommonTestingUtility {
 
     // a hbase checksum verification failure will cause unit tests to fail
     ChecksumUtil.generateExceptionForChecksumFailureForTest(true);
+    try {
+      fsTableDescriptors = new FSTableDescriptors(conf);
+    } catch (IOException e) {
+      throw new RuntimeException("Failed to initialize FSTableDescriptors", e);
+    }
   }
 
   /**
@@ -367,6 +375,17 @@ public class HBaseTestingUtility extends HBaseCommonTestingUtility {
   }
 
   /**
+   * @return META table descriptor
+   */
+  public HTableDescriptor getMetaTableDescriptor() {
+    try {
+      return fsTableDescriptors.get(TableName.META_TABLE_NAME);
+    } catch (IOException e) {
+      throw new RuntimeException("Unable to create META table descriptor", e);
+    }
+  }
+
+  /**
    * @return Where the DFS cluster will write data on the local subsystem.
    * Creates it if it does not exist already.  A subdir of {@link #getBaseTestDir()}
    * @see #getTestFileSystem()
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/TestFSTableDescriptorForceCreation.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/TestFSTableDescriptorForceCreation.java
index fe9de6a..07b9cbd 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/TestFSTableDescriptorForceCreation.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/TestFSTableDescriptorForceCreation.java
@@ -41,7 +41,7 @@ public class TestFSTableDescriptorForceCreation {
     final String name = "newTable2";
     FileSystem fs = FileSystem.get(UTIL.getConfiguration());
     Path rootdir = new Path(UTIL.getDataTestDir(), name);
-    FSTableDescriptors fstd = new FSTableDescriptors(fs, rootdir);
+    FSTableDescriptors fstd = new FSTableDescriptors(UTIL.getConfiguration(), fs, rootdir);
     HTableDescriptor htd = new HTableDescriptor(TableName.valueOf(name));
 
     assertTrue("Should create new table descriptor", fstd.createTableDescriptor(htd, false));
@@ -54,7 +54,7 @@ public class TestFSTableDescriptorForceCreation {
     FileSystem fs = FileSystem.get(UTIL.getConfiguration());
     // Cleanup old tests if any detritus laying around.
     Path rootdir = new Path(UTIL.getDataTestDir(), name);
-    FSTableDescriptors fstd = new FSTableDescriptors(fs, rootdir);
+    FSTableDescriptors fstd = new FSTableDescriptors(UTIL.getConfiguration(), fs, rootdir);
     HTableDescriptor htd = new HTableDescriptor(name);
     fstd.add(htd);
     assertFalse("Should not create new table descriptor", fstd.createTableDescriptor(htd, false));
@@ -66,7 +66,7 @@ public class TestFSTableDescriptorForceCreation {
     final String name = "createNewTableNew2";
     FileSystem fs = FileSystem.get(UTIL.getConfiguration());
     Path rootdir = new Path(UTIL.getDataTestDir(), name);
-    FSTableDescriptors fstd = new FSTableDescriptors(fs, rootdir);
+    FSTableDescriptors fstd = new FSTableDescriptors(UTIL.getConfiguration(), fs, rootdir);
     HTableDescriptor htd = new HTableDescriptor(TableName.valueOf(name));
     fstd.createTableDescriptor(htd, false);
     assertTrue("Should create new table descriptor",
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/TestHColumnDescriptor.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/TestHColumnDescriptor.java
index 132004d..4e51d13 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/TestHColumnDescriptor.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/TestHColumnDescriptor.java
@@ -37,7 +37,11 @@ public class TestHColumnDescriptor {
   @Test
   public void testPb() throws DeserializationException {
     HColumnDescriptor hcd = new HColumnDescriptor(
-      HTableDescriptor.META_TABLEDESC.getColumnFamilies()[0]);
+        new HColumnDescriptor(HConstants.CATALOG_FAMILY)
+            .setInMemory(true)
+            .setScope(HConstants.REPLICATION_SCOPE_LOCAL)
+            .setBloomFilterType(BloomType.NONE)
+            .setCacheDataInL1(true));
     final int v = 123;
     hcd.setBlocksize(v);
     hcd.setTimeToLive(v);
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/TestHTableDescriptor.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/TestHTableDescriptor.java
index c52eecb..a2809c8 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/TestHTableDescriptor.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/TestHTableDescriptor.java
@@ -46,7 +46,7 @@ public class TestHTableDescriptor {
 
   @Test
   public void testPb() throws DeserializationException, IOException {
-    HTableDescriptor htd = new HTableDescriptor(HTableDescriptor.META_TABLEDESC);
+    HTableDescriptor htd = new HTableDescriptor(TableName.META_TABLE_NAME);
     final int v = 123;
     htd.setMaxFileSize(v);
     htd.setDurability(Durability.ASYNC_WAL);
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/TestTableDescriptor.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/TestTableDescriptor.java
index 19c1136..a179c47 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/TestTableDescriptor.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/TestTableDescriptor.java
@@ -41,7 +41,7 @@ public class TestTableDescriptor {
 
   @Test
   public void testPb() throws DeserializationException, IOException {
-    HTableDescriptor htd = new HTableDescriptor(HTableDescriptor.META_TABLEDESC);
+    HTableDescriptor htd = new HTableDescriptor(TableName.META_TABLE_NAME);
     final int v = 123;
     htd.setMaxFileSize(v);
     htd.setDurability(Durability.ASYNC_WAL);
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestAdmin.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestAdmin.java
index 6fe6ede..5025c49 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestAdmin.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestAdmin.java
@@ -1151,13 +1151,13 @@ public class TestAdmin {
   public void testCreateBadTables() throws IOException {
     String msg = null;
     try {
-      this.admin.createTable(HTableDescriptor.META_TABLEDESC);
+      this.admin.createTable(new HTableDescriptor(TableName.META_TABLE_NAME));
     } catch(TableExistsException e) {
       msg = e.toString();
     }
     assertTrue("Unexcepted exception message " + msg, msg != null &&
       msg.startsWith(TableExistsException.class.getName()) &&
-      msg.contains(HTableDescriptor.META_TABLEDESC.getTableName().getNameAsString()));
+      msg.contains(TableName.META_TABLE_NAME.getNameAsString()));
 
     // Now try and do concurrent creation with a bunch of threads.
     final HTableDescriptor threadDesc =
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/master/TestMasterFailover.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/master/TestMasterFailover.java
index 7537e35..042f924 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/master/TestMasterFailover.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/master/TestMasterFailover.java
@@ -227,7 +227,7 @@ public class TestMasterFailover {
 
     FileSystem filesystem = FileSystem.get(conf);
     Path rootdir = FSUtils.getRootDir(conf);
-    FSTableDescriptors fstd = new FSTableDescriptors(filesystem, rootdir);
+    FSTableDescriptors fstd = new FSTableDescriptors(conf, filesystem, rootdir);
     fstd.createTableDescriptor(offlineTable);
 
     HRegionInfo hriOffline = new HRegionInfo(offlineTable.getTableName(), null, null);
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestGetClosestAtOrBefore.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestGetClosestAtOrBefore.java
index 3f31611..988d40b 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestGetClosestAtOrBefore.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestGetClosestAtOrBefore.java
@@ -69,10 +69,10 @@ public class TestGetClosestAtOrBefore extends HBaseTestCase {
     FileSystem filesystem = FileSystem.get(conf);
     Path rootdir = testDir;
     // Up flush size else we bind up when we use default catalog flush of 16k.
-    HTableDescriptor.META_TABLEDESC.setMemStoreFlushSize(64 * 1024 * 1024);
+    fsTableDescriptors.get(TableName.META_TABLE_NAME).setMemStoreFlushSize(64 * 1024 * 1024);
 
     HRegion mr = HRegion.createHRegion(HRegionInfo.FIRST_META_REGIONINFO,
-      rootdir, this.conf, HTableDescriptor.META_TABLEDESC);
+      rootdir, this.conf, fsTableDescriptors.get(TableName.META_TABLE_NAME));
     try {
     // Write rows for three tables 'A', 'B', and 'C'.
     for (char c = 'A'; c < 'D'; c++) {
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestHRegionInfo.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestHRegionInfo.java
index e0746fb..c7142fd 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestHRegionInfo.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestHRegionInfo.java
@@ -40,6 +40,7 @@ import org.apache.hadoop.hbase.master.RegionState;
 import org.apache.hadoop.hbase.protobuf.generated.HBaseProtos;
 import org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.RegionInfo;
 import org.apache.hadoop.hbase.util.Bytes;
+import org.apache.hadoop.hbase.util.FSTableDescriptors;
 import org.apache.hadoop.hbase.util.MD5Hash;
 import org.junit.Assert;
 import org.junit.Test;
@@ -63,13 +64,14 @@ public class TestHRegionInfo {
     HRegionInfo hri = HRegionInfo.FIRST_META_REGIONINFO;
     Path basedir = htu.getDataTestDir();
     // Create a region.  That'll write the .regioninfo file.
+    FSTableDescriptors fsTableDescriptors = new FSTableDescriptors(htu.getConfiguration());
     HRegion r = HRegion.createHRegion(hri, basedir, htu.getConfiguration(),
-      HTableDescriptor.META_TABLEDESC);
+      fsTableDescriptors.get(TableName.META_TABLE_NAME));
     // Get modtime on the file.
     long modtime = getModTime(r);
     HRegion.closeHRegion(r);
     Thread.sleep(1001);
-    r = HRegion.openHRegion(basedir, hri, HTableDescriptor.META_TABLEDESC,
+    r = HRegion.openHRegion(basedir, hri, fsTableDescriptors.get(TableName.META_TABLE_NAME),
         null, htu.getConfiguration());
     // Ensure the file is not written for a second time.
     long modtime2 = getModTime(r);
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/wal/TestLogRollingNoCluster.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/wal/TestLogRollingNoCluster.java
index 413aa8d..fb45c00 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/wal/TestLogRollingNoCluster.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/wal/TestLogRollingNoCluster.java
@@ -31,6 +31,7 @@ import org.apache.hadoop.hbase.HConstants;
 import org.apache.hadoop.hbase.HRegionInfo;
 import org.apache.hadoop.hbase.HTableDescriptor;
 import org.apache.hadoop.hbase.KeyValue;
+import org.apache.hadoop.hbase.TableName;
 import org.apache.hadoop.hbase.testclassification.RegionServerTests;
 import org.apache.hadoop.hbase.testclassification.SmallTests;
 import org.apache.hadoop.hbase.util.Bytes;
@@ -126,8 +127,8 @@ public class TestLogRollingNoCluster {
           byte[] bytes = Bytes.toBytes(i);
           edit.add(new KeyValue(bytes, bytes, bytes, now, EMPTY_1K_ARRAY));
           this.wal.append(HRegionInfo.FIRST_META_REGIONINFO,
-              HTableDescriptor.META_TABLEDESC.getTableName(),
-              edit, now, HTableDescriptor.META_TABLEDESC, sequenceId);
+              TableName.META_TABLE_NAME,
+              edit, now, TEST_UTIL.getMetaTableDescriptor(), sequenceId);
         }
         String msg = getName() + " finished";
         if (isException())
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/replication/TestReplicationWALEntryFilters.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/replication/TestReplicationWALEntryFilters.java
index b116341..407f4e2 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/replication/TestReplicationWALEntryFilters.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/replication/TestReplicationWALEntryFilters.java
@@ -59,13 +59,13 @@ public class TestReplicationWALEntryFilters {
 
     // meta
     HLogKey key1 = new HLogKey( HRegionInfo.FIRST_META_REGIONINFO.getEncodedNameAsBytes(),
-      HTableDescriptor.META_TABLEDESC.getTableName());
+      TableName.META_TABLE_NAME);
     HLog.Entry metaEntry = new Entry(key1, null);
 
     assertNull(filter.filter(metaEntry));
 
     // ns table
-    HLogKey key2 = new HLogKey(new byte[] {}, HTableDescriptor.NAMESPACE_TABLEDESC.getTableName());
+    HLogKey key2 = new HLogKey(new byte[] {}, TableName.NAMESPACE_TABLE_NAME);
     HLog.Entry nsEntry = new Entry(key2, null);
     assertNull(filter.filter(nsEntry));
 
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/util/TestFSTableDescriptors.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/util/TestFSTableDescriptors.java
index 23b1310..29c896c 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/util/TestFSTableDescriptors.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/util/TestFSTableDescriptors.java
@@ -76,7 +76,7 @@ public class TestFSTableDescriptors {
     HTableDescriptor htd = new HTableDescriptor(TableName.valueOf("testCreate"));
     TableDescriptor td = new TableDescriptor(htd, TableState.State.ENABLED);
     FileSystem fs = FileSystem.get(UTIL.getConfiguration());
-    FSTableDescriptors fstd = new FSTableDescriptors(fs, testdir);
+    FSTableDescriptors fstd = new FSTableDescriptors(UTIL.getConfiguration(), fs, testdir);
     assertTrue(fstd.createTableDescriptor(td));
     assertFalse(fstd.createTableDescriptor(td));
     FileStatus [] statuses = fs.listStatus(testdir);
@@ -98,7 +98,7 @@ public class TestFSTableDescriptors {
         TableName.valueOf("testSequenceidAdvancesOnTableInfo"));
     TableDescriptor td = new TableDescriptor(htd);
     FileSystem fs = FileSystem.get(UTIL.getConfiguration());
-    FSTableDescriptors fstd = new FSTableDescriptors(fs, testdir);
+    FSTableDescriptors fstd = new FSTableDescriptors(UTIL.getConfiguration(), fs, testdir);
     Path p0 = fstd.updateTableDescriptor(td);
     int i0 = FSTableDescriptors.getTableInfoSequenceId(p0);
     Path p1 = fstd.updateTableDescriptor(td);
@@ -159,7 +159,7 @@ public class TestFSTableDescriptors {
     FileSystem fs = FileSystem.get(UTIL.getConfiguration());
     // Cleanup old tests if any detrius laying around.
     Path rootdir = new Path(UTIL.getDataTestDir(), name);
-    TableDescriptors htds = new FSTableDescriptors(fs, rootdir);
+    TableDescriptors htds = new FSTableDescriptors(UTIL.getConfiguration(), fs, rootdir);
     HTableDescriptor htd = new HTableDescriptor(TableName.valueOf(name));
     htds.add(htd);
     assertNotNull(htds.remove(htd.getTableName()));
@@ -172,7 +172,7 @@ public class TestFSTableDescriptors {
     HTableDescriptor htd = new HTableDescriptor(TableName.valueOf(name));
     TableDescriptor td = new TableDescriptor(htd, TableState.State.ENABLED);
     Path rootdir = UTIL.getDataTestDir(name);
-    FSTableDescriptors fstd = new FSTableDescriptors(fs, rootdir);
+    FSTableDescriptors fstd = new FSTableDescriptors(UTIL.getConfiguration(), fs, rootdir);
     fstd.createTableDescriptor(td);
     TableDescriptor td2 =
       FSTableDescriptors.getTableDescriptorFromFs(fs, rootdir, htd.getTableName());
@@ -183,14 +183,14 @@ public class TestFSTableDescriptors {
     final String name = "testReadingOldHTDFromFS";
     FileSystem fs = FileSystem.get(UTIL.getConfiguration());
     Path rootdir = UTIL.getDataTestDir(name);
-    FSTableDescriptors fstd = new FSTableDescriptors(fs, rootdir);
+    FSTableDescriptors fstd = new FSTableDescriptors(UTIL.getConfiguration(), fs, rootdir);
     HTableDescriptor htd = new HTableDescriptor(TableName.valueOf(name));
     TableDescriptor td = new TableDescriptor(htd, TableState.State.ENABLED);
     Path descriptorFile = fstd.updateTableDescriptor(td);
     try (FSDataOutputStream out = fs.create(descriptorFile, true)) {
       out.write(htd.toByteArray());
     }
-    FSTableDescriptors fstd2 = new FSTableDescriptors(fs, rootdir);
+    FSTableDescriptors fstd2 = new FSTableDescriptors(UTIL.getConfiguration(), fs, rootdir);
     TableDescriptor td2 = fstd2.getDescriptor(htd.getTableName());
     assertEquals(td, td2);
     FileStatus descriptorFile2 =
@@ -209,7 +209,7 @@ public class TestFSTableDescriptors {
     FileSystem fs = FileSystem.get(UTIL.getConfiguration());
     // Cleanup old tests if any debris laying around.
     Path rootdir = new Path(UTIL.getDataTestDir(), name);
-    FSTableDescriptors htds = new FSTableDescriptors(fs, rootdir) {
+    FSTableDescriptors htds = new FSTableDescriptors(UTIL.getConfiguration(), fs, rootdir) {
       @Override
       public HTableDescriptor get(TableName tablename)
           throws TableExistsException, FileNotFoundException, IOException {
@@ -256,7 +256,7 @@ public class TestFSTableDescriptors {
     FileSystem fs = FileSystem.get(UTIL.getConfiguration());
     // Cleanup old tests if any detrius laying around.
     Path rootdir = new Path(UTIL.getDataTestDir(), name);
-    TableDescriptors htds = new FSTableDescriptors(fs, rootdir);
+    TableDescriptors htds = new FSTableDescriptors(UTIL.getConfiguration(), fs, rootdir);
     assertNull("There shouldn't be any HTD for this table",
       htds.get(TableName.valueOf("NoSuchTable")));
   }
@@ -267,7 +267,7 @@ public class TestFSTableDescriptors {
     FileSystem fs = FileSystem.get(UTIL.getConfiguration());
     // Cleanup old tests if any detrius laying around.
     Path rootdir = new Path(UTIL.getDataTestDir(), name);
-    TableDescriptors htds = new FSTableDescriptors(fs, rootdir);
+    TableDescriptors htds = new FSTableDescriptors(UTIL.getConfiguration(), fs, rootdir);
     HTableDescriptor htd = new HTableDescriptor(TableName.valueOf(name));
     htds.add(htd);
     htds.add(htd);
@@ -304,7 +304,8 @@ public class TestFSTableDescriptors {
   public void testReadingArchiveDirectoryFromFS() throws IOException {
     FileSystem fs = FileSystem.get(UTIL.getConfiguration());
     try {
-      new FSTableDescriptors(fs, FSUtils.getRootDir(UTIL.getConfiguration()))
+      new FSTableDescriptors(UTIL.getConfiguration(), fs,
+          FSUtils.getRootDir(UTIL.getConfiguration()))
           .get(TableName.valueOf(HConstants.HFILE_ARCHIVE_DIRECTORY));
       fail("Shouldn't be able to read a table descriptor for the archive directory.");
     } catch (Exception e) {
@@ -320,7 +321,7 @@ public class TestFSTableDescriptors {
         "testCreateTableDescriptorUpdatesIfThereExistsAlready"));
     TableDescriptor td = new TableDescriptor(htd, TableState.State.ENABLED);
     FileSystem fs = FileSystem.get(UTIL.getConfiguration());
-    FSTableDescriptors fstd = new FSTableDescriptors(fs, testdir);
+    FSTableDescriptors fstd = new FSTableDescriptors(UTIL.getConfiguration(), fs, testdir);
     assertTrue(fstd.createTableDescriptor(td));
     assertFalse(fstd.createTableDescriptor(td));
     htd.setValue(Bytes.toBytes("mykey"), Bytes.toBytes("myValue"));
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/util/TestHBaseFsck.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/util/TestHBaseFsck.java
index 3dc2868..f169e71 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/util/TestHBaseFsck.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/util/TestHBaseFsck.java
@@ -166,7 +166,7 @@ public class TestHBaseFsck {
 
     // Now let's mess it up and change the assignment in hbase:meta to
     // point to a different region server
-    Table meta = new HTable(conf, HTableDescriptor.META_TABLEDESC.getTableName(),
+    Table meta = new HTable(conf, TableName.META_TABLE_NAME,
         executorService);
     Scan scan = new Scan();
     scan.setStartRow(Bytes.toBytes(table+",,"));
@@ -1475,7 +1475,7 @@ public class TestHBaseFsck {
         Bytes.toBytes("C"), true, true, false);
 
       // Create a new meta entry to fake it as a split parent.
-      meta = new HTable(conf, HTableDescriptor.META_TABLEDESC.getTableName(),
+      meta = new HTable(conf, TableName.META_TABLE_NAME,
           executorService);
       HRegionInfo hri = location.getRegionInfo();
 
@@ -1550,7 +1550,7 @@ public class TestHBaseFsck {
       TEST_UTIL.getHBaseAdmin().flush(table);
       HRegionLocation location = tbl.getRegionLocation("B");
 
-      meta = new HTable(conf, HTableDescriptor.META_TABLEDESC.getTableName());
+      meta = new HTable(conf, TableName.META_TABLE_NAME);
       HRegionInfo hri = location.getRegionInfo();
 
       // do a regular split
@@ -1600,7 +1600,7 @@ public class TestHBaseFsck {
       TEST_UTIL.getHBaseAdmin().flush(table);
       HRegionLocation location = tbl.getRegionLocation("B");
 
-      meta = new HTable(conf, HTableDescriptor.META_TABLEDESC.getTableName());
+      meta = new HTable(conf, TableName.META_TABLE_NAME);
       HRegionInfo hri = location.getRegionInfo();
 
       // do a regular split
@@ -2143,7 +2143,7 @@ public class TestHBaseFsck {
 
       // Mess it up by removing the RegionInfo for one region.
       final List<Delete> deletes = new LinkedList<Delete>();
-      Table meta = new HTable(conf, HTableDescriptor.META_TABLEDESC.getTableName());
+      Table meta = new HTable(conf, TableName.META_TABLE_NAME);
       MetaScanner.metaScan(conf, new MetaScanner.MetaScannerVisitor() {
 
         @Override
@@ -2391,7 +2391,7 @@ public class TestHBaseFsck {
       LOG.info("deleting hdfs .regioninfo data: " + hri.toString() + hsa.toString());
       Path rootDir = FSUtils.getRootDir(conf);
       FileSystem fs = rootDir.getFileSystem(conf);
-      Path p = new Path(rootDir + "/" + HTableDescriptor.META_TABLEDESC.getNameAsString(),
+      Path p = new Path(rootDir + "/" + TableName.META_TABLE_NAME.getNameAsString(),
           hri.getEncodedName());
       Path hriPath = new Path(p, HRegionFileSystem.REGION_INFO_FILE);
       fs.delete(hriPath, true);
@@ -2401,7 +2401,7 @@ public class TestHBaseFsck {
       LOG.info("deleting hdfs data: " + hri.toString() + hsa.toString());
       Path rootDir = FSUtils.getRootDir(conf);
       FileSystem fs = rootDir.getFileSystem(conf);
-      Path p = new Path(rootDir + "/" + HTableDescriptor.META_TABLEDESC.getNameAsString(),
+      Path p = new Path(rootDir + "/" + TableName.META_TABLE_NAME.getNameAsString(),
           hri.getEncodedName());
       HBaseFsck.debugLsr(conf, p);
       boolean success = fs.delete(p, true);
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/util/TestMergeTable.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/util/TestMergeTable.java
index 418f977..5cc9be2 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/util/TestMergeTable.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/util/TestMergeTable.java
@@ -99,7 +99,7 @@ public class TestMergeTable {
 
     // Create regions and populate them at same time.  Create the tabledir
     // for them first.
-    new FSTableDescriptors(fs, rootdir).createTableDescriptor(desc);
+    new FSTableDescriptors(UTIL.getConfiguration(), fs, rootdir).createTableDescriptor(desc);
     HRegion [] regions = {
       createRegion(desc, null, row_70001, 1, 70000, rootdir),
       createRegion(desc, row_70001, row_80001, 70001, 10000, rootdir),
@@ -164,7 +164,7 @@ public class TestMergeTable {
   throws IOException {
     HRegion meta =
       HRegion.createHRegion(HRegionInfo.FIRST_META_REGIONINFO, rootdir,
-      UTIL.getConfiguration(), HTableDescriptor.META_TABLEDESC);
+      UTIL.getConfiguration(), UTIL.getMetaTableDescriptor());
     for (HRegion r: regions) {
       HRegion.addRegionToMETA(meta, r);
     }
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/util/TestMergeTool.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/util/TestMergeTool.java
index 38924e6..10d7f0c 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/util/TestMergeTool.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/util/TestMergeTool.java
@@ -147,7 +147,7 @@ public class TestMergeTool extends HBaseTestCase {
     try {
       // Create meta region
       createMetaRegion();
-      new FSTableDescriptors(this.fs, this.testDir).createTableDescriptor(
+      new FSTableDescriptors(this.conf, this.fs, this.testDir).createTableDescriptor(
           new TableDescriptor(this.desc));
       /*
        * Create the regions we will merge
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/util/hbck/OfflineMetaRebuildTestCore.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/util/hbck/OfflineMetaRebuildTestCore.java
index ce01281..165cac3 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/util/hbck/OfflineMetaRebuildTestCore.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/util/hbck/OfflineMetaRebuildTestCore.java
@@ -279,7 +279,7 @@ public class OfflineMetaRebuildTestCore {
    */
   protected int scanMeta() throws IOException {
     int count = 0;
-    HTable meta = new HTable(conf, HTableDescriptor.META_TABLEDESC.getTableName());
+    HTable meta = new HTable(conf, TableName.META_TABLE_NAME);
     ResultScanner scanner = meta.getScanner(new Scan());
     LOG.info("Table: " + Bytes.toString(meta.getTableName()));
     for (Result res : scanner) {
